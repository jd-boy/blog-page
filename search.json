[{"title":"Java 实现 HmacSHA256","url":"/posts/19466/","content":"@SneakyThrowspublic static String hmacSHA256(String origin, String secret) &#123;  Mac hmacSHA256 = Mac.getInstance(&quot;HmacSHA256&quot;);  hmacSHA256.init(new SecretKeySpec(secret.getBytes(StandardCharsets.UTF_8), &quot;HmacSHA256&quot;));  byte[] targetBytes = hmacSHA256.doFinal(origin.getBytes(StandardCharsets.UTF_8));  StringBuilder result = new StringBuilder();  for (byte b : targetBytes) &#123;    result.append(Integer.toHexString(b &amp; 0xFF | 0x100).substring(1, 3));  &#125;  return result.toString();&#125;\n\n","categories":["Java"]},{"title":"Java 环境变量设置 Options","url":"/posts/14498/","content":"JDK_JAVA_OPTIONS文档：https://docs.oracle.com/en/java/javase/11/tools/java.html#GUID-3B1CE181-CD30-4178-9602-230B800D4FAE\nJAVA_TOOL_OPTIONShttps://blog.51cto.com/u_11791718/4832223\n\nJDK_JAVA_OPTIONS is only picked up by the java launcher, so use it for options that you only want to apply (or only make sense for) the java startup command. This variable is also new on JDK 9+, and will be ignored by earlier JDK versions. Hence, it’s useful when migrating from older versions to 9+.\nJAVA_TOOL_OPTIONS is picked up also by other java tools like jar and javac so it should be used for flags that you want to apply (and are valid) to all those java tools.\n\n","categories":["Java"]},{"title":"Javaagent","url":"/posts/54537/","content":"JVM启动前静态InstrumentJavaagent 是什么？\nJavaagent是java命令的一个参数。参数 javaagent 可以用于指定一个 jar 包，并且对该 java 包有2个要求：\n\n这个 jar 包的 MANIFEST.MF 文件必须指定 Premain-Class 项。\nPremain-Class 指定的那个类必须实现 premain() 方法。\n\npremain 方法，从字面上理解，就是运行在 main 函数之前的的类。当Java 虚拟机启动时，在执行 main 函数之前，JVM 会先运行-javaagent所指定 jar 包内 Premain-Class 这个类的 premain 方法 。\n在命令行输入 java可以看到相应的参数，其中有 和 java agent相关的：\nCopy-agentlib:&lt;libname&gt;[=&lt;选项&gt;] 加载本机代理库 &lt;libname&gt;, 例如 -agentlib:hprof\t另请参阅 -agentlib:jdwp=help 和 -agentlib:hprof=help-agentpath:&lt;pathname&gt;[=&lt;选项&gt;]\t按完整路径名加载本机代理库-javaagent:&lt;jarpath&gt;[=&lt;选项&gt;]\t加载 Java 编程语言代理, 请参阅 java.lang.instrument\n\n在上面-javaagent参数中提到了参阅java.lang.instrument，这是在rt.jar 中定义的一个包，该路径下有两个重要的类：\n\n该包提供了一些工具帮助开发人员在 Java 程序运行时，动态修改系统中的 Class 类型。其中，使用该软件包的一个关键组件就是 Javaagent。从名字上看，似乎是个 Java 代理之类的，而实际上，他的功能更像是一个Class 类型的转换器，他可以在运行时接受重新外部请求，对Class类型进行修改。\n从本质上讲，Java Agent 是一个遵循一组严格约定的常规 Java 类。 上面说到 javaagent命令要求指定的类中必须要有premain()方法，并且对premain方法的签名也有要求，签名必须满足以下两种格式：\nCopypublic static void premain(String agentArgs, Instrumentation inst)    public static void premain(String agentArgs)\n\nJVM 会优先加载 带 Instrumentation 签名的方法，加载成功忽略第二种，如果第一种没有，则加载第二种方法。这个逻辑在sun.instrument.InstrumentationImpl 类中：\n\nInstrumentation 类 定义如下：\nCopypublic interface Instrumentation &#123;        //增加一个Class 文件的转换器，转换器用于改变 Class 二进制流的数据，参数 canRetransform 设置是否允许重新转换。    void addTransformer(ClassFileTransformer transformer, boolean canRetransform);    //在类加载之前，重新定义 Class 文件，ClassDefinition 表示对一个类新的定义，如果在类加载之后，需要使用 retransformClasses 方法重新定义。addTransformer方法配置之后，后续的类加载都会被Transformer拦截。对于已经加载过的类，可以执行retransformClasses来重新触发这个Transformer的拦截。类加载的字节码被修改后，除非再次被retransform，否则不会恢复。    void addTransformer(ClassFileTransformer transformer);    //删除一个类转换器    boolean removeTransformer(ClassFileTransformer transformer);    boolean isRetransformClassesSupported();    //在类加载之后，重新定义 Class。这个很重要，该方法是1.6 之后加入的，事实上，该方法是 update 了一个类。    void retransformClasses(Class&lt;?&gt;... classes) throws UnmodifiableClassException;    boolean isRedefineClassesSupported();        void redefineClasses(ClassDefinition... definitions)        throws  ClassNotFoundException, UnmodifiableClassException;    boolean isModifiableClass(Class&lt;?&gt; theClass);    @SuppressWarnings(&quot;rawtypes&quot;)    Class[] getAllLoadedClasses();      @SuppressWarnings(&quot;rawtypes&quot;)    Class[] getInitiatedClasses(ClassLoader loader);    //获取一个对象的大小    long getObjectSize(Object objectToSize);       void appendToBootstrapClassLoaderSearch(JarFile jarfile);        void appendToSystemClassLoaderSearch(JarFile jarfile);        boolean isNativeMethodPrefixSupported();        void setNativeMethodPrefix(ClassFileTransformer transformer, String prefix);&#125;\n\n最为重要的是上面注释的几个方法，下面我们会用到。\n如何使用javaagent？\n使用 javaagent 需要几个步骤：\n\n定义一个 MANIFEST.MF 文件，必须包含 Premain-Class 选项，通常也会加入Can-Redefine-Classes 和 Can-Retransform-Classes 选项。\n创建一个Premain-Class 指定的类，类中包含 premain 方法，方法逻辑由用户自己确定。\n将 premain 的类和 MANIFEST.MF 文件打成 jar 包。\n使用参数 -javaagent: jar包路径 启动要代理的方法。\n\n在执行以上步骤后，JVM 会先执行 premain 方法，大部分类加载都会通过该方法，注意：是大部分，不是所有。当然，遗漏的主要是系统类，因为很多系统类先于 agent 执行，而用户类的加载肯定是会被拦截的。也就是说，这个方法是在 main 方法启动前拦截大部分类的加载活动，既然可以拦截类的加载，那么就可以去做重写类这样的操作，结合第三方的字节码编译工具，比如ASM，javassist，cglib等等来改写实现类。\n通过上面的步骤我们用代码实现来实现。实现 javaagent 你需要搭建两个工程，一个工程是用来承载 javaagent类，单独的打成jar包；一个工程是javaagent需要去代理的类。即javaagent会在这个工程中的main方法启动之前去做一些事情。\n1.首先来实现javaagent工程。\n工程目录结构如下：\nCopy-java-agent----src--------main--------|------java--------|----------com.rickiyang.learn--------|------------PreMainTraceAgent--------|resources-----------META-INF--------------MANIFEST.MF\n\n第一步是需要创建一个类，包含premain 方法：\nCopyimport java.lang.instrument.ClassFileTransformer;import java.lang.instrument.IllegalClassFormatException;import java.lang.instrument.Instrumentation;import java.security.ProtectionDomain;/** * @author: rickiyang * @date: 2019/8/12 * @description: */public class PreMainTraceAgent &#123;    public static void premain(String agentArgs, Instrumentation inst) &#123;        System.out.println(&quot;agentArgs : &quot; + agentArgs);        inst.addTransformer(new DefineTransformer(), true);    &#125;    static class DefineTransformer implements ClassFileTransformer&#123;        @Override        public byte[] transform(ClassLoader loader, String className, Class&lt;?&gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) throws IllegalClassFormatException &#123;            System.out.println(&quot;premain load Class:&quot; + className);            return classfileBuffer;        &#125;    &#125;&#125;\n\n上面就是我实现的一个类，实现了带Instrumentation参数的premain()方法。调用addTransformer()方法对启动时所有的类进行拦截。\n然后在 resources 目录下新建目录：META-INF，在该目录下新建文件：MANIFREST.MF：\nCopyManifest-Version: 1.0Can-Redefine-Classes: trueCan-Retransform-Classes: truePremain-Class: PreMainTraceAgent\n\n注意到第5行有空行。\n说一下MANIFREST.MF文件的作用，这里如果你不去手动指定的话，直接 打包，默认会在打包的文件中生成一个MANIFREST.MF文件：\nCopyManifest-Version: 1.0Implementation-Title: test-agentImplementation-Version: 0.0.1-SNAPSHOTBuilt-By: yangyueImplementation-Vendor-Id: com.rickiyang.learnSpring-Boot-Version: 2.0.9.RELEASEMain-Class: org.springframework.boot.loader.JarLauncherStart-Class: com.rickiyang.learn.LearnApplicationSpring-Boot-Classes: BOOT-INF/classes/Spring-Boot-Lib: BOOT-INF/lib/Created-By: Apache Maven 3.5.2Build-Jdk: 1.8.0_151Implementation-URL: https://projects.spring.io/spring-boot/#/spring-bo ot-starter-parent/test-agent\n\n这是默认的文件，包含当前的一些版本信息，当前工程的启动类，它还有别的参数允许你做更多的事情，可以用上的有：\n\nPremain-Class ：包含 premain 方法的类（类的全路径名）\nAgent-Class ：包含 agentmain 方法的类（类的全路径名）\nBoot-Class-Path ：设置引导类加载器搜索的路径列表。查找类的特定于平台的机制失败后，引导类加载器会搜索这些路径。按列出的顺序搜索路径。列表中的路径由一个或多个空格分开。路径使用分层 URI 的路径组件语法。如果该路径以斜杠字符（“/”）开头，则为绝对路径，否则为相对路径。相对路径根据代理 JAR 文件的绝对路径解析。忽略格式不正确的路径和不存在的路径。如果代理是在 VM 启动之后某一时刻启动的，则忽略不表示 JAR 文件的路径。（可选）\nCan-Redefine-Classes ：true表示能重定义此代理所需的类，默认值为 false（可选）\nCan-Retransform-Classes ：true 表示能重转换此代理所需的类，默认值为 false （可选）\nCan-Set-Native-Method-Prefix： true表示能设置此代理所需的本机方法前缀，默认值为 false（可选）\n\n即在该文件中主要定义了程序运行相关的配置信息，程序运行前会先检测该文件中的配置项。\n一个java程序中-javaagent参数的个数是没有限制的，所以可以添加任意多个javaagent。所有的java agent会按照你定义的顺序执行，例如：\nCopyjava -javaagent:agent1.jar -javaagent:agent2.jar -jar MyProgram.jar\n\n程序执行的顺序将会是：\nMyAgent1.premain -&gt; MyAgent2.premain -&gt; MyProgram.main\n说回上面的 javaagent工程，接下来将该工程打成jar包，我在打包的时候发现打完包之后的 MANIFREST.MF文件被默认配置替换掉了。所以我是手动将上面我的配置文件替换到jar包中的文件，这里你需要注意。\n另外的再说一种不去手动写MANIFREST.MF文件的方式，使用maven插件：\nCopy&lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;    &lt;version&gt;3.1.0&lt;/version&gt;    &lt;configuration&gt;        &lt;archive&gt;            &lt;!--自动添加META-INF/MANIFEST.MF --&gt;            &lt;manifest&gt;                &lt;addClasspath&gt;true&lt;/addClasspath&gt;            &lt;/manifest&gt;            &lt;manifestEntries&gt;                &lt;Premain-Class&gt;com.rickiyang.learn.PreMainTraceAgent&lt;/Premain-Class&gt;                &lt;Agent-Class&gt;com.rickiyang.learn.PreMainTraceAgent&lt;/Agent-Class&gt;                &lt;Can-Redefine-Classes&gt;true&lt;/Can-Redefine-Classes&gt;                &lt;Can-Retransform-Classes&gt;true&lt;/Can-Retransform-Classes&gt;            &lt;/manifestEntries&gt;        &lt;/archive&gt;    &lt;/configuration&gt;&lt;/plugin&gt;\n\n用这种插件的方式也可以自动生成该文件。\nagent代码就写完了，下面再重新开一个工程，你只需要写一个带 main 方法的类即可：\nCopypublic class TestMain &#123;    public static void main(String[] args) &#123;        System.out.println(&quot;main start&quot;);        try &#123;            Thread.sleep(3000);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;        System.out.println(&quot;main end&quot;);    &#125;&#125;\n\n很简单，然后需要做的就是将上面的 代理类 和 这个测试类关联起来。有两种方式：\n如果你用的是idea，那么你可以点击菜单： run-debug configuration，然后将你的代理类包 指定在 启动参数中即可：\n\n另一种方式是不用 编译器，采用命令行的方法。与上面大致相同，将 上面的测试类编译成 class文件，然后 运行该类即可：\nCopy #将该类编译成class文件 &gt; javac TestMain.java  #指定agent程序并运行该类 &gt; java -javaagent:c:/alg.jar TestMain\n\n使用上面两种方式都可以运行,输出结果如下：\nCopyD:\\soft\\jdk1.8\\bin\\java.exe -javaagent:c:/alg.jar &quot;-javaagent:D:\\soft\\IntelliJ IDEA 2019.1.1\\lib\\idea_rt.jar=54274:D:\\soft\\IntelliJ IDEA 2019.1.1\\bin&quot; -Dfile.encoding=UTF-8 -classpath D:\\soft\\jdk1.8\\jre\\lib\\charsets.jar;D:\\soft\\jdk1.8\\jre\\lib\\deploy.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\access-bridge-64.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\cldrdata.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\dnsns.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\jaccess.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\jfxrt.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\localedata.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\nashorn.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\sunec.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\sunjce_provider.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\sunmscapi.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\sunpkcs11.jar;D:\\soft\\jdk1.8\\jre\\lib\\ext\\zipfs.jar;D:\\soft\\jdk1.8\\jre\\lib\\javaws.jar;D:\\soft\\jdk1.8\\jre\\lib\\jce.jar;D:\\soft\\jdk1.8\\jre\\lib\\jfr.jar;D:\\soft\\jdk1.8\\jre\\lib\\jfxswt.jar;D:\\soft\\jdk1.8\\jre\\lib\\jsse.jar;D:\\soft\\jdk1.8\\jre\\lib\\management-agent.jar;D:\\soft\\jdk1.8\\jre\\lib\\plugin.jar;D:\\soft\\jdk1.8\\jre\\lib\\resources.jar;D:\\soft\\jdk1.8\\jre\\lib\\rt.jar;D:\\workspace\\demo1\\target\\classes;E:\\.m2\\repository\\org\\springframework\\boot\\spring-boot-starter-aop\\2.1.1.RELEASE\\spring-.........1.8.11.jar;E:\\.m2\\repository\\com\\google\\guava\\guava\\20.0\\guava-20.0.jar;E:\\.m2\\repository\\org\\apache\\commons\\commons-lang3\\3.7\\commons-lang3-3.7.jar;E:\\.m2\\repository\\com\\alibaba\\fastjson\\1.2.54\\fastjson-1.2.54.jar;E:\\.m2\\repository\\org\\springframework\\boot\\spring-boot\\2.1.0.RELEASE\\spring-boot-2.1.0.RELEASE.jar;E:\\.m2\\repository\\org\\springframework\\spring-context\\5.1.3.RELEASE\\spring-context-5.1.3.RELEASE.jar com.springboot.example.demo.service.TestMainagentArgs : nullpremain load Class     :java/util/concurrent/ConcurrentHashMap$ForwardingNodepremain load Class     :sun/nio/cs/ThreadLocalCoderspremain load Class     :sun/nio/cs/ThreadLocalCoders$1premain load Class     :sun/nio/cs/ThreadLocalCoders$Cachepremain load Class     :sun/nio/cs/ThreadLocalCoders$2premain load Class     :java/util/jar/Attributespremain load Class     :java/util/jar/Manifest$FastInputStream.........premain load Class     :java/lang/Class$MethodArraypremain load Class     :java/lang/Voidmain startpremain load Class     :sun/misc/VMSupportpremain load Class     :java/util/Hashtable$KeySetpremain load Class     :sun/nio/cs/ISO_8859_1$Encoderpremain load Class     :sun/nio/cs/Surrogate$Parserpremain load Class     :sun/nio/cs/Surrogate.........premain load Class     :sun/util/locale/provider/LocaleResources$ResourceReferencemain endpremain load Class     :java/lang/Shutdownpremain load Class     :java/lang/Shutdown$LockProcess finished with exit code 0\n\n上面的输出结果我们能够发现：\n\n执行main方法之前会加载所有的类，包括系统类和自定义类；\n在ClassFileTransformer中会去拦截系统类和自己实现的类对象；\n如果你有对某些类对象进行改写，那么在拦截的时候抓住该类使用字节码编译工具即可实现。\n\n下面是使用javassist来动态将某个方法替换掉：\nCopypackage com.rickiyang.learn;import javassist.*;import java.io.IOException;import java.lang.instrument.ClassFileTransformer;import java.security.ProtectionDomain;/** * @author rickiyang * @date 2019-08-06 * @Desc */public class MyClassTransformer implements ClassFileTransformer &#123;    @Override    public byte[] transform(final ClassLoader loader, final String className, final Class&lt;?&gt; classBeingRedefined,final ProtectionDomain protectionDomain, final byte[] classfileBuffer) &#123;        // 操作Date类        if (&quot;java/util/Date&quot;.equals(className)) &#123;            try &#123;                // 从ClassPool获得CtClass对象                final ClassPool classPool = ClassPool.getDefault();                final CtClass clazz = classPool.get(&quot;java.util.Date&quot;);                CtMethod convertToAbbr = clazz.getDeclaredMethod(&quot;convertToAbbr&quot;);                //这里对 java.util.Date.convertToAbbr() 方法进行了改写，在 return之前增加了一个 打印操作                String methodBody = &quot;&#123;sb.append(Character.toUpperCase(name.charAt(0)));&quot; +                        &quot;sb.append(name.charAt(1)).append(name.charAt(2));&quot; +                        &quot;System.out.println(\\&quot;sb.toString()\\&quot;);&quot; +                        &quot;return sb;&#125;&quot;;                convertToAbbr.setBody(methodBody);                // 返回字节码，并且detachCtClass对象                byte[] byteCode = clazz.toBytecode();                //detach的意思是将内存中曾经被javassist加载过的Date对象移除，如果下次有需要在内存中找不到会重新走javassist加载                clazz.detach();                return byteCode;            &#125; catch (Exception ex) &#123;                ex.printStackTrace();            &#125;        &#125;        // 如果返回null则字节码不会被修改        return null;    &#125;&#125;\n\nJVM启动后动态Instrument上面介绍的Instrumentation是在 JDK 1.5中提供的，开发者只能在main加载之前添加手脚，在 Java SE 6 的 Instrumentation 当中，提供了一个新的代理操作方法：agentmain，可以在 main 函数开始运行之后再运行。\n跟premain函数一样， 开发者可以编写一个含有agentmain函数的 Java 类：\nCopy//采用attach机制，被代理的目标程序VM有可能很早之前已经启动，当然其所有类已经被加载完成，这个时候需要借助Instrumentation#retransformClasses(Class&lt;?&gt;... classes)让对应的类可以重新转换，从而激活重新转换的类执行ClassFileTransformer列表中的回调public static void agentmain (String agentArgs, Instrumentation inst)public static void agentmain (String agentArgs)\n\n同样，agentmain 方法中带Instrumentation参数的方法也比不带优先级更高。开发者必须在 manifest 文件里面设置“Agent-Class”来指定包含 agentmain 函数的类。\n在Java6 以后实现启动后加载的新实现是Attach api。Attach API 很简单，只有 2 个主要的类，都在 com.sun.tools.attach 包里面：\n\n\nVirtualMachine 字面意义表示一个Java 虚拟机，也就是程序需要监控的目标虚拟机，提供了获取系统信息(比如获取内存dump、线程dump，类信息统计(比如已加载的类以及实例个数等)， loadAgent，Attach 和 Detach （Attach 动作的相反行为，从 JVM 上面解除一个代理）等方法，可以实现的功能可以说非常之强大 。该类允许我们通过给attach方法传入一个jvm的pid(进程id)，远程连接到jvm上 。\n代理类注入操作只是它众多功能中的一个，通过loadAgent方法向jvm注册一个代理程序agent，在该agent的代理程序中会得到一个Instrumentation实例，该实例可以 在class加载前改变class的字节码，也可以在class加载后重新加载。在调用Instrumentation实例的方法时，这些方法会使用ClassFileTransformer接口中提供的方法进行处理。\n\nVirtualMachineDescriptor 则是一个描述虚拟机的容器类，配合 VirtualMachine 类完成各种功能。\n\n\nattach实现动态注入的原理如下：\n通过VirtualMachine类的attach(pid)方法，便可以attach到一个运行中的java进程上，之后便可以通过loadAgent(agentJarPath)来将agent的jar包注入到对应的进程，然后对应的进程会调用agentmain方法。\n\n既然是两个进程之间通信那肯定的建立起连接，VirtualMachine.attach动作类似TCP创建连接的三次握手，目的就是搭建attach通信的连接。而后面执行的操作，例如vm.loadAgent，其实就是向这个socket写入数据流，接收方target VM会针对不同的传入数据来做不同的处理。\n我们来测试一下agentmain的使用：\n工程结构和 上面premain的测试一样，编写AgentMainTest，然后使用maven插件打包 生成MANIFEST.MF。\nCopypackage com.rickiyang.learn;import java.lang.instrument.ClassFileTransformer;import java.lang.instrument.IllegalClassFormatException;import java.lang.instrument.Instrumentation;import java.security.ProtectionDomain;/** * @author rickiyang * @date 2019-08-16 * @Desc */public class AgentMainTest &#123;    public static void agentmain(String agentArgs, Instrumentation instrumentation) &#123;        instrumentation.addTransformer(new DefineTransformer(), true);    &#125;        static class DefineTransformer implements ClassFileTransformer &#123;        @Override        public byte[] transform(ClassLoader loader, String className, Class&lt;?&gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) throws IllegalClassFormatException &#123;            System.out.println(&quot;premain load Class:&quot; + className);            return classfileBuffer;        &#125;    &#125;&#125;Copy&lt;plugin&gt;  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;  &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;  &lt;version&gt;3.1.0&lt;/version&gt;  &lt;configuration&gt;    &lt;archive&gt;      &lt;!--自动添加META-INF/MANIFEST.MF --&gt;      &lt;manifest&gt;        &lt;addClasspath&gt;true&lt;/addClasspath&gt;      &lt;/manifest&gt;      &lt;manifestEntries&gt;        &lt;Agent-Class&gt;com.rickiyang.learn.AgentMainTest&lt;/Agent-Class&gt;        &lt;Can-Redefine-Classes&gt;true&lt;/Can-Redefine-Classes&gt;        &lt;Can-Retransform-Classes&gt;true&lt;/Can-Retransform-Classes&gt;      &lt;/manifestEntries&gt;    &lt;/archive&gt;  &lt;/configuration&gt;&lt;/plugin&gt;\n\n将agent打包之后，就是编写测试main方法。上面我们画的图中的步骤是：从一个attach JVM去探测目标JVM，如果目标JVM存在则向它发送agent.jar。我测试写的简单了些，找到当前JVM并加载agent.jar。\nCopypackage com.rickiyang.learn.job;import com.sun.tools.attach.*;import java.io.IOException;import java.util.List;/** * @author rickiyang * @date 2019-08-16 * @Desc */public class TestAgentMain &#123;    public static void main(String[] args) throws IOException, AttachNotSupportedException, AgentLoadException, AgentInitializationException &#123;        //获取当前系统中所有 运行中的 虚拟机        System.out.println(&quot;running JVM start &quot;);        List&lt;VirtualMachineDescriptor&gt; list = VirtualMachine.list();        for (VirtualMachineDescriptor vmd : list) &#123;            //如果虚拟机的名称为 xxx 则 该虚拟机为目标虚拟机，获取该虚拟机的 pid            //然后加载 agent.jar 发送给该虚拟机            System.out.println(vmd.displayName());            if (vmd.displayName().endsWith(&quot;com.rickiyang.learn.job.TestAgentMain&quot;)) &#123;                VirtualMachine virtualMachine = VirtualMachine.attach(vmd.id());                virtualMachine.loadAgent(&quot;/Users/yangyue/Documents/java-agent.jar&quot;);                virtualMachine.detach();            &#125;        &#125;    &#125;&#125;\n\nlist()方法会去寻找当前系统中所有运行着的JVM进程，你可以打印vmd.displayName()看到当前系统都有哪些JVM进程在运行。因为main函数执行起来的时候进程名为当前类名，所以通过这种方式可以去找到当前的进程id。\n注意：在mac上安装了的jdk是能直接找到 VirtualMachine 类的，但是在windows中安装的jdk无法找到，如果你遇到这种情况，请手动将你jdk安装目录下：lib目录中的tools.jar添加进当前工程的Libraries中。\n运行main方法的输出为：\n\n可以看到实际上是启动了一个socket进程去传输agent.jar。先打印了“running JVM start”表名main方法是先启动了，然后才进入代理类的transform方法。\ninstrument原理instrument的底层实现依赖于JVMTI(JVM Tool Interface)，它是JVM暴露出来的一些供用户扩展的接口集合，JVMTI是基于事件驱动的，JVM每执行到一定的逻辑就会调用一些事件的回调接口（如果有的话），这些接口可以供开发者去扩展自己的逻辑。JVMTIAgent是一个利用JVMTI暴露出来的接口提供了代理启动时加载(agent on load)、代理通过attach形式加载(agent on attach)和代理卸载(agent on unload)功能的动态库。而instrument agent可以理解为一类JVMTIAgent动态库，别名是JPLISAgent(Java Programming Language Instrumentation Services Agent)，也就是专门为java语言编写的插桩服务提供支持的代理。\n启动时加载instrument agent过程：\n创建并初始化 JPLISAgent；\n监听 VMInit 事件，在 JVM 初始化完成之后做下面的事情：\n创建 InstrumentationImpl 对象 ；\n监听 ClassFileLoadHook 事件 ；\n调用 InstrumentationImpl 的loadClassAndCallPremain方法，在这个方法里会去调用 javaagent 中 MANIFEST.MF 里指定的Premain-Class 类的 premain 方法 ；\n\n\n解析 javaagent 中 MANIFEST.MF 文件的参数，并根据这些参数来设置 JPLISAgent 里的一些内容。\n\n运行时加载instrument agent过程：通过 JVM 的attach机制来请求目标 JVM 加载对应的agent，过程大致如下：\n\n创建并初始化JPLISAgent；\n解析 javaagent 里 MANIFEST.MF 里的参数；\n创建 InstrumentationImpl 对象；\n监听 ClassFileLoadHook 事件；\n调用 InstrumentationImpl 的loadClassAndCallAgentmain方法，在这个方法里会去调用javaagent里 MANIFEST.MF 里指定的Agent-Class类的agentmain方法。\n\nInstrumentation的局限性大多数情况下，我们使用Instrumentation都是使用其字节码插桩的功能，或者笼统说就是类重定义(Class Redefine)的功能，但是有以下的局限性：\n\npremain和agentmain两种方式修改字节码的时机都是类文件加载之后，也就是说必须要带有Class类型的参数，不能通过字节码文件和自定义的类名重新定义一个本来不存在的类。\n类的字节码修改称为类转换(Class Transform)，类转换其实最终都回归到类重定义Instrumentation#redefineClasses()方法，此方法有以下限制：\n新类和老类的父类必须相同；\n新类和老类实现的接口数也要相同，并且是相同的接口；\n新类和老类访问符必须一致。 新类和老类字段数和字段名要一致；\n新类和老类新增或删除的方法必须是private static/final修饰的；\n可以修改方法体。\n\n\n\n除了上面的方式，如果想要重新定义一个类，可以考虑基于类加载器隔离的方式：创建一个新的自定义类加载器去通过新的字节码去定义一个全新的类，不过也存在只能通过反射调用该全新类的局限性。\n","categories":["Java"]},{"title":"Kafka Java 客户端配置 Kerberos 认证","url":"/posts/38228/","content":"一 、添加 Kafka 配置sasl.kerberos.service.name=kafkasecurity.protocol=SASL_PLAINTEXTsasl.mechanism=GSSAPI\n\n二、配置 krb5.conf作为 java 系统参数\n-Djava.security.krb5.conf=/usr/local/kafka/krb5.conf\n\nkrb5.conf 内容：[libdefaults]  default_realm = DEMO.COM  dns_lookup_realm = false  dns_lookup_kdc = false  rdns = false  dns_canonicalize_hostname = false  ticket_lifetime = 24h  forwardable = true  udp_preference_limit = 0  #default_ccache_name = KEYRING:persistent:%&#123;uid&#125;[realms]  DEMO.COM = &#123;    kdc = aa.demo.com:88    master_kdc = aa.demo.com:88    admin_server = aa.demo.com:749    kpasswd_server = aa.demo.com:464    default_domain = demo.com    pkinit_anchors = FILE:/var/lib/ipa-client/pki/kdc-ca-bundle.pem    pkinit_pool = FILE:/var/lib/ipa-client/pki/ca-bundle.pem  &#125;[domain_realm]  .demo.com = DEMO.COM  demo.com = DEMO.COM  bb.demo.com = DEMO.COM\n\n三、添加 keytab 配置在 /xxx 目录下添加 xxx.keytab\n四、动态认证 OR 静态认证1. 动态认证添加 kafka 配置\nsasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true storeKey=true keyTab=\\&quot;/xxx/xxx.keytab\\&quot; principal=\\&quot;xxx@xxxx.COM\\&quot;;\n\n2. 静态认证添加 java 系统参数\n-Djava.security.auth.login.config=/xxxxx/kafka_client_jaas.conf\n\nkafka_client_jaas.conf 内容：KafkaClient &#123;   com.sun.security.auth.module.Krb5LoginModule required   useKeyTab=true   renewTicket=true   serviceName=&quot;kafka&quot;   keyTab=&quot;/xxxx/xxx.keytab&quot;   principal=&quot;xxx/bb.demo.com@DEMO.COM&quot;;&#125;;Client &#123;   com.sun.security.auth.module.Krb5LoginModule required   useKeyTab=true   serviceName=&quot;kafka&quot;   useTicketCache=false   keyTab=&quot;/xxxxx/xxx.keytab&quot;   principal=&quot;xxx/bb.demo.com@DEMO.COM&quot;;&#125;;\n\n相关资料https://blog.csdn.net/justchnmm/article/details/123832385\n","categories":["Java"]},{"title":"Log4j2 Disruptor 配置","url":"/posts/2219/","content":"官方文档：https://logging.apache.org/log4j/2.x/manual/async.html#SysPropsMixedSync-Async\n1、队列满时丢弃默认是 Blocking \n-Dlog4j2.AsyncQueueFullPolicy=Discard\n\n2、更改 Disruptor 队列长度队列默认大小 4096\n-Dlog4j2.asyncLoggerRingBufferSize=262144-Dlog4j2.asyncLoggerConfigRingBufferSize=262144 \n\n3、队列满时丢弃的日志级别-Dlog4j2.discardThreshold=INFO","categories":["Java"]},{"title":"Log4j2 监控 Disruptor RingBuffer 容量","url":"/posts/36705/","content":"参考文章\nimport io.micrometer.core.instrument.Gauge;import io.micrometer.prometheus.PrometheusMeterRegistry;import java.lang.management.ManagementFactory;import javax.management.ObjectName;import lombok.RequiredArgsConstructor;import lombok.extern.slf4j.Slf4j;import org.apache.commons.lang3.StringUtils;import org.apache.logging.log4j.LogManager;import org.apache.logging.log4j.core.LoggerContext;import org.apache.logging.log4j.core.jmx.RingBufferAdminMBean;import org.springframework.context.ApplicationListener;import org.springframework.context.annotation.Configuration;import org.springframework.context.event.ContextRefreshedEvent;/** * @Auther jd */@Slf4j@Configuration@RequiredArgsConstructorpublic class Log4j2AutoConfiguration implements ApplicationListener&lt;ContextRefreshedEvent&gt; &#123;  private final PrometheusMeterRegistry prometheusMeterRegistry;  private volatile boolean initialized = false;  @Override  public synchronized void onApplicationEvent(ContextRefreshedEvent event) &#123;    if (initialized) &#123;      return;    &#125;    //通过 LogManager 获取 LoggerContext，从而获取配置    LoggerContext loggerContext = (LoggerContext) LogManager.getContext(false);    org.apache.logging.log4j.core.config.Configuration configuration = loggerContext.getConfiguration();    //获取 LoggerContext 的名称，因为 Mbean 的名称包含这个    String ctxName = loggerContext.getName();    for (String logName : configuration.getLoggers().keySet()) &#123;      //针对 RootLogger，它的 cfgName 是空字符串，为了显示好看，我们在 prometheus 中将它命名为 root      String cfgName = StringUtils.isBlank(logName) ? &quot;&quot; : logName;      String gaugeName = StringUtils.isBlank(logName) ? &quot;root&quot; : logName;      Gauge.builder(gaugeName + &quot;_logger_ring_buffer_remaining_capacity&quot;, () -&gt; &#123;        try &#123;          return (Number) ManagementFactory.getPlatformMBeanServer()              .getAttribute(new ObjectName(                  //按照 Log4j2 源码中的命名方式组装名称                  String.format(RingBufferAdminMBean.PATTERN_ASYNC_LOGGER_CONFIG, ctxName, cfgName)                  //获取剩余大小，注意这个是严格区分大小写的              ), &quot;RemainingCapacity&quot;);        &#125; catch (Exception e) &#123;          log.error(&quot;Get &#123;&#125; ringbuffer remaining size error&quot;, logName, e);        &#125;        return -1;      &#125;).register(prometheusMeterRegistry);    &#125;    initialized = true;  &#125;&#125;\n\n","categories":["Java"]},{"title":"ReentrantLock 实现双线程顺序打印1-100","url":"/posts/53751/","content":"使用 ReentrantLock 实现双线程顺序打印1-100\n\n\nimport java.util.concurrent.locks.Condition;import java.util.concurrent.locks.ReentrantLock;public class Test &#123;    private static ReentrantLock lock = new ReentrantLock();    private static Condition even = lock.newCondition();    private static Condition odd = lock.newCondition();    private static boolean flag = true;    public static void main(String[] args) throws InterruptedException &#123;        Thread aThread = new Thread(() -&gt; &#123;            try &#123;                lock.lock();                if (flag) &#123;                    even.await();                &#125;                lock.unlock();                for (int i = 2; i &lt;= 100; i += 2) &#123;                    lock.lock();                    System.out.println(i);                    odd.signal();                    even.await();                    lock.unlock();                &#125;            &#125; catch (Exception e) &#123;            &#125;            System.out.println(&quot;even end&quot;);        &#125;);        Thread bThread = new Thread(() -&gt; &#123;            try &#123;                for (int i = 1; i &lt; 100; i += 2) &#123;                    lock.lock();                    if (flag) &#123;                        flag = false;                    &#125;                    System.out.println(i);                    even.signal();                    odd.await();                    lock.unlock();                &#125;            &#125; catch (Exception e) &#123;            &#125;            lock.lock();            even.signal();            lock.unlock();            System.out.println(&quot;odd end&quot;);        &#125;);        aThread.start();        bThread.start();        aThread.join();    &#125;&#125;\n\n","categories":["Java"],"tags":["面试"]},{"title":"byte为什么要与上0xff？","url":"/posts/37071/","content":"无意间翻看之间的代码，发现了一段难以理解的代码。\nbyte[] bs = digest.digest(origin.getBytes(Charset.forName(charsetName))) ;  for (int i = 0; i &lt; bs.length; i++) &#123;    int c = bs[i] &amp; 0xFF ;  if(c &lt; 16)&#123;     sb.append(&quot;0&quot;);    &#125;    sb.append(Integer.toHexString(c)) ;  &#125;  return sb.toString() ;\n\nbs是由一段字符串经过MD5加密后，输出的byte数组。我起初难以理解为什么在接下来的循环中要将bs[i]&amp;oxFF再赋值给int类型呢？\nbs[i]是8位二进制，0xFF转化成8位二进制就是11111111，那么bs[i]&amp;0xFF不是还是bs[i]本身吗？有意思吗？\n后来我又写了一个demo\npublic class Test &#123;    public static void main(String[] args) &#123;        byte[] a = new byte[10];        a[0]= -127;        System.out.println(a[0]);        int c = a[0]&amp;0xff;        System.out.println(c);    &#125;&#125;\n\n我先打印a[0],在打印a[0]&amp;0xff后的值，本来我想结果应该都是-127\n但是结果真的是出人意料啊！\n\n-127\n129\n\n到底是为什么呢？&amp;0xff反而不对了。\n楼主真的是不懂啊，后来往补码那个方向想了想。\n记得在学计算机原理的时候，了解到计算机内的存储都是利用二进制的补码进行存储的。\n原码、反码、补码对于正数（00000001）原码来说，首位表示符号位，反码 补码都是本身\n对于负数（100000001）原码来说，反码是对原码除了符号位之外作取反运算即（111111110），补码是对反码作+1运算即（111111111）\n当将-127赋值给a[0]时候，a[0]作为一个byte类型，其计算机存储的补码是10000001（8位）。\n将a[0] 作为int类型向控制台输出的时候，jvm作了一个补位的处理，因为int类型是32位所以补位后的补码就是1111111111111111111111111 10000001（32位），这个32位二进制补码表示的也是-127.\n发现没有，虽然byte-&gt;int计算机背后存储的二进制补码由10000001（8位）转化成了1111111111111111111111111 10000001（32位）很显然这两个补码表示的十进制数字依然是相同的。\n但是我做byte-&gt;int的转化 所有时候都只是为了保持 十进制的一致性吗？\n不一定吧？好比我们拿到的文件流转成byte数组，难道我们关心的是byte数组的十进制的值是多少吗？我们关心的是其背后二进制存储的补码吧\n所以大家应该能猜到为什么byte类型的数字要&amp;0xff再赋值给int类型，其本质原因就是想保持二进制补码的一致性。\n当byte要转化为int的时候，高的24位必然会补1，这样，其二进制补码其实已经不一致了，&amp;0xff可以将高的24位置为0，低8位保持原样。这样做的目的就是为了保证二进制数据的一致性。\n当然拉，保证了二进制数据性的同时，如果二进制被当作byte和int来解读，其10进制的值必然是不同的，因为符号位位置已经发生了变化。\n象例2中，int c = a[0]&0xff;  a[0]&amp;0xff=1111111111111111111111111 10000001&amp;11111111=000000000000000000000000 10000001 ，这个值算一下就是129，\n所以c的输出的值就是129。有人问为什么上面的式子中a[0]不是8位而是32位，因为当系统检测到byte可能会转化成int或者说byte与int类型进行运算的时候，就会将byte的内存空间高位补1（也就是按符号位补位）扩充到32位，再参与运算。上面的0xff其实是int类型的字面量值，所以可以说byte与int进行运算。\n","categories":["Java"]},{"title":"两线程交替顺序打印1-100","url":"/posts/43133/","content":"import lombok.SneakyThrows;public class Test &#123;    private static int count;    private static Object aa = new Object();    @SneakyThrows    public static void main(String[] args) &#123;        new Thread(() -&gt; &#123;            try &#123;                while (print()) &#123;                    synchronized (aa) &#123;                        aa.notify();                        aa.wait();                    &#125;                &#125;                synchronized (aa) &#123;                    aa.notifyAll();                &#125;            &#125; catch (Exception e)&#123;            &#125;        &#125;, &quot;A&quot;).start();        new Thread(() -&gt; &#123;            try &#123;                synchronized (aa) &#123;                    aa.wait();                &#125;                while (print()) &#123;                    synchronized (aa) &#123;                        aa.notify();                        aa.wait();                    &#125;                &#125;            &#125; catch (Exception e) &#123;                e.printStackTrace();            &#125;        &#125;, &quot;B&quot;).start();    &#125;    private static boolean print() &#123;        if (count &gt;= 100) &#123;            return false;        &#125;        System.out.printf(&quot;%s = %d\\n&quot;, Thread.currentThread().getName(), ++count);        return true;    &#125;&#125;\n\n","categories":["Java"],"tags":["面试"]},{"title":"GitOps flux cd","url":"/posts/39629/","content":"官方仓库\n官方安装文档\n官方文档\n一、安装flux使用 Homebrew 在 Mac 上安装\nbrew install fluxcd/tap/flux\n\n检查是否具备运行 Flux 所需的一切：\nflux check --pre\n\n输出类似于：\n► checking prerequisites✔ kubernetes 1.22.2 &gt;=1.20.6✔ prerequisites checks passed\n\n二、在 K8S 中部署 flux1.设置环境变量将 github 的 access token 设置为环境变量。\nexport GITHUB_TOKEN=ghp_ooooooxxxxxxxPxxxxxxxxxpppppp\n\n2. 部署到集群该步骤会在仓库中自动创建 clusters/demo-cluster 目录，相关 yaml 也会 push\nflux bootstrap github \\  --hostname=github.com \\  --owner=jd-boy \\  --repository=gitops \\  --branch=master \\  --path=clusters/demo-cluster\n\n三、创建 sourceurl中指定的是存放 yaml 的仓库地址，可以与上一步的仓库地址不一样\nflux create source git podinfo \\  --url=ssh://git@github.com/jd-boy/gitops \\  --branch=master \\  --interval=30s \\  --private-key-file=/Users/jd/.ssh/mac-github-rsa\n\n查看 source 状态\nflux get sources git --watch\n\n四、创建 kustomization配置读取第三步 source 仓库中 apps/demo-cluster 路径下的 yaml\nflux create kustomization podinfo \\  --target-namespace=test \\  --source=podinfo \\  --path=&quot;./apps/demo-cluster&quot; \\  --prune=true \\  --interval=1m\n\n查看 kustomizations 状况：\nflux get kustomizations\n\n仓库中的目录结构为：\ngitops└── clusters/    └── demo-cluster/        └── flux-system/                                    ├── gotk-components.yaml            ├── gotk-sync.yaml            └── kustomization.yaml└── apps/    └── demo-cluster/        └──nginx-deployment.yaml\n\n此时去集群的 test 命名空间下查看nginx已经部署。\n五、暂停 kustomizations此时修改k8s中的资源，并不会被还原成 Git 中的状态。\nflux suspend kustomization &lt;name&gt;\n恢复更新可以运行一下命令\nflux resume kustomization &lt;name&gt;","categories":["Kubernetes"]},{"title":"K8S + SpringBoot 零宕机发布","url":"/posts/11562/","content":"一、SpringBoot 配置依赖&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;\n\n配置spring:  application:    name: &lt;xxx&gt;  profiles:    active: @profileActive@  lifecycle:    timeout-per-shutdown-phase: 30s     # 停机过程超时时长设置30s，超过30s，直接停机server:  port: 8080  shutdown: graceful                    # 默认为IMMEDIATE，表示立即关机；GRACEFUL表示优雅关机management:  server:    port: 50000                         # 启用独立运维端口  endpoint:    shutdown:      enabled: true                     # 开启shutdown端点，curl -X POST 127.0.0.1:50000/actuator/shutdown    health:      probes:        enabled: true                   # 开启health端点，将暴露 http://127.0.0.1:50000/actuator/health/readiness、http://127.0.0.1:50000/actuator/health/liveness  endpoints:    web:      exposure:        base-path: /actuator            # 指定上下文路径，启用相应端点        include: health,shutdown,metrics,prometheus\n\n二、Dockerfile确保 dockerfile 中集成了 curl 工具\nFROM openjdk:8-jdk-alpine#构建参数ARG JAR_FILEARG WORK_PATH=&quot;/app&quot;ARG EXPOSE_PORT=8080#环境变量ENV JAVA_OPTS=&quot;&quot;\\    JAR_FILE=$&#123;JAR_FILE&#125;#设置时区RUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo &#x27;Asia/Shanghai&#x27; &gt;/etc/timezoneRUN sed -i &#x27;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g&#x27; /etc/apk/repositories  \\    &amp;&amp; apk add --no-cache curl#将maven目录的jar包拷贝到docker中，并命名为for_docker.jarCOPY target/$JAR_FILE $WORK_PATH/#设置工作目录WORKDIR $WORK_PATH# 指定于外界交互的端口EXPOSE $EXPOSE_PORT# 配置容器，使其可执行化ENTRYPOINT exec java $JAVA_OPTS -jar $JAR_FILE\n\n三、K8S 配置apiVersion: apps/v1kind: Deploymentmetadata:  name: &#123;APP_NAME&#125;  labels:    app: &#123;APP_NAME&#125;spec:  selector:    matchLabels:      app: &#123;APP_NAME&#125;  replicas: &#123;REPLICAS&#125;                            # Pod副本数  strategy:    type: RollingUpdate                           # 滚动更新策略    rollingUpdate:      maxSurge: 1                                 # 升级过程中最多可以比原先设置的副本数多出的数量      maxUnavailable: 0                           # 升级过程中最多有多少个POD处于无法提供服务的状态  template:    metadata:      name: &#123;APP_NAME&#125;      labels:        app: &#123;APP_NAME&#125;      annotations:        timestamp: &#123;TIMESTAMP&#125;        prometheus.io/port: &quot;50000&quot;               # 不能动态赋值        prometheus.io/path: /actuator/prometheus        prometheus.io/scrape: &quot;true&quot;              # 基于pod的服务发现    spec:      affinity:                                   # 设置调度策略，采取多主机/多可用区部署        podAntiAffinity:          preferredDuringSchedulingIgnoredDuringExecution:          - weight: 100            podAffinityTerm:              labelSelector:                matchExpressions:                - key: app                  operator: In                  values:                  - &#123;APP_NAME&#125;              topologyKey: &quot;kubernetes.io/hostname&quot; # 多可用区为&quot;topology.kubernetes.io/zone&quot;      terminationGracePeriodSeconds: 30             # 优雅终止宽限期      containers:      - name: &#123;APP_NAME&#125;        image: &#123;IMAGE_URL&#125;        imagePullPolicy: Always        ports:        - containerPort: &#123;APP_PORT&#125;        - name: management-port          containerPort: 50000         # 应用管理端口        lifecycle:          preStop:       # 结束回调钩子，需保证镜像中包含 curl 工具            exec:              command: [&quot;curl&quot;, &quot;-XPOST&quot;, &quot;127.0.0.1:50000/actuator/shutdown&quot;]        readinessProbe:                # 就绪探针          httpGet:            path: /actuator/health/readiness            port: management-port          initialDelaySeconds: 30      # 延迟加载时间          periodSeconds: 10            # 重试时间间隔          timeoutSeconds: 1            # 超时时间设置          successThreshold: 1          # 健康阈值          failureThreshold: 9          # 不健康阈值        livenessProbe:                 # 存活探针          httpGet:            path: /actuator/health/liveness            port: management-port          initialDelaySeconds: 30      # 延迟加载时间          periodSeconds: 10            # 重试时间间隔          timeoutSeconds: 1            # 超时时间设置          successThreshold: 1          # 健康阈值          failureThreshold: 6          # 不健康阈值        resources:                     # 容器资源管理          limits:                      # 资源限制（监控使用情况）            cpu: 0.5            memory: 1Gi          requests:                    # 最小可用资源（灵活调度）            cpu: 0.1            memory: 200Mi        env:          - name: TZ            value: Asia/Shanghai---kind: HorizontalPodAutoscaler            # 弹性伸缩控制器apiVersion: autoscaling/v2beta2metadata:  name: &#123;APP_NAME&#125;spec:  scaleTargetRef:    apiVersion: apps/v1    kind: Deployment    name: &#123;APP_NAME&#125;  minReplicas: &#123;REPLICAS&#125;                # 最小副本数  maxReplicas: 6                         # 最大副本数  metrics:    - type: Resource      resource:        name: cpu                        # 指定资源指标        target:          type: Utilization          averageUtilization: 50\n\n","categories":["Kubernetes"]},{"title":"K8S ExitCode","url":"/posts/17169/","content":"一、如何查看退出码查看pod中的容器退出码\n$ kubectl describe pod xxxPort:           &lt;none&gt;    Host Port:      &lt;none&gt;    State:          Running      Started:      Tue, 26 May 2020 20:01:04 +0800    Last State:     Terminated      Reason:       Error      Exit Code:    137      Started:      Tue, 26 May 2020 19:58:40 +0800      Finished:     Tue, 26 May 2020 20:01:04 +0800    Ready:          True    Restart Count:  2363\n\ndocker查看\n$ docker ps --filter &quot;status=exited&quot;$ docker inspect &lt;container-id&gt; --format=&#x27;&#123;&#123;.State.ExitCode&#125;&#125;&#x27;\n\n手动输出\ndocker container run alpine sh -c &quot;exit 1&quot;$ docker container ls -aCONTAINER ID   IMAGE    COMMAND            CREATED              STATUS                       61c6880xxxxx   alpine   &quot;sh -c &#x27;exit 1&#x27;&quot;   About a minute ago   Exited (1) 3 seconds ago\n\n二、常见退出码Exit Code 0\n退出代码0表示特定容器没有附加前台进程。\n该退出代码是所有其他后续退出代码的例外。\n这不一定意味着发生了不好的事情。如果开发人员想要在容器完成其工作后自动停止其容器，则使用此退出代码。\n\nExit Code 1\n程序错误，或者Dockerfile中引用不存在的文件，如 entrypoint中引用了错误的包\n程序错误可以很简单，例如 “除以0”，也可以很复杂，比如空引用或者其他程序 crash\n\nExit Code 137\n此状态码一般是因为 pod 中容器内存达到了它的资源限制（resources.limits），一般是内存溢出（OOM），CPU达到限制只需要不分时间片给程序就可以。因为限制资源是通过 linux 的 cgroup 实现的，所以 cgroup 会将此容器强制杀掉，类似于 kill -9\n还可能是宿主机本身资源不够用了（OOM），内核会选取一些进程杀掉来释放内存\n不管是 cgroup 限制杀掉进程还是因为节点机器本身资源不够导致进程死掉，都可以从系统的 dmesg 中看到 oom 日志( journalctl -k )\n\nExit Code 139\n表明容器收到了 SIGSEGV 信号，无效的内存引用，对应 kill -11\n一般是代码有问题，或者 docker 的基础镜像有问题\n\nExit Code 143\n表明容器收到了 SIGTERM 信号，终端关闭，对应 kill -15\n一般对应 docker stop 命令\n有时 docker stop 也会导致 Exit Code 137。发生在与代码无法处理 SIGTERM 的情况下，docker 进程等待十秒钟然后发出 SIGKILL 强制退出。\n\n不常用的一些 Exit Code\nExit Code 126: 权限问题或命令不可执行\nExit Code 127: Shell 脚本中可能出现错字且字符无法识别的情况\nExit Code 1 或 255：因为很多程序员写异常退出时习惯用 exit(1) 或 exit(-1)，-1 会根据转换规则转成 255。这个一般是自定义 code，要看具体逻辑。\n\n退出状态码的区间\n必须在 0-255 之间，0 表示正常退出\n外界将程序中断退出，状态码在 129-255\n程序自身异常退出，状态码一般在 1-128\n假如写代码指定的退出状态码时不在 0-255 之间，例如: exit(-1)，这时会自动做一个转换，最终呈现的状态码还是会在 0-255 之间。我们把状态码记为 code，当指定的退出时状态码为负数，那么转换公式如下：256 – (|code| % 256)\n\n","categories":["Kubernetes"]},{"title":"K8S 拉取私有Docker仓库镜像","url":"/posts/36130/","content":"一、创建保存授权令牌的 SecretKubernetes 集群使用 docker-registry 类型的 Secret 来通过容器仓库的身份验证，进而提取私有映像。\n在 test 命名空间创建 Secret，命名为 regcred：\nkubectl create secret docker-registry docker-secret \\  --docker-server=&lt;你的镜像仓库服务器&gt; \\  --docker-username=&lt;你的用户名&gt; \\  --docker-password=&lt;你的密码&gt; \\  --docker-email=&lt;你的邮箱地址&gt;\n\n\n&lt;your-registry-server&gt; 是你的私有 Docker 仓库全限定域名（FQDN）。 DockerHub 使用 https://index.docker.io/v1/。\n&lt;your-name&gt; 是你的 Docker 用户名。\n&lt;your-pword&gt; 是你的 Docker 密码。\n&lt;your-email&gt; 是你的 Docker 邮箱。\n\n二、检查 Secret要了解你创建的 regcred Secret 的内容，可以用 YAML 格式进行查看：\nkubectl -n test get secret docker-secret --output=yaml\n\n内容如下：\napiVersion: v1data:  .dockerconfigjson: eyJhdXRocyI6eyJyZWdpc3R.........9fX0=kind: Secretmetadata:  creationTimestamp: &quot;2022-03-20T12:54:05Z&quot;  name: docker-secret  namespace: default  resourceVersion: &quot;44636&quot;  uid: d23ef2b3-eee8-41f2-9a0a-5baf4efb5107type: kubernetes.io/dockerconfigjson\n\n.dockerconfigjson 字段的值是 Docker 凭证的 base64 表示。\n要了解 dockerconfigjson 字段中的内容，请将 Secret 数据转换为可读格式：\nkubectl -n test get secret docker-secret --output=&quot;jsonpath=&#123;.data.\\.dockerconfigjson&#125;&quot; | base64 --decode\n\n内容如下：\n&#123;&quot;auths&quot;:&#123;&quot;yourprivateregistry.com&quot;:&#123;&quot;username&quot;:&quot;janedoe&quot;,&quot;password&quot;:&quot;xxxxxxxxxxx&quot;,&quot;email&quot;:&quot;jdoe@example.com&quot;,&quot;auth&quot;:&quot;c3R...zE2&quot;&#125;&#125;&#125;\n\n要了解 auth 字段中的内容，请将 base64 编码过的数据转换为可读格式：\necho &quot;c3R...zE2&quot; | base64 --decode\n\n输出结果中，用户名和密码用 : 链接，类似下面这样：\njanedoe:xxxxxxxxxxx\n\n三、创建使用你的 Secret 的 Podcat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: vertx-demo  name: vertx-demo  namespace: testspec:  replicas: 1  minReadySeconds: 5  strategy:    rollingUpdate:      maxSurge: 25%      maxUnavailable: 25%    type: RollingUpdate  selector:    matchLabels:      app: vertx-demo  template:    metadata:      labels:        app: vertx-demo    spec:      imagePullSecrets:        - name: docker-secret      containers:        - name: vertx-demo          image: &#x27;registry.xxx.com/jd-zeus/demo&#x27;          imagePullPolicy: &#x27;Always&#x27;          resources:            limits:              cpu: &#x27;1&#x27;              memory: &#x27;400M&#x27;            requests:              cpu: &#x27;0.5&#x27;              memory: &#x27;200M&#x27;          ports:            - name: tcp-8080              containerPort: 8080              protocol: TCP            - name: metrics              containerPort: 8001              protocol: TCP      restartPolicy: AlwaysEOF\n\n","categories":["Kubernetes"]},{"title":"K8S 生成kubeconfig","url":"/posts/59459/","content":"1.创建namespacekubectl create namespace test\n\n2.创建 ServiceAccount创建yaml\ncat &gt;&gt; serviceAccount.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata:  name: test-userEOF\n\n创建资源\nkubectl create -f serviceAccount.yaml -n test\n\n3.创建Role创建一个role，并通过RBAC控制上面生成的service account只能访问该namespace里的资源。\n创建yaml\ncat &gt;&gt; role.yaml&lt;&lt;EOFapiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:  name: test-role  namespace: testrules:  - apiGroups:      - &quot;&quot;    resources:      - &quot;namespaces&quot;    verbs:      - &quot;get&quot;      - &quot;list&quot;---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: test-rolebinding  namespace: testroleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: test-rolesubjects:  - namespace: test    kind: ServiceAccount    name: test-userEOF\n\n创建资源\nkubectl create -f role.yaml\n\n4.获取ServiceAccount的secret$ kubectl describe serviceAccounts test-user -n testName:                test-userNamespace:           testLabels:              &lt;none&gt;Annotations:         &lt;none&gt;Image pull secrets:  &lt;none&gt;Mountable secrets:   test-user-token-jfj9gTokens:              test-user-token-jfj9gEvents:              &lt;none&gt;\n\n5.获取secret对应tokenkubectl describe secret test-user-token-jfj9g -n test\n\n6.获取集群信息kubectl config view --flatten --minify\n\n7.按以下格式生成kubeconfigapiVersion: v1kind: Configusers:- name: test-user  user:    token: &#123;步骤5获取的token&#125;  clusters:- cluster:    certificate-authority-data: &#123;步骤6获取&#125;    server: &#123;步骤6获取&#125;  name: &#123;步骤6获取的集群名&#125;contexts:- context:    cluster: &#123;步骤6获取的集群名&#125;    user: test-user  name: test-user-contextcurrent-context: test-user-context\n\n9.为namespace设置quota创建yaml\n$ cat &gt;&gt; quota.yaml&lt;&lt;EOFapiVersion: v1kind: Listitems:- apiVersion: v1  kind: ResourceQuota  metadata:    name: quota  spec:    hard:      cpu: &quot;20&quot;    # CPU      memory: 10Gi  # 内存      pods: &quot;50&quot;  # pod数EOF\n\n创建资源\n$ kubectl create -f quota.yaml -n testresourcequota/quota created\n\n\n\napiVersion: v1kind: Configusers:- name: test-user  user:    token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjdrLVloT2NDd191VndTREh2TUpmM21EdkNPWkZFOEZXaklxZEl5Qms3dE0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InRlc3QtdXNlci10b2tlbi12Y3BodiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ0ZXN0LXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI0NTg0ZGVjYi01M2YxLTQ4ZjAtYWZiZS01NmY5ODY5ZTc3YzgiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDp0ZXN0LXVzZXIifQ.H-Sn6QbRfnZs03YTYe338MJpRuoSHwFIynQGMeKqDBwxy5EZXGMjTtIKHCpK46XSxtdchsbsPpbktsZs0QFrluDLO03MYDsBWVoT0cV0YXUgnz7OHYUX-4pfO1zXeBL_CmHAT-Gn06O4Vcs_cOZXJd-Rlc4Sw6M3AhuzouKvWOKK8zPbahhbYyOOAU1VAAYv_DdS8Txq74vs5In5nqtvLhW5wca9LTkLPysxPsAB9V_UxJWedalf27OEl1vssvWaD9K0cKPBn3ZSxyJNAper6Ivu254KVuQj2V6p2xqqSzJ_f_UPeQXAS53WhB4jxJGLmJ7BefZUjSR-IUlo1fzUYwclusters:- cluster:    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCakNDQWU2Z0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwdGFXNXAKYTNWaVpVTkJNQjRYRFRJeE1USXdOakUwTXprME9Wb1hEVE14TVRJd05URTBNemswT1Zvd0ZURVRNQkVHQTFVRQpBeE1LYldsdWFXdDFZbVZEUVRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTFJzCjZIY3N6Y0lWZVIxa1oraGRPb2d1NlRtTEw5VWhtV0N5U21FYVNiZFlzWlpacmo0VEFOaGdabFpPOUhvUDhqeTUKaTh1K2g0cDBZR0ZHd05COHRMSmdyN0puMW9MYzFRWFJXajZoNSt4SXdRdER3TmpYUnFNRURWcUF5RnNGV1pETgpWTlZGTWdTSS9IdWFEUUVYV2xyMGFUd3Jzb3VXeG1vbmtyamt4dWJzUHhiTnJWajJtaDdISFpSdTBMak1kTGplCnZUSE80dWVydEZJTFAySGJBK0I3ZUlRNDFLU051WGxRUmN0Y3BCQll6ZzB1ODVWMmx4a2MwNkRaSy9weHZ4ZE4KejRDVXh3KzVkdTdDZlhVeEFCT3VReS9YdExELzhiRXUvd1B5REk0M1Byb1gvVmtzNjk2KzNXcGFDR1lRMVZYUgpNVU5zcm9OSW9pb2R3djhNZGowQ0F3RUFBYU5oTUY4d0RnWURWUjBQQVFIL0JBUURBZ0trTUIwR0ExVWRKUVFXCk1CUUdDQ3NHQVFVRkJ3TUNCZ2dyQmdFRkJRY0RBVEFQQmdOVkhSTUJBZjhFQlRBREFRSC9NQjBHQTFVZERnUVcKQkJSR2RoeFVrVGVrbnRhdHBXaFBEQkxCUzhBcWVEQU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFiZFFwOTQxZQpmaVJjdTBSaTdrWkVXZkpqNDBWa2hqQTdTNjV4NEUyN3hYYXFxRG9tc2dkSFVpaHNyM0pXRU5iNjZYS0dFUWhlCmFDQUlHcE9tNjdJSUFacW15dUNOYVpwUGVKSXpzTTBOMVRLSnppNGhCczR4Rm5BQTljbHh0bm9jZmZLVnpxV1cKRUQ2d01BT1J1Z1pUTC9ldUgzandON2NXdEdhRkdPMFJxdXFqRW9rdGlYbkZwN1ZhT1ZaNUtpZ0RzUWZibmhJVQo2OFJoQTJJZWlYTWh2L3RiTHZwL3VQNUh5VnVSUGU2S1ZXRFB3N1NXNHV4Z0QvM29ocm8wWjVNTDF2dXk1b2Z1Ck5EWk1nUmR3dXFLT2JWektCTElaTXJKd2RWUHhvNE5xbFlnU0xGSGZjZ3ArOWUrOWVMUzZhKy9ORzkxeVRJS04KQlRkNFVUbi9QS01VbkE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==    server: https://127.0.0.1:49570  name: minikubecontexts:- context:    cluster: minikube    user: test-user  name: testKubeconfigcurrent-context: testKubeconfig\n\n","categories":["Kubernetes"]},{"title":"K8S 部署代理访问内网服务","url":"/posts/19720/","content":"Github\nDockerhub\n一、DockerfileFROM alpine:3.9MAINTAINER David Salgado &lt;david.salgado@digital.justice.gov.uk&gt;LABEL MAINTAINER David Salgado &lt;david.salgado@digital.justice.gov.uk&gt;RUN apk add --update --no-cache socat \\\t&amp;&amp; rm -rf /var/cache/apk/RUN addgroup -g 1001 -S appgroup &amp;&amp; \\    adduser -u 1001 -S appuser -G appgroupARG DEF_REMOTE_PORT=8000ARG DEF_LOCAL_PORT=8000ENV REMOTE_PORT=$DEF_REMOTE_PORTENV LOCAL_PORT=$DEF_LOCAL_PORT## By default container listens on $LOCAL_PORT (8000)EXPOSE 8000USER 1001CMD socat tcp-listen:$LOCAL_PORT,reuseaddr,fork tcp:$REMOTE_HOST:$REMOTE_PORT &amp; pid=$! &amp;&amp; trap &quot;kill $pid&quot; SIGINT &amp;&amp; \\\techo &quot;Socat started listening on $LOCAL_PORT: Redirecting traffic to $REMOTE_HOST:$REMOTE_PORT ($pid)&quot; &amp;&amp; wait $pid\n\n二、用法Define the following environment variables to configure port-forwarding.\n\n\n\nVariable\nDescription\nOptional\n\n\n\nREMOTE_HOST\nIP or address of the host you want to forward traffic to\nno\n\n\nREMOTE_PORT\nPort on remote host to forward traffic to\nyes (80)\n\n\nLOCAL_PORT\nPort where container listens\nyes (80)\n\n\nThe socat process within the container will listen by default to port 80, use -pdocker flag to map the port of the local machine where it will listen to traffic to be forwarded.\ndocker run -e REMOTE_HOST=&lt;remote_host&gt; -e REMOTE_PORT=&lt;remote_port&gt; -e LOCAL_PORT=&lt;local_port&gt; -p &lt;exposed_local_port&gt;:&lt;local_port&gt; marcnuri/port-forward\n\n三、样例The following commands will all forward 8080 traffic to a remote machine located at www.marcnuri.com in the http port\ndocker run -e REMOTE_HOST=www.marcnuri.com -e REMOTE_PORT=80 -e LOCAL_PORT=80 -p 8080:80 marcnuri/port-forwarddocker run -e REMOTE_HOST=www.marcnuri.com -e REMOTE_PORT=80 -p 8080:80 marcnuri/port-forwarddocker run -e REMOTE_HOST=www.marcnuri.com -p 8080:80 marcnuri/port-forward\n\n四、在 K8S 中使用kubectl run -n test --env REMOTE_HOST=127.0.233.11 --env REMOTE_PORT=9199 --env LOCAL_PORT=9199 --port 9199 --image marcnuri/port-forward &lt;name&gt;\n\n","categories":["Kubernetes"]},{"title":"K8s可视化监控之 Weave Scope","url":"/posts/29341/","content":"官方文档：https://www.weave.works/docs/scope/latest/installing/\nGitHub：https://github.com/weaveworks/scope\n一、背景在生成环境中k8s应用部署众多，需要一款可视化工具方便日常获知集群的实时状态，并为故障排查提供及时和准确的数据支持。在此背景下，Weaveworks 的项目Weave Scope 应运而生，其是一款 Docker 和 Kubernetes 可视化监控工具。Scope 提供了至上而下的集群基础设施和应用的完整视图，用户可以轻松对分布式的容器化应用进行实时监控和问题诊断,以确保容器应用程序的稳定性和性能，通过查看容器上下文的度量/标记，以及原数据，在容器内部的进程之间可以轻松的确定运行服务消耗最多CPU/内存资源的容器，\n二、组件详解2.1 功能Weave Scope可以监控kubernetes集群中的一系列资源的状态、资源使用情况、应用拓扑、scale、还可以直接通过浏览器进入容器内部调试等,其提供的功能包括:\n\n交互式拓扑界面\n图形模式和表格模式\n过滤功能\n搜索功能\n实时度量\n容器排错\n插件扩展\n\n2.2 组成Weave Scope由 App 和 Probe 两部分组成:\n\nProbe Agent负责收集容器和宿主的信息，并发送给 App\nApp 负责处理这些信息，并生成相应的报告，并以交互界面的形式展示\n\n2.3 部署模式\nweave-scope-agent，集群每个节点上都会运行的 scope agent 程序，负责收集数据，其部署为DaemonSet模式。\nweave-scope-app，scope 应用，从 agent 获取数据，通过 Web UI 展示并与用户交互，其部署为Deployment模式。\nweave-scope-app，默认是 ClusterIP 类型，为了方便已通过 kubectl edit 修改为 NodePort。\n\n三、安装部署3.1 Node节点部署最初Weave Scope可以在运行Docker容器的宿主机进行安装部署\nsudo curl -L git.io/scope -o /usr/local/bin/scopesudo chmod a+x /usr/local/bin/scopescope launch复制代码\n\n之后访问**http://宿主机IP:4040**进行访问\n3.2 K8s 资源文件部署kubectl apply -f &quot;https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d &#x27;\\n&#x27;)&quot;# 修改svc 为NodePortkubectl patch svc $(kubectl get svc -n weave |grep weave-scope-app |awk &#x27;&#123;print $1&#125;&#x27;) -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;type&quot;: &quot;NodePort&quot;&#125;&#125;&#x27; -n weave[root@master ~]# kubectl get svc -n weave |grep weave-scope-app weave-scope-app   NodePort   10.96.244.177   &lt;none&gt;        80:30156/TCP   52s# 查看weave 的pod[root@master ~]# kubectl get pod -n weave -o wideNAME                                         READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATESweave-scope-agent-49csj                      1/1     Running   0          7m58s   10.234.2.204   master   &lt;none&gt;           &lt;none&gt;weave-scope-agent-lxjkb                      1/1     Running   0          7m58s   10.234.2.132   node1    &lt;none&gt;           &lt;none&gt;weave-scope-agent-wp22d                      1/1     Running   0          7m58s   10.234.2.160   node2    &lt;none&gt;           &lt;none&gt;weave-scope-app-85966885c8-9gf86             1/1     Running   0          7m59s   10.244.2.246   node2    &lt;none&gt;           &lt;none&gt;weave-scope-cluster-agent-5c9765fff6-hsl9r   1/1     Running   0          7m58s   10.244.2.247   node2    &lt;none&gt;           &lt;none&gt;复制代码\n\n3.2 helm 部署目前helm 仓库提供了weave-scope编写好的charts，可以fetch到本地，进行简单的修改即可安装，非常方便快捷\n[root@master common-service]# helm fetch stable/weave-scope[root@master common-service]# tar -zxvf weave-scope-1.1.8.tgz[root@master common-service]# sed -i &quot;s@\\ type:\\ \\&quot;ClusterIP\\&quot;@ type: \\&quot;NodePort\\&quot;@&quot; weave-scope/values.yaml [root@master common-service]# helm install -n weave-scope --namespace common-service -f weave-scope/values.yaml weave-scope/NAME:   weave-scopeLAST DEPLOYED: Thu Jan 30 18:37:14 2020NAMESPACE: common-serviceSTATUS: DEPLOYEDRESOURCES:==&gt; v1/ConfigMapNAME                           DATA  AGEweave-scope-weave-scope-tests  1     1s==&gt; v1/DaemonSetNAME                           DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE SELECTOR  AGEweave-scope-agent-weave-scope  4        4        0      4           0          &lt;none&gt;         1s==&gt; v1/DeploymentNAME                                   READY  UP-TO-DATE  AVAILABLE  AGEweave-scope-cluster-agent-weave-scope  0/1    1           0          1sweave-scope-frontend-weave-scope       0/1    1           0          1s==&gt; v1/Pod(related)NAME                                                    READY  STATUS             RESTARTS  AGEweave-scope-agent-weave-scope-5xskf                     0/1    ContainerCreating  0         1sweave-scope-agent-weave-scope-76t2s                     0/1    ContainerCreating  0         1sweave-scope-agent-weave-scope-qb74g                     0/1    ContainerCreating  0         1sweave-scope-agent-weave-scope-zggff                     0/1    ContainerCreating  0         1sweave-scope-cluster-agent-weave-scope-5b8bf77f5f-7jkhq  0/1    ContainerCreating  0         1sweave-scope-frontend-weave-scope-6d985ccff6-xmz2d       0/1    ContainerCreating  0         1s==&gt; v1/ServiceNAME                     TYPE      CLUSTER-IP   EXTERNAL-IP  PORT(S)       AGEweave-scope-weave-scope  NodePort  10.233.41.4  &lt;none&gt;       80:31709/TCP  1s==&gt; v1/ServiceAccountNAME                                   SECRETS  AGEweave-scope-cluster-agent-weave-scope  1        1s==&gt; v1beta1/ClusterRoleNAME                                   AGEweave-scope-cluster-agent-weave-scope  1s==&gt; v1beta1/ClusterRoleBindingNAME                     AGEweave-scope-weave-scope  1sNOTES:You should now be able to access the Scope frontend in your web browser, bygoing to the address or hostname of any node in the cluster, using http and the port given by:SCOPE_PORT=$(kubectl -n common-service get svc weave-scope-weave-scope \\-o jsonpath=&#x27;&#123;.spec.ports[?(@.name==http)].nodePort&#125;&#x27;); echo $SCOPE_PORTMost likely one or more of the URLs given by this pipeline will work:SCOPE_PORT=$(kubectl -n common-service get svc  \\-o jsonpath=&#x27;&#123;.spec.ports[?(@.name==http)].nodePort&#125;&#x27;); \\kubectl get nodes -o jsonpath=&#x27;&#123;.items[0].status.addresses[*].address&#125;&#x27; | \\xargs -I&#123;&#125; -d&quot; &quot; echo http://&#123;&#125;:$SCOPE_PORTFor more details on using Weave Scope, see the Weave Scope documentation:https://www.weave.works/docs/scope/latest/introducing/","categories":["Kubernetes"]},{"title":"Kubernetes 性能问题分析报告","url":"/posts/34115/","content":"生产环境的 Kubernetes 上运行的服务，相对比在同一台服务器上直接以JVM进程运行，性能下降了约30%。本文记录了查找性能下降原因的过程，以及结论。\n1、实验环境1.1 压测客户端服务器：192.168.203.135\n软件：wrk-4.2.0\n1.2 实验服务器\n\n\n服务器信息\n数值\n\n\n\n服务器\n192.168.203.123\n\n\n系统版本\nLinux version 3.10.0-1127.el7.x86 64\n\n\nCPU\n56核\n\n\n内存\n512 G\n\n\nKubernetes版本\nv1.24.4\n\n\n1.3 其它工具\n\n\n命令\n描述\n\n\n\nnumactl\nNUMA（Non Uniform Memory Access) 架构测试工具\n\n\n2、实验步骤2.1 实验数据观测在实验服务器上，分别以本地 JVM进程、Kubernetes 方式多次压测，观察实验结果以及资源占用情况。其中内存占用以 numctl 工具观测。\n实验一 JVM * 2JVM 方式压测结果\n[root@jd test]# wrk -c 120 -d 15 -t 40 --latency -s json2.lua http://192.168.203.134Running 15s test @ http://192.168.203.134 40 threads and 120 connections Thread Stats Avg Stdev Max +/- Stdev Latency 3.15ms 1.87ms 42.81ms 90.18% Req/Sec 1.00k 101.07 1.53k 71.86% Latency Distribution 50% 2.57ms 75% 3.73ms 90% 5.00ms 99% 9.72ms 597409 requests in 15.10s, 136.17MB readRequests/sec: 39564.21Transfer/sec: 9.02MB\n\nJVM 方式内存分配情况\n[root@jd ~]# numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 28 29 30 31 32 33 34 35 36 37 3839 40 41node 0 size: 257438 MBnode 0 free: 172578 MBnode 1 cpus: 14 15 16 17 18 19 20 21 22 23 24 25 26 27 42 43 44 45 46 47 4849 50 51 52 53 54 55node 1 size: 258026 MBnode 1 free: 171627 MBnode distances:node 0 1 0: 10 21 1: 21 10\n\n实验二 Kubernetes Pod * 2Kubernetes方式压测结果\n[root@jd test]# wrk -c 120 -d 15 -t 40 --latency -s json2.luahttp://192.168.203.134Running 15s test @ http://192.168.203.134 40 threads and 120 connections Thread Stats Avg Stdev Max +/- Stdev Latency 3.86ms 1.82ms 46.31ms 83.74% Req/Sec 795.75 82.55 1.52k 72.61% Latency Distribution 50% 3.41ms 75% 4.62ms 90% 5.94ms 99% 10.09ms 476081 requests in 15.10s, 108.51MB readRequests/sec: 31528.93Transfer/sec: 7.19MB\n\nKubernetes 方式内存占用情况\n[root@jd ~]# numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 28 29 30 31 32 33 34 35 36 37 3839 40 41node 0 size: 257438 MBnode 0 free: 216685 MBnode 1 cpus: 14 15 16 17 18 19 20 21 22 23 24 25 26 27 42 43 44 45 46 47 4849 50 51 52 53 54 55node 1 size: 258026 MBnode 1 free: 141479 MBnode distances:node 0 1 0: 10 21 1: 21 10\n\n2.2 实验数据分析\n背景知识：\n测试服务器包含2组NUMA节点，每组NUMA节点自身的CPU和内存交互性能最优。跨NUMA节点的CPU和内存交互性能会下降。\n\n实验结果1在同等条件下，Kubernetes 相比 JVM 性能下降 20+%，说明 Kubernetes 与 JVM 性能差异确实存在。\n实验结果2以 JVM 方式启动时，内存在 2 个 NUMA Node 上的分配非常均匀；\n以 Kubernetes 方式启动时，内存在 Numa Node 上的分配不均\n2.3 问题原因猜想猜想：\nKubernetes 默认在调度 Pod 的资源时，没有根据 NUMA Node 对 CPU 和内存进行合理的分配导致 CPU 和内存不能在同一组 NUMA Node 内对齐，从而导致的性能损失。\n2.4 实验验证第一步：验证 NUMA Node 的内存对齐，是不是影响性能的直接原因。\n实验设计：因 Kubernetes 实为调度容器运行，所以直接在容器上测试即可。\n实验一：以默认方式启动 Docker 并压测以默认docker方式运行两个容器\ndocker run -d --network host --name has-adserving-api-1 ...docker run -d --network host --name has-adserving-api-2 ...\n\n[root@jd test]# wrk -c 120 -d 15 -t 40 --latency -s json2.luahttp://192.168.203.134Running 15s test @ http://192.168.203.134 40 threads and 120 connections Thread Stats Avg Stdev Max +/- Stdev Latency 3.86ms 1.82ms 46.31ms 83.74% Req/Sec 795.75 82.55 1.52k 72.61% Latency Distribution 50% 3.41ms 75% 4.62ms 90% 5.94ms 99% 10.09ms 476081 requests in 15.10s, 108.51MB readRequests/sec: 31528.93Transfer/sec: 7.19MB\n\n实验二：以指定 CPU 和内存的方式启动 Docker 并压测（即人工 NUMA Node对齐）# 指定容器运行在 NUMA Node 0 上docker run -d --cpuset-mems 0 --cpuset-cpus 0-13,28-41 --network host --name has-adserving-api-1 ...# 指定容器1运行在 NUMA Node 1 上docker run -d --cpuset-mems 0 --cpuset-cpus 14-27,42-55 --network host --name has-adserving-api-2 ...\n\n[root@jd test]# wrk -c 120 -d 15 -t 40 --latency -s json2.luahttp://192.168.203.134Running 15s test @ http://192.168.203.134 40 threads and 120 connections Thread Stats Avg Stdev Max +/- Stdev Latency 3.12ms 1.52ms 30.10ms 86.09% Req/Sec 0.99k 93.87 1.49k 70.20% Latency Distribution 50% 2.60ms 75% 3.77ms 90% 5.02ms 99% 8.63ms 591922 requests in 15.10s, 134.92MB readRequests/sec: 39200.59Transfer/sec: 8.93MB\n\n2.5 结论NUMA Node 未对齐是性能损失的根本原因。\n3、实验结果与解决方案3.1 性能损失原因Kubernetes 的 CPU 和内存分配策略导致 NUMA 未对齐。\n3.2 解决方案寻找 Kubernetes 的最佳配置方法，使得内存对齐，消除性能影响因素。\n背景知识：\nKubernetes 使用NUMA感知的内存管理器：https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/memory-manager\nKubernetes CPU管理策略：https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/cpu-management-policies主要过程：在 Kubernetes 上，根据以上文档对 kubelet 的参数进行多次修改验证。最终得出最佳配置，使得 NUMA对齐，完全解决性能损失问题。\n通过升级Kubernetes &amp; 调整kubelet参数，可以完全解决性能问题，具体如下:\n3.2.1 Kubernetes版本实验验证版本：v1.24.4+\n3.2.2 设置方式调整每个Worker上的 kubelet 进程启动参数，最佳kubelet参数如下\n# 静态CPU策略--cpu-manager-policy=static# 静态Memory策略，注意:static首字母与CPU策略--memory-manager-policy=Static--kube-reserved &#x27;cpu=400m,memory=2Gi&#x27;--system-reserved &#x27;cpu=400m,memory=1948Mi&#x27;--reserved-memory &#x27;0:memory=2Gi;1:memory=2Gi&#x27;\n\n4、内存与CPU设置指引4.1 内存设置指引内置配置必须符合公式，否则无法启动\n# 内存设置公式kube-reserved + system-reserved + eviction-hard(default 100Mi) = reserved\u0002memory(0) + reserved-memory(1)# 示例2Gi + 1948Mi + 100Mi = 2Gi + 2Gi\n\nQ：Pod怎么设置内存请求和限制？\n假设服务器 Numa Node 数量为 2，每个 Node 内存为 64G\n 则每个 Node 剩余内存为：64Gi - 2Gi = 62Gi\n单个 Node 所有 Pod 的最大内存分配 request = limit = 62Gi\n4.2 CPU设置指引无特别限制，但 CPU 需要额外预留出1核，即可用 CPU = 总CPU核数 - 1\nQ：Pod 怎么设置 CPU 请求和限制？\n假设服务器 Numa Node 数量为2，每个 Node CPU 为 28\n则每个 Node 剩余 CPU 为：(56-0.4-0.4-1) / 2 = 27\n单个 Node 所有 Pod 的最大 CPU 分配：request = limit = 27\n","categories":["Kubernetes"]},{"title":"Mac 安装 Minikube","url":"/posts/22705/","content":"官方文档\n一、安装 kubectlbrew install kubectl\n\n二、安装 Minikube1.安装 minikubebrew install minikube\n\n2.查看 minikube 版本jz@Bean-You ~ % minikube versionminikube version: v1.24.0commit: 76b94fb3c4e8ac5062daf70d60cf03ddcc0a741b\n\n三、启动集群minikube start --image-mirror-country=&#x27;cn&#x27; --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers\n\n启动后将会看到会自动选择 docker 驱动\n如果启动失败，可以参考 官方的 Driver 选择文档 ，通过 --vm-driver= 参数指定驱动。\n四、minikube 常用命令1.删除集群jz@Bean-You bin % minikube delete🔥  正在删除 docker 中的“minikube”…🔥  正在删除容器 &quot;minikube&quot; ...🔥  正在移除 /Users/jd/.minikube/machines/minikube…💀  Removed all traces of the &quot;minikube&quot; cluster.\n\n2.集群状态jz@Bean-You bin % minikube statusminikubetype: Control Planehost: Runningkubelet: Runningapiserver: Runningkubeconfig: Configured\n\n3.停止集群minikube stop\n","categories":["Kubernetes"]},{"title":"Pod 与 Service 的 DNS","url":"/posts/48384/","content":"Kubernetes 为 Service 和 Pod 创建 DNS 记录。 你可以使用一致的 DNS 名称而非 IP 地址访问 Service。\n原文连接\n一、介绍Kubernetes DNS 除了在集群上调度 DNS Pod 和 Service， 还配置 kubelet 以告知各个容器使用 DNS Service 的 IP 来解析 DNS 名称。\n集群中定义的每个 Service （包括 DNS 服务器自身）都被赋予一个 DNS 名称。 默认情况下，客户端 Pod 的 DNS 搜索列表会包含 Pod 自身的名字空间和集群的默认域。\nService 的名字空间DNS 查询可能因为执行查询的 Pod 所在的名字空间而返回不同的结果。 不指定名字空间的 DNS 查询会被限制在 Pod 所在的名字空间内。 要访问其他名字空间中的 Service，需要在 DNS 查询中指定名字空间。\n例如，假定名字空间 test 中存在一个 Pod，prod 名字空间中存在一个服务 data。\nPod 查询 data 时没有返回结果，因为使用的是 Pod 的名字空间 test。\nPod 查询 data.prod 时则会返回预期的结果，因为查询中指定了名字空间。\nDNS 查询可以使用 Pod 中的 /etc/resolv.conf 展开。kubelet 会为每个 Pod 生成此文件。例如，对 data 的查询可能被展开为 data.test.svc.cluster.local。 search 选项的取值会被用来展开查询。要进一步了解 DNS 查询，可参阅 resolv.conf 手册页面。\nnameserver 10.32.0.10search &lt;namespace&gt;.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5\n\n概括起来，名字空间 test 中的 Pod 可以成功地解析 data.prod 或者 data.prod.svc.cluster.local。\nDNS 记录哪些对象会获得 DNS 记录呢？\n\nServices\nPods\n\n以下各节详细介绍已支持的 DNS 记录类型和布局。 其它布局、名称或者查询即使碰巧可以工作，也应视为实现细节， 将来很可能被更改而且不会因此发出警告。 有关最新规范请查看 Kubernetes 基于 DNS 的服务发现。\nServicesA/AAAA 记录“普通” Service（除了无头 Service）会以 my-svc.my-namespace.svc.cluster-domain.example 这种名字的形式被分配一个 DNS A 或 AAAA 记录，取决于 Service 的 IP 协议族。 该名称会解析成对应 Service 的集群 IP。\n“无头（Headless）” Service （没有集群 IP）也会以 my-svc.my-namespace.svc.cluster-domain.example 这种名字的形式被指派一个 DNS A 或 AAAA 记录， 具体取决于 Service 的 IP 协议族。 与普通 Service 不同，这一记录会被解析成对应 Service 所选择的 Pod IP 的集合。 客户端要能够使用这组 IP，或者使用标准的轮转策略从这组 IP 中进行选择。\nSRV 记录Kubernetes 根据普通 Service 或 Headless Service 中的命名端口创建 SRV 记录。每个命名端口， SRV 记录格式为 _my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example。 普通 Service，该记录会被解析成端口号和域名：my-svc.my-namespace.svc.cluster-domain.example。 无头 Service，该记录会被解析成多个结果，及该服务的每个后端 Pod 各一个 SRV 记录， 其中包含 Pod 端口号和格式为 auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example 的域名。\n二、PodsA/AAAA 记录一般而言，Pod 会对应如下 DNS 名字解析：\npod-ip-address.my-namespace.pod.cluster-domain.example\n\n例如，对于一个位于 default 名字空间，IP 地址为 172.17.0.3 的 Pod， 如果集群的域名为 cluster.local，则 Pod 会对应 DNS 名称：\n172-17-0-3.default.pod.cluster.local.\n通过 Service 暴露出来的所有 Pod 都会有如下 DNS 解析名称可用：\npod-ip-address.service-name.my-namespace.svc.cluster-domain.example.\nPod 的 hostname 和 subdomain 字段当前，创建 Pod 时其主机名取自 Pod 的 metadata.name 值。\nPod 规约中包含一个可选的 hostname 字段，可以用来指定 Pod 的主机名。 当这个字段被设置时，它将优先于 Pod 的名字成为该 Pod 的主机名。 举个例子，给定一个 hostname 设置为 “my-host“ 的 Pod， 该 Pod 的主机名将被设置为 “my-host“。\nPod 规约还有一个可选的 subdomain 字段，可以用来指定 Pod 的子域名。 举个例子，某 Pod 的 hostname 设置为 “foo”，subdomain 设置为 “bar”， 在名字空间 “my-namespace” 中对应的完全限定域名（FQDN）为 “foo.bar.my-namespace.svc.cluster-domain.example”。\n示例：\napiVersion: v1kind: Servicemetadata:  name: default-subdomainspec:  selector:    name: busybox  clusterIP: None  ports:  - name: foo # 实际上不需要指定端口号    port: 1234    targetPort: 1234---apiVersion: v1kind: Podmetadata:  name: busybox1  labels:    name: busyboxspec:  hostname: busybox-1  subdomain: default-subdomain  containers:  - image: busybox:1.28    command:      - sleep      - &quot;3600&quot;    name: busybox---apiVersion: v1kind: Podmetadata:  name: busybox2  labels:    name: busyboxspec:  hostname: busybox-2  subdomain: default-subdomain  containers:  - image: busybox:1.28    command:      - sleep      - &quot;3600&quot;    name: busybox\n\n如果某无头 Service 与某 Pod 在同一个名字空间中，且它们具有相同的子域名， 集群的 DNS 服务器也会为该 Pod 的全限定主机名返回 A 记录或 AAAA 记录。 例如，在同一个名字空间中，给定一个主机名为 “busybox-1”、 子域名设置为 “default-subdomain” 的 Pod，和一个名称为 “default-subdomain” 的无头 Service，Pod 将看到自己的 FQDN 为 “busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example“。 DNS 会为此名字提供一个 A 记录或 AAAA 记录，指向该 Pod 的 IP。 “busybox1” 和 “busybox2” 这两个 Pod 分别具有它们自己的 A 或 AAAA 记录。\nEndpoints 对象可以为任何端点地址及其 IP 指定 hostname。\n说明： 由于不是为 Pod 名称创建 A 或 AAAA 记录的，因此 Pod 的 A 或 AAAA 需要 hostname。 没有设置 hostname 但设置了 subdomain 的 Pod 只会为 无头 Service 创建 A 或 AAAA 记录（default-subdomain.my-namespace.svc.cluster-domain.example） 指向 Pod 的 IP 地址。 另外，除非在服务上设置了 publishNotReadyAddresses=True，否则只有 Pod 进入就绪状态 才会有与之对应的记录。\nPod 的 setHostnameAsFQDN 字段特性状态： Kubernetes v1.22 [stable]\n当 Pod 配置为具有全限定域名 (FQDN) 时，其主机名是短主机名。 例如，如果你有一个具有完全限定域名 busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example 的 Pod， 则默认情况下，该 Pod 内的 hostname 命令返回 busybox-1，而 hostname --fqdn 命令返回 FQDN。\n当你在 Pod 规约中设置了 setHostnameAsFQDN: true 时，kubelet 会将 Pod 的全限定域名（FQDN）作为该 Pod 的主机名记录到 Pod 所在名字空间。 在这种情况下，hostname 和 hostname --fqdn 都会返回 Pod 的全限定域名。\n说明：\n在 Linux 中，内核的主机名字段（struct utsname 的 nodename 字段）限定 最多 64 个字符。\n如果 Pod 启用这一特性，而其 FQDN 超出 64 字符，Pod 的启动会失败。 Pod 会一直出于 Pending 状态（通过 kubectl 所看到的 ContainerCreating）， 并产生错误事件，例如 “Failed to construct FQDN from Pod hostname and cluster domain, FQDN long-FQDN is too long (64 characters is the max, 70 characters requested).” （无法基于 Pod 主机名和集群域名构造 FQDN，FQDN long-FQDN 过长，至多 64 字符，请求字符数为 70）。 对于这种场景而言，改善用户体验的一种方式是创建一个 准入 Webhook 控制器， 在用户创建顶层对象（如 Deployment）的时候控制 FQDN 的长度。\nPod 的 DNS 策略DNS 策略可以逐个 Pod 来设定。目前 Kubernetes 支持以下特定 Pod 的 DNS 策略。 这些策略可以在 Pod 规约中的 dnsPolicy 字段设置：\n\nDefault： Pod 从运行所在的节点继承名称解析配置。参考 相关讨论 获取更多信息。\nClusterFirst：与配置的集群域后缀不匹配的任何 DNS 查询（例如 “www.kubernetes.io&quot;） 都将转发到从节点继承的上游名称服务器。集群管理员可能配置了额外的存根域和上游 DNS 服务器。 参阅相关讨论 了解在这些场景中如何处理 DNS 查询的信息。\nClusterFirstWithHostNet：对于以 hostNetwork 方式运行的 Pod，应显式设置其 DNS 策略 ClusterFirstWithHostNet。\n注意：这在 Windows 上不支持。 有关详细信息，请参见下文。\n\n\nNone：此设置允许 Pod 忽略 Kubernetes 环境中的 DNS 设置。Pod 会使用其 dnsConfig 字段 所提供的 DNS 设置。 参见 Pod 的 DNS 配置节。\n\n说明： Default 不是默认的 DNS 策略。如果未明确指定 dnsPolicy，则使用 ClusterFirst。\n下面的示例显示了一个 Pod，其 DNS 策略设置为 ClusterFirstWithHostNet， 因为它已将 hostNetwork 设置为 true。\napiVersion: v1kind: Podmetadata:  name: busybox  namespace: defaultspec:  containers:  - image: busybox:1.28    command:      - sleep      - &quot;3600&quot;    imagePullPolicy: IfNotPresent    name: busybox  restartPolicy: Always  hostNetwork: true  dnsPolicy: ClusterFirstWithHostNet\n\nPod 的 DNS 配置特性状态： Kubernetes v1.14 [stable]\nPod 的 DNS 配置可让用户对 Pod 的 DNS 设置进行更多控制。\ndnsConfig 字段是可选的，它可以与任何 dnsPolicy 设置一起使用。 但是，当 Pod 的 dnsPolicy 设置为 “None“ 时，必须指定 dnsConfig 字段。\n用户可以在 dnsConfig 字段中指定以下属性：\n\nnameservers：将用作于 Pod 的 DNS 服务器的 IP 地址列表。 最多可以指定 3 个 IP 地址。当 Pod 的 dnsPolicy 设置为 “None“ 时， 列表必须至少包含一个 IP 地址，否则此属性是可选的。 所列出的服务器将合并到从指定的 DNS 策略生成的基本名称服务器，并删除重复的地址。\nsearches：用于在 Pod 中查找主机名的 DNS 搜索域的列表。此属性是可选的。 指定此属性时，所提供的列表将合并到根据所选 DNS 策略生成的基本搜索域名中。 重复的域名将被删除。Kubernetes 最多允许 6 个搜索域。\noptions：可选的对象列表，其中每个对象可能具有 name 属性（必需）和 value 属性（可选）。 此属性中的内容将合并到从指定的 DNS 策略生成的选项。 重复的条目将被删除。\n\n以下是具有自定义 DNS 设置的 Pod 示例：\napiVersion: v1kind: Podmetadata:  namespace: default  name: dns-examplespec:  containers:    - name: test      image: nginx  dnsPolicy: &quot;None&quot;  dnsConfig:    nameservers:      - 1.2.3.4    searches:      - ns1.svc.cluster-domain.example      - my.dns.search.suffix    options:      - name: ndots        value: &quot;2&quot;      - name: edns0\n\n创建上面的 Pod 后，容器 test 会在其 /etc/resolv.conf 文件中获取以下内容：\nnameserver 1.2.3.4search ns1.svc.cluster-domain.example my.dns.search.suffixoptions ndots:2 edns0\n\n对于 IPv6 设置，搜索路径和名称服务器应按以下方式设置：\nkubectl exec -it dns-example -- cat /etc/resolv.conf\n\n输出类似于：\nnameserver fd00:79:30::asearch default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.exampleoptions ndots:5\n\n扩展 DNS 配置特性状态： Kubernetes 1.22 [alpha]\n对于 Pod DNS 配置，Kubernetes 默认允许最多 6 个 搜索域（ Search Domain） 以及一个最多 256 个字符的搜索域列表。\n如果启用 kube-apiserver 和 kubelet 的特性门控 ExpandedDNSConfig，Kubernetes 将可以有最多 32 个 搜索域以及一个最多 2048 个字符的搜索域列表。\n三、Windows 节点上的 DNS 解析\n在 Windows 节点上运行的 Pod 不支持 ClusterFirstWithHostNet。 Windows 将所有带有 . 的名称视为全限定域名（FQDN）并跳过全限定域名（FQDN）解析。\n在 Windows 上，可以使用的 DNS 解析器有很多。 由于这些解析器彼此之间会有轻微的行为差别，建议使用 Resolve-DNSName powershell cmdlet 进行名称查询解析。\n在 Linux 上，有一个 DNS 后缀列表，当解析全名失败时可以使用。 在 Windows 上，你只能有一个 DNS 后缀， 即与该 Pod 的命名空间相关联的 DNS 后缀（例如：mydns.svc.cluster.local）。 Windows 可以解析全限定域名（FQDN），和使用了该 DNS 后缀的 Services 或者网络名称。 例如，在 default 命名空间中生成一个 Pod，该 Pod 会获得的 DNS 后缀为 default.svc.cluster.local。 在 Windows 的 Pod 中，你可以解析 kubernetes.default.svc.cluster.local 和 kubernetes， 但是不能解析部分限定名称（kubernetes.default 和 kubernetes.default.svc）。\n\n","categories":["Kubernetes"]},{"title":"k8s 使用 nsenter 抓包","url":"/posts/3066/","content":"一、格式nsenter [options] [&lt;program&gt; [&lt;argument&gt;...]]Run a program with namespaces of other processes.Options: -a, --all              enter all namespaces -t, --target &lt;pid&gt;     target process to get namespaces from -m, --mount[=&lt;file&gt;]   enter mount namespace -u, --uts[=&lt;file&gt;]     enter UTS namespace (hostname etc) -i, --ipc[=&lt;file&gt;]     enter System V IPC namespace -n, --net[=&lt;file&gt;]     enter network namespace -p, --pid[=&lt;file&gt;]     enter pid namespace -C, --cgroup[=&lt;file&gt;]  enter cgroup namespace -U, --user[=&lt;file&gt;]    enter user namespace -T, --time[=&lt;file&gt;]    enter time namespace -S, --setuid &lt;uid&gt;     set uid in entered namespace -G, --setgid &lt;gid&gt;     set gid in entered namespace     --preserve-credentials do not touch uids or gids -r, --root[=&lt;dir&gt;]     set the root directory -w, --wd[=&lt;dir&gt;]       set the working directory -F, --no-fork          do not fork before exec&#x27;ing &lt;program&gt; -Z, --follow-context   set SELinux context according to --target PID\n\n二、抓包1.获取pod节点信息kubectl get pod pod-name -n namespace -o wide\n\n然后进入pod所在node\n2.获取pod容器idContainerd 运行时\n$ kubectl get pod pod-name -n namespace -o yaml | grep containerID  - containerID: containerd://1904197bb55ded3bcb1b75939f595993be9d34d42a732b707d8c083196f03a44\n\nDocker 运行时\n$ kubectl get pod pod-name -n namespace -o yaml | grep containerID  - containerID: docker://1904197bb55ded3bcb1b75939f595993be9d34d42a732b707d8c083196f03a44\n\n3.获取pod PIDContainerd 运行时\n$ crictl inspect 1904197bb55ded3bcb1b75939f595993be9d34d42a732b707d8c083196f03a44 | grep -i pid    &quot;pid&quot;: 2209352,    &quot;pid&quot;: 1    &quot;type&quot;: &quot;pid&quot;\n\nDocker 运行时\n$ docker inspect 1904197bb55ded3bcb1b75939f595993be9d34d42a732b707d8c083196f03a44 | grep -i pid    &quot;pid&quot;: 2209352,    &quot;PidMode&quot;: 1    &quot;PidsLimit&quot;: &quot;0&quot;\n\n4.抓包nsenter -t 2209352 -n tcpdump -i eth0 -n -nn dst port 8083\n\n然后在node节点上就能看到抓包的文件了。\n","categories":["Kubernetes"]},{"title":"k8s 版本回退","url":"/posts/30022/","content":"1. 查看历史记录kubectl -n monitor rollout history deployment/myDep\n\n2. 查看某个历史详情kubectl -n monitor rollout history deployment/myDep --revision=10\n\n3. 回滚到上个版本kubectl -n monitor rollout undo deployment/myDep\n\n4. 回滚到指定版本kubectl -n monitor rollout undo deployment/myDep --to-revision=11\n\n","categories":["Kubernetes"]},{"title":"k8s 命令","url":"/posts/37339/","content":"1、重启 podkubectl get pod &#123;podname&#125; -n &#123;namespace&#125; -o yaml | kubectl replace --force -f -\n\n2、查看代理模式$ kubectl get pods -n kube-system -o wide | grep proxykube-proxy-flnqq                                  1/1     Running     0          10d     10.172.5.3      10.172.5.3    &lt;none&gt;           &lt;none&gt;kube-proxy-wlzxh                                  1/1     Running     0          10d     10.172.5.10     10.172.5.10   &lt;none&gt;           &lt;none&gt;\n\n$ kubectl logs kube-proxy-wlzxh -n kube-systemFri Dec 31 14:05:26 CST 2021: set iptables to nft modeI1231 14:05:26.800004       1 flags.go:33] FLAG: --add-dir-header=&quot;false&quot;I1231 14:05:26.800036       1 flags.go:33] FLAG: --alsologtostderr=&quot;false&quot;I1231 14:05:26.800039       1 flags.go:33] FLAG: --bind-address=&quot;0.0.0.0&quot;I1231 14:05:26.800044       1 flags.go:33] FLAG: --cleanup=&quot;false&quot;I1231 14:05:26.800051       1 flags.go:33] FLAG: --cleanup-ipvs=&quot;true&quot;\n\n3、强制删除 podkubectl delete pod pod-name -n test --force --grace-period=0\n\n4、批量删除 podkubectl get pod -n test -o wide | grep &quot;Terminating&quot; | awk &#x27;&#123;system(&quot;kubectl delete pod &quot;$1&quot; -n test --force --grace-period=0&quot;)&#125;&#x27;\n","categories":["Kubernetes"]},{"title":"A记录和CNAME记录的区别","url":"/posts/1487/","content":"1、什么是域名解析？域名解析就是国际域名或者国内域名以及中文域名等域名申请后做的到IP地址的转换过程。IP地址是网路上标识您站点的数字地址，为了简单好记，采用域名来代替ip地址标识站点地址。域名的解析工作由DNS服务器完成。\n\n2、什么是A记录？A (Address) 记录是用来指定主机名（或域名）对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。\n\n3、什么是CNAME记录？即：别名记录。这种记录允许您将多个名字映射到另外一个域名。通常用于同时提供WWW和MAIL服务的计算机。例如，有一台计算机名为“host.mydomain.com”（A记录）。它同时提供WWW和MAIL服务，为了便于用户访问服务。可以为该计算机设置两个别名（CNAME）：WWW和MAIL。这两个别名的全称就http://www.mydomain.com/和“mail.mydomain.com”。实际上他们都指向“[host.mydomain.com](http://host.mydomain.com/)”。\n\n4、使用A记录和CNAME进行域名解析的区别A记录就是把一个域名解析到一个IP地址（Address，特制数字IP地址），而CNAME记录就是把域名解析到另外一个域名。其功能是差不多，CNAME将几个主机名指向一个别名，其实跟指向IP地址是一样的，因为这个别名也要做一个A记录的。但是使用CNAME记录可以很方便地变更IP地址。如果一台服务器有100个网站，他们都做了别名，该台服务器变更IP时，只需要变更别名的A记录就可以了。\n\n5、使用A记录和CNAME哪个好？域名解析CNAME记录A记录哪一种比较好？如果论对网站的影响，就没有多大区别。但是：CNAME有一个好处就是稳定，就好像一个IP与一个域名的区别。服务商从方便维护的角度，一般也建议用户使用CNAME记录绑定域名的。如果主机使用了双线IP，显然使用CNAME也要方便一些。\nA记录也有一些好处，例如可以在输入域名时不用输入WWW.来访问网站哦！从SEO优化角度来看，一些搜索引擎如alex或一些搜索查询工具网站等等则默认是自动去掉WWW.来辨别网站，CNAME记录是必须有如：WWW(别名)前缀的域名，有时候会遇到这样的麻烦，前缀去掉了默认网站无法访问。\n有人认为，在SEO优化网站的时候，由于搜索引擎找不到去掉WWW.的域名时，对网站权重也会有些影响。因为有些网民客户也是不喜欢多写三个W来访问网站的，网站无法访问有少量网民客户会放弃继续尝试加WWW.访问域名了，因此网站访问浏览量也会减少一些。\n也有人认为同一个域名加WWW.和不加WWW.访问网站也会使网站权重分散，这也是个问题。但是可以使用301跳转把不加WWW.跳转到加WWW.的域名，问题就解决了。\n","categories":["运维","Linux"]},{"title":"CDN 原理","url":"/posts/3416/","content":"CDN 概述CDN 全称 Content Delivery Network，即内容分发网络。其基本思路是尽可能避开互联网上有可能影响数据传输速度和稳定性的瓶颈和环节，使内容传输的更快、更稳定。\nCDN 的工作原理 就是将源站的资源缓存CDN各个节点上，当请求命中了某个节点的资源缓存时，立即返回客户端，避免每个请求的资源都通过源站获取，避免网络拥塞、缓解源站压力，保证用户访问资源的速度和体验。\n举一个生活中的例子，我们在某东上购买商品，快递能做到当日送达，其根本原理是通过在全国各地建设本地仓库。当用户购买商品时，通过智能仓配模式，为消费者选择就近仓库发货，从而缩短物流配送时间。\n\n而商品库存的分配，流程可以参考下图，从 工厂(源站) -&gt; 地域仓库(二级缓存) -&gt; 本地仓库 (一级缓存)\n\n内容分发网络就像前面提到的智能仓配网络 一样，解决了因分布、带宽、服务器性能带来的访问延迟问题，适用于站点加速、点播、直播等场景。使用户可就近取得所需内容，解决 Internet网络拥挤的状况，提高用户访问网站的响应速度和成功率。\n\nCDN的诞生\nCDN 诞生于二十多年前，为解决内容源服务器和传输骨干网络压力过大的问题，在 1995 年，麻省理工学院教授，互联网发明者之一 Tom Leighton 带领着研究生 Danny Lewin 和其他几位顶级研究人员一起尝试用数学问题解决网络拥堵问题。\n他们使用数学算法，处理内容的动态路由安排，并最终解决了困扰 Internet 使用者的难题。后来，史隆管理学院的 MBA 学生 Jonathan Seelig 加入了 Leighton 的队伍中，从那以后他们开始实施自己的商业计划，最终于 1998 年 8 月 20 日正式成立公司，命名为 Akamai。Akamai 公司通过智能化的互联网分发，结束了 “World Wide Wait” 的尴尬局面。\n同年 1998 年，中国第一家 CDN 公司 ChinaCache 成立\nCDN工作原理接入CDN在接入CDN前，当我们访问某个域名,直接拿到第一个真实服务器的IP地址，整个流程如下(图有点简陋)：\n\n当我们需要加速网站时，通过向运营商注册自己加速域名，源站域名，然后进入到自己域名的DNS配置信息，将 A 记录修改成 CNAME 记录即可。阿里云加速申请参考如下：\n\nCDN访问过程\n1、用户访问图片内容，先经过 本地DNS 解析，如果 LDNS 命中，直接返回给用户。\n2、LDNS MISS，转发 授权DNS 查询\n3、返回域名 CNAME picwebws.pstatp.com.wsglb0.com. 对应IP地址(实际就是DNS调度系统的ip地址)\n4、域名解析请求发送至DNS调度系统，DNS调度系统为请求分配最佳节点IP地址。\n5、返回的解析IP地址\n6、用户向缓存服务器发起请求，缓存服务器响应用户请求，将用户所需内容传送到用户终端。\n图：华为云全站加速示意图\n\nCDN解决了什么问题骨干网压力过大Tom Leighton在 1995 年， 带领团队尝试用数学问题解决网络拥堵问题，从而解决骨干网络压力过大的问题。由于上网冲浪 的少年越来越多，造成骨干网的核心节点流量吞吐不足以支撑互联网用户的增长，通过CDN可以避免用户流量流经骨干网。\n骨干网是一个全球性的局域网,一级互联网服务提供商(ISP)将其高速光纤网络连接在一起，形成互联网的骨干网，实现在不同地理区域之间高效地传输流量。\n1、局域网\n局域网(Local Area Network，LAN)是指在某一区域内由多台计算机互联成的计算机组，比如：在大学时期，晚上12点后断网了，我们仍然能够通过路由器开黑打CS，魔兽。那就是基于局域网互联，实现资料共享与信息之间的通信。\n\n2、骨干网\n这里引用一下中国电信全网架构，骨干网可以理解成是一个全国性的局域网，通过核心节点的流量互通，实现全网网络的互通。这也是为什么我们称为互联网 的原因。\n北京、上海、广州，是ChinaNet的超级核心。除了超级核心之外，ChinaNet还有天津、西安、南京、杭州、武汉、成都等普通核心。\n\n三公里之 middlemile通常网络访问中会有”三公里”路程\n\n第一公里为:源站到ISP接入点\n第二公里为:源站ISP接入点到访问用户的ISP接入点\n第三公里(最后一公里)为:用户ISP接入点到用户客户端\n\nCDN网络层主要用来加速第二公里(middlemile),\n在 CDN 的基础架构中，通常使用两级 server 做加速：\n\nL1(下层)：距离用户(或俗称网民)越近越好，通常用于缓存那些可缓存的静态数据，称之为 lastmile(最后一公里)。\nL2(上层)：距离源站越近越好，称之为 firstmile(第一公里)，当 L1 无法命中缓存，或内容不可缓存时，请求会通过 L1 透传给 L2，若 L2 仍然没有命中缓存或内容不可缓存，则会继续透传给 L2 的 upstream(有可能是源站，也有可能是 L3)，同时 L2 还可以做流量、请求数的量级收敛，减少回源量(如果可缓存)，降低源站压力。\nL1 和 L2 之间的部分，是 CDN 的 ”内部网络“，称之为 middlemile(中间一公里)。\n\n\nCDN的组成全局负载均衡系统 GLB(Global Load Balance)\n\n\n当用户访问加入CDN服务的网站时，域名解析请求将最终由 “智能调度DNS”负责处理。\n它通过一组预先定义好的策略，将当时最接近用户的节点地址提供给用户，使用户可以得到快速的服务。\n同时它需要与分布在各地的CDN节点保持通信，跟踪各节点的健康状态、容量等信息，确保将用户的请求分配到就近可用的节点上.\n\n缓存服务器缓存服务器主要的功能就是缓存热点数据，数据类型包括：静态资源(html,js,css等)，多媒体资源(img,mp3,mp4等)，以及动态数据(边缘渲染)等。\n众所周知耳熟能详的与 CDN 有关的开源软件有：\n\nSquid\nVarnish\nNginx\nOpenResty\nATS\nHAProxy\n\n具体对比可参考：https://blog.csdn.net/joeyon1985/article/details/46573281\nCDN的分层架构\n\n源站源站指发布内容的原始站点。添加、删除和更改网站的文件，都是在源站上进行的;另外缓存服务器所抓取的对象也全部来自于源站。\nCDN 调度策略DNS 调度基于请求端 local DNS 的出口 IP 归属地以及运营商的 DNS 调度。\nDNS 调度的问题：\n\nDNS 缓存时间在 TTL 过期前是不会刷新的， 这样会导致节点异常的时候自动调度延时很大，会直接影响线上业务访问。\n大量的 local DNS 不支持 EDNS 协议，拿不到客户的真实IP，CDN 绝大多数时候只能通过local DNS IP来做决策，经常会出现跨区域调度的情况。\n\nHTTP DNS 调度客户端请求固定的 HTTP DNS 地址，根据返回获取解析结果。可以提高解析的准确性(不像DNS调度，只能通过local DNS IP来做决策)，能很好的避免劫持等问题。\n当然这种模式也有一些问题，例如客户端每次加载URL都可能产生一次HTTP DNS查询，这就对性能和网络接入要求很高。\n302调度基于客户端 IP 和 302 调度集群进行实时的流量调度。\n我们来看一个例子：\n\n访问 URL 链接后，此时请求到了调度群集上，我们能拿到的客户端信息有 客户端的出口IP(绝大多情况下是相同的)，接下来算法和基于 DNS 的调度可以是一样的，只是判断依据由 local DNS 出口 ip 变成了客户端的出口IP。\n浏览器收到302回应，跟随 Location 中的 URL，继续发起 http 请求，这次请求的目标 IP 是CDN 边缘节点，CDN节点会响应实际的文件内容。\n\n302 调度的优势：\n\n实时调度，因为没有 local DNS 缓存的，适合 CDN 的削峰处理，对于成本控制意义重大;\n准确性高，直接获取客户端出口 IP 进行调度。\n\n302 调度的劣势：\n\n每次都要跳转，对于延时敏感的业务不友好。一般只适用于大文件。\n\nAnyCast BGP路由调度基于 BGP AnyCast 路由策略，只提供极少的对外 IP，路由策略可以很快的调整。\n目前 AWS CloudFront、CloudFlare 都使用了这种方式，在路由层面进行调度。\n这种方式可以很好地抵御 DDOS 攻击，降低网络拥塞。\n当然这种方式的成本和方案设计都比较复杂，所以国内的 CDN 目前还都是用 UniCast 的方式。\n一些概念CDN运作原理本地缓存的数据，通过key-value 的形式，将url 和本地缓存进行映射,存储结构与 Map相似，采用 hash+链表形式进行缓存。\n\nCDN命中率衡量我们CDN服务质量的一个核心标准，当用户访问的资源恰好在缓存系统里，可以直接返回给用户，说明CDN命中;如果CDN缓存中，没有命中资源，那么会触发回源动作。\nCDN回源当CDN本地缓存没有命中时，触发回源动作,\n\n一级缓存 访问二级缓存是否有相关数据，如果有，返回一级缓存。\n二级缓存 Miss，触发 二级缓存 回源请求，请求源站对应数据。获取结果后，缓存到本地缓存，返回数据到一级缓存。\n一级缓存 获取数据，缓存本地后，返回给用户。\n\nCDN预热数据上面说的访问模式，都是基于Pull模式，由用户决策哪部分热点数据会最终存留在CDN缓存中;对于大促场景，我们往往需要预先将活动相关资源预热 到 边缘节点(L1),避免大促开启后，大量用户访问，造成源站压力过大。这时候采用的是 Push模式。\nCDN的特点总结1、资源访问加速: 本地Cache加速，提高了企业站点(尤其含有大量图片和静态页面站点)的访问速度，并大大提高以上性质站点的稳定性\n2、消除运营商间网络互联的瓶颈问题: 镜像服务消除了不同运营商之间互联的瓶颈造成的影响，实现了跨运营商的网络加速，保证不同网络中的用户都能得到良好的访问质量。\n3、远程加速: 远程访问用户根据DNS负载均衡技术 智能自动选择Cache服务器，选择最快的Cache服务器，加快远程访问的速度\n4、带宽优化: 自动生成服务器的远程Mirror(镜像)cache服务器，远程用户访问时从cache服务器上读取数据，减少远程访问的带宽、分担网络流量、减轻原站点WEB服务器负载等功能。\n5、集群抗攻击: 广泛分布的CDN节点加上节点之间的智能冗余机制，可以有效地预防黑客入侵以及降低各种D.D.o.S攻击对网站的影响，同时保证较好的服务质量 。\n","categories":["运维","Linux"]},{"title":"Linux TIME_WAIT 相关系统参数","url":"/posts/57336/","content":"Linux TIME_WAIT 主要有三个相关参数:\n\nnet.ipv4.tcp_tw_reuse = 1\n表示开启重用。允许将 TIME-WAIT sockets 重新用于新的TCP连接，默认为0，表示关闭\n\n\nnet.ipv4.tcp_tw_recycle = 1\n表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭\n\n\nnet.ipv4.tcp_fin_timeout = 60\n表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间\n\n\n\n国外的一篇英文文档详细介绍了TCP TIME-WAIT 相关内容，题为《Coping with the TCP TIME-WAIT state on busy Linux servers》 有兴趣的同学可以移步参考。\nnet.ipv4.tcp_tw_reuseTIME-WAIT状态是为了防止不相关的延迟请求包被接收。但在某些特定条件下，很有可能出现，新建立的TCP连接请求包，被旧的连接（同样的四元组，暂时还是TIME-WAIT状态，回收中）误处理。RFC 1323 实现了TCP扩展，以提高高带宽链路的性能。除此之外，它还定义了一个带有两个四字节时间戳字段的新TCP选项，第一个字节是TCP发送方的当前时钟时间戳，而第二个字节是从远程主机接收到的最新时间戳。启用net.ipv4.tcp_tw_reuse后，如果新的时间戳，比以前存储的时间戳更大，那么linux将会从TIME-WAIT状态的存活连接中，选取一个，重新分配给新的连接出去的TCP连接。此时处在TIME-WAIT状态连接，仅仅1秒后就可以被重用了。\n作为客户端因为有端口65535限制的问题，TIME-WAIT状态的连接过多直接影响处理能力，打开tw_reuse 即可解决该问题。同时不建议打开tw_recycle，没有任何作用。net.ipv4.tcp_tw_reuse参数能够帮助客户端1s完成TCP连接的快速回收，此时单机就可实现6w/s的极限请求，如果还需要再增加连接数此时就需要增加IP数量来提高连接数。\nnet.ipv4.tcp_tw_recycleTCP快速回收是一种链接资源快速回收和重用的机制，当TCP链接进入到TIME_WAIT状态时，通常需要等待2MSL的时长，但是一旦启用TCP快速回收，则只需等待一个重传时间（RTO）后就能够快速的释放这个链接，以被重新使用。\n在一些高并发的网站服务器上，为了端口能够快速回收，打开了net.ipv4.tcp_tw_recycle。\nnet.ipv4.tcp_tw_recycle禁用时，kernal 是不会检查对端机器的包的时间戳。\n但启用net.ipv4.tcp_tw_recycle后，kernel就会检查时间戳，如果发来的包的时间戳是乱跳的，就会出现把带了“倒退”的时间戳的包当作是recycle的tw连接的重传数据，不是新的请求，于是丢掉不回包，造成大量丢包。\n当多个客户端处于同一个NAT环境时，同时访问服务器，不同客户端的时间可能不一致，此时服务端接收到同一个NAT发送的请求，就会出现时间戳错乱的现象，于是后面的数据包就被丢弃了，具体的表现通常是是客户端明明发送的SYN，但服务端就是不响应ACK。在服务器借助下面的命令可以来确认数据包是否有不断被丢弃的现象。\nnetstat -s | grep rejects\n\n下面我们来具体说下，首先解释下 TCP 的 TIME_WAIT 状态：\n\n通信双方建立TCP连接后，主动关闭连接的一方就会进入TIME_WAIT状态。比如，客户端主动关闭连接时，会发送最后一个ack后，然后会进入TIME_WAIT状态，再停留2个MSL时间，进入CLOSED状态。TIME_WAIT状态的作用是为了保证连接正常关闭，且不影响其他新建的链接。\n\n\nTCP断开过程\n\n开启net.ipv4.tcp_tw_recycle的目的，就是希望能够加快TIME_WAIT状态的回收，当然这个选项的生效也依赖于net.ipv4.tcp_timestamps的开启（缺省就是开启的）。当开启了net.ipv4.tcp_tw_recycle选项后，当连接进入TIME_WAIT状态后，会记录对应远端主机最后到达分节的时间戳。如果同样的主机有新的分节到达，且时间戳小于之前记录的时间戳，即视为无效，相应的数据包会被丢弃。\n当客户终端经过NAT代理时，客户端TCP请求到达NAT网关，修改目的地址（IP+端口号）后便转发给后端服务器，而客户端时间戳数据没有变化。对于后端Web Server，请求的源地址是NAT网关，所以从后端服务器的角度看，原本不同客户端的请求经过NAT的转发，就可能会被认为是同一个连接，加之不同客户端的时间可能不一致，所以就会出现时间戳错乱的现象，于是后面的数据包就被丢弃了。\n\n\n注：重新修改回该值的初始值必须在 /etc/sysctl.conf 中修改net.ipv4.tcp_tw_recycle = 0 然后再执行命令：sysctl -p 之后才能生效。\n\n总结最合适的解决方案是增加更多的四元组数目，比如，服务器可用端口，或服务器IP，让服务器能容纳足够多的TIME-WAIT状态连接。在我们常见的互联网架构中（NGINX反代跟NGINX，NGINX跟FPM，FPM跟redis、mysql、memcache等），减少TIME-WAIT状态的TCP连接，最有效的是使用长连接，不要用短连接，尤其是负载均衡跟web服务器之间。\n\n在服务端，不要启用net.ipv4.tcp_tw_recycle\n除非能确保你的服务器网络环境不是NAT。在服务端上启用net.ipv4.tw_reuse对于连接进来的TCP连接来说，并没有任何用处。\n\n\n在客户端上启用net.ipv4.tcp_tw_reuse\n还算稍微安全的解决TIME-WAIT的方案。再开启net.ipv4.tcp_tw_recycle的话，对客户端（或以客户端形式）的回收，也没有什么用处，反而会发生很多诡异的事情（尤其是FPM这种服务器上，相对nginx是服务端，相对redis是客户端）。\n\n\n\n可以使用如下命令查看当前主机的链接状态统计：\nnetstat -n | awk &#x27;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&#x27;\n\n\n相关链接\n\nhttps://cloud.tencent.com/developer/article/1471414\n","categories":["运维","Linux"]},{"title":"Linux 创建 root 权限用户","url":"/posts/56412/","content":"1.创建用户adduser demo\n\n2.修改密码# passwd demoChanging password for user rta.New password:Retype new password:\n\n3.添加 root 权限在 /etc/passwd 中将用户ID修改为0\nrta:x:0:1000::/home/demo:/bin/bash\n\n","categories":["运维","Linux"]},{"title":"Linux 定时执行任务","url":"/posts/27329/","content":"编辑定时任务crontab -e\n\n","categories":["运维","Linux"]},{"title":"Linux 查看 Socket 情况","url":"/posts/16895/","content":"1. 同一外部目的IP的 Socket 连接数量ss -t -a | awk &#x27;&#123;print $5&#125;&#x27; | cut -d &quot;:&quot; -f4- | sort -n | uniq -c | sort -nr\n\n2. Socket 状态数量统计第一种：\nss -t -a | awk &#x27;&#123;print $1&#125;&#x27; | sort -n | uniq -c | sort -nr\n\n第二种：\nnetstat -n | awk &#x27;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&#x27; \n\n3. 查看内存cat /proc/meminfo\n\n","categories":["运维","Linux"]},{"title":"Linux 查看磁盘IO读写情况","url":"/posts/43872/","content":"一、iostatyum install -y sysstat\n\n二、iotopyum install -y iotop\n\n当前 IO 情况iotop -oP\n\n查看某个进程的 IO 情况iotop -p $PID -d 1\n\n三、pidstatyum install -y pidstat\n\npidstat  和iotop效果一致，不过这个可以监控内存\npidstat -p $PID -d 1\n\n","categories":["运维","Linux"]},{"title":"Linux 查看磁盘占用情况","url":"/posts/4155/","content":"1、查看磁盘状态[root@delivery-test ~]# df -hFilesystem      Size  Used Avail Use% Mounted ondevtmpfs        3.9G     0  3.9G   0% /devtmpfs           3.9G     0  3.9G   0% /dev/shmtmpfs           3.9G  410M  3.5G  11% /runtmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup/dev/sda1        40G   40G   20K 100% /tmpfs           799M     0  799M   0% /run/user/0overlay          40G   40G   20K 100% /var/lib/docker/overlay2/2f948926a76c2201c01918b47d54bb87db7deac28ac2a1586820e2a0163fe471/mergedshm              64M     0   64M   0% /var/lib/docker/containers/fc77375a461d6a68d3d7af41519c194662d405ce131d50793963e734b8131f42/shm\n\n2、查看各个文件磁盘占用情况命令中的 / 表示跟目录，也可以试 /home 等，可根据情况自行修改。\n[root@delivery-test ~]# du -sh /*0\t/bin154M\t/boot0\t/dev36M\t/etc37G\t/home0\t/lib4.3M\t/root1.5M\t/tmp1.6G\t/usr530M\t/var\n\n3、查看inodes容量当磁盘容量或inode容量任意一个不足时就都会提示 No space left on device。\n查看后进入占用率高的目录删除无用文件即可。\n[root@delivery-test /]# df -ihFilesystem     Inodes IUsed IFree IUse% Mounted ondevtmpfs         996K   384  995K    1% /devtmpfs            998K     1  998K    1% /dev/shmtmpfs            998K   566  998K    1% /runtmpfs            998K    16  998K    1% /sys/fs/cgroup/dev/sda1        106K  106K   923  100% /tmpfs            998K     1  998K    1% /run/user/0overlay          106K  106K   923  100% /var/lib/docker/overlay2/2f9489a0163fe471/mergedshm              998K     1  998K    1% /var/lib/docker/containers/fc7834b8131f42/shm\n\n4、查看文件数量inodes 爆满那就查看一下哪些地方文件数量过多。\n根据上边的信息 / 目录的 inodes 占用率为 100% 所以查看其下文件，将无用文件删除即可，进行删除即可。\nfor i in /*; do echo $i; find $i |wc -l; done\n\n5、僵尸进程若磁盘容量和 inodes 容量都正常但还是没有磁盘空间，则需查看是否存在已被删除的文件，状态为 deleted\nlsof | grep deleted\n\n根据进程号，将其 kill 掉即可。若不清楚该进程作用请勿随意 kill\n原理：\n在linux上被删除的文件仍被其他进程所使用，文件句柄没有完全释放出来，导致空间无法释放出来,在Linux或者Unix系统中，通过rm或者文件管理器删除文件将会从文件系统的目录结构上解除链接(unlink).然而如果文件是被打开的（有一个进程正在使用），那么进程将仍然可以读取该文件，磁盘空间也一直被占用。\n","categories":["运维","Linux"],"tags":["Linux 常用命令"]},{"title":"Linux 查看端口占用","url":"/posts/59330/","content":"一、lsoflsof(list open files)是一个列出当前系统打开文件的工具。\nlsof 查看端口占用语法格式：\nlsof -i:端口号\n\n实例查看服务器 8000 端口的占用情况：\n# lsof -i:8000COMMAND   PID USER   FD   TYPE   DEVICE SIZE/OFF NODE NAMEnodejs  26993 root   10u  IPv4 37999514      0t0  TCP *:8000 (LISTEN)\n\n可以看到 8000 端口已经被轻 nodejs 服务占用。\nlsof -i 需要 root 用户的权限来执行。\n\nCOMMAND：进程的名称\nPID：进程标识符\nUSER：进程所有者\nFD：文件描述符\nTYPE：文件类型\nDEVICD：指定磁盘的名称\nSIZE：文件大小\nNODE：索引节点（文件在磁盘上的标识）\nNAME：打开文件的确切名称\n\n更多 lsof 的命令如下：lsof -i:8080：查看8080端口占用lsof abc.txt：显示开启文件abc.txt的进程lsof -c abc：显示abc进程现在打开的文件lsof -c -p 1234：列出进程号为1234的进程所打开的文件lsof -g gid：显示归属gid的进程情况lsof +d /usr/local/：显示目录下被进程开启的文件lsof +D /usr/local/：同上，但是会搜索目录下的目录，时间较长lsof -d 4：显示使用fd为4的进程lsof -i -U：显示所有打开的端口和UNIX domain文件\n\n二、netstatnetstat -tunlp 用于显示 tcp，udp 的端口和进程等相关情况。\nnetstat 查看端口占用语法格式：\nnetstat -tunlp | grep 端口号\n\n\n-t (tcp) 仅显示tcp相关选项\n-u (udp)仅显示udp相关选项\n-n 拒绝显示别名，能显示数字的全部转化为数字\n-l 仅列出在Listen(监听)的服务状态\n-p 显示建立相关链接的程序名\n\n例如查看 8000 端口的情况，使用以下命令：\n# netstat -tunlp | grep 8000tcp        0      0 0.0.0.0:8000            0.0.0.0:*               LISTEN      26993/nodejs   \n\n更多命令：\nnetstat -ntlp   //查看当前所有tcp端口netstat -ntulp | grep 80   //查看所有80端口使用情况netstat -ntulp | grep 3306   //查看所有3306端口使用情况","categories":["运维","Linux"],"tags":["Linux 常用命令"]},{"title":"NUMA 架构","url":"/posts/29679/","content":"相关文章：\nhttps://zhuanlan.zhihu.com/p/62795773\n基本概念SMP VS. AMP\nSMP(Symmetric Multiprocessing)， 即对称多处理器架构，是目前最常见的多处理器计算机架构。\nAMP(Asymmetric Multiprocessing)， 即非对称多处理器架构，则是与SMP相对的概念。\n\n那么两者之间的主要区别是什么呢？ 总结下来有这么几点，\n\nSMP的多个处理器都是同构的，使用相同架构的CPU；而AMP的多个处理器则可能是异构的。\nSMP的多个处理器共享同一内存地址空间；而AMP的每个处理器则拥有自己独立的地址空间。\nSMP的多个处理器操通常共享一个操作系统的实例；而AMP的每个处理器可以有或者没有运行操作系统， 运行操作系统的CPU也是在运行多个独立的实例。\nSMP的多处理器之间可以通过共享内存来协同通信；而AMP则需要提供一种处理器间的通信机制。\n\nSMP和AMP的深入介绍很多经典文章书籍可参考，此处不再赘述。现今主流的x86多处理器服务器都是SMP架构的， 而很多嵌入式系统则是AMP架构的。\nNUMA VS. UMANUMA(Non-Uniform Memory Access) 非均匀内存访问架构是指多处理器系统中，内存的访问时间是依赖于处理器和内存之间的相对位置的。 这种设计里存在和处理器相对近的内存，通常被称作本地内存；还有和处理器相对远的内存， 通常被称为非本地内存。\nUMA(Uniform Memory Access) 均匀内存访问架构则是与NUMA相反，所以处理器对共享内存的访问距离和时间是相同的。\n由此可知，不论是NUMA还是UMA都是SMP架构的一种设计和实现上的选择。\n阅读文档时，也常常能看到**ccNUMA(Cache Coherent NUMA)**，即缓存一致性NUMA架构。 这种架构主要是在NUMA架构之上保证了多处理器之间的缓存一致性。降低了系统程序的编写难度。\nx86多处理器发展历史上，早期的多核和多处理器系统都是UMA架构的。这种架构下， 多个CPU通过同一个北桥(North Bridge)芯片与内存链接。北桥芯片里集成了内存控制器(Memory Controller)，\n下图是一个典型的早期 x86 UMA 系统，四路处理器通过 FSB (前端系统总线, Front Side Bus) 和主板上的内存控制器芯片 (MCH, Memory Controller Hub) 相连，DRAM 是以 UMA 方式组织的，延迟并无访问差异。\n]\n\n注：\n\nPCH(Platform Controller Hub)，Intel 于 2008 年起退出的一系列晶片组，用于取代以往的 I/O Controller Hub（ICH)\n\n\n在 UMA 架构下，CPU 和内存控制器之间的前端总线 (FSB) 在系统 CPU 数量不断增加的前提下， 成为了系统性能的瓶颈。因此，AMD 在引入 64 位 x86 架构时，实现了 NUMA 架构。之后， Intel 也推出了 x64 的 Nehalem 架构，x86 终于全面进入到 NUMA 时代。x86 NUMA 目前的实现属于 ccNUMA。\n从 Nehalem 架构开始，x86 开始转向 NUMA 架构，内存控制器芯片被集成到处理器内部，多个处理器通过 QPI 链路相连，从此 DRAM 有了远近之分。 而 Sandybridge 架构则更近一步，将片外的 IOH 芯片也集成到了处理器内部，至此，内存控制器和 PCIe Root Complex 全部在处理器内部了。 下图就是一个典型的 x86 的 NUMA 架构：\n\n\nNUMA HierarchyNUMA Node 内部一个NUMA Node内部是由一个物理CPU和它所有的本地内存(Local Memory)组成的。广义得讲， 一个NUMA Node内部还包含本地IO资源，对大多数Intel x86 NUMA平台来说，主要是PCIe总线资源。 ACPI规范就是这么抽象一个NUMA Node的。\n物理 CPU一个CPU Socket里可以由多个CPU Core和一个Uncore部分组成。每个CPU Core内部又可以由两个CPU Thread组成。 每个CPU thread都是一个操作系统可见的逻辑CPU。对大多数操作系统来说，一个八核HT打开的CPU会被识别为16个CPU。 下面就说一说这里面相关的概念，\n\nSocket\n一个Socket对应一个物理CPU。 这个词大概是从CPU在主板上的物理连接方式上来的，可以理解为 Socket 就是主板上的 CPU 插槽。处理器通过主板的Socket来插到主板上。 尤其是有了多核(Multi-core)系统以后，Multi-socket系统被用来指明系统到底存在多少个物理CPU。\n\nNode\nNUMA体系结构中多了Node的概念，这个概念其实是用来解决core的分组的问题。每个node有自己的内部CPU，总线和内存，同时还可以访问其他node内的内存，NUMA的最大的优势就是可以方便的增加CPU的数量。通常一个 Socket 有一个 Node，也有可能一个 Socket 有多个 Node。\n\nCore\nCPU的运算核心。 x86的核包含了CPU运算的基本部件，如逻辑运算单元(ALU), 浮点运算单元(FPU), L1和L2缓存。 一个Socket里可以有多个Core。如今的多核时代，即使是Single Socket的系统， 也是逻辑上的SMP系统。但是，一个物理CPU的系统不存在非本地内存，因此相当于UMA系统。\n\nUncore\nIntel x86物理CPU里没有放在Core里的部件都被叫做Uncore。Uncore里集成了过去x86 UMA架构时代北桥芯片的基本功能。 在Nehalem时代，内存控制器被集成到CPU里，叫做iMC(Integrated Memory Controller)。 而PCIe Root Complex还做为独立部件在IO Hub芯片里。到了SandyBridge时代，PCIe Root Complex也被集成到了CPU里。 现今的Uncore部分，除了iMC，PCIe Root Complex，还有QPI(QuickPath Interconnect)控制器， L3缓存，CBox(负责缓存一致性)，及其它外设控制器。\n\nThreads\n这里特指CPU的多线程技术。在Intel x86架构下，CPU的多线程技术被称作超线程(Hyper-Threading)技术。 Intel的超线程技术在一个处理器Core内部引入了额外的硬件设计模拟了两个逻辑处理器(Logical Processor)， 每个逻辑处理器都有独立的处理器状态，但共享Core内部的计算资源，如ALU，FPU，L1，L2缓存。 这样在最小的硬件投入下提高了CPU在多线程软件工作负载下的性能，提高了硬件使用效率。 x86的超线程技术出现早于NUMA架构。\n\n\n本地内存在Intel x86平台上，所谓本地内存，就是CPU可以经过Uncore部件里的iMC访问到的内存。而那些非本地的， 远程内存(Remote Memory)，则需要经过QPI的链路到该内存所在的本地CPU的iMC来访问。 曾经在Intel IvyBridge的NUMA平台上做的内存访问性能测试显示，远程内存访问的延时时本地内存的一倍。\n可以假设，操作系统应该尽量利用本地内存的低访问延迟特性来优化应用和系统的性能。\n本地 IO 资源如前所述，Intel自从SandyBridge处理器开始，已经把PCIe Root Complex集成到CPU里了。 正因为如此，从CPU直接引出PCIe Root Port的PCIe 3.0的链路可以直接与PCIe Switch或者PCIe Endpoint相连。 一个PCIe Endpoint就是一个PCIe外设。这就意味着，对某个PCIe外设来说，如果它直接于哪个CPU相连， 它就属于哪个CPU所在的NUMA Node。\n与本地内存一样，所谓本地IO资源，就是CPU可以经过Uncore部件里的PCIe Root Complex直接访问到的IO资源。 如果是非本地IO资源，则需要经过QPI链路到该IO资源所属的CPU，再通过该CPU PCIe Root Complex访问。 如果同一个NUMA Node内的CPU和内存和另外一个NUMA Node的IO资源发生互操作，因为要跨越QPI链路， 会存在额外的访问延迟问题。\n其它体系结构里，为降低外设访问延迟，也有将IB(Infiniband)总线集成到CPU里的。 这样IB设备也属于NUMA Node的一部分了。\n可以假设，操作系统如果是NUMA Aware的话，应该会尽量针对本地IO资源低延迟的优点进行优化。\n\nNUMA Node 互联在Intel x86上，NUMA Node之间的互联是通过 QPI((QuickPath Interconnect) Link的。 CPU的Uncore部分有QPI的控制器来控制CPU到QPI的数据访问。\n下图就是一个利用 QPI Switch 互联的 8 NUMA Node 的 x86 系统，\n\nNUMA AffinityNUMA Affinity(亲和性)是和NUMA Hierarchy(层级结构)直接相关的。对系统软件来说， 以下两个概念至关重要，\n\nCPU NUMA Affinity\nCPU NUMA的亲和性是指从CPU角度看，哪些内存访问更快，有更低的延迟。如前所述， 和该CPU直接相连的本地内存是更快的。操作系统如果可以根据任务所在CPU去分配本地内存， 就是基于CPU NUMA亲和性的考虑。因此，CPU NUMA亲和性就是要尽量让任务运行在本地的NUMA Node里。\n\nDevice NUMA Affinity\n设备NUMA亲和性是指从PCIe外设的角度看，如果和CPU和内存相关的IO活动都发生在外设所属的NUMA Node， 将会有更低延迟。这里有两种设备NUMA亲和性的问题，\n\nDMA Buffer NUMA Affinity\n大部分PCIe设备支持DMA功能的。也就是说，设备可以直接把数据写入到位于内存中的DMA缓冲区。 显然，如果DMA缓冲区在PCIe外设所属的NUMA Node里分配，那么将会有最低的延迟。 否则，外设的DMA操作要跨越QPI链接去读写另外一个NUMA Node里的DMA缓冲区。 因此，操作系统如果可以根据PCIe设备所属的NUMA node分配DMA缓冲区， 将会有最好的DMA操作的性能。\n\nInterrupt NUMA Affinity\n设备DMA操作完成后，需要在CPU上触发中断来通知驱动程序的中断处理例程(ISR)来读写DMA缓冲区。 很多时候，ISR触发下半部机制(SoftIRQ)来进入到协议栈相关(Network，Storage)的代码路径来传送数据。 对大部分操作系统来说，硬件中断(HardIRQ)和下半部机制的代码在同一个CPU上发生。 因此，DMA缓冲区的读写操作发生的位置和设备硬件中断(HardIRQ)密切相关。假设操作系统可以把设备的硬件中断绑定到自己所属的NUMA node， 那之后中断处理函数和协议栈代码对DMA缓冲区的读写将会有更低的延迟。\n\n\n\n\nFirmware 接口由于NUMA的亲和性对应用的性能非常重要，那么硬件平台就需要给操作系统提供接口机制来感知硬件的NUMA层级结构。 在x86平台，ACPI规范提供了以下接口来让操作系统来检测系统的NUMA层级结构。\nACPI 5.0a规范的第17章是有关NUMA的章节。ACPI规范里，NUMA Node被第9章定义的Module Device所描述。 ACPI规范里用Proximity Domain对NUMA Node做了抽象，两者的概念大多时候等同。\n\nSRAT(System Resource Affinity Table)\n主要描述了系统boot时的CPU和内存都属于哪个Proximity Domain(NUMA Node)。 这个表格里的信息时静态的，如果是启动后热插拔，需要用OSPM的_PXM方法去获得相关信息。\n\nSLIT(System Locality Information Table)\n提供CPU和内存之间的位置远近信息。在SRAT表格里，只能告诉给定的CPU和内存是否在一个NUMA Node。 对某个CPU来说，不在本NUMA Node里的内存，即远程内存们是否都是一样的访问延迟取决于NUMA的拓扑有多复杂(QPI的跳数)。 总之，对于不能简单用远近来描述的NUMA系统(QPI存在0，1，2等不同跳数)， 需要SLIT表格给出进一步的说明。同样的，这个表格也是静态表格，热插拔需要使用OSPM的_SLI方法。\n\nDSDT(Differentiated System Description Table)\n从Device NUMA角度看，这个表格给出了系统boot时的外设都属于哪个Proximity Domain(NUMA Node)。\n\n\nACPI规范OSPM(Operating System-directed configuration and Power Management) 和OSPM各种方法就是操作系统里的ACPI驱动和ACPI firmware之间的一个互动的接口。 x86启动OS后，没有ACPI之前，firmware(BIOS)的代码是无法被执行了，除非通过SMI中断处理程序。 但有了ACPI，BIOS提前把ACPI的一些静态表格和AML的bytecode代码装载到内存， 然后ACPI驱动就会加载AML的解释器，这样OS就可以通过ACPI驱动调用预先装载的AML代码。 AML(ACPI Machine Language)是和Java类似的一种虚拟机解释型语言，所以不同操作系统的ACPI驱动， 只要有相同的虚拟机解释器，就可以直接从操作系统调用ACPI写好的AML的代码了。 所以，前文所述的所有热插拔的OSPM方法，其实就是对应ACPI firmware的AML的一段函数代码而已。 (关于ACPI的简单介绍，这里给出两篇延伸阅读：1 和2。)\nNUMA Optimization\nhttps://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/\nhttps://frankdenneman.nl/2016/07/08/numa-deep-dive-part-2-system-architecture/\nhttp://oliveryang.net/2016/02/linux-numa-optimization-1/\n\n相关文章推荐\n\n【计算机体系结构】Cache Memory\nMelodies Are Just Math\nKey Numbers Every Programmer Should Know\n\n原文：\nhttps://houmin.cc/posts/b893097a/\n","categories":["运维","Linux"]},{"title":"NUMA 架构相关命令","url":"/posts/31066/","content":"相关文章：\nhttps://zhuanlan.zhihu.com/p/62795773\n查看是否支持 NUMA 架构dmesg | grep -i numa\n\n查看 NUMA node 情况numactl --hardware\n\n进程使用指定 node 的内存numactl --membind=0 java -jar app.jar\n\n查看 NUMA 当前状况numastat\n\n实用工具 lstopolstopo --of png &gt; server.png\n\n","categories":["运维","Linux"],"tags":["Linux 常用命令"]},{"title":"awk 命令","url":"/posts/10379/","content":"AWK官方文档\nawk 是什么awk是linux环境下的一个命令行工具，但是由于awk强大的能力，我们可以为awk工具传递一个字符串，该字符串的内容类似一种编程语言的语法，我们可以称其为Awk语言，而awk工具本身则可以看作是Awk语言的解析器。就好比python解析器与Python语言的关系。我们一般使用awk来做什么，awk又适合做什么工作呢。由于awk天生提供对文件中文本分列进行处理，所以如果一个文件中的每行都被特定的分隔符(常见的是空格)隔开，我们可以将这个文件看成是由很多列的文本组成，这样的文件最适合用awk进行处理，其实awk在工作中很多时候被用来处理log文件，进行一些统计工作等。\nawk 命令的一般组成awk最常用的工作一般是遍历一个文件中的每一行，然后分别对文件的每一行进行处理，一个完整的awk命令形式如下：\nawk [options] &#x27;BEGIN&#123; commands &#125; pattern&#123; commands &#125; END&#123; commands &#125;&#x27; file\n\n其中options表示awk的可选的命令行选项，其中最常用的恐怕是 -F 它指定将文件中每一行分隔成列的分隔符号。而紧接着后面的单引号里面的所有内容是awk的程序脚本，awk需要对文件每一行分割后的每一列做处理。file则是awk要处理的文件名称。让我们通过demo来体会awk的功能。\nawk 对每一行进行分割处理echo &#x27;11 22 33 44&#x27; | awk &#x27;&#123;print $3&quot; &quot;$2&quot; &quot;$1&#125;&#x27;33 22 11\n\n我们将字符串 11 22 33 44 通过管道传递给awk命令，相当于awk处理一个文件，该文件的内容就是 11 22 33 44 上面的命令中我们并没有添加 -F 指定分割符号，实际上默认情况下awk使用空格分割每一行，如果需要指定别的字符则使用-F显示指定。上面的命令是将 11 22 33 44 通过空格(不管列之间有多少个空格都将当作一个空格处理)分割成4列，在awk中有一种通过 $数字 引用的变量，这种变量引用的内容就是当前行中分割的每一列的内容，数字的序号从1开始，例如$1表示第1列的内容，$2表示第二列，以此类推。$0 表示当前整行的内容。print是awk的内置函数，用于打印出变量的值。 而我们在$3 $2 $1 之间添加了用双引号引起来的空格，如果没有，则这些变量的值打印出来会连在一起。这里的awk命令中{}里面的内容实际上是我们上面完整模式的中间部分，我们省略了上面的BEGIN块，END块，并且中间的程序块我们也省略掉了pattern部分，也就是如果不添加BEGIN或者END说明那么该程序块就是上面完整模式中的中间的那个程序块，该中间的程序块所执行的操作就是循环处理文件内容的每一行，如果文件有10行，那么中间的程序块要运行10次，每一次处理一行的内容，并且处理完当前行之后，下次循环会自动依次处理接下来的行内容。\n我们来看看，如果有两列是什么效果呢，例如：\necho -e &#x27;11 22 33 44\\naa bb cc dd&#x27; | awk &#x27;&#123;print $3&quot; &quot;$2&quot; &quot;$1&#125;&#x27;33 22 11cc bb aa\n\n注意这里 echo 命令使用了 -e 选项的目的就是为了保持字符串中的\\n的格式能够生效，否则该换行将被忽略。那么上面的命令是如何执行的呢，我们模拟一下awk的执行过程，首先awk读取第一行的内容，使用空格分隔该行中的列，并将字符串11赋值给$1，22赋值给$2，33赋值给$3，44赋值给$4。然后通过print打印出来。接着读取第二行的内容，同样执行上面的操作。\n使用parttern部分我们已经学习了awk最简单的命令，下面我们再加一点东西进去，在程序块的前面添加pattern部分，例如：\necho -e &#x27;1 2 3 4\\n5 6 7 8&#x27; | awk &#x27;$1&gt;2&#123;print $3&quot; &quot;$2&quot; &quot;$1&#125;&#x27;7 6 5\n\n该程序与上面的程序几乎一样，只不过我们在程序块前面添加了 $1&gt;2 表示如果当前行的第1列的值大于2则处理当前行，否则不处理。说白了pattern部分是用来从文件中筛选出需要处理的行进行处理的，如果没有则循环处理文件中的所有行。pattern部分可以是任何条件表达式的判断结果，例如&gt;，&lt;，==，&gt;=，&lt;=，!= 同时还可以使用+，-，*，/运算与条件表达式相结合的复合表达式，逻辑 &amp;&amp;，||，! 同样也可以使用进来。另外pattern部分还可以使用 /正则/ 选择需要处理的行。\nawk 的BEGIN语句块BEGIN语句块是在匹配文件第一行之前运行的语句块。由于是匹配第一行之前运行，实际上在BEGIN语句块中 $n 是不可用的。一般情况下可以在BEGIN语句块中做一些变量(awk中可以自定义变量，直接为一个变量赋值就定义了一个变量，awk中没有专门定义变量的关键字)初始化的工作，以及一些只需要在开始仅打印一次的输出信息(例如输出表的表头)。例如：\necho -e &#x27;1 2 3 4\\n5 6 7 8&#x27; | awk &#x27;BEGIN&#123;print &quot;c1 c2 c3&quot;;print &quot;&quot;&#125;&#123;print $3&quot; &quot;$2&quot; &quot;$1&#125;&#x27;c1 c2 c33 2 17 6 5\n\n注意一个语句块(花括号包围)中可以有多条语句，使用分号隔开，这与C语言一样。如果需要单独打印空行，需要使用 print &quot;&quot; 我们上面就实现了输出表头的效果。\nawk 的END语句块END语句块是在awk循环执行完所有行的处理之后，才执行的，与BEGIN一样，END语句块也只执行一次，我们看看完整的例子。\necho -e &#x27;1\\n2\\n3&#x27; | awk &#x27;BEGIN&#123;print &quot;begin&quot;&#125;&#123;print $1&#125;END&#123;print &quot;end&quot;&#125;&#x27;begin123end\n\nawk定义变量对列求和test.txt 的内容如下：\n11 22 3323 45 3422 32 43\n\nawk &#x27;BEGIN&#123;sum=0&#125;&#123;sum+=$1&#125;END&#123;print sum&#125;&#x27; test.txt56\n\n首先在BEGIN语句块中为变量sum赋值0，然后在循环语句块中将每一行的第1列加到sum中，当文件的所有行全部循环处理完成之后，打印出sum变量的值。当然这个例子中BEGIN语句块是可以省略的，我们可以直接在循环语句块中使用sum变量，此时sum第一次使用，该变量会自动被建立，默认的初始值是0。\nawk中的判断语句awk的所有语句块中都可以使用判断语句，其判断语句语法与C语言一样。\n//test.txt内容如下\n1 2 34 5 67 8 910 11 12\n\nawk &#x27;&#123;if($1%2==0)print $1&quot; &quot;$2&quot; &quot;$3&#125;&#x27; test.txt4 5 610 11 12\n\nawk中的循环while循环awk &#x27;BEGIN&#123;count=0;while(count&lt;5)&#123;print count;count ++;&#125;&#125;&#x27;01234\n\n可以看出awk的一个语句块中可以有比较复杂的复合语句，其使用与C语言几乎差不多，多条语句之间用分号隔开，复合语句块用花括号括起来做分隔。\ndo..while循环awk &#x27;BEGIN&#123;count=0;do&#123;print count;count++&#125;while(count&lt;5)&#125;&#x27;01234\n\nfor 循环awk &#x27;BEGIN&#123;for(count=0;count&lt;5;count++)print count&#125;&#x27;01234\n\n可以看到这几种循环的形式与C语言是一样的，对于我们理解没有任何障碍。awk中也使用break退出循环，使用continue跳过本次循环，其含义与C语言一样。\n使用数组分组求和，for..in循环awk中的数组基本上可以看作是字典，看下面的例子：test.txt的文件内容\nzhangsan 2 3lisi 5 6zhangsan 8 9lisi 11 12wangwu 33 11\n\n将所有第一列相同的分成一个组，并将该组中的第二列求和。\nawk &#x27;&#123;sum[$1]+=$2&#125;END&#123;for(k in sum)print k&quot; &quot;sum[k]&#125;&#x27; test.txtzhangsan 10lisi 16wangwu 33\n\n这个例子里面使用了for..in循环来遍历数组的key，同时通过key来得到数组的值。对于key不是数字的数组，是不能通过普通的for循环来以数字索引访问数组元素的。我们可以通过length()函数来获得数组的元素个数，例如length(array)\nawk 中使用shell变量值有的时候我们在shell中计算出来的变量值需要被awk命令使用，我们当然不能在awk中直接使用 $VAR，因为美元符号在awk中本来就是特殊符号，在awk中可以使用 $n 引用当前行的第n列的值，所以直接这么使用是不行的，awk提供了一个选项 -v 来指定变量，在awk中有两种变量，一种是 $n 形式的变量，这个是在循环文件的行的时候，用来引用当前行的第n列的值，还有一种变量，不用定义可以直接使用，不需要用美元符号来引用。下面看看shell中的变量值如何在awk中使用：\na=22b=33awk -v x=$a -v y=$b &#x27;BEGIN&#123;print x&quot; &quot;y&#125;&#x27;\n\n可以看到我们只需要在使用awk的时候通过 -v 指定awk中将会用到的变量即可，而变量值则可以通过引用shell变量得到，也就是说我们只能在awk的options部分引用shell的变量，在awk的语句块中使用美元符号引用变量会被awk解析成自己的变量而不是shell的变量。\nawk 中的操作符与优先级列表\nawk 的内置函数awk定义了很多内置函数，下面我们根据函数类型列出常用的函数，下面的函数只是一部分，完整的函数列表则需要查阅awk的官方文档。\n算术：\natan2(y,x) 返回 y/x 的反正切\ncos(x) 返回 x 的余弦；x 是弧度\nsin(x) 返回 x 的正弦；x 是弧度\nexp(x) 返回 x 幂函数\nlog(x) 返回 x 的自然对数\nsqrt(x) 返回 x 平方根\nint(x) 返回 x 的截断至整数的值\nrand() 返回任意数字 n，其中 0 &lt;= n &lt; 1\nsrand([expr]) 将 rand 函数的种子值设置为 Expr 参数的值，或如果省略 Expr 参数则使用某天的时间。返回先前的种子值。\n\n字符串：\ngsub(reg,str1,str2) 使用str1替换所有str2中符合正则表达式reg的子串\nsub(reg,str1,str2) 含义与gsub相同，只不过gsub是替换所有匹配，sub只替换第一个匹配\nindex(str,substr) 返回substr在str中第一次出现的索引，注意索引从1开始计算，如果没有则返回0\nlength(str) 返回str字符串的长度，length函数还可以返回数组元素的个数\nblength(str) 返回字符串的字节数\nmatch(str,reg) 与index函数一样，只不过reg使用正则表达式，例如match(“hello”,/lo/)\nsplit(str,array,reg)将str分隔成数组保存到array中，分隔使用正则reg，或者字符串都可以，返回数组长度\ntolower(str) 转换为小写\ntoupper(str) 转换为大写\nsubstr(str,start,length) 截取字符串，从start索引开始的length个字符，如不指定length则截取到末尾，索引从1开始\n\n其他：\nsystem(command) 执行系统命令，返回退出码\nmktime( YYYY MM dd HH MM ss[ DST]) 生成时间格式\nstrftime(format,timestamp) 格式化时间输出，将时间戳转换为时间字符串\nsystime() 得到时间戳,返回从1970年1月1日开始到当前时间(不计闰年)的整秒数\n\nawk 的内置变量awk 中同样定义了很多内置变量，我们可以直接像使用普通变量一样使用他们，由于awk的版本众多，有些内置变量并不是得到所有awk版本的支持。\n\n$n 当前记录的第n个字段，比如n为1表示第一个字段，n为2表示第二个字段\n$0 这个变量包含执行过程中当前行的文本内容。\n\nawk 支持的变量\nFILENAME 当前输入文件的名\nFS 字段分隔符（默认是任何空格）\nNF 表示字段数，在执行过程中对应于当前的字段数\nNR 表示记录数，在执行过程中对应于当前的行号\nOFMT 数字的输出格式（默认值是%.6g）\nOFS 输出字段分隔符（默认值是一个空格）\nORS 输出记录分隔符（默认值是一个换行符）\nRS 记录分隔符（默认是一个换行符）\n\nnawk 支持的变量\nARGC 命令行参数的数目\nARGV 包含命令行参数的数组\nERRNO 最后一个系统错误的描述\nRSTART 由match函数所匹配的字符串的第一个位置\nRLENGTH 由match函数所匹配的字符串的长度\nSUBSEP 数组下标分隔符（默认值是34）\n\nPOSIXawk 支持的变量\nENVIRON 环境变量关联数组\nFNR 同NR，但相对于当前文件\n\ngawk 支持的变量\nARGIND 命令行中当前文件的位置（从0开始算）\nCONVFMT 数字转换格式（默认值为%.6g）\nFIELDWIDTHS 字段宽度列表（用空格键分隔）\nIGNORECASE 如果为真，则进行忽略大小写的匹配\n\n","categories":["运维","Linux"],"tags":["Linux 常用命令"]},{"title":"cat 多行写入文件防止变量被替换","url":"/posts/16115/","content":"1. 在分界符前加反斜杠cat &gt;&gt; a.sh &lt;&lt; \\EOFecho `hostname`echo $HOMEEOF\n\n2. 给分界符加双引号cat &gt;&gt; a.sh &lt;&lt; &quot;EOF&quot;echo `hostname`echo $HOMEEOF\n\n3. 给分界符加单引号cat &gt;&gt; a.sh &lt;&lt; &#x27;EOF&#x27;echo `hostname`echo $HOMEEOF\n\n","categories":["运维","Linux"]},{"title":"curl 命令","url":"/posts/39748/","content":"分析 HTTP 请求耗时HTTP 请求过程：\n\n每次http请求经过这些过程： 客户端发起请求–&gt;DNS解析–&gt;TCP连接–&gt;SSL等协议握手–&gt;服务器处理–&gt;内容传输–&gt;完成\n查看curl 命令的手册，curl命令支持以下阶段的时间统计：\n\ntime_namelookup : 从请求开始到DNS解析完成的耗时\ntime_connect : 从请求开始到TCP三次握手完成耗时\ntime_appconnect : 从请求开始到TLS握手完成的耗时\ntime_pretransfer : 从请求开始到向服务器发送第一个GET请求开始之前的耗时\ntime_redirect : 重定向时间，包括到内容传输前的重定向的DNS解析、TCP连接、内容传输等时间\ntime_starttransfer : 从请求开始到内容传输前的时间\ntime_total : 从请求开始到完成的总耗时\nspeed_download：下载速度，单位-字节每秒\n\n我们常关注的HTTP性能指标有：\n\nDNS请求耗时 ： 域名的NS及本地使用DNS的解析速度\nTCP建立耗时 ： 服务器网络层面的速度\nSSL握手耗时 ： 服务器处理HTTPS等协议的速度\n服务器处理请求时间 ： 服务器处理HTTP请求的速度\nTTFB ： 服务器从接收请求到开始到收到第一个字节的耗时\n服务器响应耗时 ：服务器响应第一个字节到全部传输完成耗时\n请求完成总耗时\n\n注意： 如果想分析HTTP性能的瓶颈，不建议使用带有重定向的请求进行分析，重定向会导致建立多次TCP连接或多次HTTP请求，多次请求的数据混在一起，数据不够直观，因此 time_redirect 对实际分析意义不大。\n其中的运算关系：\n\nDNS请求耗时 = time_namelookup\nTCP三次握手耗时 = time_connect - time_namelookup\nSSL握手耗时 = time_appconnect - time_connect\n服务器处理请求耗时 = time_starttransfer - time_pretransfer\nTTFB耗时 = time_starttransfer - time_appconnect\n服务器传输耗时 = time_total - time_starttransfer\n总耗时 = time_total\n\n用curl命令统计以上时间：\ncurl -w &#x27;\\ntime_namelookup=%&#123;time_namelookup&#125;\\ntime_connect=%&#123;time_connect&#125;\\ntime_appconnect=%&#123;time_appconnect&#125;\\ntime_redirect=%&#123;time_redirect&#125;\\ntime_pretransfer=%&#123;time_pretransfer&#125;\\ntime_starttransfer=%&#123;time_starttransfer&#125;\\ntime_total=%&#123;time_total&#125;\\n\\n&#x27; -o /dev/null -s -L &#x27;https://www.nixops.me/&#x27;\n\n以上内容不够直观，curl -w参数支持模板，新建一个文件timing.txt,内容如下：\ntime_namelookup=%&#123;time_namelookup&#125;\\ntime_connect=%&#123;time_connect&#125;\\ntime_appconnect=%&#123;time_appconnect&#125;\\ntime_redirect=%&#123;time_redirect&#125;\\ntime_pretransfer=%&#123;time_pretransfer&#125;\\ntime_starttransfer=%&#123;time_starttransfer&#125;\\ntime_total=%&#123;time_total&#125;\\n\n\n使用模板在执行一次：\ncurl -w &quot;@timing.txt&quot; -o /dev/null -s -L &#x27;https://www.nixops.me/&#x27;\n\n将上述功能生成脚本stat.sh\n#!/bin/bashDefault_URL=https://www.nixops.meURL=$&#123;1:-$Default_URL&#125;Result=`curl -o /dev/null -s $URL \\        -w \\        &#x27;time_namelookup=%&#123;time_namelookup&#125;time_connect=%&#123;time_connect&#125;time_appconnect=%&#123;time_appconnect&#125;time_redirect=%&#123;time_redirect&#125;time_pretransfer=%&#123;time_pretransfer&#125;time_starttransfer=%&#123;time_starttransfer&#125;time_total=%&#123;time_total&#125;&#x27;`declare $Resultcurl_timing()&#123;    printf &quot;\\e[92mcURL Timing: \\e[0m\\n&quot;    for i in $Result    do              IFS=&#x27;=&#x27;            printf &quot;\\e[96m%18s \\e[0m: %10s \\n&quot; $i    done&#125;stat_timing()&#123;    Result_TCP=`printf &quot;%.6f&quot; $(echo $time_connect - $time_namelookup |bc -l)`    Result_TLS=`printf &quot;%.6f&quot; $(echo $time_appconnect - $time_connect |bc -l)`    Result_Server=`printf &quot;%.6f&quot; $(echo $time_starttransfer - $time_pretransfer |bc -l)`    Result_TTFB=`printf &quot;%.6f&quot; $(echo $time_starttransfer - $time_appconnect |bc -l)`    Result_Transfer=`printf &quot;%.6f&quot; $(echo $time_total - $time_starttransfer |bc -l)`    printf &quot;\\n\\e[92mResource Timing: \\e[0m\\n&quot;    printf &quot;\\e[96m%18s \\e[0m: %.6f \\n&quot; &quot;DNS Lookup&quot; $time_namelookup    printf &quot;\\e[96m%18s \\e[0m: %.6f \\n&quot; &quot;TCP Connection&quot; $Result_TCP        if  [ `echo &quot;$time_appconnect == 0&quot;|bc` -eq 0 ]    then        printf &quot;\\e[96m%18s \\e[0m: %.6f \\n&quot; &quot;TLS Handshake&quot; $Result_TLS    fi    printf &quot;\\e[96m%18s \\e[0m: %.6f \\n&quot; &quot;Server Processing&quot; $Result_Server    printf &quot;\\e[96m%18s \\e[0m: %.6f \\n&quot; &quot;TTFB&quot; $Result_TTFB    printf &quot;\\e[96m%18s \\e[0m: %.6f \\n&quot; &quot;Content Transfer&quot; $Result_Transfer    printf &quot;\\e[96m%18s \\e[0m: %.6f \\n&quot; &quot;Finish&quot; $time_total&#125;curl_timingstat_timing \n\n执行一下：\n# ./stat.sh https://www.baidu.comcURL Timing:    time_namelookup :   0.004087       time_connect :   0.006480    time_appconnect :   0.022001      time_redirect :   0.000000   time_pretransfer :   0.022026 time_starttransfer :   0.025635         time_total :   0.025658 Resource Timing:         DNS Lookup : 0.004087     TCP Connection : 0.002393      TLS Handshake : 0.015521  Server Processing : 0.003609               TTFB : 0.003634   Content Transfer : 0.000023             Finish : 0.025658 \n\n从上述结果，就可以直观的分析出http各阶段的耗时，方便我们进行性能瓶颈。\n参数\n--keepalive：强制开启 keep-alive（默认开启）\n--no-keepalive：禁用 keep-alive\n\n","categories":["运维","Linux"],"tags":["Linux 常用命令"]},{"title":"expect 登陆脚本","url":"/posts/34610/","content":"1 安装expect工具expect是建立在tcl基础上的一个自动化交互套件, 在一些需要交互输入指令的场景下, 可通过脚本设置自动进行交互通信. 其交互流程是:\nspawn启动指定进程 -&gt; expect获取指定关键字 -&gt; send想指定进程发送指定指令 -&gt; 执行完成, 退出.\n由于expect是基于tcl的, 所以需要确保系统中安装了tcl:\n# 检查是否安装了tcl:[root@localhost ~]# whereis tcltcl: /usr/lib64/tcl8.5 /usr/include/tcl.h /usr/share/tcl8.5# 如果没有安装, 使用yum安装tcl和expect:[root@localhost ~]# yum install -y tcl[root@localhost ~]# yum install -y expect# 查看expect的安装路径:[root@localhost ~]# command -v expect/usr/bin/expect\n\n2 expect的常用命令\n\n\n命 令\n说 明\n\n\n\nspawn\n启动新的交互进程, 后面跟命令或者指定程序\n\n\nexpect\n从进程中接收信息, 如果匹配成功, 就执行expect后的动作\n\n\nsend\n向进程发送字符串\n\n\nsend exp_send\n用于发送指定的字符串信息\n\n\nexp_continue\n在expect中多次匹配就需要用到\n\n\nsend_user\n用来打印输出 相当于shell中的echo\n\n\ninteract\n允许用户交互\n\n\nexit\n退出expect脚本\n\n\neof\nexpect执行结束, 退出\n\n\nset\n定义变量\n\n\nputs\n输出变量\n\n\nset timeout\n设置超时时间\n\n\n[lindex $argv 0]\n获取expect脚本的第1个参数\n\n\n3 作用原理简介3.1 示例脚本这里以ssh远程登录某台服务器的脚本为例进行说明, 假设此脚本名称为remote_login.sh:\n#!/usr/bin/expectset a [lindex $argv 0]set timeout 30spawn ssh -l root 172.16.22.131expect &quot;password*&quot;send &quot;123456\\r&quot;interact\n\n3.2 脚本功能解读(1) #!/usr/bin/expect\n上述内容必须位于脚本文件的第一行, 用来告诉操作系统, 此脚本需要使用系统的哪个脚本解析引擎来执行.\n具体路径可通过command -v expect命令查看.\n\n注意:\n这里的expect和Linux的bash、Windows的cmd等程序一样, 都是一种脚本执行引擎.\n脚本需要有可执行权限(chmod +x remote_login.sh, 或chmod 755 auto_login.sh), 然后通过命令./remote_login.sh运行即可;\n如果输入sh remote_login.sh, 意义就不一样了: 明确调用sh引擎去执行此脚本, 此时首行的#!/usr/bin/expect就失效了.\n\n(2) set timeout 30\n设置连接的超时时间为30秒.\n(3) spawn ssh -l root 172.16.22.131\nspawn、send等命令是expect工具中的内部命令, 如果没有安装expect工具, 就会出现”spawn not found”等错误.\n\n不要用which spawn之类的命令去找spawn, 因为并没有这样的程序.\n\n(4) expect &quot;password*&quot;\n这个命令用来判断上次输出结果里是否包含”password*”的字符串, 如果有则立即返回, 否则就等待一段时间后返回. 这里的等待时长就是前面设置的timeout, 也就是30秒.\n(5) send &quot;123456\\r&quot;\n这里就是执行交互动作, 作用等同于手工输入密码.\n提示: 命令字符串结尾加上\\r, 这样的话, 如果出现异常等待的状态就能够停留下来, 作进一步的核查.\n(6) interact\nexpect执行完成后保持用户的交互状态, 这个时候用户就可以手工操作了.\n如果没有这一句, expect执行完成后就会退出脚本刚刚远程登录过去的终端, 用户也就不能继续操作了.\n4 其他脚本使用示例4.1 直接通过expect执行多条命令注意首行内容, 这种情况下就只能通过./script.sh来执行这类脚本了:\n#!/usr/bin/expect -fset timeout 10# 切换到root用户, 然后执行ls和df命令:spawn su - rootexpect &quot;Password*&quot;send &quot;123456\\r&quot;expect &quot;]*&quot;         # 通配符send &quot;ls\\r&quot;expect &quot;#*&quot;         # 通配符的另一种形式send &quot;df -Th\\r&quot;send &quot;exit\\r&quot;       # 退出spawn开启的进程expect eof          # 退出此expect交互程序\n\n4.2 通过shell调用expect执行多条命令注意首行内容, 这种情况下可通过sh script.sh、bash script.sh 或./script.sh, 都可以执行这类脚本:\n#!/bin/baship=&quot;172.16.22.131&quot;username=&quot;root&quot;password=&quot;123456&quot;# 指定执行引擎/usr/bin/expect &lt;&lt;EOF    set time 30    spawn ssh $username@$ip df -Th    expect &#123;        &quot;*yes/no&quot; &#123; send &quot;yes\\r&quot;; exp_continue &#125;        &quot;*password:&quot; &#123; send &quot;$password\\r&quot; &#125;    &#125;    expect eofEOF\n\n5 spawn not found 的解决出现这个错误的基本上都是初学者: Linux 执行shell脚本有两种方式:\n\n一种是将脚本作为sh的命令行参数, 如sh remote_login.sh, 或sh /data/remote_login.sh;\n一种是将脚本作为具有执行权限的可执行脚本, 如./remote_login.sh, 或/data/remote_login.sh.\n\n而作为sh命令行参数来运行, 就会导致脚本第一行的#!/usr/bin/expect失效, 也就出现了spawn not found、send not found等错误.\n要解决这个问题, 只需要通过以下命令运行脚本即可: ./automate_expect.sh\n\n参考资料Linux expect 介绍和用法\nexpect spawn、linux expect 用法小记\n\n","categories":["运维","Linux"]},{"title":"shell 中脚本参数传递的两种方式","url":"/posts/2309/","content":"方式一：$0,$1,$2..采用$0,$1,$2..等方式获取脚本命令行传入的参数，值得注意的是，$0获取到的是脚本路径以及脚本名，后面按顺序获取参数，当参数超过10个时(包括10个)，需要使用${10},${11}….才能获取到参数，但是一般很少会超过10个参数的情况。\n示例：新建一个test.sh的文件\n#!/bin/bashecho &quot;脚本$0&quot;echo &quot;第一个参数$1&quot;echo &quot;第二个参数$2&quot;\n\n在shell中执行脚本，结果如下\n$ ./test.sh 1 2\n\nshell中将会输出：\n脚本./test.sh第一个参数1第二个参数2\n\n下面是参数超过10个的情况，在test.sh文件写入\n#!/bin/bashecho &quot;脚本名$0&quot;echo &quot;第一个参数$1&quot;echo &quot;第二个参数$2&quot;echo &quot;第三个参数$3&quot;echo &quot;第四个参数$4&quot;……echo &quot;第十个参数$10&quot;echo &quot;第十个参数$&#123;10&#125;&quot;\n\n在shell中执行脚本，结果如下\n$ ./test.sh a b c d e f g h i j\n\nshell中将会输出：\n脚本名./test.sh第一个参数a第二个参数b第三个参数c第四个参数d第五个参数e第六个参数f第七个参数g第八个参数h第九个参数i第十个参数a0第十个参数j\n\n可以看到${10}正确读取到了第十个参数，而$10被分成$1读取到第一个参数a然后拼接字符串0，于是输出a0。\n方式二：getopts语法格式：getopts [option[:]] [DESCPRITION] VARIABLE\n\noption：表示为某个脚本可以使用的选项\n:：如果某个选项（option）后面出现了冒号（”:”），则表示这个选项后面可以接参数（即一段描述信息DESCPRITION）\nVARIABLE：表示将某个选项保存在变量VARIABLE中\n\n同样新建一个test.sh文件\nwhile getopts &quot;:a:b:c:&quot; optdo    case $opt in        a)        echo &quot;参数a的值$OPTARG&quot;        ;;        b)        echo &quot;参数b的值$OPTARG&quot;        ;;        c)        echo &quot;参数c的值$OPTARG&quot;        ;;        ?)        echo &quot;未知参数&quot;        exit 1;;    esacdone\n\n用一个 while 循环加 case 分支获取不同参数，:a:b:c 相当于定义参数的变量名，有时候可能会有未知参数，所以增加一个 ? 的分支。\n在shell中执行脚本，结果如下\n$ ./test.sh -a 1 -b 2 -c 3\n\n在shell中的输出\n参数a的值1参数b的值2参数c的值3\n\n$ ./test.sh -a 1 -c 3\n\n在shell中的输出\n参数a的值1参数c的值3\n\n$ ./test.sh -a 1 -c 3 -d 4\n\n在shell中的输出\n参数a的值1参数c的值3未知参数\n\n","categories":["运维","Linux"]},{"title":"tcpdump 命令详解","url":"/posts/37051/","content":"在讲解之前，有两点需要声明：\n\n第三节到第六节里的 tcpdump 命令示例，只为了说明参数的使用，并不一定就能抓到包，如果要精准抓到你所需要的包，需要配合第五节的逻辑逻辑运算符进行组合搭配。\n不同 Linux 发行版下、不同版本的 tcpdump 可能有小许差异， 本文是基于 CentOS 7.2 的 4.5.1 版本的tcpdump 进行学习的，若在你的环境中无法使用，请参考 man tcpdump 进行针对性学习。\n\n1. tcpdump 核心参数图解大家都知道，网络上的流量、数据包，非常的多，因此要想抓到我们所需要的数据包，就需要我们定义一个精准的过滤器，把这些目标数据包，从巨大的数据包网络中抓取出来。\n所以学习抓包工具，其实就是学习如何定义过滤器的过程。\n而在 tcpdump 的世界里，过滤器的实现，都是通过一个又一个的参数组合起来，一个参数不够精准，那就再加一个，直到我们能过滤掉无用的数据包，只留下我们感兴趣的数据包。\ntcpdump 的参数非常的多，初学者在没有掌握 tcpdump 时，会对这个命令的众多参数产生很多的疑惑。\n就比如下面这个命令，我们要通过 host 参数指定 host ip 进行过滤\n$ tcpdump host 192.168.10.100\n\n主程序 + 参数名+ 参数值 这样的组合才是我们正常认知里面命令行该有的样子。\n可 tcpdump 却不走寻常路，我们居然还可以在 host 前再加一个限定词，来缩小过滤的范围？\n$ tcpdump src host 192.168.10.100\n\n从字面上理解，确实很容易理解，但是这不符合编写命令行程序的正常逻辑，导致我们会有所疑虑：\n\n除了 src ，dst，可还有其它可以用的限定词？\nsrc，host 应该如何理解它们，叫参数名？不合适，因为 src 明显不合适。\n\n如果你在网上看到有关 tcpdump 的博客、教程，无一不是给你一个参数组合，告诉你这是实现了怎样的一个过滤器？这样的教学方式，很容易让你依赖别人的文章来使用 tcpdump，而不能将 tcpdump 这样神器消化，达到灵活应用，灵活搭配过滤器的效果。\n上面加了 src 本身就颠覆了我们的认知，你可知道在 src 之前还可以加更多的条件，比如 tcp, udp, icmp 等词，在你之前的基础上再过滤一层。\n$ tcpdump tcp src host 192.168.10.100\n\n这种参数的不确定性，让大多数人对 tcpdump 的学习始终无法得其精髓。\n因此，在学习 tcpdump 之前，我觉得有必要要先让你知道：tcpdump 的参数是如何组成的？这非常重要。\n为此，我画了一张图，方便你直观的理解 tcpdump 的各种参数：\n\n\noption 可选参数：将在后边一一解释。\nproto 类过滤器：根据协议进行过滤，可识别的关键词有： tcp, udp, icmp, ip, ip6, arp, rarp,ether,wlan, fddi, tr, decnet\ntype 类过滤器：可识别的关键词有：host, net, port, portrange，这些词后边需要再接参数。\ndirection 类过滤器：根据数据流向进行过滤，可识别的关键字有：src, dst，同时你可以使用逻辑运算符进行组合，比如 src or dst\n\nproto、type、direction 这三类过滤器的内容比较简单，也最常用，因此我将其放在最前面，也就是 第三节：常规过滤规则一起介绍。\n而 option 可选的参数非常多，有的甚至也不经常用到，因此我将其放到后面一点，也就是 第四节：可选参数解析\n当你看完前面六节，你对 tcpdump 的认识会上了一个台阶，至少能够满足你 80% 的使用需求。\n你一定会问了，还有 20% 呢？\n其实 tcpdump 还有一些过滤关键词，它不符合以上四种过滤规则，可能需要你单独记忆。关于这部分我会在 第六节：特殊过滤规则 里进行介绍。\n2. 理解 tcpdump 的输出2.1 输出内容结构tcpdump 输出的内容虽然多，却很规律。\n这里以我随便抓取的一个 tcp 包为例来看一下\n21:26:49.013621 IP 172.20.20.1.15605 &gt; 172.20.20.2.5920: Flags [P.], seq 49:97, ack 106048, win 4723, length 48\n\n从上面的输出来看，可以总结出：\n\n第一列：时分秒毫秒 21:26:49.013621\n第二列：网络协议 IP\n第三列：发送方的ip地址+端口号，其中172.20.20.1是 ip，而15605 是端口号\n第四列：箭头 &gt;， 表示数据流向\n第五列：接收方的ip地址+端口号，其中 172.20.20.2 是 ip，而5920 是端口号\n第六列：冒号\n第七列：数据包内容，包括Flags 标识符，seq 号，ack 号，win 窗口，数据长度 length，其中 [P.] 表示 PUSH 标志位为 1，更多标识符见下面\n\n2.2 Flags 标识符使用 tcpdump 抓包后，会遇到的 TCP 报文 Flags，有以下几种：\n\n[S] : SYN（开始连接）\n[P] : PSH（推送数据）\n[F] : FIN （结束连接）\n[R] : RST（重置连接）\n[.] : 没有 Flag （意思是除上面四种类型外的其他情况，有可能是 ACK 也有可能是 URG）\n\n3. 常规过滤规则3.1 基于IP地址过滤：host使用 host 就可以指定 host ip 进行过滤\n$ tcpdump host 192.168.10.100\n\n数据包的 ip 可以再细分为源ip和目标ip两种\n# 根据源ip进行过滤$ tcpdump -i eth2 src 192.168.10.100# 根据目标ip进行过滤$ tcpdump -i eth2 dst 192.168.10.200\n\n3.2 基于网段进行过滤：net若你的ip范围是一个网段，可以直接这样指定\n$ tcpdump net 192.168.10.0/24\n\n网段同样可以再细分为源网段和目标网段\n# 根据源网段进行过滤$ tcpdump src net 192.168# 根据目标网段进行过滤$ tcpdump dst net 192.168\n\n3.3 基于端口进行过滤：port使用 port 就可以指定特定端口进行过滤\n$ tcpdump port 8088\n\n端口同样可以再细分为源端口，目标端口\n# 根据源端口进行过滤$ tcpdump src port 8088# 根据目标端口进行过滤$ tcpdump dst port 8088\n\n如果你想要同时指定两个端口你可以这样写\n$ tcpdump port 80 or port 8088\n\n但也可以简写成这样\n$ tcpdump port 80 or 8088\n\n如果你的想抓取的不再是一两个端口，而是一个范围，一个一个指定就非常麻烦了，此时你可以这样指定一个端口段。\n$ tcpdump portrange 8000-8080$ tcpdump src portrange 8000-8080$ tcpdump dst portrange 8000-8080\n\n对于一些常见协议的默认端口，我们还可以直接使用协议名，而不用具体的端口号\n比如 http == 80，https == 443 等\n$ tcpdump tcp port http\n\n3.4 基于协议进行过滤：proto常见的网络协议有：tcp, udp, icmp, http, ip,ipv6 等\n若你只想查看 icmp 的包，可以直接这样写\n$ tcpdump icmp\n\nprotocol 可选值：ip, ip6, arp, rarp, atalk, aarp, decnet, sca, lat, mopdl, moprc, iso, stp, ipx, or netbeui\n3.5 基本IP协议的版本进行过滤当你想查看 tcp 的包，你也许会这样子写\n$ tcpdump tcp\n\n这样子写也没问题，就是不够精准，为什么这么说呢？\nip 根据版本的不同，可以再细分为 IPv4 和 IPv6 两种，如果你只指定了 tcp，这两种其实都会包含在内。\n那有什么办法，能够将 IPv4 和 IPv6 区分开来呢？\n很简单，如果是 IPv4 的 tcp 包 ，就这样写（友情提示：数字 6 表示的是 tcp 在ip报文中的编号。）\n$ tcpdump &#x27;ip proto tcp&#x27;# or$ tcpdump ip proto 6# or$ tcpdump &#x27;ip protochain tcp&#x27;# or $ tcpdump ip protochain 6\n\n而如果是 IPv6 的 tcp 包 ，就这样写\n$ tcpdump &#x27;ip6 proto tcp&#x27;# or$ tcpdump ip6 proto 6# or$ tcpdump &#x27;ip6 protochain tcp&#x27;# or $ tcpdump ip6 protochain 6\n\n关于上面这几个命令示例，有两点需要注意：\n\n跟在 proto 和 protochain 后面的如果是 tcp, udp, icmp ，那么过滤器需要用引号包含，这是因为 tcp,udp, icmp 是 tcpdump 的关键字。\n跟在ip 和 ip6 关键字后面的 proto 和 protochain 是两个新面孔，看起来用法类似，它们是否等价，又有什么区别呢？\n\n关于第二点，网络上没有找到很具体的答案，我只能通过 man tcpdump 的提示， 给出自己的个人猜测，但不保证正确。\nproto 后面跟的 &lt;protocol&gt; 的关键词是固定的，只能是 ip, ip6, arp, rarp, atalk, aarp, decnet, sca, lat, mopdl, moprc, iso, stp, ipx, or netbeui 这里面的其中一个。\n而 protochain 后面跟的 protocol 要求就没有那么严格，它可以是任意词，只要 tcpdump 的 IP 报文头部里的 protocol 字段为 &lt;protocol&gt; 就能匹配上。\n理论上来讲，下面两种写法效果是一样的\n$ tcpdump &#x27;ip &amp;&amp; tcp&#x27;$ tcpdump &#x27;ip proto tcp&#x27;\n\n同样的，这两种写法也是一样的\n$ tcpdump &#x27;ip6 &amp;&amp; tcp&#x27;$ tcpdump &#x27;ip6 proto tcp&#x27;\n\n4. 可选参数解析4.1 设置不解析域名提升速度\n-n：不把ip转化成域名，直接显示 ip，避免执行 DNS lookups 的过程，速度会快很多\n-nn：不把协议和端口号转化成名字，速度也会快很多。\n-N：不打印出host 的域名部分.。比如,，如果设置了此选现，tcpdump 将会打印’nic’ 而不是 ‘nic.ddn.mil’.\n\n4.2 过滤结果输出到文件使用 tcpdump 工具抓到包后，往往需要再借助其他的工具进行分析，比如常见的 wireshark 。\n而要使用wireshark ，我们得将 tcpdump 抓到的包数据生成到文件中，最后再使用 wireshark 打开它即可。\n使用 -w 参数后接一个以 .pcap 后缀命令的文件名，就可以将 tcpdump 抓到的数据保存到文件中。\n$ tcpdump icmp -w icmp.pcap\n\n4.3 从文件中读取包数据使用 -w 是写入数据到文件，而使用 -r 是从文件中读取数据。\n读取后，我们照样可以使用上述的过滤器语法进行过滤分析。\n$ tcpdump icmp -r all.pcap\n\n4.4 控制详细内容的输出\n-v：产生详细的输出. 比如包的TTL，id标识，数据包长度，以及IP包的一些选项。同时它还会打开一些附加的包完整性检测，比如对IP或ICMP包头部的校验和。\n-vv：产生比-v更详细的输出. 比如NFS回应包中的附加域将会被打印, SMB数据包也会被完全解码。（摘自网络，目前我还未使用过）\n-vvv：产生比-vv更详细的输出。比如 telent 时所使用的SB, SE 选项将会被打印, 如果telnet同时使用的是图形界面，其相应的图形选项将会以16进制的方式打印出来（摘自网络，目前我还未使用过）\n\n4.5 控制时间的显示\n-t：在每行的输出中不输出时间\n-tt：在每行的输出中会输出时间戳\n-ttt：输出每两行打印的时间间隔(以毫秒为单位)\n-tttt：在每行打印的时间戳之前添加日期的打印（此种选项，输出的时间最直观）\n\n4.6 显示数据包的头部\n-x：以16进制的形式打印每个包的头部数据（但不包括数据链路层的头部）\n-xx：以16进制的形式打印每个包的头部数据（包括数据链路层的头部）\n-X：以16进制和 ASCII码形式打印出每个包的数据(但不包括连接层的头部)，这在分析一些新协议的数据包很方便。\n-XX：以16进制和 ASCII码形式打印出每个包的数据(包括连接层的头部)，这在分析一些新协议的数据包很方便。\n\n4.7 过滤指定网卡的数据包\n-i：指定要过滤的网卡接口，如果要查看所有网卡，可以 -i any\n\n4.8 过滤特定流向的数据包\n-Q： 选择是入方向还是出方向的数据包，可选项有：in, out, inout，也可以使用 –direction=[direction] 这种写法\n\n4.9 其他常用的一些参数\n-A：以ASCII码方式显示每一个数据包(不显示链路层头部信息). 在抓取包含网页数据的数据包时, 可方便查看数据\n-l : 基于行的输出，便于你保存查看，或者交给其它工具分析\n-q : 简洁地打印输出。即打印很少的协议相关信息, 从而输出行都比较简短.\n-c : 捕获 count 个包 tcpdump 就退出\n-s : tcpdump 默认只会截取前 96 字节的内容，要想截取所有的报文内容，可以使用 -s number， number 就是你要截取的报文字节数，如果是 0 的话，表示截取报文全部内容。\n-S : 使用绝对序列号，而不是相对序列号\n-C：file-size，tcpdump 在把原始数据包直接保存到文件中之前, 检查此文件大小是否超过file-size. 如果超过了, 将关闭此文件,另创一个文件继续用于原始数据包的记录. 新创建的文件名与-w 选项指定的文件名一致, 但文件名后多了一个数字.该数字会从1开始随着新创建文件的增多而增加. file-size的单位是百万字节(nt: 这里指1,000,000个字节,并非1,048,576个字节, 后者是以1024字节为1k, 1024k字节为1M计算所得, 即1M=1024 ＊ 1024 ＝ 1,048,576)\n-F：使用file 文件作为过滤条件表达式的输入, 此时命令行上的输入将被忽略.\n\n4.10 对输出内容进行控制的参数\n-D : 显示所有可用网络接口的列表\n-e : 每行的打印输出中将包括数据包的数据链路层头部信息\n-E : 揭秘IPSEC数据\n-L ：列出指定网络接口所支持的数据链路层的类型后退出\n-Z：后接用户名，在抓包时会受到权限的限制。如果以root用户启动tcpdump，tcpdump将会有超级用户权限。\n-d：打印出易读的包匹配码\n-dd：以C语言的形式打印出包匹配码.\n-ddd：以十进制数的形式打印出包匹配码\n\n5. 过滤规则组合有编程基础的同学，对于下面三个逻辑运算符应该不陌生了吧\n\nand：所有的条件都需要满足，也可以表示为 &amp;&amp;\nor：只要有一个条件满足就可以，也可以表示为 ||\nnot：取反，也可以使用 !\n\n举个例子，我想需要抓一个来自10.5.2.3，发往任意主机的3389端口的包\n$ tcpdump src 10.5.2.3 and dst port 3389\n\n当你在使用多个过滤器进行组合时，有可能需要用到括号，而括号在 shell 中是特殊符号，因为你需要使用引号将其包含。例子如下：\n$ tcpdump &#x27;src 10.0.2.4 and (dst port 3389 or 22)&#x27;\n\n而在单个过滤器里，常常会判断一条件是否成立，这时候，就要使用下面两个符号\n\n=：判断二者相等\n==：判断二者相等\n!=：判断二者不相等\n\n当你使用这两个符号时，tcpdump 还提供了一些关键字的接口来方便我们进行判断，比如\n\nif：表示网卡接口名、\nproc：表示进程名\npid：表示进程 id\nsvc：表示 service class\ndir：表示方向，in 和 out\neproc：表示 effective process name\nepid：表示 effective process ID\n\n比如我现在要过滤来自进程名为 nc 发出的流经 en0 网卡的数据包，或者不流经 en0 的入方向数据包，可以这样子写\n$ tcpdump &quot;( if=en0 and proc =nc ) || (if != en0 and dir=in)&quot;\n\n6. 特殊过滤规则5.1 根据 tcpflags 进行过滤通过上一篇文章，我们知道了 tcp 的首部有一个标志位。\n\ntcpdump 支持我们根据数据包的标志位进行过滤\nproto [ expr:size ]\n\n\nproto：可以是熟知的协议之一（如ip，arp，tcp，udp，icmp，ipv6）\nexpr：可以是数值，也可以是一个表达式，表示与指定的协议头开始处的字节偏移量。\nsize：是可选的，表示从字节偏移量开始取的字节数量。\n\n接下来，我将举几个例子，让人明白它的写法，不过在那之前，有几个点需要你明白，这在后面的例子中会用到：\n1、tcpflags 可以理解为是一个别名常量，相当于 13，它代表着与指定的协议头开头相关的字节偏移量，也就是标志位，所以 tcp[tcpflags] 等价于 tcp[13] ，对应下图中的报文位置。\n\n2、tcp-fin, tcp-syn, tcp-rst, tcp-push, tcp-ack, tcp-urg 这些同样可以理解为别名常量，分别代表 1，2，4，8，16，32，64。这些数字是如何计算出来的呢？\n以 tcp-syn 为例，你可以参照下面这张图，计算出来的值 是就是 2\n\n由于数字不好记忆，所以一般使用这样的“别名常量”表示。\n因此当下面这个表达式成立时，就代表这个包是一个 syn 包。\ntcp[tcpflags] == tcp-syn\n\n要抓取特定数据包，方法有很多种。\n下面以最常见的 syn包为例，演示一下如何用 tcpdump 抓取到 syn 包，而其他的类型的包也是同样的道理。\n据我总结，主要有三种写法：\n1、第一种写法：使用数字表示偏移量\n$ tcpdump -i eth0 &quot;tcp[13] &amp; 2 != 0&quot; \n\n2、第二种写法：使用别名常量表示偏移量\n$ tcpdump -i eth0 &quot;tcp[tcpflags] &amp; tcp-syn != 0&quot; \n\n3、第三种写法：使用混合写法\n$ tcpdump -i eth0 &quot;tcp[tcpflags] &amp; 2 != 0&quot; # or$ tcpdump -i eth0 &quot;tcp[13] &amp; tcp-syn != 0&quot; \n\n如果我想同时捕获多种类型的包呢，比如 syn + ack 包\n1、第一种写法\n$ tcpdump -i eth0 &#x27;tcp[13] == 2 or tcp[13] == 16&#x27;\n\n2、第二种写法\n$ tcpdump -i eth0 &#x27;tcp[tcpflags] == tcp-syn or tcp[tcpflags] == tcp-ack&#x27;\n\n3、第三种写法\n$ tcpdump -i eth0 &quot;tcp[tcpflags] &amp; (tcp-syn|tcp-ack) != 0&quot; \n\n4、第四种写法：注意这里是 单个等号，而不是像上面一样两个等号，18（syn+ack） = 2（syn） + 16（ack）\n$ tcpdump -i eth0 &#x27;tcp[13] = 18&#x27;# or$ tcpdump -i eth0 &#x27;tcp[tcpflags] = 18&#x27;\n\ntcp 中有 类似 tcp-syn 的别名常量，其他协议也是有的，比如 icmp 协议，可以使用的别名常量有\nicmp-echoreply, icmp-unreach, icmp-sourcequench, icmp-redirect, icmp-echo, icmp-routeradvert,icmp-routersolicit, icmp-timx-ceed, icmp-paramprob, icmp-tstamp, icmp-tstampreply,icmp-ireq, icmp-ireqreply, icmp-maskreq, icmp-maskreply\n\n5.2 基于包大小进行过滤若你想查看指定大小的数据包，也是可以的\n$ tcpdump less 32 $ tcpdump greater 64 $ tcpdump &lt;= 128\n\n5.3 根据 mac 地址进行过滤例子如下，其中 ehost 是记录在 /etc/ethers 里的 name\n$ tcpdump ether host [ehost]$ tcpdump ether dst\t[ehost]$ tcpdump ether src\t[ehost]\n\n5.4 过滤通过指定网关的数据包$ tcpdump gateway [host]\n\n5.5 过滤广播/多播数据包$ tcpdump ether broadcast$ tcpdump ether multicast$ tcpdump ip broadcast$ tcpdump ip multicast$ tcpdump ip6 multicast\n\n7. 如何抓取到更精准的包？先给你抛出一个问题：如果我只想抓取 HTTP 的 POST 请求该如何写呢？\n如果只学习了上面的内容，恐怕你还是无法写法满足这个抓取需求的过滤器。\n在学习之前，我先给出答案，然后再剖析一下，这个过滤器是如何生效的，居然能让我们对包内的内容进行判断。\n$ tcpdump -s 0 -A -vv &#x27;tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4]&#x27;\n\n命令里的可选参数，在前面的内容里已经详细讲过了。这里不再细讲。\n本节的重点是引号里的内容，看起来很复杂的样子。\n将它逐一分解，我们只要先理解了下面几种用法，就能明白\n\ntcp[n]：表示 tcp 报文里 第 n 个字节\n\ntcp[n:c]：表示 tcp 报文里从第n个字节开始取 c 个字节，tcp[12:1] 表示从报文的第12个字节（因为有第0个字节，所以这里的12其实表示的是13）开始算起取一个字节，也就是 8 个bit。查看 tcp 的报文首部结构，可以得知这 8 个bit 其实就是下图中的红框圈起来的位置，而在这里我们只要前面 4个bit，也就是实际数据在整个报文首部中的偏移量。\n\n\n&amp;：是位运算里的 and 操作符，比如 0011 &amp; 0010 = 0010\n\n&gt;&gt;：是位运算里的右移操作，比如 0111 &gt;&gt; 2 = 0001\n\n0xf0：是 10 进制的 240 的 16 进制表示，但对于位操作来说，10进制和16进制都将毫无意义，我们需要的是二进制，将其转换成二进制后是：11110000，这个数有什么特点呢？前面个 4bit 全部是 1，后面4个bit全部是0，往后看你就知道这个特点有什么用了。\n\n\n分解完后，再慢慢合并起来看\n1、tcp[12:1] &amp; 0xf0 其实并不直观，但是我们将它换一种写法，就好看多了，假设 tcp 报文中的 第12 个字节是这样组成的 10110000，那么这个表达式就可以变成 10110110 &amp;&amp; 11110000 = 10110000，得到了 10110000 后，再进入下一步。\n2、tcp[12:1] &amp; 0xf0) &gt;&gt; 2 ：如果你不理解 tcp 报文首部里的数据偏移，请先点击这个前往我的上一篇文章，搞懂数据偏移的意义，否则我保证你这里会绝对会听懵了。\ntcp[12:1] &amp; 0xf0) &gt;&gt; 2 这个表达式实际是 (tcp[12:1] &amp; 0xf0) &gt;&gt; 4 ) &lt;&lt; 2 的简写形式。所以要搞懂 tcp[12:1] &amp; 0xf0) &gt;&gt; 2 只要理解了(tcp[12:1] &amp; 0xf0) &gt;&gt; 4 ) &lt;&lt; 2 就行了 。\n从上一步我们算出了 tcp[12:1] &amp; 0xf0 的值其实是一个字节，也就是 8 个bit，但是你再回去看下上面的 tcp 报文首部结构图，表示数据偏移量的只有 4个bit，也就是说 上面得到的值 10110000，前面 4 位（1011）才是正确的偏移量，那么为了得到 1011，只需要将 10110000 右移4位即可，也就是 tcp[12:1] &amp; 0xf0) &gt;&gt; 4，至此我们是不是已经得出了实际数据的正确位置呢，很遗憾还没有，前一篇文章里我们讲到 Data Offset 的单位是 4个字节，因为要将 1011 乘以 4才可以，除以4在位运算中相当于左移2位，也就是 &lt;&lt;2，与前面的 &gt;&gt;4 结合起来一起算的话，最终的运算可以简化为 &gt;&gt;2。\n至此，我们终于得出了实际数据开始的位置是 tcp[12:1] &amp; 0xf0) &gt;&gt; 2 （单位是字节）。\n找到了数据的起点后，可别忘了我们的目的是从数据中打到 HTTP 请求的方法，是 GET 呢 还是 POST ，或者是其他的？\n有了上面的经验，我们自然懂得使用 tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] 从数据开始的位置再取出四个字节，然后将结果与 GET （注意 GET最后还有个空格）的 16进制写法（也就是 0x47455420）进行比对。\n0x47   --&gt;   71    --&gt;  G0x45   --&gt;   69    --&gt;  E0x54   --&gt;   84    --&gt;  T0x20   --&gt;   32    --&gt;  空格\n\n\n如果相等，则该表达式为True，tcpdump 认为这就是我们所需要抓的数据包，将其输出到我们的终端屏幕上。\n8. 抓包实战应用例子8.1 提取 HTTP 的 User-Agent从 HTTP 请求头中提取 HTTP 的 User-Agent：\n$ tcpdump -nn -A -s1500 -l | grep &quot;User-Agent:&quot;\n\n通过 egrep 可以同时提取User-Agent 和主机名（或其他头文件）：\n$ tcpdump -nn -A -s1500 -l | egrep -i &#x27;User-Agent:|Host:&#x27;\n\n8.2 抓取 HTTP GET 和 POST 请求抓取 HTTP GET 请求包：\n$ tcpdump -s 0 -A -vv &#x27;tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x47455420&#x27;# or$ tcpdump -vvAls0 | grep &#x27;GET&#x27;\n\n可以抓取 HTTP POST 请求包：\n$ tcpdump -s 0 -A -vv &#x27;tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x504f5354&#x27;# or $ tcpdump -vvAls0 | grep &#x27;POST&#x27;\n\n注意：该方法不能保证抓取到 HTTP POST 有效数据流量，因为一个 POST 请求会被分割为多个 TCP 数据包。\n8.3 找出发包数最多的 IP找出一段时间内发包最多的 IP，或者从一堆报文中找出发包最多的 IP，可以使用下面的命令：\n$ tcpdump -nnn -t -c 200 | cut -f 1,2,3,4 -d &#x27;.&#x27; | sort | uniq -c | sort -nr | head -n 20\n\n\ncut -f 1,2,3,4 -d ‘.’ : 以 . 为分隔符，打印出每行的前四列。即 IP 地址。\nsort | uniq -c : 排序并计数\nsort -nr : 按照数值大小逆向排序\n\n8.4 抓取 DNS 请求和响应DNS 的默认端口是 53，因此可以通过端口进行过滤\n$ tcpdump -i any -s0 port 53\n\n8.5 切割 pcap 文件当抓取大量数据并写入文件时，可以自动切割为多个大小相同的文件。例如，下面的命令表示每 3600 秒创建一个新文件 capture-(hour).pcap，每个文件大小不超过 200*1000000 字节：\n$ tcpdump  -w /tmp/capture-%H.pcap -G 3600 -C 200\n\n这些文件的命名为 capture-&#123;1-24&#125;.pcap，24 小时之后，之前的文件就会被覆盖。\n8.6 提取 HTTP POST 请求中的密码从 HTTP POST 请求中提取密码和主机名：\n$ tcpdump -s 0 -A -n -l | egrep -i &quot;POST /|pwd=|passwd=|password=|Host:&quot;\n\n8.7 提取 HTTP 请求的 URL提取 HTTP 请求的主机名和路径：\n$ tcpdump -s 0 -v -n -l | egrep -i &quot;POST /|GET /|Host:&quot;\n\n8.8 抓取 HTTP 有效数据包抓取 80 端口的 HTTP 有效数据包，排除 TCP 连接建立过程的数据包（SYN / FIN / ACK）：\n$ tcpdump &#x27;tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)&#x27;\n\n8.9 结合 Wireshark 进行分析通常 Wireshark（或 tshark）比 tcpdump 更容易分析应用层协议。一般的做法是在远程服务器上先使用 tcpdump 抓取数据并写入文件(后缀为pacp)，然后再将文件拷贝到本地工作站上用 Wireshark 分析。\n还有一种更高效的方法，可以通过 ssh 连接将抓取到的数据实时发送给 Wireshark 进行分析。以 MacOS 系统为例，可以通过 brew cask install wireshark 来安装，然后通过下面的命令来分析：\n$ ssh root@remotesystem &#x27;tcpdump -s0 -c 1000 -nn -w - not port 22&#x27; | /Applications/Wireshark.app/Contents/MacOS/Wireshark -k -i -\n\n例如，如果想分析 DNS 协议，可以使用下面的命令：\n$ ssh root@remotesystem &#x27;tcpdump -s0 -c 1000 -nn -w - port 53&#x27; | /Applications/Wireshark.app/Contents/MacOS/Wireshark -k -i -\n\n抓取到的数据：\n\n-c 选项用来限制抓取数据的大小。如果不限制大小，就只能通过 ctrl-c 来停止抓取，这样一来不仅关闭了 tcpdump，也关闭了 wireshark。\n到这里，我已经将我所知道的 tcpdump 的用法全部说了一遍，如果你有认真地看完本文，相信会有不小的收获，掌握一个上手的抓包工具，对于以后我们学习网络、分析网络协议、以及定位网络问题，会很有帮助，而 tcpdump 是我推荐的一个抓包工具。\n9. 参考文章\nFreeBSD Manual Pages About tcpdump\nLinux tcpdump命令详解\n一份快速实用的 tcpdump 命令参考手册\n超详细的网络抓包神器 tcpdump 使用指南\n[译]tcpdump 示例教程\n[英]tcpdump 示例教程\n\n","categories":["运维","Linux"]},{"title":"traceroute 查看路由链路","url":"/posts/14443/","content":"traceroute（跟踪路由）是路由跟踪程序，用于确定 IP 数据报访问目标所经过的路径。traceroute 命令用 IP 存活时间 (TTL) 字段和 ICMP 错误消息来确定从一个主机到网络上其他主机的路由。\n通过 traceroute 命令可以知道数据包从你的计算机到互联网另一端的主机是走的什么路径。当然每次数据包由某一同样的出发点（source）到达某一同样的目的地 (destination) 走的路径可能会不一样，但大部分时候所走的路由是相同的。\nLinux 系统中是 traceroute, 在 Windows 中为 tracert。traceroute 通过发送小数据包到目的主机直到其返回，来测量其耗时。一条路径上的每个设备 traceroute 要测 3 次。输出结果中包括每次测试的时间 (ms) 和设备的名称及其 IP 地址。\n在大多数情况下，在 Linux 主机系统下，直接执行命令行：\ntraceroute hostname\n\n使用命令格式：\ntraceroute options host\n\n命令功能：\ntraceroute 预设数据包大小是 40Bytes，可设置。\n具体参数格式：\ntraceroute [-dFlnrvx][-f 存活数值][-g 网关...][-i 网络界面][-m 存活数值][-p 通信端口][-s 来源地址][-t 服务类型][-w 超时秒数][主机名称或 IP 地址] 数据包大小\n\n命令参数：\n-d 使用 Socket 层级的排错功能。-f 设置第一个检测数据包的存活数值 TTL 的大小。-F 设置勿离断位。-g 设置来源路由网关，最多可设置 8 个。-i 使用指定的网络界面送出数据包。-I 使用 ICMP 回应取代 UDP 资料信息。-m 设置检测数据包的最大存活数值 TTL 的大小。-n 直接使用 IP 地址而非主机名称。-p 设置 UDP 传输协议的通信端口。-r 忽略普通的 Routing Table，直接将数据包送到远端主机上。-s 设置本地主机送出数据包的 IP 地址。-t 设置检测数据包的 TOS 数值。-v 详细显示指令的执行过程。-w 设置等待远端主机回报的时间。-x 开启或关闭数据包的正确性检验。\n\n实例最常用法直接追踪路由\ntraceroute ip_or_host\n\n结果说明：\ntraceroute to 180.149.128.9 (180.149.128.9), 30 hops max, 32 byte packets 1  209.17.118.3  0.30 ms  AS59253  Singapore, greenserver.io 2  23.106.255.6  0.68 ms  AS59253  Singapore, leaseweb.com 3  23.106.255.198  1.38 ms  AS59253  Singapore, leaseweb.com 4  204.130.243.4  1.26 ms  *  United States 5  154.54.45.193  178.34 ms  AS174  United States, California, Los Angeles, cogentco.com 6  38.142.238.34  179.28 ms  AS174  United States, California, Los Angeles, cogentco.com 7  202.97.59.141  334.31 ms  AS4134  China, Beijing, ChinaTelecom 8  202.97.12.117  328.51 ms  AS4134  China, Beijing, ChinaTelecom 9  *10  *11  *12  180.149.128.9  342.12 ms  AS23724  China, Beijing, ChinaTelecom\n\n序列号从 1 开始，每条纪录就是一跳，每一跳表示一个网关，每行有三个时间，单位都是 ms，其实就是 -q 的默认参数。探测数据包向每个网关发送三个数据包后，网关响应后返回的时间；如果您用 traceroute -q 10 google.com ，表示向每个网关发送 10 个数据包。\ntraceroute 一台主机有时会看到一些行以星号表示，出现这样的情况，可能是防火墙封掉了 ICMP 的返回信息，得不到什么相关的数据包返回数据。\n有时在某一网关处延时比较长，可能是某台网关比较阻塞，也可能是物理设备本身的原因。当然如果某台 DNS 出现问题时，不能解析主机名、域名时，也会有延时长的现象；您可以加 -n 参数来避免 DNS 解析，以 IP 输出数据。\n如果在局域网中的不同网段之间，可以通过 traceroute 来排查问题所在，是主机的问题还是网关的问题。如果通过远程来访问某台服务器遇到问题时，用到 traceroute 追踪数据包所经过的网关，提交 IDC 服务商，也有助于解决问题；但目前看来在国内解决这样的问题是比较困难的，即使发现问题，IDC 服务商也不可能帮助解决。\n跳数设置traceroute -m 10 google.com\n\n不解析主机名traceroute -n google.com\n\n设置探测包数量traceroute -q 4 google.com\n\n绕过正常的路由表直接发送到网络相连的主机traceroute -r douban.com\n\n工作原理traceroute 命令利用 ICMP 及 IP header 的 TTL(Time To Live) 字段 (field)。\n\ntraceroute 送出一个 TTL 是 1 的 IP datagram 到目的地（每次送出的为 3 个 40 字节的包，包括源地址，目的地址和包发出的时间），当路径上的第一个路由器 (router) 收到这个 datagram 时，它将 TTL 减 1。此时，TTL 变为 0 了，所以该路由器会将此 datagram 丢掉，并送回一个「ICMP time exceeded」消息（包括发 IP 包的源地址，IP 包的所有内容及路由器的 IP 地址），traceroute 收到这个消息后，便知道这个路由器存在于这个路径上\n接着 traceroute 再送出另一个 TTL 是 2 的 datagram，发现第 2 个路由器\n……\ntraceroute 每次将送出的 datagram 的 TTL 加 1 来发现另一个路由器，这个重复的动作一直持续到某个 datagram 抵达目的地。当 datagram 到达目的地后，该主机并不会送回 ICMP time exceeded 消息，因为它已是目的地了。\ntraceroute 如何得知目的地到达了呢？traceroute 在送出 UDP datagrams 到目的地时，它所选择送达的 port number 是一个一般应用程序都不会用的端口 (30000 以上），所以当此 UDP datagram 到达目的地后该主机会回送一个 (ICMP port unreachable) 的消息，而当 traceroute 收到这个消息时，便知道目的地已经到达了。所以 traceroute 在 Server 端也是没有所谓的 Daemon 程式。\n\ntraceroute 提取发 ICMP TTL 到期消息设备的 IP 地址并作域名解析。每次 traceroute 都打印出一系列数据，包括所经过的路由设备的域名及 IP 地址，三个包每次来回所花时间。\n","categories":["运维","Linux"]},{"title":"xargs 命令","url":"/posts/34285/","content":"xargs（英文全拼： eXtended ARGuments）是给命令传递参数的一个过滤器，也是组合多个命令的一个工具。\nxargs 可以将管道或标准输入（stdin）数据转换成命令行参数，也能够从文件的输出中读取数据。\nxargs 也可以将单行或多行文本输入转换为其他格式，例如多行变单行，单行变多行。\nxargs 默认的命令是 echo，这意味着通过管道传递给 xargs 的输入将会包含换行和空白，不过通过 xargs 的处理，换行和空白将被空格取代。\nxargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。\n之所以能用到这个命令，关键是由于很多命令不支持|管道来传递参数，而日常工作中有有这个必要，所以就有了 xargs 命令，例如：\nfind /sbin -perm +700 |ls -l       #这个命令是错误的find /sbin -perm +700 |xargs ls -l   #这样才是正确的\n\nxargs 一般是和管道一起使用。\n命令格式：\nsomecommand |xargs -item  command\n\n参数：\n\n-a file 从文件中读入作为 stdin\n-e flag ，注意有的时候可能会是-E，flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止。\n-p 当每次执行一个argument的时候询问一次用户。\n-n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。\n-t 表示先打印命令，然后再执行。\n-i 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给 {}，可以用 {} 代替。\n-r no-run-if-empty 当xargs的输入为空的时候则停止xargs，不用再去执行了。\n-s num 命令行的最大字符数，指的是 xargs 后面那个命令的最大命令行字符数。\n-L num 从标准输入一次读取 num 行送给 command 命令。\n-l 同 -L。\n-d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符。\n-x exit的意思，主要是配合-s使用。。\n-P 修改最大的进程数，默认是1，为0时候为as many as it can ，这个例子我没有想到，应该平时都用不到的吧。\n\n实例xargs 用作替换工具，读取输入数据重新格式化后输出。\n定义一个测试文件，内有多行文本数据：\n# cat test.txta b c d e f gh i j k l m no p qr s tu v w x y z\n\n多行输入单行输出：\n# cat test.txt | xargsa b c d e f g h i j k l m n o p q r s t u v w x y z\n\n-n 选项多行输出：\n# cat test.txt | xargs -n3a b cd e fg h ij k lm n op q rs t uv w xy z\n\n-d 选项可以自定义一个定界符：\n# echo &quot;nameXnameXnameXname&quot; | xargs -dXname name name name\n\n结合 -n 选项使用：\n# echo &quot;nameXnameXnameXname&quot; | xargs -dX -n2name namename name\n\n读取 stdin，将格式化后的参数传递给命令\n假设一个命令为 sk.sh 和一个保存参数的文件 arg.txt：\n#!/bin/bash#sk.sh命令内容，打印出所有参数。echo $*\n\narg.txt文件内容：\n# cat arg.txtaaabbbccc\n\nxargs 的一个选项 -I，使用 -I 指定一个替换字符串 {}，这个字符串在 xargs 扩展时会被替换掉，当 -I 与 xargs 结合使用，每一个参数命令都会被执行一次：\n# cat arg.txt | xargs -I &#123;&#125; ./sk.sh -p &#123;&#125; -l-p aaa -l-p bbb -l-p ccc -l\n\n复制所有图片文件到 /data/images 目录下：\nls *.jpg | xargs -n1 -I &#123;&#125; cp &#123;&#125; /data/images\n\nxargs 结合 find 使用\n用 rm 删除太多的文件时候，可能得到一个错误信息：**/bin/rm Argument list too long.** 用 xargs 去避免这个问题：\nfind . -type f -name &quot;*.log&quot; -print0 | xargs -0 rm -f\n\nxargs -0 将 \\0 作为定界符。\n统计一个源代码目录中所有 php 文件的行数：\nfind . -type f -name &quot;*.php&quot; -print0 | xargs -0 wc -l\n\n查找所有的 jpg 文件，并且压缩它们：\nfind . -type f -name &quot;*.jpg&quot; -print | xargs tar -czvf images.tar.gz\n\nxargs 其他应用\n假如你有一个文件包含了很多你希望下载的 URL，你能够使用 xargs下载所有链接：\n# cat url-list.txt | xargs wget -c","categories":["运维","Linux"],"tags":["Linux 常用命令"]},{"title":"Docker 安装Mysql","url":"/posts/15647/","content":"1.拉取镜像docker pull --platform linux/x86_64 mysql\n\n2.启动容器docker run -d --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 --platform linux/x86_64 mysql","categories":["MySQL"]},{"title":"Mac Navicat 无限试用","url":"/posts/20745/","content":"\n仅供个人学习使用，严禁商业用途\n\n一、Navicat 151. 删除注册表# 查看注册表plutil -p ~/Library/Preferences/com.prect.NavicatPremium15.plist# 删除属性plutil -remove 91F6C435D172C8163E0689D3DAD3F3E9 ~/Library/Preferences/com.prect.NavicatPremium15.plist\n\n2. 删除隐藏文件cd ~/Library/Application\\ Support/PremiumSoft\\ CyberTech/Navicat\\ CC/Navicat\\ Premiumls -al# 删除与该文件名相似的文件rm -rf .*\n\n二、Navicat 161. 删除注册表plutil -remove 91F6C435D172C8163E0689D3DAD3F3E9 ~/Library/Preferences/com.navicat.NavicatPremium.plistplutil -remove B966DBD409B87EF577C9BBF3363E9614 ~/Library/Preferences/com.navicat.NavicatPremium.plist\n\n2. 删除隐藏文件rm -rf ~/Library/Application\\ Support/PremiumSoft\\ CyberTech/Navicat\\ CC/Navicat\\ Premium/.*\n\n三、脚本#!/bin/bashset -efile=$(defaults read /Applications/Navicat\\ Premium.app/Contents/Info.plist)regex=&quot;CFBundleShortVersionString = \\&quot;([^\\.]+)&quot;[[ $file =~ $regex ]]version=$&#123;BASH_REMATCH[1]&#125;echo &quot;Detected Navicat Premium version $version&quot;case $version in    &quot;16&quot;)        file=~/Library/Preferences/com.navicat.NavicatPremium.plist        ;;    &quot;15&quot;)        file=~/Library/Preferences/com.prect.NavicatPremium15.plist        ;;    *)        echo &quot;Version &#x27;$version&#x27; not handled&quot;        exit 1       ;;esacecho -n &quot;Reseting trial time...&quot;regex=&quot;([0-9A-Z]&#123;32&#125;) = &quot;[[ $(defaults read $file) =~ $regex ]]hash=$&#123;BASH_REMATCH[1]&#125;if [ ! -z $hash ]; then    defaults delete $file $hashfiregex=&quot;\\.([0-9A-Z]&#123;32&#125;)&quot;[[ $(ls -a ~/Library/Application\\ Support/PremiumSoft\\ CyberTech/Navicat\\ CC/Navicat\\ Premium/ | grep &#x27;^\\.&#x27;) =~ $regex ]]hash2=$&#123;BASH_REMATCH[1]&#125;if [ ! -z $hash2 ]; then    rm ~/Library/Application\\ Support/PremiumSoft\\ CyberTech/Navicat\\ CC/Navicat\\ Premium/.$hash2fiecho &quot; Done&quot;\n\n","categories":["MySQL"]},{"title":"Maxwell 基于 Kafka 实现 MySQL CDC","url":"/posts/8994/","content":"Maxwell Github 仓库\nMaxwell 官方文档\nDocker 运行 Maxwelldocker run -d --rm --link mysql --link kafka --name maxwell zendesk/maxwell bin/maxwell \\    --user=root \\    --password=123456 \\    --host=mysql \\    --producer=kafka \\    --kafka.bootstrap.servers=kafka:9092 \\    --kafka_topic=maxwell \\    --filter=&quot;exclude: *.*, include: jd_test.*&quot; \\    --output_ddl\n\n","categories":["MySQL"]},{"title":"MySQL Explain","url":"/posts/45366/","content":"1.typeSystem表数据已经被加载到内存中，不需要进行磁盘IO的SQL\nconst\n命中主键或者唯一索引\n判断条件为常量值\n\n示例：id 为主键\nexplain select * from test where id = 1;\nname 为唯一索引\nexplain select * from test where name = &#x27;12&#x27;;\n\neq_ref对于前表的每一行，后表只有一行被扫描。即\n\njoin查询\n连接时命中 主键 或 非空唯一索引\n等值连接\n\n示例：test 表为 eq_ref\nexplain select * from test1 t1 left join test t on t1.test_id = t.id\n\nref字段索引为 非唯一索引 ，即同一索引值可能扫描到多条记录。\n示例：name 为普通索引\nexplain select * from test where name = &#x27;12&#x27;;\n\nrange索引上的范围查询，如 between、in、&gt;、&lt; 都是范围查询\n示例：name 为普通索引\nexplain select * from test where name like &quot;12%&quot;;explain select * from test where name != &#x27;12&#x27;;\nid 为主键\nexplain select * from test where id &gt; 1;explain select * from test where id != 1;\n\nindex需要扫描 索引 上的全部数据，例如 select count(*) from user;\nall没有索引或索引失效，全表扫描。\n示例：name 为普通索引\nexplain select * from test where name like &quot;%12&quot;;\n\nid 不是主键，无任何索引\nexplain select * from user where id = 1;","categories":["MySQL"]},{"title":"MySQL MVCC","url":"/posts/9873/","content":"Mysql 有 读未提交（RU）、读已提交（RC）、可重复读（RR）、序列化，四种隔离级别，MVCC可以作用在 读已提交 和 可重复读 两种隔离级别上，同时在 可重复读 级别上解决了幻读。\n1.隐藏字段在每一行数据中，会有三个隐藏字段 db_row_id、data_trx_id、data_roll_ptr。\n\ndb_row_id：在没有设置主键，没有唯一索引的情况下作为隐藏行号，实现索引的；\n\ndata_trx_id：最近一次对该行更新的事务的id；\n\ndata_roll_ptr：回滚指针，指向其上一个版本的 undo log。\n\n\n当对一条数据更新后，会将其原本的数据生成一条 undo log，然后将 DATA_ROLL_PTR 指向该 undo log ，形成一条版本链，方便回滚和MVCC 查找可访问的数据版本。\n2.ReadView在 ReadView 中有以下四个主要属性：\n\nmIds：生成 ReadView 时，所有正在活跃（未提交的）的事务id；\nmin_trx_id：mIds 中最小的事务id；\nmax_trx_id：生成 ReadView 时，最大的事务id，注意 不是正在活跃的最大事务id（或者说mIds中的最大id）。因为先提交的事务不一定先结束，可能当前最大事务id为100，而 mIds 中最大的事务id是98，也就是说id为98的事务还未提交，而id为99和100的事务已经提交了；\ncreator_trx_id：创建该 ReadView 的事务id。\n\n举个栗子：\n如果 mIds = &#123;5, 6, 7, 11, 13&#125;，那么 min_trx_id = 5，max_trx_id &gt; 13 或 max_trx_id = 13。\n在 RC 级别，MVCC 会在每次读时生成一个新的 ReadView；\n在 RR 级别，MVCC 只会在第一次读时生成 ReadView，当前事务之后都使用这同一个 ReadView。\n3.访问规则\n如果 data_trx_id 小于 min_trx_id ，说明生成当前版本的事务在生成 ReadView 之前就已经提交了，那么当前事务可以访问该版本数据。\n如果 data_trx_id 大于 max_trx_id 说明当前版本的数据是在生成 ReadView 之后才生成的，所以当前事务不可访问该版本数据。\n如果 data_trx_id 在 mIds 中，即大于等于 min_trx_id 且小于等于 mIds 中的最大事务id（注意不是 max_trx_id），说明创建该 ReadView 时，生成该版本数据的事务还未提交，所以不能访问。\n如果 data_trx_id 大于  mIds 中的最大事务id且小于等于 max_trx_id ，说明创建该 ReadView 时，生成该版本数据的事务已经提交，所以可以访问。\n\n当发现当前版本数据无法访问时，可以通过 data_roll_ptr 对 undo log 进行遍历，根据以上规则找到可以访问的数据版本。\n","categories":["MySQL"]},{"title":"MySQL sql优化","url":"/posts/15841/","content":"对于MySQL层优化我一般遵从五个原则：\n\n减少数据访问：设置合理的字段类型，启用压缩，通过索引访问等减少磁盘IO\n返回更少的数据：只返回需要的字段和数据分页处理 减少磁盘io及网络io\n减少交互次数：批量DML操作，函数存储等减少数据连接次数\n减少服务器CPU开销：尽量减少数据库排序操作以及全表查询，减少cpu 内存占用\n利用更多资源：使用表分区，可以增加并行操作，更大限度利用cpu资源\n\n总结到SQL优化中，就三点:\n\n最大化利用索引；\n尽可能避免全表扫描；\n减少无效数据的查询；\n\n一、SQL 执行顺序SELECT语句 - 语法顺序：1. SELECT 2. DISTINCT &lt;select_list&gt;3. FROM &lt;left_table&gt;4. &lt;join_type&gt; JOIN &lt;right_table&gt;5. ON &lt;join_condition&gt;6. WHERE &lt;where_condition&gt;7. GROUP BY &lt;group_by_list&gt;8. HAVING &lt;having_condition&gt;9. ORDER BY &lt;order_by_condition&gt;10.LIMIT &lt;limit_number&gt;\n\nSELECT语句 - 执行顺序：FROM&lt;表名&gt; # 选取表，将多个表数据通过笛卡尔积变成一个表。\nON&lt;筛选条件&gt; # 对笛卡尔积的虚表进行筛选\nJOIN# 指定join，用于添加数据到on之后的虚表中，例如left join会将左表的剩余数据添加到虚表中\nWHERE# 对上述虚表进行筛选\nGROUP BY&lt;分组条件&gt; # 分组# 用于having子句进行判断，在书写上这类聚合函数是写在having判断里面的\nHAVING&lt;分组筛选&gt; # 对分组后的结果进行聚合筛选\nSELECT&lt;返回数据列表&gt; # 返回的单列必须在group by子句中，聚合函数除外\nDISTINCT数据除重\nORDER BY&lt;排序条件&gt; # 排序\nLIMIT&lt;行数限制&gt;\n二、SQL 优化策略\n声明：以下SQL优化策略适用于数据量较大的场景下，如果数据量较小，没必要以此为准，以免画蛇添足。\n\n避免不走索引的场景1. 尽量避免在字段开头模糊查询，会导致数据库引擎放弃索引进行全表扫描。如下：\nSELECT * FROM t WHERE username LIKE &#x27;%陈%&#x27;\n\n优化方式：尽量在字段后面使用模糊查询。如下：\nSELECT * FROM t WHERE username LIKE &#x27;陈%&#x27;\n\n如果需求是要在前面使用模糊查询，\n\n使用MySQL内置函数 INSTR(str,substr) 来匹配，作用类似于java中的 indexOf()，查询字符串出现的角标位置\n使用FullText全文索引，用match against 检索\n数据量较大的情况，建议引用ElasticSearch、solr，亿级数据量检索速度秒级\n当表数据量较少（几千条儿那种），别整花里胡哨的，直接用like ‘%xx%’。\n\n2. 尽量避免使用 in 和 not in，会导致引擎走全表扫描。如下：\nSELECT * FROM t WHERE id IN (2,3)\n\n优化方式：如果是连续数值，可以用between代替。如下：\nSELECT * FROM t WHERE id BETWEEN 2 AND 3\n\n如果是子查询，可以用exists代替。如下：\n-- 不走索引select * from A where A.id in (select id from B);-- 走索引select * from A where exists (select * from B where B.id = A.id);\n\n3. 尽量避免使用 or，会导致数据库引擎放弃索引进行全表扫描。如下：\nSELECT * FROM t WHERE id = 1 OR id = 3\n\n优化方式：可以用union代替or。如下：\nSELECT * FROM t WHERE id = 1   UNIONSELECT * FROM t WHERE id = 3\n\n4. 尽量避免进行null值的判断，会导致数据库引擎放弃索引进行全表扫描。如下：\nSELECT * FROM t WHERE score IS NULL\n\n优化方式：可以给字段添加默认值0，对0值进行判断。如下：\nSELECT * FROM t WHERE score = 0` \n\n5.尽量避免在 where 条件中等号的左侧进行表达式、函数操作，会导致数据库引擎放弃索引进行全表扫描。\n可以将表达式、函数操作移动到等号右侧。如下：\n-- 全表扫描SELECT * FROM T WHERE score/10 = 9-- 走索引SELECT * FROM T WHERE score = 10*9`\n\n6. 当数据量大时，避免使用where 1=1的条件。通常为了方便拼装查询条件，我们会默认使用该条件，数据库引擎会放弃索引进行全表扫描。如下：\nSELECT username, age, sex FROM T WHERE 1=1\n\n优化方式：用代码拼装 sql 时进行判断，没 where 条件就去掉 where，有where条件就加 and。\n7. 查询条件不能用 &lt;&gt; 或者 !=\n使用索引列作为条件进行查询时，需要避免使用&lt;&gt;或者!=等判断条件。如确实业务需要，使用到不等于符号，需要在重新评估索引建立，避免在此字段上建立索引，改由查询条件中其他索引字段代替。\n8. where 条件仅包含复合索引非前置列\n如下：复合（联合）索引包含key_part1，key_part2，key_part3三列，但SQL语句没有包含索引前置列”key_part1”，按照MySQL联合索引的最左匹配原则，不会走联合索引。\nselect col1 from table where key_part2=1 and key_part3=2\n\n9. 隐式类型转换造成不使用索引\n如下SQL语句由于索引对列类型为varchar，但给定的值为数值，涉及隐式类型转换，造成不能正确走索引。\nselect col1 from table where col_varchar=123;\n\n10. order by 条件要与where中条件一致，否则order by不会利用索引进行排序\n-- 不走age索引SELECT * FROM t order by age;-- 走age索引SELECT * FROM t where age &gt; 0 order by age;\n\n对于上面的语句，数据库的处理顺序是：\n\n第一步：根据where条件和统计信息生成执行计划，得到数据。\n第二步：将得到的数据排序。当执行处理数据（order by）时，数据库会先查看第一步的执行计划，看order by 的字段是否在执行计划中利用了索引。如果是，则可以利用索引顺序而直接取得已经排好序的数据。如果不是，则重新进行排序操作。\n第三步：返回排序后的数据。\n\n当 order by 中的字段出现在where条件中时，才会利用索引而不再二次排序，更准确的说，order by 中的字段在执行计划中利用了索引时，不用排序操作。\n这个结论不仅对order by有效，对其他需要排序的操作也有效。比如group by 、union 、distinct等。\n11. 正确使用hint优化语句\nMySQL中可以使用hint指定优化器在执行时选择或忽略特定的索引。一般而言，处于版本变更带来的表结构索引变化，更建议避免使用hint，而是通过Analyze table多收集统计信息。但在特定场合下，指定hint可以排除其他索引干扰而指定更优的执行计划。\n\nUSE INDEX 在你查询语句中表名的后面，添加 USE INDEX 来提供希望 MySQL 去参考的索引列表，就可以让 MySQL 不再考虑其他可用的索引。例子: SELECT col1 FROM table USE INDEX (mod_time, name)...\nIGNORE INDEX 如果只是单纯的想让 MySQL 忽略一个或者多个索引，可以使用 IGNORE INDEX 作为 Hint。例子: SELECT col1 FROM table IGNORE INDEX (priority) ...\nFORCE INDEX 为强制 MySQL 使用一个特定的索引，可在查询中使用FORCE INDEX 作为Hint。例子: SELECT col1 FROM table FORCE INDEX (mod_time) ...\n\n在查询的时候，数据库系统会自动分析查询语句，并选择一个最合适的索引。但是很多时候，数据库系统的查询优化器并不一定总是能使用最优索引。如果我们知道如何选择索引，可以使用FORCE INDEX强制查询使用指定的索引。\n例如：\nSELECT * FROM students FORCE INDEX (idx_class_id) WHERE class_id = 1 ORDER BY id DESC;\n\nSELECT 语句其他优化1. 避免出现 select *\n首先，select * 操作在任何类型数据库中都不是一个好的SQL编写习惯。\n使用 select * 取出全部列，会让优化器无法完成索引覆盖扫描这类优化，会影响优化器对执行计划的选择，也会增加网络带宽消耗，更会带来额外的I/O,内存和CPU消耗。\n建议提出业务实际需要的列数，将指定列名以取代 select *。\n2. 避免出现不确定结果的函数\n特定针对主从复制这类业务场景。由于原理上从库复制的是主库执行的语句，使用如 now()、rand()、sysdate()、current_user() 等不确定结果的函数很容易导致主库与从库相应的数据不一致。另外不确定值的函数，产生的SQL语句无法利用 query cache。\n3.多表关联查询时，小表在前，大表在后。\n在MySQL中，执行 from 后的表关联查询是从左往右执行的（Oracle相反），第一张表会涉及到全表扫描，所以将小表放在前面，先扫小表，扫描快效率较高，在扫描后面的大表，或许只扫描大表的前100行就符合返回条件并return了。\n例如：表1有50条数据，表2有30亿条数据；如果全表扫描表2，你品，那就先去吃个饭再说吧是吧。\n4. 使用表的别名\n当在SQL语句中连接多个表时，请使用表的别名并把别名前缀于每个列名上。这样就可以减少解析的时间并减少哪些友列名歧义引起的语法错误。\n5. 用where字句替换HAVING字句\n避免使用HAVING字句，因为HAVING只会在检索出所有记录之后才对结果集进行过滤，而where则是在聚合前刷选记录，如果能通过where字句限制记录的数目，那就能减少这方面的开销。HAVING中的条件一般用于聚合函数的过滤，除此之外，应该将条件写在where字句中。\nwhere和having的区别：where后面不能使用组函数\n6.调整Where字句中的连接顺序\nMySQL采用从左往右，自上而下的顺序解析where子句。根据这个原理，应将过滤数据多的条件往前放，最快速度缩小结果集。\n增删改 DML 语句优化1. 大批量插入数据\n如果同时执行大量的插入，建议使用多个值的 INSERT 语句（方法二）。这比使用分开 INSERT 语句快（方法一），一般情况下批量插入效率有几倍的差别。\n方法一：\ninsert into T values(1,2); insert into T values(1,3); insert into T values(1,4);\n\n方法二：\nInsert into T values(1,2),(1,3),(1,4);\n\n选择后一种方法的原因有三。\n\n减少SQL语句解析的操作，MySQL没有类似Oracle的share pool，采用方法二，只需要解析一次就能进行数据的插入操作；\n在特定场景可以减少对DB连接次数\nSQL语句较短，可以减少网络传输的IO。\n\n2. 适当使用commit\n适当使用commit可以释放事务占用的资源而减少消耗，commit 后能释放的资源如下：\n\n事务占用的 undo 数据块；\n事务在 redo log中记录的数据块；\n释放事务施加的，减少锁争用影响性能。特别是在需要使用delete删除大量数据的时候，必须分解删除量并定期 commit。\n\n3. 避免重复查询更新的数据\n针对业务中经常出现的更新行同时又希望获得改行信息的需求，MySQL并不支持PostgreSQL那样的 UPDATE RETURNING 语法，在MySQL中可以通过变量实现。\n例如，更新一行记录的时间戳，同时希望查询当前记录中存放的时间戳是什么，简单方法实现：\nUpdate t1 set time=now() where col1=1; Select time from t1 where id =1;\n\n使用变量，可以重写为以下方式：\nUpdate t1 set time=now () where col1=1 and @now: = now (); Select @now;\n\n前后二者都需要两次网络来回，但使用变量避免了再次访问数据表，特别是当t1表数据量较大时，后者比前者快很多。\n4.查询优先还是更新（insert、update、delete）优先\nMySQL 还允许改变语句调度的优先级，它可以使来自多个客户端的查询更好地协作，这样单个客户端就不会由于锁定而等待很长时间。改变优先级还可以确保特定类型的查询被处理得更快。我们首先应该确定应用的类型，判断应用是以查询为主还是以更新为主的，是确保查询效率还是确保更新的效率，决定是查询优先还是更新优先。\n下面我们提到的改变调度策略的方法主要是针对只存在表锁的存储引擎，比如 MyISAM 、MEMROY、MERGE，对于Innodb 存储引擎，语句的执行是由获得行锁的顺序决定的。MySQL 的默认的调度策略可用总结如下：\n1）写入操作优先于读取操作。\n2）对某张数据表的写入操作某一时刻只能发生一次，写入请求按照它们到达的次序来处理。\n3）对某张数据表的多个读取操作可以同时地进行。MySQL 提供了几个语句调节符，允许你修改它的调度策略：\n\nLOW_PRIORITY关键字应用于DELETE、INSERT、LOAD DATA、REPLACE和UPDATE；\nHIGH_PRIORITY关键字应用于SELECT和INSERT语句；\nDELAYED关键字应用于INSERT和REPLACE语句。\n\n如果写入操作是一个 LOW_PRIORITY（低优先级）请求，那么系统就不会认为它的优先级高于读取操作。在这种情况下，如果写入者在等待的时候，第二个读取者到达了，那么就允许第二个读取者插到写入者之前。只有在没有其它的读取者的时候，才允许写入者开始操作。这种调度修改可能存在 LOW_PRIORITY写入操作永远被阻塞的情况。\nSELECT 查询的HIGH_PRIORITY（高优先级）关键字也类似。它允许SELECT 插入正在等待的写入操作之前，即使在正常情况下写入操作的优先级更高。另外一种影响是，高优先级的 SELECT 在正常的 SELECT 语句之前执行，因为这些语句会被写入操作阻塞。如果希望所有支持LOW_PRIORITY 选项的语句都默认地按照低优先级来处理，那么 请使用–low-priority-updates 选项来启动服务器。通过使用 INSERTHIGH_PRIORITY 来把 INSERT 语句提高到正常的写入优先级，可以消除该选项对单个INSERT语句的影响。\n查询条件优化1. 对于复杂的查询，可以使用中间临时表暂存数据\n2. 优化 group by 语句\n默认情况下，MySQL 会对GROUP BY分组的所有值进行排序，如 “GROUP BY col1，col2，….;” 查询的方法如同在查询中指定 “ORDER BY col1，col2，…;” 如果显式包括一个包含相同的列的 ORDER BY子句，MySQL 可以毫不减速地对它进行优化，尽管仍然进行排序。\n因此，如果查询包括 GROUP BY 但你并不想对分组的值进行排序，你可以指定 ORDER BY NULL禁止排序。例如：\nSELECT col1, col2, COUNT(*) FROM table GROUP BY col1, col2 ORDER BY NULL ;\n\n3. 优化join语句\nMySQL中可以通过子查询来使用 SELECT 语句来创建一个单列的查询结果，然后把这个结果作为过滤条件用在另一个查询中。使用子查询可以一次性的完成很多逻辑上需要多个步骤才能完成的 SQL 操作，同时也可以避免事务或者表锁死，并且写起来也很容易。但是，有些情况下，子查询可以被更有效率的连接(JOIN)..替代。\n例子：假设要将所有没有订单记录的用户取出来，可以用下面这个查询完成：\nSELECT col1 FROM customerinfo WHERE CustomerID NOT in (SELECT CustomerID FROM salesinfo )\n\n如果使用连接(JOIN).. 来完成这个查询工作，速度将会有所提升。尤其是当 salesinfo表中对 CustomerID 建有索引的话，性能将会更好，查询如下：\nSELECT col1 FROM customerinfo    LEFT JOIN salesinfoON customerinfo.CustomerID=salesinfo.CustomerID       WHERE salesinfo.CustomerID IS NULL\n\n连接(JOIN).. 之所以更有效率一些，是因为 MySQL 不需要在内存中创建临时表来完成这个逻辑上的需要两个步骤的查询工作。\n4. 优化union查询\nMySQL通过创建并填充临时表的方式来执行union查询。除非确实要消除重复的行，否则建议使用union all。原因在于如果没有all这个关键词，MySQL会给临时表加上distinct选项，这会导致对整个临时表的数据做唯一性校验，这样做的消耗相当高。\n高效：\nSELECT COL1, COL2, COL3 FROM TABLE WHERE COL1 = 10 UNION ALL SELECT COL1, COL2, COL3 FROM TABLE WHERE COL3= &#x27;TEST&#x27;;\n\n低效：\nSELECT COL1, COL2, COL3 FROM TABLE WHERE COL1 = 10 UNION SELECT COL1, COL2, COL3 FROM TABLE WHERE COL3= &#x27;TEST&#x27;;\n\n5.拆分复杂SQL为多个小SQL，避免大事务\n\n简单的SQL容易使用到MySQL的QUERY CACHE；\n减少锁表时间特别是使用MyISAM存储引擎的表；\n可以使用多核CPU。\n\n6. 使用truncate代替delete\n当删除全表中记录时，使用delete语句的操作会被记录到undo块中，删除记录也记录binlog，当确认需要删除全表时，会产生很大量的binlog并占用大量的undo数据块，此时既没有很好的效率也占用了大量的资源。\n使用truncate替代，不会记录可恢复的信息，数据不能被恢复。也因此使用truncate操作有其极少的资源占用与极快的时间。另外，使用truncate可以回收表的水位，使自增字段值归零。\n7. 使用合理的分页方式以提高分页效率\n使用合理的分页方式以提高分页效率 针对展现等分页需求，合适的分页方式能够提高分页的效率。\n案例1：\nselect * from t where thread_id = 10000 and deleted = 0    order by gmt_create asc limit 0, 15;\n\n上述例子通过一次性根据过滤条件取出所有字段进行排序返回。数据访问开销=索引IO+索引全部记录结果对应的表数据IO。因此，该种写法越翻到后面执行效率越差，时间越长，尤其表数据量很大的时候。\n适用场景：当中间结果集很小（10000行以下）或者查询条件复杂（指涉及多个不同查询字段或者多表连接）时适用。\n案例2：\nselect t.* from \t(select id from t where thread_id = 10000 and deleted = 0 order by gmt_create asc limit 0, 15) a  inner join t on a.id = t.id;\n\n上述例子必须满足t表主键是id列，且有覆盖索引 secondary key:(thread_id, deleted, gmt_create)。通过先根据过滤条件利用覆盖索引取出主键id进行排序，再进行join操作取出其他字段。数据访问开销=索引IO+索引分页后结果（例子中是15行）对应的表数据IO。因此，该写法每次翻页消耗的资源和时间都基本相同，就像翻第一页一样。\n适用场景：当查询和排序字段（即where子句和order by子句涉及的字段）有对应覆盖索引时，且中间结果集很大的情况时适用。\n建表优化1. 在表中建立索引，优先考虑 where、order by 使用到的字段。\n2. 尽量使用数字型字段（如性别，男：1 女：2），若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。\n这是因为引擎在处理查询和连接时会 逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。\n3. 查询数据量大的表会造成查询缓慢。主要的原因是扫描行数过多。这个时候可以通过程序，分段分页进行查询，循环遍历，将结果合并处理进行展示。要查询100000到100050的数据，如下：\nSELECT * FROM (SELECT ROW_NUMBER() OVER(ORDER BY ID ASC) AS rowid,*    FROM infoTab)t WHERE t.rowid &gt; 100000 AND t.rowid &lt;= 100050\n\n4. 用varchar/nvarchar 代替 char/nchar\n尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。\n不要以为 NULL 不需要空间，比如：char(100) 型，在字段建立时，空间就固定了， 不管是否插入值（NULL也包含在内），都是占用 100个字符的空间的，如果是varchar这样的变长字段， null 不占用空间。\n","categories":["MySQL"]},{"title":"Mysql Buffer Pool详解","url":"/posts/17417/","content":"一、前言1. buffer pool是什么　　咱们在使用mysql的时候，比如很简单的select * from table;这条语句，具体查询数据其实是在存储引擎中实现的，大家都知道mysql数据其实是放在磁盘里面的，如果每次查询都直接从磁盘里面查询，这样势必会很影响性能，所以一定是先把数据从磁盘中取出，然后放在内存中，下次查询直接从内存中来取。但是一台机器中往往不是只有mysql一个进程在运行的，很多个进程都需要使用内存，所以mysql中会有一个专门的区域来处理这些数据，这个专门为mysql准备的区域，就叫buffer pool。\nbuffer pool是mysql一个非常关键的核心组件。数据库中的数据实际上最终都是要存放在磁盘文件上的，如下图所示\n\n　　但是我们在对数据库执行增删改操作的时候，不可能直接更新磁盘上的数据的，因为如果你对磁盘进行随机读写操作，那速度是相当的慢，随便一个大磁盘文件的随机读写操作，可能都要几百毫秒。如果要是那么搞的话，可能你的数据库每秒也就只能处理几百个请求了！ 在对数据库执行增删改操作的时候，实际上主要都是针对内存里的Buffer Pool中的数据进行的，也就是实际上主要是对数据库的内存里的数据结构进行了增删改，如下图所示\n\n　　其实每个人都担心一个事，就是你在数据库的内存里执行了一堆增删改的操作，内存数据是更新了，但是这个时候如果数据库突然崩溃了，那么内存里更新好的数据不是都没了吗？ MySQL就怕这个问题，所以引入了一个redo log机制，你在对内存里的数据进行增删改的时候，他同时会把增删改对应的日志写入redo log中，如下图\n\n　　万一你的数据库突然崩溃了，没关系，只要从redo log日志文件里读取出来你之前做过哪些增删改操作，瞬间就可以重新把这些增删改操作在你的内存里执行一遍，这就可以恢复出来你之前做过哪些增删改操作了。 当然对于数据更新的过程，他是有一套严密的步骤的，还涉及到undo log、binlog、提交事务、buffer pool脏数据刷回磁盘，等等。\n\n　　Buffer Pool就是数据库的一个内存组件，里面缓存了磁盘上的真实数据，然后我们的系统对数据库执行的增删改操作，其实主要就是对这个内存数据结构中的缓存数据执行的。通过这种方式，保证每个更新请求，尽量就是只更新内存，然后往磁盘顺序写日志文件。\n更新内存的性能是极高的，然后顺序写磁盘上的日志文件的性能也是比较高的，因为顺序写磁盘文件，他的性能要远高于随机读写磁盘文件。\n\n2. buffer pool的工作流程\n咱们以查询语句为例\n1:在查询的时候会先去buffer pool(内存)中看看有没有对应的数据页，如果有的话直接返回\n2:如果buffer pool中没有对应的数据页，则会去磁盘中查找，磁盘中如果找到了对应的数据，则会把该页的数据直接copy一份到buffer pool中返回给客户端\n3:下次有同样的查询进来直接查找buffer pool找到对应的数据返回即可。\n大家看到这里相信应该对buffer pool有了个大概的认识，有没有感觉有点缓存的感觉，当然buffer pool可没有缓存那么简单，内部结构还是比较复杂的，不过没关系，咱们继续往下看。\n3. buffer pool缓冲池和查询缓存(query cache)在正式讲解buffer pool 之前，我们先搞清楚buffer pool缓冲池和查询缓存(query cache)简称Qcache的区别。\n如果将Mysql分为Server层和存储引擎层两大部分，那么Qcache位于Server层，Buffer Pool位于存储引擎层。\n　　如果你的Mysql 查询缓存功能是打开的，那么当一个sql进入Mysql Server之后，Mysql Server首先会从查询缓存中查看是否曾经执行过这个SQL，如果曾经执行过的话，曾经执行的查询结果之前会以key-value的形式保存在查询缓存中。key是sql语句，value是查询结果。我们将这个过程称为查询缓存！\n　　如果查询缓存中没有你要找的数据的话，MySQL才会执行后续的逻辑，通过存储引擎将数据检索出来。并且查询缓存会被shared cache for sessions，是的，它会被所有的session共享。\n　　MySQL查询缓存是查询结果缓存。它将以SEL开头的查询与哈希表进行比较，如果匹配，则返回上一次查询的结果。进行匹配时，查询必须逐字节匹配，例如 SELECT * FROM t1; 不等于select * from t1;，此外，一些不确定的查询结果无法被缓存，任何对表的修改都会导致这些表的所有缓存无效(只要有一个sql update了该表，那么表的查询缓存就会失效)。因此，适用于查询缓存的最理想的方案是只读，特别是需要检查数百万行后仅返回数行的复杂查询。如果你的查询符合这样一个特点，开启查询缓存会提升你的查询性能。\n　　MySQL查询缓存的目的是为了提升查询性能，但它本身也是有性能开销的。需要在合适的业务场景下（读写压力模型）使用，不合适的业务场景不但不能提升查询性能，查询缓存反而会变成MySQL的瓶颈。\n查询缓存的开销主要有：\n\n读查询在开始前必须先检查是否命中缓存；\n如果这个读查询可以被缓存，那么当完成执行后，MySQL若发现查询缓存中没有这个查询，会将其结果存入查询缓存，这会带来额外的系统消耗；\n当向某个表写入数据的时候，MySQL必须将对应表的所有缓存都设置失效。如果查询缓存非常大或者碎片很多，这个操作就可能带来很大的系统消耗。\n\n查询缓存的缺点：\n　　首先，查询缓存的效果取决于缓存的命中率，只有命中缓存的查询效果才能有改善，因此无法预测其性能。只要有一个sql update了该表，那么表的查询缓存就会失效，所以当你的业务对表CRUD的比例不相上下，那么查询缓存may be会影响应用的吞吐效率。\n　　其次，查询缓存的另一个大问题是它受到单个互斥锁的保护。在具有多个内核的服务器上，大量查询会导致大量的互斥锁争用。\n在mysql8.0的版本中，已经将查询缓存模块删除了。\n二、buffer pool的内存数据结构1. 数据页概念我们先了解一下数据页这个概念。它是 MySQL 抽象出来的数据单位，磁盘文件中就是存放了很多数据页，每个数据页里存放了很多行数据。\n默认情况下，数据页的大小是 16kb。\n所以对应的，在 Buffer Pool 中，也是以数据页为数据单位，存放着很多数据。但是我们通常叫做缓存页，因为 Buffer Pool 毕竟是一个缓冲池，并且里面的数据都是从磁盘文件中缓存到内存中。\n所以，默认情况下缓存页的大小也是 16kb，因为它和磁盘文件中数据页是一一对应的。\n所以，缓冲池和磁盘之间的数据交换的单位是数据页，Buffer Pool中存放的是一个一个的数据页。\n假设我们要更新一行数据，此时数据库会找到这行数据所在的数据页，然后从磁盘文件里把这行数据所在的数据页直接给加载到Buffer Pool里去。如下图。\n\n2. 那么怎么识别数据在哪个缓存页中每个缓存页都会对应着一个描述数据块，里面包含数据页所属的表空间、数据页的编号，缓存页在 Buffer Pool 中的地址等等。\n描述数据块本身也是一块数据，它的大小大概是缓存页大小的5%左右，大概800个字节左右的大小。然后假设你设置的buffer pool大小是128MB，实际上Buffer Pool真正的最终大小会超出一些，可能有个130多MB的样子，因为他里面还要存放每个缓存页的描述数据。\n在Buffer Pool中，每个缓存页的描述数据放在最前面，然后各个缓存页放在后面。所以此时我们看下面的图，Buffer Pool实际看起来大概长这个样子 。\n\n3. buffer pool的初始化与配置MySQL 启动时，是如何初始化 Buffer Pool 的呢？\n1、MySQL 启动时，会根据参数 innodb_buffer_pool_size 的值来为 Buffer Pool 分配内存区域。\n2、然后会按照缓存页的默认大小 16k 以及对应的描述数据块的 800个字节 左右大小，在 Buffer Pool 中划分中一个个的缓存页和一个个的描述数据库块。\n3、注意，此时的缓存页和描述数据块都是空的，毕竟才刚启动 MySQL 呢。\nbuffer pool的配置\n　　buffer pool通常由数个内存块加上一组控制结构体对象组成。内存块的个数取决于buffer pool instance的个数，不过在5.7版本中开始默认以128M(可配置)的chunk单位分配内存块，这样做的目的是为了支持buffer pool的在线动态调整大小。\n　　Buffer Pool默认情况下是128MB，还是有一点偏小了，我们实际生产环境下完全可以对Buffer Pool进行调整。 比如我们的数据库如果是16核32G的机器，那么你就可以给Buffer Pool分配个2GB的内存。\n1、innodb_buffer_pool_size：这个值是设置 InnoDB Buffer Pool 的总大小；\n2、innodb_buffer_pool_chunk_size：当增加或减少innodb_buffer_pool_size时，操作以块（chunk）形式执行。块大小由innodb_buffer_pool_chunk_size配置选项定义，默认值128M。\n这里面有个关系要确定一下，最好按照这个设置 innodb_buffer_pool_size=innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances*N（N&gt;=1）;\n3、innodb_buffer_pool_instances：设置 InnoDB Buffer Pool 实例的个数，每一个实例都有自己独立的 list 管理Buffer Pool；\n　　当buffer pool比较大的时候（超过1G），innodb会把buffer pool划分成几个instances，这样可以提高读写操作的并发，减少竞争。读写page都使用hash函数分配给一个instances。\n　　当增加或者减少buffer pool大小的时候，实际上是操作的chunk。buffer pool的大小必须是innodb_buffer_pool_chunk_sizeinnodb_buffer_pool_instances的整数倍，如果配置的innodb_buffer_pool_size不是innodb_buffer_pool_chunk_sizeinnodb_buffer_pool_instances的倍数，buffer pool的大小会自动调整为innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances的倍数，自动调整的值不少于指定的值。如果指定的buffer大小是9G，instances的个数是16，chunk默认的大小是128M，那么buffer会自动调整为10G。4、innodb_old_blocks_pct：默认 InnoDB Buffer Pool 中点的位置，默认值是37，最大100，也就是我们所谓的3/8的位置，可以自己设置。\n1）Buffer Pool Size 设置和生效过程　　理想情况下，在给服务器的其他进程留下足够的内存空间的情况下，Buffer Pool Size 应该设置的尽可能大。当 Buffer Pool Size 设置的足够大时，整个数据库就相当于存储在内存当中，当读取一次数据到 Buffer Pool Size 以后，后续的读操作就不用再访问磁盘。\n下面我们看一下 Buffer Pool Size 的设置方式：\n当数据库已经启动的情况下，我们可以通过在线调整的方式修改 Buffer Pool Size 的大小。通过以下语句：\nSET GLOBAL innodb_buffer_pool_size=402653184;\n\n当执行这个语句以后，并不会立即生效，而是要等所有的事务全部执行成功以后才会生效；新的连接和事务必须等其他事务完全执行成功以后，Buffer Pool Size 设置生效以后才能够连接成功，不然会一直处于等待状态。\n期间，Buffer Pool Size 要完成碎片整理，去除缓存 page 等等操作。在执行增加或者减少 Buffer Pool Size 的操作时，操作会作为一个执行块执行，innodb_buffer_pool_chunk_size 的大小会定义一个执行块的大小，默认的情况下，这个值是128M。\nBuffer Pool Size 的大小最好设置为 innodb_buffer_pool_chunk_size innodb_buffer_pool_instances 的整数倍，而且是大于等于1。\n如果你的机器配置的大小不是整数倍的话，Buffer Pool Size 的大小是会自适应修改为 innodb_buffer_pool_chunk_sizeinnodb_buffer_pool_instances 的整数倍，会略小于你配置的 Buffer Pool Size 的大小。\n比如以8G为例：\nmysqld –innodb_buffer_pool_size=8G –innodb_buffer_pool_instances=16，然后innodb_buffer_pool_instances=16 的大小刚好设置为16，是一个整数倍的关系。而且innodb_buffer_pool_chunk_size 的大小也是可以在my.cnf里面指定的。\n还有一种情况是 innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances 大于 buffer pool size 的情况下，innodb_buffer_pool_chunk_size 也会自适应为 Buffer Pool size/innodb_buffer_pool_instances，可见MySQL 的管理还是非常的智能的。\n如果我们要查 Buffer Pool 的状态的话：\nSHOW STATUS WHERE Variable_name=&#x27;InnoDB_buffer_pool_resize_status&#x27;\n\n可以帮我们查看到状态。我们可以看一下增加 Buffer Pool 的时候的一个过程，再看一下减少的时候的日志，其实还是很好理解的，我们可以看成每次增大或者减少 Buffer Pool 的时候就是进行 innodb_buffer_pool_chunk 的增加或者释放，按照 innodb_buffer_pool_chunk_size 设定值的大小增加或者释放执行块。\n增加的过程：增加执行块，指定新地址，将新加入的执行块加入到 free list（控制执行块的一个列表，可以这么理解）。\n减少的过程：重新整理 Buffer Pool 和空闲页，将数据从块中移除，指定新地址。\n2）Buffer Pool Instances在64位操作系统的情况下，可以拆分缓冲池成多个部分，这样可以在高并发的情况下最大可能的减少争用。下面我们看一下怎么配置 Buffer Pool Instances？\n配置多个 Buffer Pool Instances 能在很大程度上能够提高 MySQL 在高并发的情况下处理事物的性能，优化不同连接读取缓冲页的争用。\n我们可以通过设置 innodb_buffer_pool_instances 来设置 Buffer Pool Instances。当 InnoDB Buffer Pool 足够大的时候，你能够从内存中读取时候能有一个较好的性能，但是也有可能碰到多个线程同时请求缓冲池的瓶颈。这个时候设置多个 Buffer Pool Instances 能够尽量减少连接的争用。\n这能够保证每次从内存读取的页都对应一个 Buffer Pool Instances，而且这种对应关系是一个随机的关系。并不是热数据存放在一个 Buffer Pool Instances下，内部也是通过 hash 算法来实现这个随机数的。每一个 Buffer Pool Instances 都有自己的 free lists，LRU 和其他的一些 Buffer Poll 的数据结构，各个 Buffer Pool Instances 是相对独立的。\ninnodb_buffer_pool_instances 的设置必须大于1才算得上是多配置，但是这个功能起作用的前提是innodb_buffer_pool_size 的大小必须大于1G，理想情况下 innodb_buffer_pool_instances 的每一个 instance 都保证在1G以上。\n3）SHOW ENGINE INNODB STATUS当你的数据库启动之后，你随时可以通过上述命令，去查看当前innodb里的一些具体情况，执行SHOW ENGINE INNODB STATUS就可以了。此时你可能会看到如下一系列的东西：\nTotal memory allocated xxxx;Dictionary memory allocated xxxBuffer pool size xxxxFree buffers xxxDatabase pages xxxOld database pages xxxxModified db pages xxPending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young xxxx, not young xxxxx youngs/s, xx non-youngs/sPages read xxxx, created xxx, written xxxxx reads/s, xx creates/s, 1xx writes/sBuffer pool hit rate xxx / 1000, young-making rate xxx / 1000 not xx / 1000Pages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/sLRU len: xxxx, unzip_LRU len: xxxI/O sum[xxx]:cur[xx], unzip sum[16xx:cur[0]\n\n下面解释一下这里的东西，主要讲解这里跟buffer pool相关的一些东西。\n\n\nTotal memory allocated，这就是说buffer pool最终的总大小是多少\nBuffer pool size，这就是说buffer pool一共能容纳多少个缓存页\nFree buffers，这就是说free链表中一共有多少个空闲的缓存页是可用的\nDatabase pages和Old database pages，就是说lru链表中一共有多少个缓存页，以及冷数据区域里的缓存页数量\nModified db pages，这就是flush链表中的缓存页数量\nPending reads和Pending writes，等待从磁盘上加载进缓存页的数量，还有就是即将从lru链表中刷入磁盘的数量、即将从flush链表中刷入磁盘的数量\nPages made young和not young，这就是说已经lru冷数据区域里访问之后转移到热数据区域的缓存页的数 量，以及在lru冷数据区域里1s内被访问了没进入热数据区域的缓存页的数量\nyoungs/s和not youngs/s，这就是说每秒从冷数据区域进入热数据区域的缓存页的数量，以及每秒在冷数据区域里被访问了但是不能进入热数据区域的缓存页的数量\nPages read xxxx, created xxx, written xxx，xx reads/s, xx creates/s, 1xx writes/s，这里就是说已经读取、创建和写入了多少个缓存页，以及每秒钟读取、创建和写入的缓存页数量\nBuffer pool hit rate xxx / 1000，这就是说每1000次访问，有多少次是直接命中了buffer pool里的缓存的\nyoung-making rate xxx / 1000 not xx / 1000，每1000次访问，有多少次访问让缓存页从冷数据区域移动到了热数据区域，以及没移动的缓存页数量\nLRU len：这就是lru链表里的缓存页的数量\nI/O sum：最近50s读取磁盘页的总数\nI/O cur：现在正在读取磁盘页的数量\n\n\n三、buffer pool的空间管理　　缓冲池也是有大小限制的，那么既然缓冲池有大小限制的，每次都读入的数据页怎么来管理呢？这里我们来聊聊缓冲池的空间管理，其实对缓冲池进行管理的关键部分是如何安排进池的数据并且按照一定的策略淘汰池中的数据，保证池中的数据不“溢出”，同时还能保证常用数据留在池子中。\n\n1. 传统 LRU 淘汰法缓冲池是基于传统的 LRU 方法来进行缓存页管理的，我们先来看下如果使用 LRU 是如何管理的。\nLRU，全称是 Least Recently Used，中文名字叫作「最近最少使用」。从名字上就很容易理解了。\n这里分两种情况：\n1）缓存页已在缓冲池中这种情况下会将对应的缓存页放到 LRU 链表的头部，无需从磁盘再进行读取，也无需淘汰其它缓存页。\n如下图所示，如果要访问的数据在 6 号页中，则将 6 号页放到链表头部即可，这种情况下没有缓存页被淘汰。\n\n2）缓存页不在缓冲池中缓存页不在缓冲中，这时候就需要从磁盘中读入对应的数据页，将其放置在链表头部，同时淘汰掉末尾的缓存页 \n如下图所示，如果要访问的数据在 60 号页中，60 号页不在缓冲池中，此时加载进来放到链表的头部，同时淘汰掉末尾的 17 号缓存页。\n\n是不是看上去很简单，同时也能满足缓冲池淘汰缓存页的方法？但是我们来思考几个问题：\n预读失效\n　　上面我们提到了缓冲池的预读机制可能会预先加载相邻的数据页。假如加载了 20、21 相邻的两个数据页，如果只有页号为 20 的缓存页被访问了，而另一个缓存页却没有被访问。此时两个缓存页都在链表的头部，但是为了加载这两个缓存页却淘汰了末尾的缓存页，而被淘汰的缓存页却是经常被访问的。这种情况就是预读失效，被预先加载进缓冲池的页，并没有被访问到，这种情况是不是很不合理。\n缓冲池污染　　还有一种情况是当执行一条 SQL 语句时，如果扫描了大量数据或是进行了全表扫描，此时缓冲池中就会加载大量的数据页，从而将缓冲池中已存在的所有页替换出去，这种情况同样是不合理的。这就是缓冲池污染，并且还会导致 MySQL 性能急剧下降。\n2. 冷热数据分离这样看来，传统的 LRU 方法并不能满足缓冲池的空间管理。因此，Msyql 基于 LRU 设计了冷热数据分离的处理方案。\n也就是将 LRU 链表分为两部分，一部分为热数据区域，一部分为冷数据区域。\n\n当数据页第一次被加载到缓冲池中的时候，先将其放到冷数据区域的链表头部，1s（由 innodb_old_blocks_time 参数控制） 后该缓存页被访问了再将其移至热数据区域的链表头部。\n\n　　可能你会有疑惑了，为什么要等 1s 后才将其移至热数据区域呢？你想想，如果数据页刚被加载到冷数据区就被访问了，之后再也不访问它了呢？这不就造成热数据区的浪费了吗？要是 1s 后不访问了，说明之后可能也不会去频繁访问它，也就没有移至热缓冲区的必要了。当缓存页不够的时候，从冷数据区淘汰它们就行了。 \n　　另一种情况，当我的数据页已经在热缓冲区了，是不是缓存页只要被访问了就将其插到链表头部呢？不用我说你肯定也觉得不合理。热数据区域里的缓存页是会被经常访问的，如果每访问一个缓存页就插入一次链表头，那整个热缓冲区里就异常骚动了，你想想那个画面。\n那咋整呢？Mysql 中优化为热数据区的后 3/4 部分被访问后才将其移动到链表头部去，对于前 1/4 部分的缓存页被访问了不会进行移动。\n四、Buffer Pool 预读机制　　预读是mysql提高性能的一个重要的特性。预读就是 IO 异步读取多个页数据读入 Buffer Pool 的一个过程，并且这些页被认为是很快就会被读取到的。InnoDB使用两种预读算法来提高I/O性能：线性预读（linear read-ahead）和随机预读（randomread-ahead）\n　　为了区分这两种预读的方式，我们可以把线性预读放到以extent为单位，而随机预读放到以extent中的page为单位。线性预读着眼于将下一个extent提前读取到buffer pool中，而随机预读着眼于将当前extent中的剩余的page提前读取到buffer pool中。\n1. Linear线性预读　　线性预读的单位是extend，一个extend中有64个page。线性预读的一个重要参数是innodb_read_ahead_threshold，是指在连续访问多少个页面之后，把下一个extend读入到buffer pool中，不过预读是一个异步的操作。当然这个参数不能超过64，因为一个extend最多只有64个页面。例如，innodb_read_ahead_threshold = 56，就是指在连续访问了一个extend的56个页面之后把下一个extend读入到buffer pool中。在添加此参数之前，InnoDB仅计算当它在当前范围的最后一页中读取时是否为整个下一个范围发出异步预取请求。\n2. Random随机预读　　随机预读方式则是表示当同一个extent中的一些page在buffer pool中发现时，Innodb会将该extent中的剩余page一并读到buffer pool中。由于随机预读方式给innodb code带来了一些不必要的复杂性，同时在性能也存在不稳定性，在5.5中已经将这种预读方式废弃，默认是OFF。若要启用此功能，即将配置变量设置innodb_random_read_ahead为ON。\n\n五、Buffer Pool的三种Page和链表　　Buffer Pool 是Innodb 内存中的的一块占比较大的区域，用来缓存表和索引数据。众所周知，从内存访问会比从磁盘访问快很多。为了提高数据的读取速度，Buffer Pool 会通过三种Page 和链表来管理这些经常访问的数据，保证热数据不被置换出Buffer Pool。\n1. 三种PageFree Page（空闲页）\n表示此Page 未被使用，位于 Free 链表。\nClean Page（干净页）\n此Page 已被使用，但是页面未发生修改，位于LRU 链表。\nDirty Page（脏页）\n此Page 已被使用，页面已经被修改，其数据和磁盘上的数据已经不一致。当脏页上的数据写入磁盘后，内存数据和磁盘数据一致，那么该Page 就变成了干净页。脏页 同时存在于LRU 链表和Flush 链表。\n \n2. 三种链表1）LRU 链表\n如上图所示，是Buffer Pool里面的LRU(least recently used)链表。LRU链表是被一种叫做最近最少使用的算法管理。\nLRU链表被分成两部分，一部分是New Sublist(Young 链表)，用来存放经常被读取的页的地址，另外一部分是Old Sublist(Old 链表)，用来存放较少被使用的页面。每部分都有对应的头部 和尾部。\n默认情况下\n\nOld 链表占整个LRU 链表的比例是3/8。该比例由innodb_old_blocks_pct控制，默认值是37（3/8*100）。该值取值范围为5~95，为全局动态变量。\n当新的页被读取到Buffer Pool里面的时候，和传统的LRU算法插入到LRU链表头部不同，Innodb LRU算法是将新的页面插入到Yong 链表的尾部和Old 链表的头部中间的位置，这个位置叫做Mid Point，如上图所示。\n频繁访问一个Buffer Pool的页面，会促使页面往Young链表的头部移动。如果一个Page在被读到Buffer Pool后很快就被访问，那么该Page会往Young List的头部移动，但是如果一个页面是通过预读的方式读到Buffer Pool，且之后短时间内没有被访问，那么很可能在下次访问之前就被移动到Old List的尾部，而被驱逐了。\n随着数据库的持续运行，新的页面被不断的插入到LRU链表的Mid Point，Old 链表里的页面会逐渐的被移动Old链表的尾部。同时，当经常被访问的页面移动到LRU链表头部的时候，那些没有被访问的页面会逐渐的被移动到链表的尾部。最终，位于Old 链表尾部的页面将被驱逐。\n\n如果一个数据页已经处于Young 链表，当它再次被访问的时候，只有当其处于Young 链表长度的1/4(大约值)之后，才会被移动到Young 链表的头部。这样做的目的是减少对LRU 链表的修改，因为LRU 链表的目标是保证经常被访问的数据页不会被驱逐出去。\ninnodb_old_blocks_time 控制的Old 链表头部页面的转移策略。该Page需要在Old 链表停留超过innodb_old_blocks_time 时间，之后再次被访问，才会移动到Young 链表。这么操作是避免Young 链表被那些只在innodb_old_blocks_time时间间隔内频繁访问，之后就不被访问的页面塞满，从而有效的保护Young 链表。\n在全表扫描或者全索引扫描的时候，Innodb会将大量的页面写入LRU 链表的Mid Point位置，并且只在短时间内访问几次之后就不再访问了。设置innodb_old_blocks_time的时间窗口可以有效的保护Young List，保证了真正的频繁访问的页面不被驱逐。\ninnodb_old_blocks_time 单位是毫秒，默认值是1000。调大该值提高了从Old链表移动到Young链表的难度，会促使更多页面被移动到Old 链表，老化，从而被驱逐。\n当扫描的表很大，Buffer Pool都放不下时，可以将innodb_old_blocks_pct设置为较小的值，这样只读取一次的数据页就不会占据大部分的Buffer Pool。例如，设置innodb_old_blocks_pct = 5，会将仅读取一次的数据页在Buffer Pool的占用限制为5％。\n当经常扫描一些小表时，这些页面在Buffer Pool移动的开销较小，我们可以适当的调大innodb_old_blocks_pct，例如设置innodb_old_blocks_pct = 50。\n在SHOW ENGINE INNODB STATUS 里面提供了Buffer Pool一些监控指标，有几个我们需要关注一下：\n\nyoungs/s：该指标表示的是每秒访问Old 链表中页面，使其移动到Young链表的次数。如果MySQL实例都是一些小事务，没有大表全扫描，且该指标很小，就需要调大innodb_old_blocks_pct 或者减小innodb_old_blocks_time，这样会使得Old List 的长度更长，Old页面被移动到Old List 的尾部消耗的时间会更久，那么就提升了下一次访问到Old List里面的页面的可能性。如果该指标很大，可以调小innodb_old_blocks_pct，同时调大innodb_old_blocks_time，保护热数据。\nnon-youngs/s：该指标表示的是每秒访问Old 链表中页面，没有移动到Young链表的次数，因为其不符合innodb_old_blocks_time。如果该指标很大，一般情况下是MySQL存在大量的全表扫描。如果MySQL存在大量全表扫描，且这个指标又不大的时候，需要调大innodb_old_blocks_time，因为这个指标不大意味着全表扫描的页面被移动到Young 链表了，调大innodb_old_blocks_time时间会使得这些短时间频繁访问的页面保留在Old 链表里面。\n\n每隔1秒钟，Page Cleaner线程执行LRU List Flush的操作，来释放足够的Free Page。innodb_lru_scan_depth 变量控制每个Buffer Pool实例每次扫描LRU List的长度，来寻找对应的脏页，执行Flush操作。\n2）Flush 链表\nFlush 链表里面保存的都是脏页，也会存在于LRU 链表。\nFlush 链表是按照oldest_modification排序，值大的在头部，值小的在尾部\n当有页面访被修改的时候，使用mini-transaction，对应的page进入Flush 链表\n如果当前页面已经是脏页，就不需要再次加入Flush list，否则是第一次修改，需要加入Flush 链表\n当Page Cleaner线程执行flush操作的时候，从尾部开始scan，将一定的脏页写入磁盘，推进检查点，减少recover的时间\n\n　　SQL 的增删改查都在 Buffer Pool 中执行，慢慢地，Buffer Pool 中的缓存页因为不断被修改而导致和磁盘文件中的数据不一致了，也就是 Buffer Pool 中会有很多个脏页，脏页里面很多脏数据。\n所以，MySQL 会有一条后台线程，定时地将 Buffer Pool 中的脏页刷回到磁盘文件中。\n但是，后台线程怎么知道哪些缓存页是脏页呢，不可能将全部的缓存页都往磁盘中刷吧，这会导致 MySQL 暂停一段时间。\nMySQL 是怎么判断脏页的\n　　我们引入一个和 free 链表类似的 flush 链表。他的本质也是通过缓存页的描述数据块中的两个指针，让修改过的缓存页的描述数据块能串成一个双向链表，这两指针大家可以认为是 flush_pre 指针和 flush_next 指针。\n下面我用伪代码来描述一下：\nDescriptionDataBlock&#123;    block_id = block1;    // free 链表的    free_pre = null;    free_next = null;    // flush 链表的    flush_pre = null;    flush_next = block2;&#125;\n\nflush 链表也有对应的基础节点，也是包含链表的头节点和尾节点，还有就是修改过的缓存页的数量。\nFlushListBaseNode&#123;    start = block1;    end = block2;    count = 2;&#125;\n\n到这里，我们都知道，SQL 的增删改都会使得缓存页变为脏页，此时会修改脏页对应的描述数据块的 flush_pre 指针和 flush_next 指针，使得描述数据块加入到 flush 链表中，之后 MySQL 的后台线程就可以将这个脏页刷回到磁盘中。\n3）Free 链表\nFree 链表 存放的是空闲页面，初始化的时候申请一定数量的页面，当 MySQL 启动后，会不断地有 SQL 请求进来，此时空先的缓存页就会不断地被使用。\n在执行SQL的过程中，每次成功load 页面到内存后，会判断Free 链表的页面是否够用。如果不够用的话，就flush LRU 链表和Flush 链表来释放空闲页。如果够用，就从Free 链表里面删除对应的页面，在LRU 链表增加页面，保持总数不变。\n\nFree 链表的使用原理\nfree 链表，它是一个双向链表，链表的每个节点就是一个个空闲的缓存页对应的描述数据块。\n他本身其实就是由 Buffer Pool 里的描述数据块组成的，你可以认为是每个描述数据块里都有两个指针，一个是 free_pre 指针，一个是 free_next 指针，分别指向自己的上一个 free 链表的节点，以及下一个 free 链表的节点。\n通过 Buffer Pool 中的描述数据块的 free_pre 和 free_next 两个指针，就可以把所有的描述数据块串成一个 free 链表。\n下面我们可以用伪代码来描述一下 free 链表中描述数据块节点的数据结构：\nDescriptionDataBlock&#123;    block_id = block1;    free_pre = null;    free_next = block2;&#125;\n\nfree 链表有一个基础节点，他会引用链表的头节点和尾节点，里面还存储了链表中有多少个描述数据块的节点，也就是有多少个空闲的缓存页。\n下面我们也用伪代码来描述一下基础节点的数据结构：\nFreeListBaseNode&#123;    start = block01;    end = block03;       count = 2;&#125;\n\n到此，free 链表就介绍完了。上面我们也介绍了 MySQL 启动时 Buffer Pool 的初始流程，接下来，我会将结合刚介绍完的 free 链表，讲解一下 SQL 进来时，磁盘数据页读取到 Buffer Pool 的缓存页的过程。但是，我们先要了解一下一个新概念：数据页缓存哈希表，它的 key 是表空间+数据页号，而 value 是对应缓存页的地址。\n描述如图所示：\n\n磁盘数据页读取到 Buffer Pool 的缓存页的过程\n\n首先，SQL 进来时，判断数据对应的数据页能否在 数据页缓存哈希表里 找到对应的缓存页。\n如果找到，将直接在 Buffer Pool 中进行增删改查。\n如果找不到，则从 free 链表中找到一个空闲的缓存页，然后从磁盘文件中读取对应的数据页的数据到缓存页中，并且将数据页的信息和缓存页的地址写入到对应的描述数据块中，然后修改相关的描述数据块的 free_pre 指针和 free_next 指针，将使用了的描述数据块从 free 链表中移除。记得，还要在数据页缓存哈希表中写入对应的 key-value 对。最后也是在 Buffer Pool 中进行增删改查。\n\n3. LRU 链表和Flush链表的区别\nLRU 链表 flush，由用户线程触发(MySQL 5.6.2之前)；而Flush 链表 flush由MySQL数据库InnoDB存储引擎后台srv_master线程处理。(在MySQL 5.6.2之后，都被迁移到Page Cleaner线程中)。\nLRU 链表 flush，其目的是为了写出LRU 链表尾部的脏页，释放足够的空闲页，当Buffer Pool满的时候，用户可以立即获得空闲页面，而不需要长时间等待；Flush 链表 flush，其目的是推进Checkpoint LSN，使得InnoDB系统崩溃之后能够快速的恢复。\nLRU 链表 flush，其写出的脏页，需要从LRU链表中删除，移动到Free 链表。Flush List flush，不需要移动page在LRU链表中的位置。\nLRU 链表 flush，每次flush的脏页数量较少，基本固定，只要释放一定的空闲页即可；Flush 链表 flush，根据当前系统的更新繁忙程度，动态调整一次flush的脏页数量，量很大。\n在Flush 链表上的页面一定在LRU 链表上，反之则不成立。\n\n4. 触发刷脏页的条件\nREDO日志快用满的时候。由于MySQL更新是先写REDO日志，后面再将数据Flush到磁盘，如果REDO日志对应脏数据还没有刷新到磁盘就被覆盖的话，万一发生Crash，数据就无法恢复了。此时会从Flush 链表里面选取脏页，进行Flush。\n为了保证MySQL中的空闲页面的数量，Page Cleaner线程会从LRU 链表尾部淘汰一部分页面作为空闲页。如果对应的页面是脏页的话，就需要先将页面Flush到磁盘。\nMySQL中脏页太多的时候。innodb_max_dirty_pages_pct 表示的是Buffer Pool最大的脏页比例，默认值是75%，当脏页比例大于这个值时会强制进行刷脏页，保证系统有足够可用的Free Page。innodb_max_dirty_pages_pct_lwm参数控制的是脏页比例的低水位，当达到该参数设定的时候，会进行preflush，避免比例达到innodb_max_dirty_pages_pct 来强制Flush，对MySQL实例产生影响。\nMySQL实例正常关闭的时候，也会触发MySQL把内存里面的脏页全部刷新到磁盘。\n\nInnodb 的策略是在运行过程中尽可能的多占用内存，因此未被使用的页面会很少。当我们读取的数据不在Buffer Pool里面时，就需要申请一个空闲页来存放。如果没有足够的空闲页时，就必须从LRU 链表的尾部淘汰页面。如果该页面是干净的，可以直接拿来用，如果是脏页，就需要进行刷脏操作，将内存数据Flush到磁盘。\n所以，如果出现以下情况，是很容易影响MySQL实例的性能：\n\n一个SQL查询的数据页需要淘汰的页面过多\n实例是个写多型的MySQL，checkpoint跟不上日志产生量，会导致更新全部堵塞，TPS跌0。\n\ninnodb_io_capacity 参数定义了Innodb 后台任务的IO能力，例如刷脏操作还有Change Buffer的merge操作等。\nInnodb 的三种Page和链表的设计，保证了我们需要的热数据常驻在内存，及时淘汰不需要的数据，提升了我们的查询速度，同时不同的刷脏策略也提高了我们的恢复速度，保证了数据安全。\n","categories":["MySQL"]},{"title":"什么是 Change Data Capture ？","url":"/posts/33848/","content":"Change Data Capture (缩写为 CDC)—— 大概可以机翻为 “变动数据捕获”—— 你可以将它视为和数据库有关的架构设计模式的一种。它的核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入，更新，删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。\n一、适用场景我们可以把 CDC 认为是数据库事件驱动的一种数据 / 信息分发系统，CDC 主要适用于以下的场景：\n异构数据库之间的数据同步或备份 / 建立数据分析计算平台在 MySQL，PostgreSQL，MongoDB 等等数据库之间互相同步数据，或者把这些数据库的数据同步到 Elasticsearch 里以供全文搜索，当然也可以基于 CDC 对数据库进行备份。而数据分析系统可以通过订阅感兴趣的数据表的变更，来获取所需要的分析数据进行处理，不需要把分析流程嵌入到已有系统中，以实现解耦。\n\n微服务之间共享数据状态在微服务大行其道的今日，微服务之间信息共享一直比较复杂，CDC 也是一种可能的解决方案，微服务可以通过 CDC 来获取其他微服务数据库的变更，从而获取数据的状态更新，执行自己相应的逻辑。\n\n更新缓存 / CQRS 的 Query 视图更新通常缓存更新都比较难搞，可以通过 CDC 来获取数据库的数据更新事件，从而控制对缓存的刷新或失效。\n而 CQRS 是什么又是一个很大的话题，简单来讲，你可以把 CQRS 理解为一种高配版的读写分离的设计模式。举个例子，我们前面讲了可以利用 CDC 将 MySQL 的数据同步到 Elasticsearch 中以供搜索，在这样的架构里，所有的查询都用 ES 来查，但在想修改数据时，并不直接修改 ES 里的数据，而是修改上游的 MySQL 数据，使之产生数据更新事件，事件被消费者消费来更新 ES 中的数据，这就基本上是一种 CQRS 模式。而在其他 CQRS 的系统中，也可以利用类似的方式来更新查询视图。\n\n二、整体架构数据源数据库（即数据源）需要能完整地记录或输出数据库的事件（数据或数据表的变动）。例如 MySQL 的 binlog，MySQL 通过 binlog（二进制日志）记录数据库的变动事件，而 MySQL 本身可以借助 binlog 来实现主从复制。其实 CDC 本身也只是把这种机制扩展了一下，使之能够作为更广泛的用途。\n大部分数据库都有类似的机制：\n\nMySQL: binlog\nPostgreSQL: Streaming Replication Protocol\nMongoDB: oplog等等。\n\n发布服务（生产者）这东西现在没有一个明确的名称，这里就暂且称为发布服务。它的功能是解析数据源输出的流式数据，序列化成统一的封装格式，并输出到数据总线中。\nMaxwellzendesk/maxwell，专为 MySQL 数据源使用，它能够读取 binlog，将数据库变更序列化为 JSON，输出到 Kafka, Kinesis, RabbitMQ, Google Cloud Pub/Sub, Redis 等消息队列中。它还提供了 bootstrap 命令，支持将某张表全量数据输出作为起始数据。\nDebeziumdebezium/debezium，目前支持监听 MySQL，MongoDB，PostgreSQL，Oracle，SQL Server 等数据库的变化，但消息中间件只能使用 Kafka。它的数据交换默认采用 Avro 格式，这种格式对数据 schema 的变更比较友好。目前还处在开发阶段，所以文档不是特别完善，但似乎已经有勇敢的公司用在生产环境中了。\ncanalalibaba/canal，它能解析 binlog 并输出，可以利用它提供的 client 来实现自己的订阅消费逻辑，但这个项目不涉及到输出到通用的消息中间件，因此适用范围较窄。最近看了一下发现有了更新，支持输出到 kafka，甚至还带一些监控功能，看起来很强。\n自行实现你需要的是一个能够解析数据库输出流的库，比如 noplay/python-mysql-replication 这种，基于这样的库来实现发布服务。\n我曾经写过 py-mysql-elasticsearch-sync 这样一个工具，就是为了解决把 MySQL 的数据同步到 Elasticsearch 这样的任务，可以把这东西视为弱鸡版的 CDC（因为中间没有通过数据总线发布，而是直接写入 ES 了）。其中就使用了 python-mysql-replication 这个库来获取数据库变更，它的原理就是将自己伪装成一个 MySQL Slave 数据库，通过解析 binlog 获取数据变更的事件。（和 canal 原理上差不多）\n数据总线（消息中间件）数据总线这部分是一个消息中间件，一般会选 Kafka，RabbitMQ，Redis 之类的，但大部分做 CDC 都会选用 Kafka。\nKafka 官方并不把自己称为消息队列，而是叫 “distributed streaming platform”，的确，目前 kafka 的能力以及演化方向更像是一个数据处理的平台了，而不仅仅是消息中间件，你可以利用 Kafka Streams 来进行一些数据处理。\n在整个 CDC 里，Kafka 会作为核心的数据交换组件，或者你可以把它称为数据总线，kafka 集群的健壮性和吞吐量能够支撑海量数据的 pub/sub，并且能够将写入的数据持久化一段时间，发布服务将数据库任何数据变动写入 Kafka，由不同的消费者在上面同时进行订阅和消费，如果有需要，消费者随时可以把自己的 offset 往前调，对以往的消息重新消费。\n消费者消费者部分根据场景而不同，通常只要使用相应消息队列的客户端库实现消费者，根据生产者生成的消息格式进行解析处理即可，例如你用 Kafka 作为队列，那么在 Python 中可以用 confluentinc/confluent-kafka-python 之类的库来进行实现。而需求比较简单的话可以使用直接现成的开源工具，比如 Kafka 的一些 Sink Connectors。\n如果有特殊的需求，可以自行实现类似的生产者，消费者（或者借助已有的工具进行改造），将数据变动通过 Kafka 之类的消息总线进行发布，来自定义自己的 CDC 系统。\n","categories":["MySQL"]},{"title":"庖丁解InnoDB之REDO LOG","url":"/posts/59699/","content":"数据库故障恢复机制的前世今生中介绍了，磁盘数据库为了在保证数据库的原子性(A, Atomic) 和持久性(D, Durability)的同时，还能以灵活的刷盘策略来充分利用磁盘顺序写的性能，会记录REDO和UNDO日志，即ARIES方法。本文将重点介绍REDO LOG的作用，记录的内容，组织结构，写入方式等内容，希望读者能够更全面准确的理解REDO LOG在InnoDB中的位置。本文基于MySQL 8.0代码。\n1. 为什么需要记录REDO为了取得更好的读写性能，InnoDB会将数据缓存在内存中（InnoDB Buffer Pool），对磁盘数据的修改也会落后于内存，这时如果进程或机器崩溃，会导致内存数据丢失，为了保证数据库本身的一致性和持久性，InnoDB维护了REDO LOG。修改Page之前需要先将修改的内容记录到REDO中，并保证REDO LOG早于对应的Page落盘，也就是常说的WAL，Write Ahead Log。当故障发生导致内存数据丢失后，InnoDB会在重启时，通过重放REDO，将Page恢复到崩溃前的状态。\n2. 需要什么样的REDO那么我们需要什么样的REDO呢？首先，REDO的维护增加了一份写盘数据，同时为了保证数据正确，事务只有在他的REDO全部落盘才能返回用户成功，REDO的写盘时间会直接影响系统吞吐，显而易见，REDO的数据量要尽量少。其次，系统崩溃总是发生在始料未及的时候，当重启重放REDO时，系统并不知道哪些REDO对应的Page已经落盘，因此REDO的重放必须可重入，即REDO操作要保证幂等。最后，为了便于通过并发重放的方式加快重启恢复速度，REDO应该是基于Page的，即一个REDO只涉及一个Page的修改。\n熟悉的读者会发现，数据量小是Logical Logging的优点，而幂等以及基于Page正是Physical Logging的优点，因此InnoDB采取了一种称为Physiological Logging的方式，来兼得二者的优势。所谓Physiological Logging，就是以Page为单位，但在Page内以逻辑的方式记录。举个例子，MLOG_REC_UPDATE_IN_PLACE类型的REDO中记录了对Page中一个Record的修改，方法如下：\n\n（Page ID，Record Offset，(Filed 1, Value 1) … (Filed i, Value i) … )\n\n其中，PageID指定要操作的Page页，Record Offset记录了Record在Page内的偏移位置，后面的Field数组，记录了需要修改的Field以及修改后的Value。\n由于Physiological Logging的方式采用了物理Page中的逻辑记法，导致两个问题：\n1，需要基于正确的Page状态上重放REDO\n由于在一个Page内，REDO是以逻辑的方式记录了前后两次的修改，因此重放REDO必须基于正确的Page状态。然而InnoDB默认的Page大小是16KB，是大于文件系统能保证原子的4KB大小的，因此可能出现Page内容成功一半的情况。InnoDB中采用了Double Write Buffer的方式来通过写两次的方式保证恢复的时候找到一个正确的Page状态。这部分会在之后介绍Buffer Pool的时候详细介绍。\n2，需要保证REDO重放的幂等\nDouble Write Buffer能够保证找到一个正确的Page状态，我们还需要知道这个状态对应REDO上的哪个记录，来避免对Page的重复修改。为此，InnoDB给每个REDO记录一个全局唯一递增的标号**LSN(Log Sequence Number)**。Page在修改时，会将对应的REDO记录的LSN记录在Page上（FIL_PAGE_LSN字段），这样恢复重放REDO时，就可以来判断跳过已经应用的REDO，从而实现重放的幂等。\n3. REDO中记录了什么内容知道了InnoDB中记录REDO的方式，那么REDO里具体会记录哪些内容呢？为了应对InnoDB各种各样不同的需求，到MySQL 8.0为止，已经有多达65种的REDO记录。用来记录这不同的信息，恢复时需要判断不同的REDO类型，来做对应的解析。根据REDO记录不同的作用对象，可以将这65中REDO划分为三个大类：作用于Page，作用于Space以及提供额外信息的Logic类型。\n1，作用于Page的REDO\n这类REDO占所有REDO类型的绝大多数，根据作用的Page的不同类型又可以细分为，Index Page REDO，Undo Page REDO，Rtree PageREDO等。比如MLOG_REC_INSERT，MLOG_REC_UPDATE_IN_PLACE，MLOG_REC_DELETE三种类型分别对应于Page中记录的插入，修改以及删除。这里还是以MLOG_REC_UPDATE_IN_PLACE为例来看看其中具体的内容：\n\n其中，Type就是MLOG_REC_UPDATE_IN_PLACE类型，Space ID和Page Number唯一标识一个Page页，这三项是所有REDO记录都需要有的头信息，后面的是MLOG_REC_UPDATE_IN_PLACE类型独有的，其中Record Offset用给出要修改的记录在Page中的位置偏移，Update Field Count说明记录里有几个Field要修改，紧接着对每个Field给出了Field编号(Field Number)，数据长度（Field Data Length）以及数据（Filed Data）。\n2，作用于Space的REDO\n这类REDO针对一个Space文件的修改，如MLOG_FILE_CREATE，MLOG_FILE_DELETE，MLOG_FILE_RENAME分别对应对一个Space的创建，删除以及重命名。由于文件操作的REDO是在文件操作结束后才记录的，因此在恢复的过程中看到这类日志时，说明文件操作已经成功，因此在恢复过程中大多只是做对文件状态的检查，以MLOG_FILE_CREATE来看看其中记录的内容：\n\n同样的前三个字段还是Type，Space ID和Page Number，由于是针对Page的操作，这里的Page Number永远是0。在此之后记录了创建的文件flag以及文件名，用作重启恢复时的检查。\n3，提供额外信息的Logic REDO\n除了上述类型外，还有少数的几个REDO类型不涉及具体的数据修改，只是为了记录一些需要的信息，比如最常见的MLOG_MULTI_REC_END就是为了标识一个REDO组，也就是一个完整的原子操作的结束。\n4. REDO是如何组织的所谓REDO的组织方式，就是如何把需要的REDO内容记录到磁盘文件中，以方便高效的REDO写入，读取，恢复以及清理。我们这里把REDO从上到下分为三层：逻辑REDO层、物理REDO层和文件层。\n逻辑REDO层这一层是真正的REDO内容，REDO由多个不同Type的多个REDO记录收尾相连组成，有全局唯一的递增的偏移sn，InnoDB会在全局log_sys中维护当前sn的最大值，并在每次写入数据时将sn增加REDO内容长度。如下图所示：\n\n物理REDO层磁盘是块设备，InnoDB中也用Block的概念来读写数据，一个Block的长度OS_FILE_LOG_BLOCK_SIZE等于磁盘扇区的大小512B，每次IO读写的最小单位都是一个Block。除了REDO数据以外，Block中还需要一些额外的信息，下图所示一个Log Block的的组成，包括12字节的Block Header：前4字节中Flush Flag占用最高位bit，标识一次IO的第一个Block，剩下的31个个bit是Block编号；之后是2字节的数据长度，取值在[12，508]；紧接着2字节的First Record Offset用来指向Block中第一个REDO组的开始，这个值的存在使得我们对任何一个Block都可以找到一个合法的的REDO开始位置；最后的4字节Checkpoint Number记录写Block时的next_checkpoint_number，用来发现文件的循环使用，这个会在文件层详细讲解。Block末尾是4字节的Block Tailer，记录当前Block的Checksum，通过这个值，读取Log时可以明确Block数据有没有被完整写盘。\n\nBlock中剩余的中间498个字节就是REDO真正内容的存放位置，也就是我们上面说的逻辑REDO。我们现在将逻辑REDO放到物理REDO空间中，由于Block内的空间固定，而REDO长度不定，因此可能一个Block中有多个REDO，也可能一个REDO被拆分到多个Block中，如下图所示，棕色和红色分别代表Block Header和Tailer，中间的REDO记录由于前一个Block剩余空间不足，而被拆分在连续的两个Block中。\n\n由于增加了Block Header和Tailer的字节开销，在物理REDO空间中用LSN来标识偏移，可以看出LSN和SN之间有简单的换算关系：\nconstexpr inline lsn_t log_translate_sn_to_lsn(lsn_t sn) &#123;  return (sn / LOG_BLOCK_DATA_SIZE * OS_FILE_LOG_BLOCK_SIZE +          sn % LOG_BLOCK_DATA_SIZE + LOG_BLOCK_HDR_SIZE);&#125;\n\nSN加上之前所有的Block的Header以及Tailer的长度就可以换算到对应的LSN，反之亦然。\n文件层最终REDO会被写入到REDO日志文件中，以ib_logfile0、ib_logfile1…命名，为了避免创建文件及初始化空间带来的开销，InooDB的REDO文件会循环使用，通过参数innodb_log_files_in_group可以指定REDO文件的个数。多个文件收尾相连顺序写入REDO内容。每个文件以Block为单位划分，每个文件的开头固定预留4个Block来记录一些额外的信息，其中第一个Block称为Header Block，之后的3个Block在0号文件上用来存储Checkpoint信息，而在其他文件上留空：\n\n其中第一个Header Block的数据区域记录了一些文件信息，如下图所示，4字节的Formate字段记录Log的版本，不同版本的LOG，会有REDO类型的增减，这个信息是8.0开始才加入的；8字节的Start LSN标识当前文件开始LSN，通过这个信息可以将文件的offset与对应的lsn对应起来；最后是最长32位的Creator信息，正常情况下会记录MySQL的版本。\n\n现在我们将REDO放到文件空间中，如下图所示，逻辑REDO是真正需要的数据，用sn索引，逻辑REDO按固定大小的Block组织，并添加Block的头尾信息形成物理REDO，以lsn索引，这些Block又会放到循环使用的文件空间中的某一位置，文件中用offset索引：\n\n虽然通过LSN可以唯一标识一个REDO位置，但最终对REDO的读写还需要转换到对文件的读写IO，这个时候就需要表示文件空间的offset，他们之间的换算方式如下：\nconst auto real_offset =      log.current_file_real_offset + (lsn - log.current_file_lsn);\n\n切换文件时会在内存中更新当前文件开头的文件offset，current_file_real_offset，以及对应的LSN，current_file_lsn，通过这两个值可以方便地用上面的方式将LSN转化为文件offset。注意这里的offset是相当于整个REDO文件空间而言的，由于InnoDB中读写文件的space层实现支持多个文件，因此，可以将首位相连的多个REDO文件看成一个大文件，那么这里的offset就是这个大文件中的偏移。\n5. 如何高效地写REDO作为维护数据库正确性的重要信息，REDO日志必须在事务提交前保证落盘，否则一旦断电将会有数据丢失的可能，因此从REDO生成到最终落盘的完整过程成为数据库写入的关键路径，其效率也直接决定了数据库的写入性能。这个过程包括REDO内容的产生，REDO写入InnoDB Log Buffer，从InnoDB Log Buffer写入操作系统Page Cache，以及REDO刷盘，之后还需要唤醒等待的用户线程完成Commit。下面就通过这几个阶段来看看InnoDB如何在高并发的情况下还能高效地完成写REDO。\nREDO产生我们知道事务在写入数据的时候会产生REDO，一次原子的操作可能会包含多条REDO记录，这些REDO可能是访问同一Page的不同位置，也可能是访问不同的Page（如Btree节点分裂）。InnoDB有一套完整的机制来保证涉及一次原子操作的多条REDO记录原子，即恢复的时候要么全部重放，要不全部不重放，这部分将在之后介绍恢复逻辑的时候详细介绍，本文只涉及其中最基本的要求，就是这些REDO必须连续。InnoDB中通过min-transaction实现，简称mtr，需要原子操作时，调用mtr_start生成一个mtr，mtr中会维护一个动态增长的m_log，这是一个动态分配的内存空间，将这个原子操作需要写的所有REDO先写到这个m_log中，当原子操作结束后，调用mtr_commit将m_log中的数据拷贝到InnoDB的Log Buffer。\n写入InnoDB Log Buffer高并发的环境中，会同时有非常多的min-transaction(mtr)需要拷贝数据到Log Buffer，如果通过锁互斥，那么毫无疑问这里将成为明显的性能瓶颈。为此，从MySQL 8.0开始，设计了一套无锁的写log机制，其核心思路是允许不同的mtr，同时并发地写Log Buffer的不同位置。不同的mtr会首先调用log_buffer_reserve函数，这个函数里会用自己的REDO长度，原子地对全局偏移log.sn做fetch_add，得到自己在Log Buffer中独享的空间。之后不同mtr并行的将自己的m_log中的数据拷贝到各自独享的空间内。\n/* Reserve space in sequence of data bytes: */const sn_t start_sn = log.sn.fetch_add(len);\n\n写入Page Cache写入到Log Buffer中的REDO数据需要进一步写入操作系统的Page Cache，InnoDB中有单独的log_writer来做这件事情。这里有个问题，由于Log Buffer中的数据是不同mtr并发写入的，这个过程中Log Buffer中是有空洞的，因此log_writer需要感知当前Log Buffer中连续日志的末尾，将连续日志通过pwrite系统调用写入操作系统Page Cache。整个过程中应尽可能不影响后续mtr进行数据拷贝，InnoDB在这里引入一个叫做link_buf的数据结构，如下图所示：\n\nlink_buf是一个循环使用的数组，对每个lsn取模可以得到其在link_buf上的一个槽位，在这个槽位中记录REDO长度。另外一个线程从开始遍历这个link_buf，通过槽位中的长度可以找到这条REDO的结尾位置，一直遍历到下一位置为0的位置，可以认为之后的REDO有空洞，而之前已经连续，这个位置叫做link_buf的tail。下面看看log_writer和众多mtr是如何利用这个link_buf数据结构的。这里的这个link_buf为log.recent_written，如下图所示：\n\n图中上半部分是REDO日志示意图，write_lsn是当前log_writer已经写入到Page Cache中日志末尾，current_lsn是当前已经分配给mtr的的最大lsn位置，而buf_ready_for_write_lsn是当前log_writer找到的Log Buffer中已经连续的日志结尾，从write_lsn到buf_ready_for_write_lsn是下一次log_writer可以连续调用pwrite写入Page Cache的范围，而从buf_ready_for_write_lsn到current_lsn是当前mtr正在并发写Log Buffer的范围。下面的连续方格便是log.recent_written的数据结构，可以看出由于中间的两个全零的空洞导致buf_ready_for_write_lsn无法继续推进，接下来，假如reserve到中间第一个空洞的mtr也完成了写Log Buffer，并更新了log.recent_written*，如下图：\n\n这时，log_writer从当前的buf_ready_for_write_lsn向后遍历log.recent_written，发现这段已经连续：\n\n因此提升当前的buf_ready_for_write_lsn，并将log.recent_written的tail位置向前滑动，之后的位置清零，供之后循环复用：\n\n紧接log_writer将连续的内容刷盘并提升write_lsn。\n刷盘log_writer提升write_lsn之后会通知log_flusher线程，log_flusher线程会调用fsync将REDO刷盘，至此完成了REDO完整的写入过程。\n唤醒用户线程为了保证数据正确，只有REDO写完后事务才可以commit，因此在REDO写入的过程中，大量的用户线程会block等待，直到自己的最后一条日志结束写入。默认情况下innodb_flush_log_at_trx_commit = 1，需要等REDO完成刷盘，这也是最安全的方式。当然，也可以通过设置innodb_flush_log_at_trx_commit = 2，这样，只要REDO写入Page Cache就认为完成了写入，极端情况下，掉电可能导致数据丢失。\n大量的用户线程调用log_write_up_to等待在自己的lsn位置，为了避免大量无效的唤醒，InnoDB将阻塞的条件变量拆分为多个，log_write_up_to根据自己需要等待的lsn所在的block取模对应到不同的条件变量上去。同时，为了避免大量的唤醒工作影响log_writer或log_flusher线程，InnoDB中引入了两个专门负责唤醒用户的线程：log_wirte_notifier和log_flush_notifier，当超过一个条件变量需要被唤醒时，log_writer和log_flusher会通知这两个线程完成唤醒工作。下图是整个过程的示意图：\n\n多个线程通过一些内部数据结构的辅助，完成了高效的从REDO产生，到REDO写盘，再到唤醒用户线程的流程，下面是整个这个过程的时序图：\n\n6. 如何安全地清除REDO由于REDO文件空间有限，同时为了尽量减少恢复时需要重放的REDO，InnoDB引入log_checkpointer线程周期性的打Checkpoint。重启恢复的时候，只需要从最新的Checkpoint开始回放后边的REDO，因此Checkpoint之前的REDO就可以删除或被复用。\n我们知道REDO的作用是避免只写了内存的数据由于故障丢失，那么打Checkpiont的位置就必须保证之前所有REDO所产生的内存脏页都已经刷盘。最直接的，可以从Buffer Pool中获得当前所有脏页对应的最小REDO LSN：lwm_lsn。 但光有这个还不够，因为有一部分min-transaction的REDO对应的Page还没有来的及加入到Buffer Pool的脏页中去，如果checkpoint打到这些REDO的后边，一旦这时发生故障恢复，这部分数据将丢失，因此还需要知道当前已经加入到Buffer Pool的REDO lsn位置：dpa_lsn。取二者的较小值作为最终checkpoint的位置，其核心逻辑如下：\n/* LWM lsn for unflushed dirty pages in Buffer Pool */lsn_t lwm_lsn = buf_pool_get_oldest_modification_lwm();/* Note lsn up to which all dirty pages have already been added into Buffer Pool */const lsn_t dpa_lsn = log_buffer_dirty_pages_added_up_to_lsn(log);lsn_t checkpoint_lsn = std::min(lwm_lsn, dpa_lsn);\n\nMySQL 8.0中为了能够让mtr之间更大程度的并发，允许并发地给Buffer Pool注册脏页。类似与log.recent_written和log_writer，这里引入一个叫做recent_closed的link_buf来处理并发带来的空洞，由单独的线程log_closer来提升recent_closed的tail，也就是当前连续加入Buffer Pool脏页的最大LSN，这个值也就是上面提到的dpa_lsn。需要注意的是，由于这种乱序的存在，lwm_lsn的值并不能简单的获取当前Buffer Pool中的最老的脏页的LSN，保守起见，还需要减掉一个recent_closed的容量大小，也就是最大的乱序范围，简化后的代码如下：\n/* LWM lsn for unflushed dirty pages in Buffer Pool */const lsn_t lsn = buf_pool_get_oldest_modification_approx();const lsn_t lag = log.recent_closed.capacity();lsn_t lwm_lsn = lsn - lag;/* Note lsn up to which all dirty pages have already been added into Buffer Pool */const lsn_t dpa_lsn = log_buffer_dirty_pages_added_up_to_lsn(log);lsn_t checkpoint_lsn = std::min(lwm_lsn, dpa_lsn);\n\n这里有一个问题，由于lwm_lsn已经减去了recent_closed的capacity，因此理论上这个值一定是小于dpa_lsn的。那么再去比较lwm_lsn和dpa_lsn来获取Checkpoint位置或许是没有意义的。\n上面已经提到，ib_logfile0文件的前三个Block有两个被预留作为Checkpoint Block，这两个Block会在打Checkpiont的时候交替使用，这样来避免写Checkpoint过程中的崩溃导致没有可用的Checkpoint。Checkpoint Block中的内容如下：\n\n首先8个字节的Checkpoint Number，通过比较这个值可以判断哪个是最新的Checkpiont记录，之后8字节的Checkpoint LSN为打Checkpoint的REDO位置，恢复时会从这个位置开始重放后边的REDO。之后8个字节的Checkpoint Offset，将Checkpoint LSN与文件空间的偏移对应起来。最后8字节是前面提到的Log Buffer的长度，这个值目前在恢复过程并没有使用。\n7. 总结本文系统的介绍了InnoDB中REDO的作用、特性、组织结构、写入方式已经清理时机，基本覆盖了REDO的大多数内容。关于重启恢复时如何使用REDO将数据库恢复到正确的状态，将在之后介绍InnoDB故障恢复机制的时候详细介绍。\n","categories":["MySQL"]},{"title":"庖丁解InnoDB之Undo LOG","url":"/posts/43631/","content":"Undo Log是InnoDB十分重要的组成部分，它的作用横贯InnoDB中两个最主要的部分，并发控制（Concurrency Control）和故障恢复（Crash Recovery），InnoDB中Undo Log的实现亦日志亦数据。本文将从其作用、设计思路、记录内容、组织结构，以及各种功能实现等方面，整体介绍InnoDB中的Undo Log，文章会深入一定的代码实现，但在细节上还是希望用抽象的实现思路代替具体的代码。本文基于MySQL 8.0，但在大多数的设计思路上MySQL的各个版本都是一致的。考虑到篇幅有限，以及避免过多信息的干扰，从而能够聚焦Undo Log本身的内容，本文中一笔带过或有意省略了一些内容，包括索引、事务系统、临时表、XA事务、Virtual Column、外部记录、Blob等。\n一、Undo Log的作用数据库故障恢复机制的前世今生中提到过，Undo Log用来记录每次修改之前的历史值，配合Redo Log用于故障恢复。这也就是InnoDB中Undo Log的第一个作用：\n1. 事务回滚在设计数据库时，我们假设数据库可能在任何时刻，由于如硬件故障，软件Bug，运维操作等原因突然崩溃。这个时候尚未完成提交的事务可能已经有部分数据写入了磁盘，如果不加处理，会违反数据库对Atomic的保证，也就是任何事务的修改要么全部提交，要么全部取消。针对这个问题，直观的想法是等到事务真正提交时，才能允许这个事务的任何修改落盘，也就是No-Steal策略。显而易见，这种做法一方面造成很大的内存空间压力，另一方面提交时的大量随机IO会极大的影响性能。因此，数据库实现中通常会在正常事务进行中，就不断的连续写入Undo Log，来记录本次修改之前的历史值。当Crash真正发生时，可以在Recovery过程中通过回放Undo Log将未提交事务的修改抹掉。InnoDB采用的就是这种方式。\n既然已经有了在Crash Recovery时支持事务回滚的Undo Log，自然地，在正常运行过程中，死锁处理或用户请求的事务回滚也可以利用这部分数据来完成。\n2. MVCC（Multi-Versioin Concurrency Control）浅析数据库并发控制机制中提到过，为了避免只读事务与写事务之间的冲突，避免写操作等待读操作，几乎所有的主流数据库都采用了多版本并发控制（MVCC）的方式，也就是为每条记录保存多份历史数据供读事务访问，新的写入只需要添加新的版本即可，无需等待。InnoDB在这里复用了Undo Log中已经记录的历史版本数据来满足MVCC的需求。\n二、什么样的Undo Log庖丁解InnoDB之REDO LOG中讲过的基于Page的Redo Log可以更好的支持并发的Redo应用，从而缩短DB的Crash Recovery时间。而对于Undo Log来说，InnoDB用Undo Log来实现MVCC，DB运行过程中是允许有历史版本的数据存在的。因此，Crash Recovery时利用Undo Log的事务回滚完全可以在后台，像正常运行的事务一样异步回滚，从而让数据库先恢复服务。因此，Undo Log的设计思路不同于Redo Log，Undo Log需要的是事务之间的并发，以及方便的多版本数据维护，其重放逻辑不希望因DB的物理存储变化而变化。因此，InnoDB中的Undo Log采用了基于事务的Logical Logging的方式。\n同时，更多的责任意味着更复杂的管理逻辑，InnoDB中其实是把Undo当做一种数据来维护和使用的，也就是说，Undo Log日志本身也像其他的数据库数据一样，会写自己对应的Redo Log，通过Redo Log来保证自己的原子性。因此，更合适的称呼应该是Undo Data。\n三、Undo Record中的内容每当InnoDB中需要修改某个Record时，都会将其历史版本写入一个Undo Log中，对应的Undo Record是Update类型。当插入新的Record时，还没有一个历史版本，但为了方便事务回滚时做逆向（Delete）操作，这里还是会写入一个Insert类型的Undo Record。\nInsert类型的Undo Record这种Undo Record在代码中对应的是TRX_UNDO_INSERT_REC类型。不同于Update类型的Undo Record，Insert Undo Record仅仅是为了可能的事务回滚准备的，并不在MVCC功能中承担作用。因此只需要记录对应Record的Key，供回滚时查找Record位置即可。\n\n其中Undo Number是Undo的一个递增编号，Table ID用来表示是哪张表的修改。下面一组Key Fields的长度不定，因为对应表的主键可能由多个field组成，这里需要记录Record完整的主键信息，回滚的时候可以通过这个信息在索引中定位到对应的Record。除此之外，在Undo Record的头尾还各留了两个字节用户记录其前序和后继Undo Record的位置。\nUpdate类型的Undo Record由于MVCC需要保留Record的多个历史版本，当某个Record的历史版本还在被使用时，这个Record是不能被真正的删除的。因此，当需要删除时，其实只是修改对应Record的Delete Mark标记。对应的，如果这时这个Record又重新插入，其实也只是修改一下Delete Mark标记，也就是将这两种情况的delete和insert转变成了update操作。再加上常规的Record修改，因此这里的Update Undo Record会对应三种Type：TRX_UNDO_UPD_EXIST_REC、TRX_UNDO_DEL_MARK_REC 和 TRX_UNDO_UPD_DEL_REC。他们的存储内容也类似：\n\n除了跟Insert Undo Record相同的头尾信息，以及主键Key Fileds之外，Update Undo Record增加了：\n\nTransaction Id记录了产生这个历史版本事务Id，用作后续MVCC中的版本可见性判断\nRollptr指向的是该记录的上一个版本的位置，包括space number，page number和page内的offset。沿着Rollptr可以找到一个Record的所有历史版本。\nUpdate Fields中记录的就是当前这个Record版本相对于其之后的一次修改的Delta信息，包括所有被修改的Field的编号，长度和历史值。\n\n四、Undo Record的组织方式上面介绍了一个Undo Record中的存放的内容，每一次的修改都会产生至少一个Undo Record，那么大量Undo Record如何组织起来，来支持高效的访问和管理呢，这一小节我们将从几个层面来进行介绍：首先是在不考虑物理存储的情况下的逻辑组织方式； 之后，物理组织方式介绍如何将其存储到实际16KB物理块中；然后文件组织方式介绍整体的文件结构；最后再介绍其在内存中的组织方式。\n逻辑组织方式 - Undo Log每个事务其实会修改一组的Record，对应的也就会产生一组Undo Record，这些Undo Record收尾相连就组成了这个事务的Undo Log。除了一个个的Undo Record之外，还在开头增加了一个Undo Log Header来记录一些必要的控制信息，因此，一个Undo Log的结构如下所示：\nUndo Log Header中记录了产生这个Undo Log的事务的 Trx ID；\nTrx No 是事务的提交顺序，也会用这个来判断是否能Purge，这个在后面会详细介绍；\nDelete Mark 表明该Undo Log中有没有 TRX_UNDO_DEL_MARK_REC 类型的Undo Record，避免Purge时不必要的扫描；\nLog Start Offset 中记录Undo Log Header的结束位置，方便之后Header中增加内容时的兼容；\n之后是一些Flag信息；\nNext Undo Log 及 Prev Undo Log 标记前后两个Undo Log，这个会在接下来介绍；\n最后通过 History List Node 将自己挂载到为Purge准备的History List中。\n索引中的同一个Record被不同事务修改，会产生不同的历史版本，这些历史版本又通过Rollptr穿成一个链表，供MVCC使用。如下图所示：\n\n示例中有三个事务操作了表t上，主键id是1的记录，首先事务I插入了这条记录并且设置filed a的值是A，之后事务J和事务K分别将这条id为1的记录中的filed a的值修改为了B和C。\nI，J，K三个事务分别有自己的逻辑上连续的三条Undo Log，每条Undo Log有自己的Undo Log Header。\n从索引中的这条Record沿着Rollptr可以依次找到这三个事务Undo Log中关于这条记录的历史版本。\n同时可以看出，Insert类型Undo Record中只记录了对应的主键值：id=1，而Update类型的Undo Record中还记录了对应的历史版本的生成事务Trx_id，以及被修改的field a的历史值。\n物理组织格式 - Undo Segment上面描述了一个Undo Log的结构，一个事务会产生多大的Undo Log本身是不可控的，而最终写入磁盘却是按照固定的块大小为单位的，InnoDB中默认是16KB，那么如何用固定的块大小承载不定长的Undo Log，以实现高效的空间分配、复用，避免空间浪费。\nInnoDB的基本思路是让多个较小的Undo Log紧凑存在一个Undo Page中，而对较大的Undo Log则随着不断的写入，按需分配足够多的Undo Page分散承载。下面我们就看看这部分的物理存储方式：\n\n如上所示，是一个Undo Segment的示意图，每个写事务开始写操作之前都需要持有一个Undo Segment，一个Undo Segment中的所有磁盘空间的分配和释放，也就是16KB Page的申请和释放，都是由一个FSP的Segment管理的，这个跟索引中的Leaf Node Segment和Non-Leaf Node Segment的管理方式是一致的，这部分之后会有单独的文章来进行介绍。\nUndo Segment会持有至少一个Undo Page，每个Undo Page会在开头38字节到56字节记录Undo Page Header，其中记录Undo Page的类型、最后一条Undo Record的位置，当前Page还空闲部分的开头，也就是下一条Undo Record要写入的位置。Undo Segment中的第一个Undo Page还会在56字节到86字节记录Undo Segment Header，这个就是这个Undo Segment中磁盘空间管理的Handle；其中记录的是这个Undo Segment的状态，比如TRX_UNDO_CACHED、TRX_UNDO_TO_PURGE等；这个Undo Segment中最后一条Undo Record的位置；这个FSP Segment的Header，以及当前分配出来的所有Undo Page的链表。\nUndo Page剩余的空间都是用来存放Undo Log的，对于像上图Undo Log 1，Undo Log 2这种较短的Undo Log，为了避免Page内的空间浪费，InnoDB会复用Undo Page来存放多个Undo Log，而对于像Undo Log 3这种比较长的Undo Log可能会分配多个Undo Page来存放。需要注意的是Undo Page的复用只会发生在第一个Page。\n文件组织方式 - Undo Tablespace每一时刻一个Undo Segment都是被一个事务独占的。每个写事务都会持有至少一个Undo Segment，当有大量写事务并发运行时，就需要存在多个Undo Segment。InnoDB中的Undo 文件中准备了大量的Undo Segment的槽位，按照1024一组划分为Rollback Segment。每个Undo Tablespace最多会包含128个Rollback Segment，Undo Tablespace文件中的第三个Page会固定作为这128个Rollback Segment的目录，也就是Rollback Segment Arrary Header，其中最多会有128个指针指向各个Rollback Segment Header所在的Page。Rollback Segment Header是按需分配的，其中包含1024个Slot，每个Slot占四个字节，指向一个Undo Segment的First Page。除此之前还会记录该Rollback Segment中已提交事务的History List，后续的Purge过程会顺序从这里开始回收工作。\n可以看出Rollback Segment的个数会直接影响InnoDB支持的最大事务并发数。MySQL 8.0由于支持了最多127个独立的Undo Tablespace，一方面避免了ibdata1的膨胀，方便undo空间回收，另一方面也大大增加了最大的Rollback Segment的个数，增加了可支持的最大并发写事务数。如下图所示：\n\n内存组织结构上面介绍的都是Undo数据在磁盘上的组织结构，除此之外，在内存中也会维护对应的数据结构来管理Undo Log，如下图所示：\n\n对应每个磁盘Undo Tablespace会有一个undo::Tablespace的内存结构，其中最主要的就是一组trx_rseg_t的集合，trx_rseg_t对应的就是上面介绍过的一个Rollback Segment Header，除了一些基本的元信息之外，trx_rseg_t中维护了四个trx_undo_t的链表，Update List中是正在被使用的用于写入Update类型Undo的Undo Segment；\nUpdate Cache List中是空闲空间比较多，可以被后续事务复用的Update类型Undo Segment;\n对应的，Insert List和Insert Cache List分别是正在使用中的Insert类型Undo Segment，和空间空间较多，可以被后续复用的Insert类型Undo Segment。因此trx_undo_t对应的就是上面介绍过的Undo Segment。\n接下来，我们就从Undo的写入、Undo用于Rollback、MVCC、Crash Recovery以及如何清理Undo等方面来介绍InnoDB中Undo的角色和功能。\n五、Undo的写入当写事务开始时，会先通过trx_assign_rseg_durable分配一个Rollback Segment，该事务的内存结构trx_t也会通过rsegs指针指向对应的trx_rseg_t内存结构，这里的分配策略很简单，就是依次尝试下一个Active的Rollback Segment。\n之后当第一次真正产生修改需要写Undo Record的时，会调用trx_undo_assign_undo来获得一个Undo Segment。这里会优先复用trx_rseg_t上Cached List中的trx_undo_t，也就是已经分配出来但没有被正在使用的Undo Segment，如果没有才调用trx_undo_create创建新的Undo Segment，trx_undo_create中会轮询选择当前Rollback Segment中可用的Slot，也是就值FIL_NUL的Slot，申请新的Undo Page，初始化Undo Page Header，Undo Segment Header等信息，创建新的trx_undo_t内存结构并挂到trx_rseg_t的对应List中。\n获得了可用的Undo Segment之后，该事务会在合适的位置初始化自己的Undo Log Header，之后，其所有修改产生的Undo Record都会顺序的通过trx_undo_report_row_operation顺序的写入当前的Undo Log，其中会根据是insert还是update类型，分别调用trx_undo_page_report_insert或者trx_undo_page_report_modify。\n本文开始已经介绍过了具体的Undo Record内容。简单的讲，insert类型会记录插入Record的主键，update类型除了记录主键以外还会有一个update fileds记录这个历史值跟索引值的diff。之后指向当前Undo Record位置的Rollptr会返回写入索引的Record上。\n当一个Page写满后，会调用trx_undo_add_page来在当前的Undo Segment上添加新的Page，新Page写入Undo Page Header之后继续供事务写入Undo Record，为了方便维护，这里有一个限制就是单条Undo Record不跨page，如果当前Page放不下，会将整个Undo Record写入下一个Page。\n当事务结束（commit或者rollback）之后，如果只占用了一个Undo Page，且当前Undo Page使用空间小于page的3/4，这个Undo Segment会保留并加入到对应的insert/update cached list中。否则，insert类型的Undo Segment会直接回收，而update类型的Undo Segment会等待后台的Purge做完后回收。根据不同的情况，Undo Segment Header中的State会被从TRX_UNDO_ACTIVE改成TRX_UNDO_TO_FREE，TRX_UNDO_TO_PURGE或TRX_UNDO_CACHED，这个修改其实就是InnoDB的事务结束的标志，无论是Rollback还是Commit，在这个修改对应的Redo落盘之后，就可以返回用户结果，并且Crash Recovery之后也不会再做回滚处理。\n六、Undo for RollbackInnoDB中的事务可能会由用户主动触发Rollback；也可能因为遇到死锁异常Rollback；或者发生Crash，重启后对未提交的事务回滚。在Undo层面来看，这些回滚的操作是一致的，基本的过程就是从该事务的Undo Log中，从后向前依次读取Undo Record，并根据其中内容做逆向操作，恢复索引记录。\n回滚的入口是函数row_undo，其中会先调用trx_roll_pop_top_rec_of_trx获取并删除该事务的最后一条Undo Record。\n如下图例子中的Undo Log包括三条Undo Records，其中Record 1在Undo Page 1中，Record 2，3在Undo Page 2中，先通过从Undo Segment Header中记录的Page List找到当前事务的最后一个Undo Page的Header，并根据Undo Page 2的Header上记录的Free Space Offset定位最后一条Undo Record结束的位置，当然实际运行时，这两个值是缓存在trx_undo_t的top_page_no和top_offset中的。利用Prev Record Offset可以找到Undo Record 3，做完对应的回滚操作之后，再通过前序指针Prev Record Offset找到前一个Undo Record，依次进行处理。处理完当前Page中的所有Undo Records后，再沿着Undo Page Header中的List找到前一个Undo Page，重复前面的过程，完成一个事务所有Page上的所有Undo Records的回滚。\n\n拿到一个Undo Record之后，自然地，就是对其中内容的解析，这里会调用row_undo_ins_parse_undo_rec，从Undo Record中获取修改行的table，解析出其中记录的主键信息，如果是update类型，还会拿到一个update vector记录其相对于更新的一个版本的变化。\nTRX_UNDO_INSERT_REC类型的Undo回滚在row_undo_ins中进行，insert的逆向操作当然就是delete，根据从Undo Record中解析出来的主键，用row_undo_search_clust_to_pcur定位到对应的ROW， 分别调用row_undo_ins_remove_sec_rec和row_undo_ins_remove_clust_rec在二级索引和主索引上将当前行删除。\nupdate类型的undo包括TRX_UNDO_UPD_EXIST_REC，TRX_UNDO_DEL_MARK_REC和TRX_UNDO_UPD_DEL_REC三种情况，他们的Undo回滚都是在row_undo_mod中进行，首先会调用row_undo_mod_del_unmark_sec_and_undo_update，其中根据从Undo Record中解析出的update vector来回退这次操作在所有二级索引上的影响，可能包括重新插入被删除的二级索引记录、去除其中的Delete Mark标记，或者用update vector中的diff信息将二级索引记录修改之前的值。之后调用row_undo_mod_clust同样利用update vector中记录的diff信息将主索引记录修改回之前的值。\n完成回滚的Undo Log部分，会调用trx_roll_try_truncate进行回收，对不再使用的page调用trx_undo_free_last_page将磁盘空间交还给Undo Segment，这个是写入过程中trx_undo_add_page的逆操作。\n七、Undo for MVCC多版本的目的是为了避免写事务和读事务的互相等待，那么每个读事务都需要在不对Record加Lock的情况下， 找到对应的应该看到的历史版本。\n所谓历史版本就是假设在该只读事务开始的时候对整个DB打一个快照，之后该事务的所有读请求都从这个快照上获取。\n当然实现上不能真正去为每个事务打一个快照，这个时间空间都太高了。\nInnoDB的做法，是在读事务第一次读取的时候获取一份ReadView，并一直持有，其中记录所有当前活跃的写事务ID，由于写事务的ID是自增分配的，通过这个ReadView我们可以知道在这一瞬间，哪些事务已经提交哪些还在运行，根据Read Committed的要求，未提交的事务的修改就是不应该被看见的，对应地，已经提交的事务的修改应该被看到。\n作为存储历史版本的Undo Record，其中记录的trx_id就是做这个可见性判断的，对应的主索引的Record上也有这个值。\n当一个读事务拿着自己的ReadView访问某个表索引上的记录时，会通过比较Record上的trx_id确定是否是可见的版本，如果不可见就沿着Record或Undo Record中记录的rollptr一路找更老的历史版本。\n如下图所示，事务R开始需要查询表t上的id为1的记录，R开始时事务I已经提交，事务J还在运行，事务K还没开始，这些信息都被记录在了事务R的ReadView中。事务R从索引中找到对应的这条Record[1, C]，对应的trx_id是K，不可见。沿着Rollptr找到Undo中的前一版本[1, B]，对应的trx_id是J，不可见。继续沿着Rollptr找到[1, A]，trx_id是I可见，返回结果。\n\n前面提到过，作为Logical Log，Undo中记录的其实是前后两个版本的diff信息，而读操作最终是要获得完整的Record内容的，也就是说这个沿着rollptr指针一路查找的过程中需要用Undo Record中的diff内容依次构造出对应的历史版本，这个过程在函数row_search_mvcc中，其中trx_undo_prev_version_build会根据当前的rollptr找到对应的Undo Record位置，这里如果rollptr指向的是insert类型，或者找到了已经Purge了的位置，说明到头了，会直接返回失败。否则，就会解析对应的Undo Record，恢复出trx_id、指向下一条Undo Record的rollptr、主键信息，diff信息update vector等信息。之后通过row_upd_rec_in_place，用update vector修改当前持有的Record拷贝中的信息，获得Record的这个历史版本。之后调用自己ReadView的changes_visible判断可见性，如果可见则返回用户。完成这个历史版本的读取。\n八、Undo for Crash RecoveryCrash Recovery时，需要利用Undo中的信息将未提交的事务的所有影响回滚，以保证数据库的Failure Atomic。\n前面提到过，InnoDB中的Undo其实是像数据一样处理的，也从上面的组织结构中可以看出来，Undo本身有着比Redo Log复杂得多、按事务分配而不是顺序写入的组织结构，其本身的Durability像InnoDB中其他的数据一样，需要靠Redo来保证，像庖丁解InnoDB之REDO LOG中介绍的那样。\n除了通用的一些MLOG_2BYTES、MLOG_4BYTES类型之外，Undo本身也有自己对应的Redo Log类型：\nMLOG_UNDO_INIT类型在Undo Page舒适化的时候记录初始化；在分配Undo Log的时候，需要重用Undo Log Header或需要创建新的Undo Log Header的时候，会分别记录MLOG_UNDO_HDR_REUSE和MLOG_UNDO_HDR_CREATE类型的Redo Record；\nMLOG_UNDO_INSERT是最常见的，在Undo Log里写入新的Undo Record都对应的写这个日志记录写入Undo中的所有内容；\nMLOG_UNDO_ERASE_END 对应Undo Log跨Undo Page时抹除最后一个不完整的Undo Record的操作。\n如数据库故障恢复机制的前世今生中讲过的ARIES过程，Crash Recovery的过程中会先重放所有的Redo Log，整个Undo的磁盘组织结构，也会作为一种数据类型也会通过上面讲到的这些Redo类型的重放恢复出来。之后在trx_sys_init_at_db_start中会扫描Undo的磁盘结构，遍历所有的Rollback Segment和其中所有的Undo Segment，通过读取Undo Segment Header中的State，可以知道在Crash前，最后持有这个Undo Segment的事务状态。如果是TRX_UNDO_ACTIVE，说明当时事务需要回滚，否则说明事务已经结束，可以继续清理Undo Segment的逻辑。之后，就可以恢复出Undo Log的内存组织模式，包括活跃事务的内存结构trx_t，Rollback Segment的内存结构trx_rseg_t，以及其中的trx_undo_t的四个链表。\nCrash Recovery完成之前，会启动在srv_dict_recover_on_restart中启动异步回滚线程trx_recovery_rollback_thread，其中对Crash前还活跃的事务，通过trx_rollback_active进行回滚，这个过程更上面提到的Undo for Rollback是一致的。\n九、Undo的清理我们已经知道，InnoDB在Undo Log中保存了多份历史版本来实现MVCC，当某个历史版本已经确认不会被任何现有的和未来的事务看到的时候，就应该被清理掉。因此就需要有办法判断哪些Undo Log不会再被看到。\nInnoDB中每个写事务结束时都会拿一个递增的编号trx_no作为事务的提交序号，而每个读事务会在自己的ReadView中记录自己开始的时候看到的最大的trx_no为m_low_limit_no。那么，如果一个事务的trx_no小于当前所有活跃的读事务Readview中的这个m_low_limit_no，说明这个事务在所有的读开始之前已经提交了，其修改的新版本是可见的， 因此不再需要通过undo构建之前的版本，这个事务的Undo Log也就可以被清理了。\n如下图所所以，由于ReadView List中最老的ReadView在获取时，Transaction J就已经Commit，因此所有的读事务都一定能被Index中的版本或者第一个Undo历史版本满足，不需要更老的Undo，因此整个Transaction J的Undo Log都可以清理了。\n\nUndo的清理工作是由专门的后台线程srv_purge_coordinator_thread进行扫描和分发， 并由多个srv_worker_thread真正清理的。coordinator会首先在函数trx_purge_attach_undo_recs中扫描innodb_purge_batch_size配置个Undo Records，作为一轮清理的任务分发给worker。\n扫描一批要清理Undo Records事务结束的时候，对于需要Purge的Update类型的Undo Log，会按照事务提交的顺序trx_no，挂载到Rollback Segment Header的History List上。Undo Log回收的基本思路，就是按照trx_no从小到大，依次遍历所有Undo Log进行清理操作。\n前面介绍了，InnoDB中有多个Rollback Segment，那么就会有多个History List，每个History List内部事务有序，但还需要从多个History List上找一个trx_no全局有序的序列，如下图所示：\n\n图中的事务编号是按照InnoDB这里引入了一个堆结构purge_queue，用来依次从所有History List中找到下一个拥有最小trx_no的事务。purge_queue中记录了所有等待Purge的Rollback Segment和其History中trx_no最小的事务，trx_purge_choose_next_log依次从purge_queue中pop出拥有全局最小trx_no的Undo Log。调用trx_purge_get_next_rec遍历对应的Undo Log，处理每一条Undo Record。之后继续调用trx_purge_rseg_get_next_history_log从purge_queue中获取下一条trx_no最小的Undo Log，并且将当前Rollback Segment上的下一条Undo Log继续push进purge_queue，等待后续的顺序处理。对应上图的处理过程和对应的函数调用，如下图所示：\n[trx_purge_choose_next_log] Pop T1 from purge_queue;[trx_purge_get_next_rec] Iterator T1;[trx_purge_rseg_get_next_history_log] Get T1 next: T5;[trx_purge_choose_next_log] Push T5 into purge_queue;[trx_purge_choose_next_log] Pop T4 from purge_queue;[trx_purge_get_next_rec] Iterator T4;[trx_purge_rseg_get_next_history_log] Get T4 next: ...;[trx_purge_choose_next_log] Push ... into purge_queue;[trx_purge_choose_next_log] Pop T5 from purge_queue;[trx_purge_get_next_rec] Iterator T5;[trx_purge_rseg_get_next_history_log] Get T5 next: T6;[trx_purge_choose_next_log] Push T6 into purge_queue;......\n\n其中，trx_purge_get_next_rec会从上到下遍历一个Undo Log中的所有Undo Record，这个跟前面讲过的Rollback时候从下到上的遍历方向是相反的，还是以同样的场景为例，要Purge的Undo Log横跨两个Undo Page，Undo Record 1在Page 1中，而Undo Record 2，3在Page 2中。如下图所示，首先会从当前的Undo Log Header中找到第一个Undo Record的位置Log Start Offset，处理完Undo Record1之后沿着Next Record Offset去找下一个Undo Record，当找到Page末尾时，要通过Page List Node找下一个Page，找到Page内的第一个Undo Record，重复上面的过程直到找出所有的Undo Record。\n\n对每个要Purge的Undo Record，在真正删除它本身之前，可能还需要处理一些索引上的信息，这是由于正常运行过程中，当需要删除某个Record时，为了保证其之前的历史版本还可以通过Rollptr找到，Record是没有真正删除的，只是打了Delete Mark的标记，并作为一种特殊的Update操作记录了Undo Record。那么在对应的TRX_UNDO_DEL_MARK_REC类型的Undo Record被清理之前，需要先从索引上真正地删除这个Delete Mark的记录。因此Undo Record的清理工作会分为两个过程：\n\nTRX_UNDO_DEL_MARK_REC类型Undo Record对应的Record的真正删除，称为Undo Purge；\n以及Undo Record本身从旧到新的删除，称为Undo Truncate。\n\n除此之外，当配置的独立Undo Tablespace大于两个的时候，InnoDB支持通过重建来缩小超过配置大小的Undo Tablespace：\n\nUndo Tablespace的重建缩小，称为Undo Tablespace Truncate\n\nUndo Purge这一步主要针对的是TRX_UNDO_DEL_MARK_REC类型的Undo Record，用来真正的删除索引上被标记为Delete Mark的Record。worker线程会在row_purge函数中，循环处理coordinator分配来的每一个Undo Records，先通过row_purge_parse_undo_rec，依次从Undo Record中解析出type、table_id、rollptr、对应记录的主键信息以及update vector。之后，针对TRX_UNDO_DEL_MARK_REC类型，调用row_purge_remove_sec_if_poss将需要删除的记录从所有的二级索引上删除，调用row_purge_remove_clust_if_poss从主索引上删除。另外，TRX_UNDO_UPD_EXIST_REC类型的Undo虽然不涉及主索引的删除，但可能需要做二级索引的删除，也是在这里处理的。\nUndo Truncatecoordinator线程会等待所有的worker完成一批Undo Records的Purge工作，之后尝试清理不再需要的Undo Log，trx_purge_truncate函数中会遍历所有的Rollback Segment中的所有Undo Segment，如果其状态是TRX_UNDO_TO_PURGE，调用trx_purge_free_segment释放占用的磁盘空间并从History List中删除。否则，说明该Undo Segment正在被使用或者还在被cache（TRX_UNDO_CACHED类型），那么只通过trx_purge_remove_log_hd将其从History List中删除。\n需要注意的是，Undo Truncate的动作并不是每次都会进行的，它的频次是由参数innodb_rseg_truncate_frequency控制的，也就是说要攒innodb_rseg_truncate_frequency个batch才进行一次，前面提到每一个batch中会处理innodb_purge_batch_size个Undo Records，这也就是为什么我们从show engine innodb status中看到的Undo History List的缩短是跳变的。\nUndo Tablespace Truncate如果innodb_trx_purge_truncate配置打开，在函数trx_purge_truncate中还会去尝试重建Undo Tablespaces以缩小文件空间占用。Undo Truncate之后，会在函数trx_purge_mark_undo_for_truncate中扫描所有的Undo Tablespace，文件大小大于配置的innodb_max_undo_log_size的Tablespace会被标记为inactive，每一时刻最多有一个Tablespace处于inactive，inactive的Undo Tablespace上的所有Rollback Segment都不参与给新事物的分配，等该文件上所有的活跃事务退出，并且所有的Undo Log都完成Purge之后，这个Tablespace就会被通过trx_purge_initiate_truncate重建，包括重建Undo Tablespace中的文件结构和内存结构，之后被重新标记为active，参与分配给新的事务使用。\n总结：本文首先概括地介绍了Undo Log的角色，之后介绍了一个Undo Record中的内容，紧接着介绍它的逻辑组织方式、物理组织方式、文件组织方式以及内存组织方式，详细描述了Undo Tablespace、Rollback Segment、Undo Segment、Undo Log和Undo Record的之间的关系和层级。这些组织方式都是为了更好的使用和维护Undo信息。最后在此基础上，介绍了Undo在各个重要的DB功能中的作用和实现方式，包括事务回滚、MVCC、Crash Recovery、Purge等。\n","categories":["MySQL"]},{"title":"数据库事务隔离发展历史","url":"/posts/38370/","content":"事务隔离是数据库系统设计中根本的组成部分，本文主要从标准层面来讨论隔离级别的发展历史，首先明确隔离级别划分的目标；之后概述其否定之否定的发展历程；进而引出 Adya给出的比较合理的隔离级别定义，最终总结隔离标准一路走来的思路。\n一、目标事务隔离是事务并发产生的直接需求，最直观的、保证正确性的隔离方式，显然是让并发的事务依次执行，或是看起来像是依次执行。但在真实的场景中，有时并不需要如此高的正确性保证，因此希望牺牲一些正确性来提高整体性能。通过区别不同强度的隔离级别使得使用者可以在正确性和性能上自由权衡。随着数据库产品数量以及使用场景的膨胀，带来了各种隔离级别选择的混乱，数据库的众多设计者和使用者亟需一个对隔离级别划分的共识，这就是标准出现的意义。一个好的隔离级别定义有如下两个重要的目标：\n\n正确：每个级别的定义，应该能够将所有损害该级别想要保证的正确性的情况排除在外。也就是说，只要实现满足某一隔离级别定义，就一定能获得对应的正确性保证。\n实现无关：常见的并发控制的实现方式包括，锁、OCC以及多版本 。而一个好的标准不应该限制其实现方式。\n\n二、ANSI SQL标准(1992)：基于异象1992年ANSI首先尝试指定统一的隔离级别标准，其定义了不同级别的异象(phenomenas)， 并依据能避免多少异象来划分隔离标准。异象包括：\n\n脏读（Dirty Read）: 读到了其他事务还未提交的数据；\n不可重复读（Non-Repeatable/Fuzzy Read）：由于其他事务的修改，对某数据的两次读取结果不同（因为其他事物已提交，所以在该事务中可见）；\n幻读（Phantom Read）：在自己的事务中，在两个连续的查找之间，一个并发的事务增加或删除了数据，导致这两个查询返回了不同的结果（注：不可重复读与幻读很相似，不可重复读的重点是修改，而幻读的重点在于新增或者删除）。\n\n通过阻止不同的异象发生，得到了四种不同级别的隔离标准：\nANSI SQL标准看起来是非常直观的划分方式，不想要什么就排除什么，并且做到了实现无关。然而，现实并不像想象美好。因为它并不正确。\n三、A Critique of ANSI(1995)：基于锁几年后，微软的研究员们在A Critique of ANSI SQL Isolation Levels一文中对ANSI的标准进行了批判，指出其存在两个致命的问题：\n1，不完整，缺少对Dirty Write的排除\nANSI SQL标准中所有的隔离级别都没有将Dirty Write这种异象排除在外，所谓Dirty Write指的是两个未提交的事务先后对同一个对象进行了修改。而Dirty Write之所以是一种异象，主要因为他会导致下面的一致性问题：\n\nH0: w1[x] w2[x] w2[y] c2 w1[y] c1\n\n这段历史中，假设有相关性约束x=y，T1尝试将二者都修改为1，T2尝试将二者都修改为2，顺序执行的结果应该是二者都为1或者都为2，但由于Dirty Write的发生，最终结果变为x=2，y=1，不一致。\n2，歧义\nANSI SQL的英文表述有歧义。以Phantom为例，如下图历史H3：\n\nH3：r1[P] w2[insert y to P] r2[z] w2[z] c2 r1[z] c1\n\n假设T1根据条件P查询所有的雇员列表，之后T2增加了一个雇员并增加了雇员人数值z，之后T1读取雇员人数z，最终T1的列表中的人数比z少，不一致。但T1并没有在T2修改链表后再使用P中的值，是否就不属于ANSI中对Phantom的定义了呢？这也导致了对ANSI的表述可能有严格和宽松两种解读。对于Read Dirty和Non-Repeatable/Fuzzy Read也有同样的问题。\n那么，如何解决上述两个问题呢？Critique of ANSI的答案是：宁可错杀三千，不可放过一个，即给ANSI标准中的异象最严格的定义。Critique of ANSI改造了异象的定义：\n\nP0: w1[x]…w2[x]…(c1 or a1) (Dirty Write)\nP1: w1[x]…r2[x]…(c1 or a1) (Dirty Read)\nP2: r1[x]…w2[x]…(c1 or a1) (Fuzzy or Non-Repeatable Read)\nP3: r1[P]…w2[y in P]…(c1 or a1) (Phantom)\n\n此时定义已经很严格了，直接阻止了对应的读写组合顺序。仔细可以看出，此时得到的其实就是基于锁的定义:\n\nRead Uncommitted，阻止P0：整个事务阶段对x加长写锁\nRead Commited，阻止P0，P1：短读锁 + 长写锁\nRepeatable Read，阻止P0，P1，P2：长读锁 + 短谓词锁 + 长写锁\nSerializable，阻止P0，P1，P2，P3：长读锁 + 长谓词锁 + 长写锁\n\n四、问题本质可以看出，这种方式的隔离性定义保证了正确性，但却产生了依赖实现方式的问题：太过严格的隔离性定义，阻止了Optimize或Multi-version的实现方式中的一些正常的情况：\n\n针对P0：Optimize的实现方式可能会让多个事务各自写自己的本地副本，提交的时候只要顺序合适是可以成功的，只在需要的时候才abort，但这种选择被P0阻止；\n针对P2：只要T1没有在读x，后续没有与x相关的操作，且先于T2提交。在Optimize的实现中是可以接受的，却被P2阻止。\n\n回忆Critique of ANSI中指出的ANSI标准问题，包括Dirty Write和歧义，其实都是由于多Object之间有相互约束关系导致的，如下图所示，图中黑色部分表示的是ANSI中针对某一个异象描述的异常情况，灰色部分由于多Object约束导致的异常部分，但这部分在传统的异象定义方式中并不能描述，因此其只能退而求其次，扩大限制的范围到黄色部分，从而限制了正常的情况。：\n\n由此，可以看出问题的本质：由于异象的描述只针对单个object，缺少描述多object之间的约束关系，导致需要用锁的方式来作出超出必须的限制。相应地，解决问题的关键：要有新的定义异象的模型，使之能精准的描述多object之间的约束关系，从而使得我们能够精准地限制上述灰色部分，而将黄色的部分解放出来。Adya给出的答案是序列化图。\n五、A Generalized Theory(1999)：基于序列化图Adya在Weak Consistency: A Generalized Theory and Optimistic Implementations for Distributed Transactions中给出了基于序列化图得定义，思路为先定义冲突关系；并以冲突关系为有向边形成序列化图；再以图中的环类型定义不同的异象；最后通过阻止不同的异象来定义隔离级别。\n序列化图（Direct Serialization Graph, DSG）序列化图是用有向图的方式来表示事务相互之间的依赖关系，图中每个节点表示一个事务，有向边表示存在一种依赖关系，事务需要等到所有指向其的事务先行提交，如下图所示历史的合法的提交顺序应该为：T1，T2，T3：\n\n这里的有向边包括三种情况：\n\n写写冲突ww（Directly Write-Depends）：表示两个事务先后修改同一个数据库Object(w1[x]…w2[x]…)；\n先写后读冲突wr（Directly Read-Depends）：一个事务修改某个数据库Object后，另一个对该Object进行读操作（w1[x]…r2[x]…）；\n先读后写冲突rw（Directly Anti-Depends）：一个事务读取某个Object或者某个Range后，另一个事务进行了修改（r1[x]…w2[x]… or r1[P]…w2[y in P]）；\n\n\n基于序列化图的异象定义：根据有向图的定义，我们可以将事务对不同Object的依赖关系表示到一张同一张图中，而所谓异象就是在图中找不到一个正确的序列化顺序，即存在某种环。而这种基于环的定义其实就是将基于Lock定义的异象最小化到图中灰色部分：\n1，P0(Dirty Write) 最小化为 G0(Write Cycles)：序列化图中包含两条边都为ww冲突组成的环，如H0：\n\nH0: w1[x] w2[x] w2[y] c2 w1[y] c1\n\n可以看出T1在x上与T2写写冲突，T2又在y上与T1写写冲突，形成了如下图所示的环。\n\n2，P1(Dirty Read) 最小化为 G1：Dirty Read异象的最小集包括三个部分G1a(Aborted Reads)，读到的uncommitted数据最终被abort；G1b(Intermediate Reads) ：读到其他事务中间版本的数据；以及G1c(Circular Information Flow)：DSG中包含ww冲突和wr冲突形成的环。\n3，P2(Fuzzy or Non-Repeatable Read) 最小化为 G2-item(Item Anti-dependency Cycles) ：DSG中包含环，且其中至少有一条关于某个object的rw冲突\n4，P3(Phantom) 最小化为 G2(Anti-dependency Cycles): DSG中包含环，并且其中至少有一条是rw冲突，仍然以上面的H3为例：\n\nH3：r1[P] w2[insert y to P] r2[z] w2[z] c2 r1[z] c1\n\nT1在谓词P上与T2 rw冲突，反过来T2又在z上与T1wr冲突，如下图所示：\n\n对应的隔离级别：通过上面的讨论可以看出，通过环的方式我们成功最小化了异象的限制范围，那么排除这些异象就得到了更宽松的，通用的隔离级别定义：\n\nPL-1（Read Uncommitted）：阻止G0\nPL-2（Read Commited）：阻止G1\nPL-2.99（Repeatable Read）：阻止G1，G2-item\nPL-3（Serializable）：阻止G1，G2\n\n六、其他隔离级别：除了上述的隔离级别外，在正确性的频谱中还有着大量空白，也就存在着各种其他隔离级别的空间，商业数据库的实现中有两个比较常见：\n1，Cursor Stability该隔离界别介于Read Committed和Repeatable Read之间，通过对游标加锁而不是对object加读锁的方式避免了Lost Write异象。\n2， Snapshot Ioslation事务开始的时候拿一个Start-Timestamp的snapshot，所有的操作都在这个snapshot上做，当commit的时候拿Commit-Timestamp，检查所有有冲突的值不能再[Start- Timestamp, Commit-Timestamp]被提交，否则abort。长久以来，Snapshot Ioslation一直被认为是Serializable，但其实Snapshot Ioslation下还会出现Write Skew的异象。之后的文章会详细介绍如何从Snapshot Ioslation出发获得Serializable。\n总结对于事务隔离级别的标准，数据库的前辈们进行了长久的探索：\n\nANSI isolation levels定义了异象标准，并根据所排除的异象，定义了，Read Uncommitted、Read Committed、Repeatable Read、Serializable四个隔离级别；\nA Critique of ANSI SQL Isolation Levels认为ANSI的定义并没将有多object约束的异象排除在外，并选择用更严格的基于Lock的定义扩大了每个级别限制的范围；\nWeak Consistency: A Generalized Theory and Optimistic Implementations for Distributed Transactions认为基于Lock的定义过多的扩大了限制的范围，导致正常情况被排除在外，从而限制了Optimize类型并行控制的使用；指出解决该问题的关键是要有模型能准确地描述这种多Object约束；并给出了基于序列化图的定义方式，将每个级别限制的范围最小化。\n\n","categories":["MySQL"]},{"title":"数据库故障恢复机制的前世今生","url":"/posts/26383/","content":"背景在数据库系统发展的历史长河中，故障恢复问题始终伴随左右，也深刻影响着数据库结构的发展变化。\n通过故障恢复机制，可以实现数据库的两个至关重要的特性：Durability of Updates 以及 Failure Atomic，也就是我们常说的的ACID中的A和D。\n磁盘数据库由于其卓越的性价比一直以来都占据数据库应用的主流位置。然而，由于需要协调内存和磁盘两种截然不同的存储介质，在处理故障恢复问题时也增加了很多的复杂度。随着学术界及工程界的共同努力及硬件本身的变化，磁盘数据库的故障恢复机制也不断的迭代更新，尤其近些年来，随着NVM的浮现，围绕新硬件的研究也如雨后春笋出现。\n本文希望通过分析不同时间点的关键研究成果，来梳理数据库故障恢复问题的本质，其发展及优化方向，以及随着硬件变化而发生的变化。\n 文章将首先描述故障恢复问题本身；然后按照基本的时间顺序介绍传统数据库中故障恢复机制的演进及优化；之后思考新硬件带来的机遇与挑战；并引出围绕新硬件的两个不同方向的研究成果；最后进行总结。\n问题数据库系统运行过程中可能遇到的故障类型主要包括，Transaction Failure，Process Failure，System Failure以及Media Failure。其中Transaction Failure可能是主动回滚或者冲突后强制Abort；Process Failure指的是由于各种原因导致的进程退出，进程内存内容会丢失；System Failure来源于操作系统或硬件故障；而Media Failure则是存储介质的不可恢复损坏。数据库系统需要正确合理的处理这些故障，从而保证系统的正确性。为此需要提供两个特性：\n\nDurability of Updates：已经Commit的事务的修改，故障恢复后仍然存在；\nFailure Atomic：失败事务的所有修改都不可见。\n\n因此，故障恢复的问题描述为：即使在出现故障的情况下，数据库依然能够通过提供Durability及Atomic特性，保证恢复后的数据库状态正确。然而，要解决这个问题并不是一个简单的事情，为了不显著牺牲数据库性能，长久以来人们对故障恢复机制进行了一系列的探索。\n一、Shadow Paging1981年，JIM GRAY等人在《The Recovery Manager of the System R Database Manager》中采用了一种非常直观的解决方式Shadow Paging[1]。System R的磁盘数据采用Page为最小的组织单位，一个File由多个Page组成，并通过称为Direcotry的元数据进行索引，每个Directory项纪录了当前文件的Page Table，指向其包含的所有Page。采用Shadow Paging的文件称为Shadow File，如下图中的File B所示，这种文件会包含两个Directory项，Current及Shadow。\n事务对文件进行修改时，会获得新的Page，并加入Current的Page Table，所有的修改都只发生在Current Directory；事务Commit时，Current指向的Page刷盘，并通过原子的操作将Current的Page Table合并到Shadow Directory中，之后再返回应用Commit成功；事务Abort时只需要简单的丢弃Current指向的Page；如果过程中发生故障，只需要恢复Shadow Directory，相当于对所有未提交事务的回滚操作。Shadow Paging很方便的实现了：\n\nDurability of Updates：事务完成Commit后，所有修改的Page已经落盘，合并到Shadow后，其所有的修改可以在故障后恢复出来。\nFailure Atomic：回滚的事务由于没有Commit，从未影响Shadow Directory，因此其所有修改不可见。\n\n虽然Shadow Paging设计简单直观，但它的一些缺点导致其并没有成为主流，首先，不支持Page内并发，一个Commit操作会导致其Page上所有事务的修改被提交，因此一个Page内只能包含一个事务的修改；其次，不断修改Page的物理位置，导致很难将相关的页维护在一起，破坏局部性；另外，对大事务而言，Commit过程在关键路径上修改Shadow Directory的开销可能很大，同时这个操作还必须保证原子；最后，增加了垃圾回收的负担，包括对失败事务的Current Pages和提交事务的Old Pages的回收。\n二、WAL由于传统磁盘顺序访问性能远好于随机访问，采用Logging的故障恢复机制意图利用顺序写的Log来记录对数据库的操作，并在故障恢复后通过Log内容将数据库恢复到正确的状态。简单的说，每次修改数据内容前先顺序写对应的Log，同时为了保证恢复时可以从Log中看到最新的数据库状态，要求Log先于数据内容落盘，也就是常说的Write Ahead Log，WAL。除此之外，事务完成Commit前还需要在Log中记录对应的Commit标记，以供恢复时了解当前的事务状态，因此还需要关注Commit标记和事务中数据内容的落盘顺序。根据Log中记录的内容可以分为三类：Undo-Only，Redo-Only，Redo-Undo。\nUndo-Only Logging\nUndo-Only Logging的Log记录可以表示未&lt;T, X, v&gt;，事务T修改了X的值，X的旧值是v。事务提交时，需要通过强制Flush保证Commit标记落盘前，对应事务的所有数据落盘，即落盘顺序为Log记录-&gt;Data-&gt;Commit标记。恢复时可以根据Commit标记判断事务的状态，并通过Undo Log中记录的旧值将未提交事务的修改回滚。我们来审视一下Undo-Only对Durability及Atomic的保证：\n\nDurability of Updates：Data强制刷盘保证，已经Commit的事务由于其所有Data都已经在Commit标记之前落盘，因此会一直存在；\nFailure Atomic：Undo Log内容保证，失败事务的已刷盘的修改会在恢复阶段通过Undo日志回滚，不再可见。\n\n然而Undo-Only依然有不能Page内并发的问题，如果两个事务的修改落到一个Page中，一个事务提交前需要的强制Flush操作，会导致同Page所有事务的Data落盘，可能会早于对应的Log项从而损害WAL。同时，也会导致关键路径上过于频繁的磁盘随机访问。\nRedo-Only Logging\n不同于Undo-Only，采用Redo-Only的Log中记录的是修改后的新值。对应地，Commit时需要保证，Log中的Commit标记在事务的任何数据之前落盘，即落盘顺序为Log记录-&gt;Commit标记-&gt;Data。恢复时同样根据Commit标记判断事务状态，并通过Redo Log中记录的新值将已经Commit，但数据没有落盘的事务修改重放。\n\nDurability of Updates：Redo Log内容保证，已提交事务的未刷盘的修改，利用Redo Log中的内容重放，之后可见；\nFailure Atomic：阻止Commit前Data落盘保证，失败事务的修改不会出现在磁盘上，自然不可见。\n\nRedo-Only同样有不能Page内并发的问题，Page中的多个不同事务，只要有一个未提交就不能刷盘，这些数据全部都需要维护在内存中，造成较大的内存压力。\nRedo-Undo Logging\n可以看出的只有Undo或Redo的问题，主要来自于对Commit标记及Data落盘顺序的限制，而这种限制归根结底来源于Log信息中对新值或旧值的缺失。因此Redo-Undo采用同时记录新值和旧值的方式，来消除Commit和Data之间刷盘顺序的限制。\n\nDurability of Updates：Redo 内容保证，已提交事务的未刷盘的修改，利用Redo Log中的内容重放，之后可见；\nFailure Atomic：Undo内容保证，失败事务的已刷盘的修改会在恢复阶段通过Undo日志回滚，不再可见。\n\n如此一来，同Page的不同事务提交就变得非常简单。同时可以将连续的数据攒着进行批量的刷盘已利用磁盘较高的顺序写性能。\n三、Force and Steal从上面看出，Redo和Undo内容分别可以保证Durability和Atomic两个特性，其中一种信息的缺失需要用严格的刷盘顺序来弥补。这里关注的刷盘顺序包含两个维度：\n\nForce or No-Force：Commit时是否需要强制刷盘，采用Force的方式由于所有的已提交事务的数据一定已经存在于磁盘，自然而然地保证了Durability；\nNo-Steal or Steal，Commit前数据是否可以提前刷盘，采用No-Steal的方式由于保证事务提交前修改不会出现在磁盘上，自然而然地保证了Atomic。\n\n总结一下，实现Durability可以通过记录Redo信息或要求Force刷盘顺序，实现Atomic需要记录Undo信息或要求No-Steal刷盘顺序，组合得到如下四种模式，如下图所示：\n\n四、ARIES，一统江湖1992年，IBM的研究员们发表了《ARIES: a transaction recovery method supporting fine-granularity locking and partial rollbacks using write-ahead logging》[2]，其中提出的ARIES逐步成为磁盘数据库实现故障恢复的标配，ARIES本质是一种Redo-Undo的WAL实现。 Normal过程：修改数据之前先追加Log记录，Log内容同时包括Redo和Undo信息，每个日志记录产生一个标记其在日志中位置的递增LSN（Log Sequence Number）；数据Page中记录最后修改的日志项LSN，以此来判断Page中的内容的新旧程度，实现幂等。故障恢复阶段需要通过Log中的内容恢复数据库状态，为了减少恢复时需要处理的日志量，ARIES会在正常运行期间周期性的生成Checkpoint，Checkpoint中除了当前的日志LSN之外，还需要记录当前活跃事务的最新LSN，以及所有脏页，供恢复时决定重放Redo的开始位置。需要注意的是，由于生成Checkpoint时数据库还在正常提供服务（Fuzzy Checkpoint），其中记录的活跃事务及Dirty Page信息并不一定准确，因此需要Recovery阶段通过Log内容进行修正。\nRecover过程：故障恢复包含三个阶段：Analysis，Redo和Undo。Analysis阶段的任务主要是利用Checkpoint及Log中的信息确认后续Redo和Undo阶段的操作范围，通过Log修正Checkpoint中记录的Dirty Page集合信息，并用其中涉及最小的LSN位置作为下一步Redo的开始位置RedoLSN。同时修正Checkpoint中记录的活跃事务集合（未提交事务），作为Undo过程的回滚对象；Redo阶段从Analysis获得的RedoLSN出发，重放所有的Log中的Redo内容，注意这里也包含了未Commit事务；最后Undo阶段对所有未提交事务利用Undo信息进行回滚，通过Log的PrevLSN可以顺序找到事务所有需要回滚的修改。\n除此之外，ARIES还包含了许多优化设计，例如通过特殊的日志记录类型CLRs避免嵌套Recovery带来的日志膨胀，支持细粒度锁，并发Recovery等。[3]认为，ARIES有两个主要的设计目标：\n\nFeature：提供丰富灵活的实现事务的接口：包括提供灵活的存储方式、提供细粒度的锁、支持基于Savepoint的事务部分回滚、通过Logical Undo以获得更高的并发、通过Page-Oriented Redo实现简单的可并发的Recovery过程。\nPerformance：充分利用内存和磁盘介质特性，获得极致的性能：采用No-Force避免大量同步的磁盘随机写、采用Steal及时重用宝贵的内存资源、基于Page来简化恢复和缓存管理。\n\n五、NVM带来的机遇与挑战从Shadow Paging到WAL，再到ARIES，一直围绕着两个主题：减少同步写以及尽量用顺序写代替随机写。而这些正是由于磁盘性能远小于内存，且磁盘顺序访问远好于随机访问。然而随着NVM磁盘的出现以及对其成为主流的预期，使得我们必须要重新审视我们所做的一切。相对于传统的HDD及SSD，NVM最大的优势在于：\n\n接近内存的高性能\n顺序访问和随机访问差距不大\n按字节寻址而不是Block\n\n在这种情况下，再来看ARIES的实现：\n\nNo-force and Steal：同时维护Redo， Undo和数据造成的三倍写放大，来换取磁盘顺序写的性能，但在NVM上这种取舍变得很不划算；\nPages：为了迁就磁盘基于Block的访问接口，采用Page的存储管理方式，而内存本身是按字节寻址的，因此，这种适配也带来很大的复杂度。在同样按字节寻址的NVM上可以消除。\n\n近年来，众多的研究尝试为NVM量身定制更合理的故障恢复机制，我们这里介绍其中两种比较有代表性的研究成果，MARS希望充分利用NVM并发及内部带宽的优势，将更多的任务交给硬件实现；而WBL则尝试重构当前的Log方式。\n六、MARS发表在2013年的SOSP上的《“From ARIES to MARS: Transaction support for next-generation, solid-state drives.” 》提出了一种尽量保留ARIES特性，但更适合NVM的故障恢复算法MARS[3]。MARS取消了Undo Log，保留的Redo Log也不同于传统的Append-Only，而是可以随机访问的。如下图所示，每个事务会占有一个唯一的TID，对应的Metadata中记录了Log和Data的位置。正常访问时，所有的数据修改都在对应的Redo Log中进行，不影响真实数据，由于没有Undo Log，需要采用No-Steal的方式，阻止Commit前的数据写回；Commit时会先设置事务状态为COMMITTED，之后利用NVM的内部带宽将Redo中的所有内容并发拷贝回Metadata中记录的数据位置。如果在COMMITED标记设置后发生故障，恢复时可以根据Redo Log中的内容重放。其本质是一种Redo加No-Steal的实现方式：\n\nDurability of Updates： Redo实现，故障后重放Redo；\nFailure Atomic：未Commit事务的修改只存在于Redo Log，重启后会被直接丢弃。\n\n可以看出，MARS的Redo虽然称之为Log，但其实已经不同于传统意义上的顺序写文件，允许随机访问，更像是一种临时的存储空间，类似于Shadow的东西。之所以在Commit时进行数据拷贝而不是像Shadow Paging一样的元信息修改，笔者认为是为了保持数据的局部性，并且得益于硬件优异的内部带宽。\n七、WBL不同于MARS保留Redo信息的思路，2016年VLDB上的《 “Write Behind Logging” 》只保留了Undo信息。笔者认为这篇论文中关于WBL的介绍里，用大量笔墨介绍了算法上的优化部分，为了抓住其本质，这里先介绍最基本的WBL算法：WBL去掉了传统的Append Only的Redo和Undo日志，但仍然需要保留Undo信息用来回滚未提交事务。事务Commit前需要将其所有的修改强制刷盘，之后在Log中记录Commit标记，也就是这里说的Write Behind Log。恢复过程中通过分析Commit标记将为提交的事务通过Undo信息回滚。可以看出WBL算法本身非常简单，在这个基础上，WBL做了如下优化：\n\nGroup Commit：周期性的检查内存中的修改，同样在所有修改刷盘之后再写Log，Log项中记录Commit并落盘的最新事务TimeStamp cp，保证早于cp的所有事务修改都已经落盘；同时记录当前分配出去的最大TimeStamp cd；也就是说此时所有有修改但未提交的事务Timestamp都落在cp和cd之间。Reovery的时候只需对这部分数据进行回滚；\n针对多版本数据库，多版本信息已经起到了undo的作用，因此不需要再额外记录undo信息；\n延迟回滚：Recovery后不急于对未提交事务进行回滚，而是直接提供服务，一组(cp, cd)称为一个gap，每一次故障恢复都可能引入新的gap，通过对比事务Timestamp和gap集合，来判断数据的可见性，需要依靠后台垃圾回收线程真正的进行回滚和对gap的清理，如下图所示；\n可以看出，WBL本质并没有什么新颖，是一个Force加Undo的实现方式，其正确性保证如下：\nDurability of Updates：Commit事务的数据刷盘后才进行Commit，因此Commit事务的数据一定在Recovery后存在\nFailure Atomic：通过记录的Undo信息或多版本时的历史版本信息，在Recovery后依靠后台垃圾回收线程进行回滚。\n\n总结数据库故障恢复问题本质是要在容忍故障发生的情况下保证数据库的正确性，而这种正确性需要通过提供Durability of Updates和Failure Atomic来保证。其中Duribility of Update要保证Commit事务数据在恢复后存在，可以通过Force机制或者通过Redo信息回放来保证；对应的，Failure Atomic需要保证未Commit事务的修改再恢复后消失，可以通过No-Steal机制或者通过Undo信息回滚来保证。根据保证Durability和Atomic的不同方式，对本文提到的算法进行分类，如下：\n\n\nShadow Paging可以看做是采用了Force加No-Steal的方式，没有Log信息，在Commit时，通过原子的修改Directory元信息完成数据的持久化更新，但由于其对Page内并发的限制等问题，没有成为主流；\nLogging的实现方式增加了Redo或Undo日志，记录恢复需要的信息，从而放松Force或No-Steal机制对刷盘顺序的限制，从而尽量用磁盘顺序写代替随机写获得较好的性能。ARIES算法是在这一方向上的集大成者，其对上层应用提供了丰富灵活的接口，采用了No-Force加Steal机制，将传统磁盘上的性能发挥到极致，从而成为传统磁盘数据故障恢复问题的标准解决方案；\n随着NVM设备的逐步出现，其接近内存的性能、同样优异的顺序访问和随机访问表现，以及基于字节的寻址方式，促使人们开始重新审视数据库故障恢复的实现。其核心思路在于充分利用NVM硬件特性，减少Log机制导致的写放大以及设计较复杂的问题；\nMARS作为其中的佼佼者之一，在NVM上维护可以随机访问的Redo日志，同时采用Force加Steal的缓存策略，充分利用NVM优异的随机写性能和内部带宽。\nWBL从另一个方向进行改造，保留Undo信息，采用No-Force加No-Steal的缓存策略，并通过Group Commit及延迟回滚等优化，减少日志信息，缩短恢复时间。\n\n本文介绍了磁盘数据库一路走来的核心发展思路，但距离真正的实现还有巨大的距离和众多的设计细节，如对Logical Log或Physical Log的选择、并发Recovery、Fuzzy Checkpoing、Nested Top Actions等，之后会用单独的文章以InnoDB为例来深入探究其设计和实现细节。\n","categories":["MySQL"]},{"title":"更新id最大的行","url":"/posts/64986/","content":"update id_store set `version` = 1 WHERE `version` = 0 ORDER BY id LIMIT 1;\n\n","categories":["MySQL"]},{"title":"查看 navicat 已保存连接的密码","url":"/posts/65463/","content":"在 navicat 中点击 文件 菜单，选择 导出连接，选择要获取密码的连接，确认后会导出一个后缀为 .cnx 的文件，文件内容大致如下：\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Connections Ver=&quot;1.4&quot;&gt;\t&lt;Connection ConnectionName=&quot;mysql&quot; ProjectUUID=&quot;&quot; ConnType=&quot;MYSQL&quot; OraConnType=&quot;&quot; ServiceProvider=&quot;Default&quot; Host=&quot;172.18.80.104&quot; Port=&quot;3306&quot; Database=&quot;&quot; OraServiceNameType=&quot;&quot; TNS=&quot;&quot; MSSQLAuthenMode=&quot;&quot; MSSQLAuthenWindowsDomain=&quot;&quot; DatabaseFileName=&quot;&quot; UserName=&quot;root&quot; Password=&quot;E55FD1F510B6BC98B66F7F24F48ED2F5&quot; SavePassword=&quot;true&quot; SettingsSavePath=&quot;/Users/zhangzhongen/Library/Application Support/PremiumSoft CyberTech/Navicat CC/Common/Settings/0/0/MySQL/mysql&quot; SessionLimit=&quot;0&quot; Encoding=&quot;0&quot; Keepalive=&quot;false&quot; KeepaliveInterval=&quot;240&quot; MySQLCharacterSet=&quot;true&quot; Compression=&quot;false&quot; AutoConnect=&quot;false&quot; NamedPipe=&quot;false&quot; NamedPipeSocket=&quot;&quot; OraRole=&quot;&quot; OraOSAuthen=&quot;false&quot; SQLiteEncrypt=&quot;false&quot; SQLiteEncryptPassword=&quot;&quot; SQLiteSaveEncryptPassword=&quot;false&quot; UseAdvanced=&quot;false&quot; SSL=&quot;false&quot; SSL_Authen=&quot;false&quot; SSL_PGSSLMode=&quot;REQUIRE&quot; SSL_ClientKey=&quot;&quot; SSL_ClientCert=&quot;&quot; SSL_CACert=&quot;&quot; SSL_Clpher=&quot;&quot; SSL_PGSSLCRL=&quot;&quot; SSL_WeakCertValidation=&quot;false&quot; SSL_AllowInvalidHostName=&quot;false&quot; SSL_PEMClientKeyPassword=&quot;&quot; SSH=&quot;false&quot; SSH_Host=&quot;&quot; SSH_Port=&quot;22&quot; SSH_UserName=&quot;&quot; SSH_AuthenMethod=&quot;PASSWORD&quot; SSH_Password=&quot;&quot; SSH_SavePassword=&quot;false&quot; SSH_PrivateKey=&quot;&quot; SSH_Passphrase=&quot;&quot; SSH_SavePassphrase=&quot;false&quot; SSH_Compress=&quot;false&quot; HTTP=&quot;false&quot; HTTP_URL=&quot;&quot; HTTP_PA=&quot;&quot; HTTP_PA_UserName=&quot;&quot; HTTP_PA_Password=&quot;&quot; HTTP_PA_SavePassword=&quot;&quot; HTTP_EQ=&quot;&quot; HTTP_CA=&quot;&quot; HTTP_CA_ClientKey=&quot;&quot; HTTP_CA_ClientCert=&quot;&quot; HTTP_CA_CACert=&quot;&quot; HTTP_CA_Passphrase=&quot;&quot; HTTP_Proxy=&quot;&quot; HTTP_Proxy_Host=&quot;&quot; HTTP_Proxy_Port=&quot;&quot; HTTP_Proxy_UserName=&quot;&quot; HTTP_Proxy_Password=&quot;&quot; HTTP_Proxy_SavePassword=&quot;&quot;/&gt;&lt;/Connections&gt;\n\n在上述 XML 中 Connection 节的 Password 属性保存的就是连接密码了，只不过这个密码是加密过的。\n这时我们可以使用网上大佬写的脚本来进行解密，Github 链接为：\n\nhttps://github.com/tianhe1986/FatSmallTools\n\n脚本如下：\n&lt;?php namespace FatSmallTools; class NavicatPassword&#123;    protected $version = 0;    protected $aesKey = &#x27;libcckeylibcckey&#x27;;    protected $aesIv = &#x27;libcciv libcciv &#x27;;    protected $blowString = &#x27;3DC5CA39&#x27;;    protected $blowKey = null;    protected $blowIv = null;        public function __construct($version = 12)    &#123;        $this-&gt;version = $version;        $this-&gt;blowKey = sha1(&#x27;3DC5CA39&#x27;, true);        $this-&gt;blowIv = hex2bin(&#x27;d9c7c3c8870d64bd&#x27;);    &#125;        public function encrypt($string)    &#123;        $result = FALSE;        switch ($this-&gt;version) &#123;            case 11:                $result = $this-&gt;encryptEleven($string);                break;            case 12:                $result = $this-&gt;encryptTwelve($string);                break;            default:                break;        &#125;                return $result;    &#125;        protected function encryptEleven($string)    &#123;        $round = intval(floor(strlen($string) / 8));        $leftLength = strlen($string) % 8;        $result = &#x27;&#x27;;        $currentVector = $this-&gt;blowIv;                for ($i = 0; $i &lt; $round; $i++) &#123;            $temp = $this-&gt;encryptBlock($this-&gt;xorBytes(substr($string, 8 * $i, 8), $currentVector));            $currentVector = $this-&gt;xorBytes($currentVector, $temp);            $result .= $temp;        &#125;                if ($leftLength) &#123;            $currentVector = $this-&gt;encryptBlock($currentVector);            $result .= $this-&gt;xorBytes(substr($string, 8 * $i, $leftLength), $currentVector);        &#125;                return strtoupper(bin2hex($result));    &#125;        protected function encryptBlock($block)    &#123;        return openssl_encrypt($block, &#x27;BF-ECB&#x27;, $this-&gt;blowKey, OPENSSL_RAW_DATA|OPENSSL_NO_PADDING);     &#125;        protected function decryptBlock($block)    &#123;        return openssl_decrypt($block, &#x27;BF-ECB&#x27;, $this-&gt;blowKey, OPENSSL_RAW_DATA|OPENSSL_NO_PADDING);     &#125;        protected function xorBytes($str1, $str2)    &#123;        $result = &#x27;&#x27;;        for ($i = 0; $i &lt; strlen($str1); $i++) &#123;            $result .= chr(ord($str1[$i]) ^ ord($str2[$i]));        &#125;                return $result;    &#125;        protected function encryptTwelve($string)    &#123;        $result = openssl_encrypt($string, &#x27;AES-128-CBC&#x27;, $this-&gt;aesKey, OPENSSL_RAW_DATA, $this-&gt;aesIv);        return strtoupper(bin2hex($result));    &#125;        public function decrypt($string)    &#123;        $result = FALSE;        switch ($this-&gt;version) &#123;            case 11:                $result = $this-&gt;decryptEleven($string);                break;            case 12:                $result = $this-&gt;decryptTwelve($string);                break;            default:                break;        &#125;                return $result;    &#125;        protected function decryptEleven($upperString)    &#123;        $string = hex2bin(strtolower($upperString));                $round = intval(floor(strlen($string) / 8));        $leftLength = strlen($string) % 8;        $result = &#x27;&#x27;;        $currentVector = $this-&gt;blowIv;                for ($i = 0; $i &lt; $round; $i++) &#123;            $encryptedBlock = substr($string, 8 * $i, 8);            $temp = $this-&gt;xorBytes($this-&gt;decryptBlock($encryptedBlock), $currentVector);            $currentVector = $this-&gt;xorBytes($currentVector, $encryptedBlock);            $result .= $temp;        &#125;                if ($leftLength) &#123;            $currentVector = $this-&gt;encryptBlock($currentVector);            $result .= $this-&gt;xorBytes(substr($string, 8 * $i, $leftLength), $currentVector);        &#125;                return $result;    &#125;        protected function decryptTwelve($upperString)    &#123;        $string = hex2bin(strtolower($upperString));        return openssl_decrypt($string, &#x27;AES-128-CBC&#x27;, $this-&gt;aesKey, OPENSSL_RAW_DATA, $this-&gt;aesIv);    &#125;&#125;  use FatSmallTools\\NavicatPassword; //需要指定版本，11 或 12，12 以上用 12，以下用 11$navicatPassword = new NavicatPassword(12);//$navicatPassword = new NavicatPassword(11); //解密$decode = $navicatPassword-&gt;decrypt(&#x27;&lt;pwd&gt;&#x27;);echo $decode.&quot;\\n&quot;;\n\n将脚本倒数第二行的 &lt;pwd&gt; 替换成 XML 中加密的密码，然后执行该脚本就会输出解密后的密码。如果你不想部署 PHP 运行环境，可以直接把脚本放到下面网页中进行在线执行：\n\nhttps://tool.lu/coderunner/\n\n","categories":["MySQL"]},{"title":"浅析数据库并发控制机制","url":"/posts/10983/","content":"数据库事务隔离发展标准一文中，从标准制定的角度介绍了数据库的隔离级别，介绍了Read Uncommitted、Read Committed、Repeatable Read、Serializable等隔离级别的定义。本文就来看看究竟有哪些常见的实现事务隔离的机制，称之为并发控制（Concurrency Control）。\n原理所谓并发控制，就是保证并发执行的事务在某一隔离级别上的正确执行的机制。需要指出的是并发控制由数据库的调度器负责，事务本身并不感知，如下图所示，Scheduler将多个事务的读写请求，排列为合法的序列，使之依次执行：\n\n这个过程中，对可能破坏数据正确性的冲突事务，调度器可能选择下面两种处理方式：\n\nDelay：延迟某个事务的执行到合法的时刻\nAbort：直接放弃事务的提交，并回滚该事务可能造成的影响\n\n可以看出Abort比Delay带来更高的成本，接下来我们就介绍不同的并发控制机制在不同情况下的处理方式。\n分类\n这里从两个维度，对常见的并发控制机制进行分类：\n1. 乐观程度不同的实现机制，基于不同的对发生冲突概率的假设，悲观方式认为只要两个事务访问相同的数据库对象，就一定会发生冲突，因而应该尽早阻止；而乐观的方式认为，冲突发生的概率不大，因此会延后处理冲突的时机。如上图横坐标所示，乐观程度从左向右增高：\n\n基于Lock：最悲观的实现，需要在操作开始前，甚至是事务开始前，对要访问的数据库对象加锁，对冲突操作Delay；\n基于Timestamp：乐观的实现，每个事务在开始时获得全局递增的时间戳，期望按照开始时的时间戳依次执行，在操作数据库对象时检查冲突并选择Delay或者Abort；\n基于Validation：更乐观的实现，仅在Commit前进行Validate，对冲突的事务Abort\n\n可以看出，不同乐观程度的机制本质的区别在于，检查或预判冲突的时机，Lock在事务开始时，Timestamp在操作进行时，而Validation在最终Commit前。相对于悲观的方式，乐观机制可以获得更高的并发度，而一旦冲突发生，Abort事务也会比Delay带来更大的开销。\n2. 单版本 VS 多版本如上图纵坐标所示，相同的乐观程度下，还存在多版本的实现。所谓多版本，就是在每次需要对数据库对象修改时，生成新的数据版本，每个对象的多个版本共存。读请求可以直接访问对应版本的数据，从而避免读写事务和只读事务的相互阻塞。当然多版本也会带来对不同版本的维护成本，如需要垃圾回收机制来释放不被任何事物可见的版本。\n需要指出的是这些并发控制机制并不与具体的隔离级别绑定，通过冲突判断的不同规则，可以实现不同强度的隔离级别，下面基于Serializable具体介绍每种机制的实现方式。\n基于Lock基于Lock实现的Scheduler需要在事务访问数据前加上必要的锁保护，为了提高并发，会根据实际访问情况分配不同模式的锁，常见的有读写锁，更新锁等。最简单地，需要长期持有锁到事务结束，为了尽可能的在保证正确性的基础上提高并行度，数据库中常用的加锁方式称为两阶段锁（2PL），Growing阶段可以申请加锁，Shrinking阶段只能释放，即在第一次释放锁之后不能再有任何加锁请求。需要注意的是2PL并不能解决死锁的问题，因此还需要有死锁检测及处理的机制，通常是选择死锁的事务进行Abort。\n\nScheduler对冲突的判断还需要配合Lock Table，如下图所示是一个可能得Lock Table信息示意，每一个被访问的数据库对象都会在Lock Table中有对应的表项，其中记录了当前最高的持有锁的模式、是否有事务在Delay、以及持有或等待对应锁的事务链表；同时对链表中的每个事务记录其事务ID，请求锁的模式以及是否已经持有该锁。Scheduler会在加锁请求到来时，通过查找Lock Table判断能否加锁或是Delay，如果Delay需要插入到链表中。对应的当事务Commit或Abort后需要对其持有的锁进行释放，并按照不同的策略唤醒等待队列中Delay的事务。\n\n基于Timestamp基于Timestamp的Scheduler会在事务开始时候分配一个全局自增的Timestamp，这个Timestamp通常由物理时间戳或系统维护的自增id产生，用于区分事务开始的先后。同时，每个数据库对象需要增加一些额外的信息，这些信息会由对应的事务在访问后更新，包括:\n\nRT(X): 最大的读事务的Timestamp\nWT(X): 最大的写事务的Timestamp\nC(X): 最新修改的事务是否已经提交\n\n基于Timestamp假设开始时Timestamp的顺序就是事务执行的顺序，当事务访问数据库对象时，通过对比事务自己的Timestamp和该对象的信息，可以发现与这种与开始顺序不一致的情况，并作出应对：\n\nRead Late：比自己Timestamp晚的事务在自己想要Read之前对该数据进行了写入，并修改了WT(X)，此时会Read不一致的数据。\nWrite Late: 比自己Timestamp晚的事务在自己想要Write之前读取了该数据，并修改了RT(X)，如果继续写入会导致对方读到不一致数据。\n\n这两种情况都是由于实际访问数据的顺序与开始顺序不同导致的，Scheduler需要对冲突的事务进行Abort。\n\nRead Dirty：通过对比C(X)，可以发现是否看到的是已经Commit的数据，如果需要保证Read Commit，则需要Delay事务到对方Commit之后再进行提交。\n\n基于Validation（OCC）基于Validation的方式，有时也称为Optimistic Concurrency Control(OCC)，大概是因为它比基于Timestamp的方式要更加的乐观，将冲突检测推迟到Commit前才进行。不同于Timestamp方式记录每个对象的读写时间，Validation的方式记录的是每个事物的读写操作集合。并将事物划分为三个阶段：\n\nRead阶段：从数据库中读取数据并在私有空间完成写操作，这个时候其实并没有实际写入数据库。维护当前事务的读写集合，RS、WS；\nValidate阶段：对比当前事务与其他有时间重叠的事务的读写集合，判断能否提交；\nWrite阶段：若Validate成功，进入Write阶段，这里才真正写入数据库。\n\n\n同时，Scheduler会记录每个事务的开始时间START(T)，验证时间VAL(T)，完成写入时间FIN(T)\n基于Validataion的方式假设事务Validation的顺序就是事务执行的顺序，因此验证的时候需要检查访问数据顺序可能得不一致：\n\nRS(T)和WS(U) 是否有交集，对任何事务U，FIN(U) &gt; START(T)，如果有交集，则T的读可能与U的写乱序；\nWS(T)和WS({U) 是否有交集，对任何事务U， Fin(U) &gt; VAL(T)，如果有交集，则T的写可能与U的写乱序。\n\nMultiversion（MVCC）对应上述每种乐观程度，都可以有多版本的实现方式，多版本的优势在于，可以让读写事务与只读事务互不干扰，因而获得更好的并行度，也正是由于这一点成为几乎所有主流数据库的选择。为了实现多版本的并发控制，需要给每个事务在开始时分配一个唯一标识TID，并对数据库对象增加以下信息：\n\ntxd-id，创建该版本的事务TID\nbegin-ts及end-ts分别记录该版本创建和过期时的事务TID\npointer: 指向该对象其他版本的链表\n\n\n其基本的实现思路是，每次对数据库对象的写操作都生成一个新的版本，用自己的TID标记新版本begin-ts及上一个版本的end-ts，并将自己加入链表。读操作对比自己的TID与数据版本的begin-ts，end-ts，找到其可见最新的版本进行访问。根据乐观程度多版本的机制也分为三类：\n1. Two-phase Locking (MV2PL)与单版本的2PL方式类似，同样需要Lock Table跟踪当前的加锁及等待信息，另外给数据库对象增加了多版本需要的begin-ts和end-ts信息。写操作需要对最新的版本加写锁，并生成新的数据版本。读操作对找到的最新的可见版本加读锁访问。\n2. Timestamp Ordering (MVTO)对比单版本的Timestamp方式对每个数据库对象记录的Read TimeStamp(RT)，Write TimeStamp(WT)，Commited flag(C)信息外增加了标识版本的begin-ts和end-ts，同样在事务开始前获得唯一递增的Start TimeStamp（TS），写事务需要对比自己的TS和可见最新版本的RT来验证顺序，写入是创建新版本，并用自己的TS标记新版本的WT，不同于单版本，这个WT信息永不改变。读请求读取自己可见的最新版本，并在访问后修改对应版本的RT，同样通过判断C flag信息避免Read Uncommitted。\n3. Optimistic Concurrency Control (MVOCC)对比单版本的Validataion（OCC）方式，同样分为三个阶段，Read阶段根据begin-ts，end-ts找到可见最新版本，不同的是在多版本下Read阶段的写操作不在私有空间完成，而是直接生成新的版本，并在其之上进行操作，由于其commit前begin-ts为INF，所以不被其他事务课件；Validation阶段分配新的Commit TID，并以之判断是否可以提交；通过Validation的事务进入Write阶段将begin-ts修改为Commit TID。\n总结相对于悲观的锁实现，乐观的机制可以在冲突发生较少的情况下获得更好的并发效果，然而一旦冲突，需要事务回滚带来的开销要远大于悲观实现的阻塞，因此他们各自适应于不同的场景。而多版本由于避免读写事务与只读事务的互相阻塞， 在大多数数据库场景下都可以取得很好的并发效果，因此被大多数主流数据库采用。可以看出无论是乐观悲观的选择，多版本的实现，读写锁，两阶段锁等各种并发控制的机制，归根接地都是在确定的隔离级别上尽可能的提高系统吞吐，可以说隔离级别选择决定上限，而并发控制实现决定下限。\n本文从乐观悲观的程度以及单版本多版本选择上对可用的并发控制机制选择进行了划分，并介绍了各种机制大体的设计思路，而距离真正的实现还有比较大的距离，包括实现细节和配套机制。比如常用的各种类型的MVCC中，由于多版本的存在而带来的一些列如垃圾回收、索引管理、版本存储等相关问题。我们之后将以MyRocks为例看看并发控制在工程上的具体实现。\n","categories":["MySQL"]},{"title":"Spring 配置加载","url":"/posts/28500/","content":"官方文档：https://docs.spring.io/spring-boot/docs/2.1.9.RELEASE/reference/html/boot-features-external-config.html\nSpringApplication 从以下位置的 application.properties 文件中加载属性并将它们添加到 Spring 环境中:\n\nA /config subdirectory of the current directory\nThe current directory\nA classpath /config package\nThe classpath root\n\n该列表按优先级排序（在列表中较高位置定义的属性会覆盖在较低位置定义的属性）。\n\nYou can also use YAML (‘.yml’) files as an alternative to ‘.properties’.\n\n如果你不喜欢 application.properties 作为配置文件名，你可以通过指定 spring.config.name 环境属性来切换到另一个文件名。 您还可以使用 spring.config.location 环境属性（以逗号分隔的目录位置或文件路径列表）来引用显式位置。 以下示例显示如何指定不同的文件名：\n$ java -jar myproject.jar --spring.config.name=myproject\n\n以下示例显示如何指定两个位置：\n$ java -jar myproject.jar --spring.config.location=classpath:/default.properties,classpath:/override.properties\n\n\n\n\nSpring.config.name 和 spring.config.location 很早就被用来确定需要加载哪些文件。\n它们必须定义为环境属性（通常是操作系统环境变量、系统属性或命令行参数）。\n\n如果 spring.config.location 包含目录（而不是文件），它们应该以 / 结尾（并且在运行时，在加载之前附加从 spring.config.name 生成的名称，包括 profile-specific 文件的文件名）。\nspring.config.location 中指定的文件将被原样使用，不支持 profile-specific 变量，并且被任何 profile-specific 的属性覆盖。\n配置位置以相反的顺序搜索。 默认情况下，配置的位置是classpath:/,classpath:/config/,file:./,file:./config/。生成的搜索顺序如下：\n\nfile:./config/\nfile:./\nclasspath:/config/\nclasspath:/\n\n当使用 spring.config.location 配置自定义配置位置时，它们会替换默认位置。例如，如果 spring.config.location 配置了值 classpath:/custom-config/,file:./custom-config/，则搜索顺序如下：\n\nfile:./custom-config/\nclasspath:custom-config/\n\n或者，当使用 spring.config.additional-location 配置自定义配置位置时，除了默认位置之外，还会使用它们。 \nspring.config.additional-location 配置的自定义配置将在默认位置之前搜索。例如，如果配置了 spring.config.additional-location 为 classpath:/custom-config/,file:./custom-config/ ，则搜索顺序变为以下：\n\nfile:./custom-config/\nclasspath:custom-config/\nfile:./config/\nfile:./\nclasspath:/config/\nclasspath:/\n\n这种搜索顺序允许您在一个配置文件中指定默认值，然后在另一个配置文件中选择性地覆盖这些值。\n您可以在默认位置之一的“application.properties”（或您使用 spring.config.name 选择的任何其他基本名称）中为您的应用程序提供默认值。然后可以在运行时使用位于自定义位置之一的不同文件覆盖这些默认值。\n\n如果您使用环境变量而不是系统属性，大多数操作系统不允许使用句点分隔的键名，但您可以使用下划线代替（例如，SPRING_CONFIG_NAME 而不是 spring.config.name）。\n\n\n如果您的应用程序在容器中运行，则可以使用 JNDI 属性（在 java:comp/env 中）或 servlet 上下文初始化参数来代替环境变量或系统属性，或者也可以使用环境变量或系统属性。\n\n","categories":["Spring"]},{"title":"SpringBoot actuator 调整日志级别","url":"/posts/56907/","content":"发送请求\ncurl &#x27;http://localhost:8080/actuator/loggers/org.jd.kafka.KafkaManager&#x27; -i -X POST -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123;&quot;configuredLevel&quot;:&quot;off&quot;&#125;&#x27;\n\n","categories":["Spring"]},{"title":"分布式系统、拜占庭将军问题与区块链","url":"/posts/15204/","content":"从技术的角度看，区块链是一种与分布式系统有关的技术。它与分布式系统的各个概念之间有什么联系？今天本文就借这个机会，跟大家一起讨论一下分布式系统的核心问题和概念。最后，我们将尽量沿着逻辑上前后一贯的思路，讨论一下区块链技术。\n分布式系统和一致性问题一个由区块链技术支撑的系统，比如比特币网络或以太坊，从技术上看，是一个很庞大的分布式系统。因此，我们首先从分布式系统开始说起。\n对于技术人员来说，特别是服务器开发人员，几乎每个人都经常和分布式系统打交道。当服务的规模越来越大的时候，它必然发展成一个复杂的分布式系统。很典型的，就是各种分布式数据库，它们通常能将数据以某种方式在多个节点上存储，在高可用的基础上保证数据的一致性。\n实际上，一致性问题(consensus problem)是分布式系统需要解决的一个核心问题。分布式系统一般是由多个地位相等的节点组成，各个节点之间的交互就好比几个人聚在一起讨论问题。让我们设想一个更具体的场景，比如三个人讨论中午去哪里吃饭，第一个人说附近刚开了一个火锅店，听说味道非常不错；但第二个人说，不好，吃火锅花的时间太久了，还是随便喝点粥算了；而第三个人说，那个粥店我昨天刚去过，太难喝了，还不如去吃麦当劳。结果，三个人僵持不下，始终达不成一致。\n有人说，这还不好解决，投票呗。于是三个人投了一轮票，结果每个人仍然坚持自己的提议，问题还是没有解决。有人又想了个主意，干脆我们选出一个leader，这个leader说什么，我们就听他的，这样大家就不用争了。于是，大家开始投票选leader。结果很悲剧，每个人都觉得自己应该做这个leader。三个人终于发现，「选leader」这件事仍然和原来的「去哪里吃饭」这个问题在本质上是一样的，同样难以解决。\n这时恐怕有些读者们心里在想，这三个人是有毛病吧……就吃个饭这么点小事，用得着争成这样吗？实际上，在分布式系统中的每个节点之间，如果没有某种严格定义的规则和协议，它们之间的交互就真的有可能像上面说的情形一样。整个系统达不成一致，就根本没法工作。\n所以，就有聪明人设计出了一致性协议(consensus protocol)，像我们常见的比如Paxos、Raft、Zab之类。与前面几个人商量问题类似，如果翻译成Paxos的术语，相当于每个节点可以提出自己的提议(称为proposal，里面包含提议的具体值)，协议的最终目标就是各个节点根据一定的规则达成相同的proposal。但以谁的提议为准呢？我们容易想到的一个规则可能是这样：哪个节点先提出提议，就以谁的为准，后提出的提议无效。但是，在一个分布式系统中的情况可比几个人聚在一起讨论问题复杂多了，这里边还有网络延迟的问题，导致你很难对发生的所有事件进行全局地排序。举个简单的例子，假设节点A和B分别几乎同时地向节点X和Y发出了自己的proposal，但由于消息在网络中的延迟情况不同，最后结果是：X先收到了A的proposal，后收到了B的proposal；但是Y正好相反，它先收到了B的proposal，后收到了A的proposal。这样在X和Y分别看来，谁先谁后就难以达成一致了。\n此外，如果考虑到节点宕机和消息丢失的可能性，情况还会更复杂。节点宕机可以看成是消息丢失的特例，相当于发给这个节点的消息全部丢失了。这在CAP的理论框架下，相当于发生了网络分割(network partitioning)，也就是对应CAP中的P。为什么节点宕机和消息丢失都能归结到网络分割的情况上去呢？是因为这几种情况实际上无法区分。比如，有若干个节点联系不上了，也就是说，对于其它节点来说，它们发送给这些节点的消息收不到任何回应。真正的原因，可能是网络中间不通了，也可能是那些目的节点宕机了，也可能是消息无限期地被延迟了。总之，就是系统中有些节点联系不上了，它们不能再参与决策，但也不代表它们过一段时间不能重新联系上。\n为了表达上更直观，下面我们还是假设某些节点宕机了。那在这个时候，剩下的节点在缺少了某些节点参与决策的情况下，还能不能对于提议达成一致呢？即使是达成了一致，那么在那些宕机的节点重新恢复过来之后（注意这时候它们对于其它节点之间已经达成一致的提议可能一无所知），它们会不会对于已经达成的一致提议重新提出异议，从而造成混乱？所有这些问题，都是分布式一致性协议需要解决的。\n我们这里没有足够的篇幅来详细讨论这些协议具体的实现了，感兴趣的读者可以去查阅相关的论文。实际上，理解问题本身比理解问题的答案要重要的多。总之，我们需要知道的是，我们已经有了一些现成的分布式一致性算法，它们能解决上面讨论的这些问题，保证在一个去中心化的网络中，各个节点之间最终能够对于提议达成一致。而且，这些协议都具有一定的容错性。一般来说，只要网络中的大部分节点(或一个quorum)仍然存活（即它们相互间可以收发消息），这个一致性的提议就可以达成。\n拜占庭将军问题我们前面讨论的一致性协议，有一个重要的前提条件，就是：各个节点都是可以信任的，它们都严格遵守同样的一套规则。这个条件，在一个公司的内部网络中可以认为是基本能满足的。但如果这个条件不满足会怎么样呢？假设网络中有些节点是恶意的，它们不但不遵守协议，还故意捣乱（比如胡乱发送消息），那么其它正常的节点还能够顺利工作吗？\n在分布式系统理论中，这个问题被抽象成了一个著名的问题——拜占庭将军问题(Byzantine Generals Problem)。这个问题由大名鼎鼎的Leslie Lamport提出，也就是Paxos的作者。同时，Lamport还是2013年的图灵奖得主。\n这要从一个故事开始说起（当然这个故事是Lamport编出来的）。拜占庭帝国的几支军队攻打到了敌人的城市外面，然后分开驻扎。每一支军队由一位拜占庭将军(Byzantine general)率领。为了制定出一个统一的作战计划，每一位将军需要通过信差(messenger)与其它将军互通消息。但是，在拜占庭将军之间可能出现了叛徒(traitor)。这些叛徒将军的目的是阻挠其他忠诚的将军(loyal generals)达成一致的作战计划。为了这一目的，他们可能做任何事，比如串通起来，故意传出虚假消息，或者不传出任何消息。\n我们来看一个简单的例子。假设有5位将军，他们投票来决定是进攻还是撤退。其中两位认为应该进攻，还有两位认为应该撤退，这时候进攻和撤退的票数是2:2打平了。第五位将军恰好是个叛徒，他告诉前两位应该进攻，但告诉后两位应该撤退，结果前两位将军最终决定进攻，而后两位将军却决定撤退。没有达成一致的作战计划。\n这个问题显然比我们在前一章讨论的可信任环境下的一致性问题要更难。要解决这个问题，我们是希望能找到一个算法，保证在存在叛徒阻挠的情况下，我们仍然能够达成如下目标：\n\nA. 所有忠诚的将军都得到了相同（一致）的作战计划。比如都决定进攻，或都决定撤退，而不是有些将军认为应该进攻，其他将军却决定撤退。\nB. 忠诚的将军不仅得到了相同的作战计划，还应该保证得到的作战计划是合理的(reasonable)。比如，本来进攻是更有利的作战计划，但由于叛徒的阻挠，最终却制定出了一起撤退的计划。这样我们的算法也算失败了。\n\n可以看出，上面的目标A，是比较明确的，至少给定一个算法很容易判定它有没有达到这个目标。但目标B却让人无从下手。一个作战计划是不是「合理」的，本来就不好定义。即使没有叛徒的存在，忠诚的将军们也未必就一定能制定出合理的计划。这涉及到科学研究中一个非常重要的问题，如果一个事情不能用一种形式化的方式清晰的定义出来，对于它的研究也就无从谈起，这个事情本身也无法上升到科学的层面。Lamport在对拜占庭将军问题的研究中，一个突出的贡献就是，把这个看似不太好界定的问题，巧妙地归约到了一个能用数学语言精确描述的问题上去。下面我们就看一下这个过程是怎么做的。\n首先我们考虑一下将军们制定作战计划的过程（先假设没有叛徒）。每一位将军根据自己对战局的观察，给出他建议的作战计划——是进攻还是撤退，这称为作战提议。然后，每位将军把自己的作战提议通过信差传达给其他每一位将军。现在每一位将军都知道了其他将军的作战提议，再加上他自己的作战提议，他需要根据所有这些信息得到最终的一个作战计划。为了表达上更清晰，我们给每位将军进行编号，分别是1, 2, …, n，每位将军提出的作战提议记为v(1), v(2), …, v(n)，一共是n个值，这其中有些代表「进攻」，有些代表「撤退」。经过信差传递消息之后，每位将军都看到了相同的作战提议序列v(1), v(2), …, v(n)，当然这其中的一个是当前这位将军自己提出来的。然后只要每位将军采用同样的方法，对所有的v(1), v(2), …, v(n)这些信息进行汇总，就能得到同样的最终作战计划。比如，容易想到的一个方法是投票法，即对v(1), v(2), …, v(n)中不同的作战提议进行投票，最后选择得票最多的作为最终作战计划。\n当然，这样得到的最终作战计划也不能保证就是最好的，但这应该是我们能做到的最好的了。我们现在仍然假设将军里没有叛徒。我们发现，前面提到的目标A和目标B的要求可以适当「降低」一些：我们不再关注将军们是否能达成最终一致的作战计划，并且这个计划是不是「合理」；我们只关注每个将军是否收到了完全相同的作战提议v(1), v(2), …, v(n)。只要每位将军收到的这些作战提议是完全相同的，他们再用同样的方法进行汇总，就很容易得到最终一致的作战计划。至于这个最终的作战计划是不是最好的，那就跟很多「人为」的因素有关了，我们不去管它。\n现在我们考虑将军中出现了叛徒。遵循前面的思路，我们仍然希望每位将军能够收到完全相同的作战提议v(1), v(2), …, v(n)。现在我们仔细审视一下其中的一个值，v(i)，在前面的描述中，它表示来自第i个将军的作战提议。如果第i个将军是忠诚的，那么这个定义没有什么问题。但是，如果第i个将军是叛徒，那么就有问题了。为什么呢？因为叛徒可以为所欲为，他为了扰乱整个作战计划的制定，完全可能向不同的将军给出不同的作战提议。这样的话，不同的忠诚将军收到的来自第i个将军的v(i)可能是不同的值。这样v(i)这个定义就不对了，它需要改一改。\n不管怎么样，即使存在叛徒，我们还是希望每位将军最终是基于完全相同的作战提议来做汇总，这些作战提议仍然记为v(1), v(2), …, v(n)。不过，这里的v(i)不再表示来自第i个将军的作战提议，而是表示经过我们设计的某个一致性算法处理之后，每位将军最终看到的第i个提议。这里需要分两种情况讨论。首先第一种情况，如果第i个将军是忠诚的，那么我们自然希望这个v(i)就是第i个将军发送出来的作战提议。换句话说，我们希望经过一致性算法处理之后，第i个将军如果是忠诚的，那么它的提议能够被如实地传达给其他将军，而不会被叛徒的行为所干扰。这是可能制定出「合理」作战计划的前提。第二种情况，如果第i个将军是叛徒，那么他有可能向不同的将军发送不同的提议。这时候我们不能够只听他的一面之词，而是希望经过一致性算法处理之后，各个将军之间充分交换意见，然后根据其他各个将军转述的信息，综合判断得到一个v(i)。这个v(i)是进攻还是撤退，并不太重要，关键是要保证每位将军得到的v(i)是相同的。只有这样，各位将军经过汇总所有的v(1), v(2), …, v(n)之后才能得到最终的完全一致的作战计划。\n根据上面的分析，我们发现，在这两种情况中，我们都只需要关注单个将军（也就是第i个将军）所发出的提议如何传达给其他将军。重点终于来了！至此，我们就能够把原来的问题归约到一个子问题上。这个子问题，才是Leslie Lamport在他的论文中被真正命名为「拜占庭将军问题(Byzantine Generals Problem)」的那个问题。在这个问题中，我们只关注发送命令的单个将军，称他为主将(commanding general)，而其他接受命令的将军称为副官(lieutenant)。下面是「拜占庭将军问题」的精确描述。\n一个主将发送命令给n-1个副官，如何才能确保下面两个条件：\n\n(IC1) 所有忠诚的副官最终都接受相同的命令。\n(IC2) 如果主将是忠诚的，那么所有忠诚的副官都接受主将发出的命令。\n\n这其实正好对应了我们前面已经讨论过的两种情况。如果主将是忠诚的，那么条件IC2保证了命令如实地传递，这时候条件IC1自然也满足了；如果主将是叛徒，那么条件IC2没有意义了，而条件IC1保证了，即使叛徒主将对每个副官发出不同的命令，每个副官仍然能最终获得一致的命令。\n这里有两个地方可能让人产生疑惑。\n第一，有些人会问了，难道主将还能是叛徒？主将都是叛徒了，还有啥搞头啊？其实是这样的，这个「拜占庭将军问题」只是原问题的一个子问题。当n个将军通过传递消息来决策作战计划的时候，可以分解成n个「拜占庭将军问题」，即分别以每位将军作为主将，以其余n-1位将军作为副官。如果有一个算法能够解决「拜占庭将军问题」，那么同时运行n个算法实例，就能使得每位将军都获得完全相同的作战提议序列，即前面我们提到的v(1), v(2), …, v(n)。最后，每位将军将v(1), v(2), …, v(n)使用相同的方法进行汇总（比如按多数投票），就能得到最终的作战计划。\n第二，当主将是叛徒的时候，他可以向不同的副官发送不同的命令，怎么可能每个副官仍然能最终获得一致的命令呢？这正是算法需要解决的。其实这也容易解释（我们前面也提到过这个思路），由于主将可能向不同的副官发送不同的命令，所以副官不能直接采用主将发来的命令，而是也要看看其他副官转述来的主将的命令是什么。然后，一个副官综合了由所有副官转述的命令（再加上主将直接发来的命令）之后，就可能得到比较全面的信息，从而做出一致的判断（在实际中是个不断迭代的过程）。\n好了，我们用了这么多篇幅，终于把「拜占庭将军问题」本身描述清楚了。这实际上也是最难的部分。我们上一章提到过，理解问题本身比理解问题的答案更重要。只要问题本身分析清楚了，如何设计一个能解决它的算法就只是细节问题了。我们这里不深入算法的细节了，感兴趣的读者可以去查阅下列论文：\n\n《The Byzantine Generals Problem》，下载地址：http://lamport.azurewebsites.net/pubs/byz.pdf\n《Reaching Agreement in the Presence of Faults》，下载地址：http://lamport.azurewebsites.net/pubs/reaching.pdf\n\n我们这里只提一下论文给出的算法的结论。\n使用不同的消息模型，「拜占庭将军问题」有不同的解法。\n\n如果将军之间使用口头消息(oral messages)，也就是说，消息被转述的时候是可能被篡改的，那么要对付m个叛徒，需要至少有3m+1个将军（其中至少2m+1个将军是忠诚的）。\n如果将军之间使用签名消息(signed messages)，也就是说，消息被发出来之后是无法伪造的，只要被篡改就会被发现，那么对付m个叛徒，只需要至少m+2个将军，也就是说至少2个忠诚的将军（如果只有1个忠诚的将军，显然这个问题没有意义）。这种情况实际相当于对忠诚将军的数目没有限制。\n\n容错性(fault tolerance)我们前面提到过，以Paxos为代表的分布式一致性协议，是在可信任的环境下运行的。而在「拜占庭将军问题」中，网络中则存在恶意节点。因此我们很容易产生一个想法：Paxos是不是「拜占庭将军问题」在叛徒数为零时的一个特例解？\n这样看其实有点问题。在「拜占庭将军问题」中，除了叛徒，剩下的是忠诚的将军。「忠诚」这个词，其实暗含了一个意思：他是能够正常工作的（即你可以随时通过消息跟他进行交互）。为什么这么说呢？我们知道，一个叛徒可以做任何事，包括发送错误消息，也包括不发送任何消息。「不发送任何消息」，相当于不能正常工作，或者说，发生了某种故障。所以，不仅仅是故意的恶意行为，即使是单纯的故障，也应该能归入叛徒的行为。这在其他将军看来没有区别。\n按照这种理解，「忠诚」这个词并不是很恰当。叛徒数为零，相当于网络中每个节点都在正常工作。但是Paxos的设计也是能够容错的，就像我们在前面讨论的一样，网络中的少数节点发生故障（比如宕机），Paxos仍然能正常工作。可见，Paxos并不能看成是「拜占庭将军问题」在叛徒数为零时的一个特例解。\n那「拜占庭将军问题」和Paxos这类分布式一致性算法的关系应该如何看待呢？我们可以从容错性的强弱程度上来分析。\n一般来说，设计一个计算机系统，小到一块芯片，大到一个分布式网络，都需要考虑一定的容错性(fault tolerance)。但根据错误不同的性质，可以分为两大类：\n\n拜占庭错误(Byzantine fault)。这种错误，在不同的观察者看来，会有前后不一致的表现。\n非拜占庭错误(non-Byzantine fault)。从字面意思看，是指那些不属于前一类错误的其它错误。\n\n这两类错误的含义并没有字面上那么好理解。\n先说说拜占庭错误。在「拜占庭将军问题」中，叛徒的恶意行为固然是属于这一类错误的。在不同的将军看来，叛徒可能发送完全不一致的作战提议。而在计算机系统中，出现故障的节点或部件也可能表现出前后不一致的行为，虽然这并非恶意，但也属于这一类错误。比如信道不稳定，导致节点发送给其它节点的消息发生了随机错误，或者说，消息损坏了(corrupted)。再比如，在数据库系统中，commit之后的数据明明已经同步给磁盘了（通过操作系统的fsync），但由于突然断电等原因，最终数据还是没有真正落盘成功，甚至出现数据错乱。\n再看一下非拜占庭错误。Lamport在他关于Paxos的一篇论文中也使用了non-Byzantine这个词（见《Paxos Made Simple》）。但是这个词的命名的确让人有点不好理解。在分布式系统中，如果节点宕机了，或者网络不通了，都会导致某些节点不能工作。其它节点其实没法区分这两种情况，在它看来，只是发现某个节点暂时联系不上了（即接收消息超时了）。至于是因为那个节点本身出问题了，还是网络不通了，或者是消息出现了严重的延迟，是无法区分的。而且，过一会之后，节点可能会重新恢复（或是自己恢复了，或经过了人工干预）。换句话说，对于出现这种错误的节点，我们只是收不到它的消息了，而不会收到来自它的错误消息。相反，只要收到了来自它的消息，那么消息本身是「忠实」的。\n可见，拜占庭错误是更强的一类错误。在「拜占庭将军问题」中，叛徒发送前后不一致的作战提议，属于拜占庭错误；而不发送任何消息，属于非拜占庭错误。所以，解决「拜占庭将军问题」的算法，既能处理拜占庭错误，又能处理非拜占庭错误。这听起来稍微有些奇怪，不过这只是命名带来的问题。\n总之，「拜占庭将军问题」的解法应该是最强的一类分布式一致性算法，它理论上能够处理任何错误。而Paxos只能处理非拜占庭错误。通常把能够处理拜占庭错误的这种容错性称为「Byzantine fault tolerance」，简称为BFT。\n这样说来，BFT的算法应该可以解决任何错误下的分布式一致性问题，也包括Paxos所解决的问题。那为什么不统一使用BFT的算法来解决所有的分布式一致性问题呢？为什么还需要再费力气设计Paxos之类的一些算法呢？我们前面没有仔细讨论解决「拜占庭将军问题」的算法，所以这里也不做仔细的分析了。但容易想象的是，提供BFT这么强的错误容忍性，肯定需要付出很高的代价。比如需要消息的大量传递。而Paxos不需要提供那么强的容错性，因此可以比较高效地运行。另外，具体到Lamport在论文中给出的解决「拜占庭将军问题」的算法，它还对系统的记时假设(timing assumption)有更强的要求。这也容易理解，既然算法的容错性要求这么高，自然对于运行环境的假设(assumption)也有可能要高一点。由于这个问题是分布式系统中一个挺关键的问题，所以我们在这里单独拿出来讨论一下。在Lamport在论文中，算法对于系统的假设有这么一条：\n\nThe absence of a message can be detected.\n\n这条假设要求，如果某位叛徒将军没有发送任何消息（当然也可能是消息丢失了），那么这件事是可以检测出来的。显然，这只能依赖某种超时机制(time-out)，依赖节点之间的时钟达到一定程度的同步，即时钟的偏移不能超过一个最大值。这实际上是一种同步模型(synchronous model)。而Paxos的系统假设在这一点上就没有这么强，它是基于异步模型(asynchronous model)，对系统时钟没有特定的要求。我在之前的另一篇文章《基于Redis的分布式锁到底安全吗？》一文中也有提到过这个问题。这有时候会成为一些分布式算法产生争议的根源。\n具体来说，根据Paxos的论文所说，Paxos的设计是基于异步(asynchronous)、非拜占庭(non-Byzantine)的系统模型，即：\n\n节点可以以任意速度运行，可能宕机、重启。但是，算法执行过程中需要记录的一些变量，在重启后应该能够恢复。\n消息可以延迟任意长时间，可以重复(duplicated)，可以丢失(lost)，但不能损坏(corrupted)。\n\n上面第一条其实是要求数据在数据库中持久化，并且要保证在落盘过程中没有发生拜占庭错误（我们前面刚提到过）。但实际中由于突然断电、磁盘缓存等现实问题，拜占庭错误是有可能发生的（虽然概率很低），所以这就要求工程上做一些特殊处理。\n上面第二条，消息损坏，属于拜占庭错误。所以Paxos要求不能有消息损坏发生。这在使用TCP协议进行消息传输的情况下，可以认为是能够满足要求的。\n综上分析，解决「拜占庭将军问题」的算法，提供了最强的容错性，即BFT，而Paxos只能容忍非拜占庭错误。但是，在只有非拜占庭错误出现的前提下，Paxos基于异步模型，是比同步模型更弱的系统假设，因此算法更鲁棒。当然，Paxos也更高效。\n区块链在现实中，真正需要达到BFT容错性的系统很少，除非是一些容错性要求非常高的系统，比如波音飞机上的控制系统，或者SpaceX Dragon太空船这类系统（参见https://www.weusecoins.com/bitcoin-byzantine-generals-problem/）。\n我们平常能接触到的BFT的一个典型的例子，就是区块链了。一个区块链网络是一个完全开放的网络，其中的矿工节点(miner)是可以自由加入和自由退出的。这些节点当然有可能是恶意的，所以区块链网络在设计的时候必须要考虑这个问题。这实际上就是典型的「拜占庭将军问题」。\n接下来为了将区块链与「拜占庭将军问题」之间的联系讨论得更加清楚，我们先来非常粗略地介绍一下区块链技术。\n以比特币网络为例，它的核心操作就是进行比特币交易，即某个比特币的拥有者将自己一定数量的比特币转移给其他人。首先，比特币的拥有者要发起交易(transaction)，他需要先用自己的私钥对交易进行签名，然后将交易请求发给矿工节点。矿工将收到的所有交易打包到一个区块(block)当中，并通过一系列复杂度很高的运算找到一个nonce值，保证对于它和区块内其它信息进行hash计算后的结果能够符合预定的要求。这一步对于整个区块链网络至关重要，被称为工作量证明(Proof of Work)。然后矿工把该区块在全网发布，由其它矿工来验证这个区块。这个验证既包括对交易签名进行验证（使用比特币拥有者的公钥），也包括对工作量证明的有效性进行验证。如果验证通过，就把这个区块挂在当前最长的区块链上。\n如果两个矿工几乎同时完成了区块的打包和工作量证明，它们可能都会将区块进行发布，这时区块链就会分叉(fork)。但矿工们会不停地产生新区块，并将新区块挂在当前最长的区块链上，所以最终哪个分叉变得更长，哪个分叉就会被多数矿工节点承认。这么看来，区块链其实不是一个链，而是一棵树。我们知道，在树这种数据结构中，从根节点到叶子节点只有唯一的一条路径。因此，当前有效的区块链其实是这棵树中从根节点到叶子节点最长的那条路径。只要一个区块在最长的链上，那么它就是有效的，它里面包含的所有交易就被固化下来了（被多数节点承认）。\n我们只是非常粗略地介绍了一下区块链的工作原理。如果想了解细节，建议研究一下以太坊的官方wiki，地址是：\n\nhttps://github.com/ethereum/wiki/wiki\n\n下面我们开始讨论区块链技术与「拜占庭将军问题」的关联。\n前面我们讨论「拜占庭将军问题」的时候，得到过以下结论：\n\n如果使用口头消息，那么至少需要多于2/3的将军是忠诚的。\n如果使用签名消息，那么对忠诚将军的数量是没有要求的。\n\n根据前面的介绍，我们在区块链中使用的消息应该属于签名消息。具体体现在：每一个区块中的交易都进行了签名，保证无法被篡改，也能保证这个消息只能是由最初的发起者发出的。那么，这属于上述第二种情况，难道说区块链网络中忠诚节点的数目没有要求？显然不是这样。比如在比特币网络中，要求恶意节点不能掌握多于50%的算力。为什么两者之间似乎不一致呢？这是因为，「拜占庭将军问题」只是关注一个子问题，它关注的是其中一个将军（称为主将）向其他所有将军（称为副官）发送命令的情况。而最终对所有命令进行汇总则要求所有忠诚的将军达成共识。如果忠诚的将军数目太少，不管最终确定的作战计划是什么，还是会失败，因为叛徒可能不执行这个作战计划。这类似于比特币网络中的情况，其中对于最长链的选择过程，就相当于将军们对所有命令进行汇总的操作（按多数投票）。\n在「拜占庭将军问题」中，一个叛徒可能向不同的将军发送不一致的命令。如果算法设计得不好，就可能造成最终无法达成一致。在区块链网络中，类似的行为将会成本很高。这是由于矿工节点发布区块的消息必须经过工作量证明，它如果发布不一致的区块，每个区块都需要工作量证明，这将耗费它大量的算力。另外，这样做也没有动机，它只会产生更多分叉，不会产生最长链。\n在「拜占庭将军问题」的框架下，如何看待工作量证明呢？它其实相当于提高了做叛徒的成本，从而极大降低了叛徒超过半数的可能性。这里可以做一个对比，假设历史上存在真实的拜占庭将军问题，那么可以想象，敌军的间谍打入拜占庭将军这个群体中的成本应该是很高的。所以，可以认为将军中的叛徒不至于太多。但对应到计算机网络中，如果没有类似工作量证明的机制，那么成为叛徒矿工的成本就是非常低的。这就很有可能使得叛徒比忠诚的矿工还要更多。\n当然，从经济学的角度看，在需要工作量证明的前提下，成为叛徒矿工也是不明智的。因为它既然拥有比较强的算力，还不如按照合理的方式通过挖矿赚取收益更为稳妥。不过这是技术之外的因素了。\n除了工作量证明这种机制(Proof of Work)之外，还有一种被称为Proof of Stake的机制。虽然有人质疑这种机制存在缺点(比如nothing at stake)，但站在「拜占庭将军问题」的角度，它也是相当于提高了做叛徒的成本。这就好比一个间谍要混入董事会，成本肯定是比较高的，因为他需要首先持有大量股票。\n区块链到底是什么？有人说是个无法篡改的超级账本，也有人说是个去中心化的交易系统，还有人说它是构建数字货币的底层工具。但是，从技术的角度来说，它首先是个解决了拜占庭将军问题的分布式网络，在完全开放的环境中，实现了数据的一致性和安全性。而其它的属性，都附着于这一技术本质之上。\n","categories":["分布式"]},{"title":"到底什么是一致性？","url":"/posts/27504/","content":"凡是做服务器开发的技术同学，估计都对分布式系统以及相关的理论感兴趣。而对于分布式理论，大家讨论的最多的恐怕就是「分布式一致性」问题了。然而，不管是学术界还是业界的发展历史上，对于「一致性」这个概念的理解，始终充满了混乱。\n如果你问一个技术同学，到底什么是分布式一致性，估计会得到五花八门的答案。这其中比较常见的说法可能是这样的：一致性就是多个服务器节点中的数据保持一致（至少百度百科上差不多就是这么说的）。而如果再讨论得深入一点，可能就会谈到所谓的分布式一致性协议，比如Paxos之类的；还有CAP定理，也跟「一致性」有关。\n但是，「一致性」这个词是非常有迷惑性的。如果用英文来表达的话，跟「一致性」有关的至少有两个词：consistency和consensus。它们经常都被翻译成「一致性」，这进一步加剧了这个概念被滥用的程度。为了接下来讨论方便，我们先简单地澄清一下：\n\n网上通常提到的诸如Paxos之类的分布式一致性协议，其实是consensus这个词。它如果被翻译成「共识」，可能会更好一些。为了表达清晰，本文后面在讨论consensus问题的时候，尽量使用「共识」这个词。\nACID或CAP里C，用的都是consistency这个词，但真实含义迥然不同。\n此外，还经常会听到人们关于「强一致性」的说法，而且这种说法通常都会牵涉到CAP定理或者「分布式事务」的概念。「强一致性」与CAP定理确实关系密切，但与「分布式事务」的关系却不知从何而来。\n\n下面，我们就对这些概念进行详细的解析。\nACID中的一致性ACID是数据库事务的四个特性，分别是原子性 (Atomicity)、一致性 (Consistency)、隔离性 (Isolation)和持久性 (Durability)。\n我们现在关注的是其中的C，即一致性Consistency。它是什么意思呢？通俗地说，它指的是任何一个数据库事务的执行，都应该让整个数据库保持在「一致」的状态。那怎样的状态才算「一致」呢？举个例子，假设在银行账户之间进行转账。显然，「转账」这个操作应该确保在转账前后账户总额保持不变，这是任何一个转账操作必须要遵守的规定。现在假设要从账户A向账户B转账100元，于是我们启动了一个数据库事务。在这个事务中，可以先从账号A中减去100元，再往账户B中增加100元。这样的一个事务操作，满足了“转账前后账户总额保持不变”的规定，因此我们说：这个事务操作保持了数据库的「一致性」；同时，在这个事务执行前后，数据库都处于一种「一致」的状态。\n从上一段的描述中，我们容易看出：\n\nACID中的「一致性」，是对于整个数据库的「一致」状态的维持。抽象来看，对数据库每进行一次事务操作，它的状态就发生一次变化。这相当于把数据库看成了状态机，只要数据库的起始状态是「一致」的，并且每次事务操作都能保持「一致性」，那么数据库就能始终保持在「一致」的状态上 (Consistency Preservation)。\n所谓状态是不是「一致」，其实是由业务层规定的。比如前面这个转账的例子，“转账前后账户总额保持不变”，这个规定只对于「转账」这个特定的业务场景有效。如果换一个业务场景，「一致」的概念就不是这样规定了。所以说，ACID中的「一致性」，其实是体现了业务逻辑上的合理性，并不是由数据库本身的技术特性所决定的。\n\n我们再来看一下，为了让事务总是能保持ACID的一致性，我们需要在实现上考虑哪些因素呢？\n至少两个方面需要考虑：一个是出错情况 (failure/error)；一个是并发 (concurrency) 行为。\n首先，对于任何系统来说，错误都是在所难免的。而错误又可以细分为两类。\n第一类，事务本身的实现逻辑可能存在错误。比如，从账户A向账户B转账100元，在这个事务中，如果我们只从账号A中减去了100元，但忘记了往账户B中增加100元，那么这个事务就是错误的。显然，避免第一类错误，是保持一致性的前提，这需要应用层进行恰当的编码来保证。\n第二类，则是意想不到的各种软硬件错误。比如，还是从账户A向账户B转账100元，事务本身的实现逻辑没有问题，它先执行了从账号A中减去了100元，但在执行往账户B中增加100元之前，却发生了意想不到的错误，比如进程突然crash了，或是磁盘满了，或是网络突然不通了，或是其它任何可能的硬件错误。这时候，事务只执行了前一半，势必会破坏数据库整体状态的一致性。那怎么办呢？这其实就需要ACID中的A（原子性）来保障了。简言之，原子性保障了事务的执行要么全部成功，要么全部失败，而不允许出现“只执行了一半”这种“部分成功”的情况。\n其次，并发行为也可能会影响事务的一致性。在数据库系统中，并发行为体现在可能存在多个事务同时操作同一份数据的情况。还是拿前面转账的例子来说，假设有两个事务：事务1从账户A向账户B转账100元，事务2从账户A向账户C转账50元。如果两个事务先后顺序执行，自然没有问题。但如果两个事务同时执行了，那么可能会出现下面的执行序列（假设账号A的初始余额为x元）：\n\n&lt;事务1&gt;：读取账户A的余额，读到了x元；\n&lt;事务2&gt;：读取账户A的余额，也读到了x元；\n&lt;事务1&gt;：向账户A中写入(x-100)元；\n&lt;事务2&gt;：向账户A中写入(x-50)元；\n……\n\n上面的执行过程，账户A中最后被写入的值是(x-50)元，显然是不对的（事务的一致性会被破坏）。如果两个转账的事务能正确执行完，那么账户A的余额应该是(x-150)元才对。\n这个并发的问题怎么处理呢？这就需要ACID中的I（隔离性）来保障了。什么是隔离性呢？它对于并发执行的多个事务进行合理的排序，保障了不同事务的执行互不干扰。换言之，隔离性这种特性，能够让并发执行的多个事务就好像是按照「先后顺序」执行的一样。\n经过上面的分析，现在关于ACID中的一致性，我们可以得到一些结论了：\n\nACID中的一致性，是个很偏应用层的概念。这跟ACID中的原子性、隔离性和持久性有很大的不同。原子性、隔离性和持久性，都是数据库本身所提供的技术特性；而一致性，则是由特定的业务场景规定的。怪不得《Designing Data-Intensive Applications》[1]一书的作者在书中写道：”The letter C doesn’t really belong in ACID“。\n要真正做到ACID中的一致性，它是要依赖数据库的原子性和隔离性的（应对错误和并发）。但是，就算数据库提供了所有你所需要的技术特性，也不一定能保证ACID的一致性。这还取决于你在应用层对于事务本身的实现逻辑是否正确无误。\n最后，ACID中的一致性，甚至跟分布式都没什么直接关系。它跟分布式的唯一关联在于，在分布式环境下，它所依赖的数据库原子性和隔离性更难实现。\n\n总之，ACID中的一致性，是一个非常特殊的概念。除了数据库事务处理，它很难扩展到其它场景，也跟分布式理论中的其它「一致性」概念没有什么关系。\n分布式事务与共识算法的关系先说共识问题 (consensus problem)。这是分布式系统中的一个十分基础而核心的问题，它表示如何在分布式系统中的多个节点之间就某事达成共识。\n网上通常提到的「分布式一致性协议」，或者「分布式一致性算法」，一般来说就是解决这里的共识问题的算法。用词的不同，是由于中英文翻译造成的。这些算法或协议，经常包含Paxos之类，但也可能包括两阶段提交协议(2PC)或三阶段提交协议(3PC)。\n人们既然经常将Paxos、2PC、3PC这些算法放在一起讨论，那么它们之间势必存在着某种相似性的。但这种相似性是怎么来的呢？我们仔细分析一下。Paxos，是解决共识问题的通用算法。它允许每个节点提出自己的提议(称为proposal），而Paxos算法能够不借助于任何中心化节点，保证各个节点之间对于提议最终达成一致。这里的proposal，是一个抽象的概念，它可以包含任何你想达成共识的数值。2PC和3PC，则是为了解决分布式事务提交问题的。\n这样从表面看起来，Paxos和2PC、3PC，这两类算法似乎没有多少相似性。2PC和3PC是跟分布式事务强相关的，而Paxos跟分布式事务没有什么特别的关系。为了分析更深层次的本质，我们探究一下2PC和3PC产生的背景。\n回到事务的概念。事务本来和分布式没什么直接关系的，就算在一个单节点的数据库上，要实现出事务的ACID特性，也不是那么容易的。只是，如同在前一章节的结尾我们提到的，在分布式环境下，事务的ACID特性更难实现。在前一章中我们主要关注ACID中的一致性，现在我们关注一下ACID中的原子性 (Atomicity)。\nACID中的原子性，要求事务的执行要么全部成功，要么全部失败，而不允许出现“部分成功”的情况。在分布式事务中，这要求参与事务的所有节点，要么全部执行Commit操作，要么全部执行Abort操作。换句话说，参与事务的所有节点，需要在“执行Commit还是Abort”这一点上达成一致（其实就是共识）。这个问题在学术界被称为原子提交问题（Atomic Commitment Problem）[2]，而能够解决原子提交问题的算法，则被称为原子提交协议（Atomic Commitment Protocal，简称ACP）[3]。2PC和3PC，属于原子提交协议两种不同的具体实现。\n分析到这里，我们似乎发现了原子提交问题与共识问题的关联性：\n\n共识问题，解决的是如何在分布式系统中的多个节点之间就某个提议达成共识。\n原子提交问题，解决的是参与分布式事务的所有节点在“执行Commit还是Abort”这一点上达成共识。\n所以，原子提交问题是共识问题的一个特例。\n\n这个酷似「三段论」式的论述，看起来“合情合理”。实际上，学术界在很长一段时间内都认为，分布式事务的原子提交问题是拜占庭将军问题（Byzantine Generals Problem）的一个退化形式[4]。什么是拜占庭将军问题呢？简单来说，它也是分布式系统的一种共识问题，而且是容错性要求最高的一种共识问题。我们在这里不展开讨论拜占庭将军问题了，如果你对细节感兴趣，欢迎阅读我之前的另一篇文章“漫谈分布式系统、拜占庭将军问题与区块链”。总之，学术界以前的这种观点，跟我们刚刚分析得到的结论（原子提交问题是共识问题的一个特例）差不多。\n但是，分布式系统的诡异之处就要体现在这里，一些细节的不同，可能导致非常大的差异。如果你仔细看前文的描述，会发现这样一个细节：当我们描述共识问题的时候，我们说的是在多个节点之间达成共识；而当我们描述原子提交问题的时候，我们说的是在所有节点之间达成共识。这个细微的差别，让这两类问题，几乎变成了完全不同的问题（谁也替代不了谁）。\n从两类问题各自的应用场景来看，这个差异是合理的，也是容易理解的。以解决共识问题的Paxos协议为例，它只要求网络中的大部分节点达成共识就可以了，这样Paxos才能提供一定的容错性，只要网络中发生故障的节点不超过一半仍然能够正常工作（不会被阻塞）。然而，解决原子提交问题的2PC或3PC则不同。即使只有一个节点发生故障了，其它节点也不能擅自决策进行Commit操作。因为这样的话，这个事务就只是「部分地执行成功了」，违反了ACID原子性的要求。所以，原子提交协议必须保证在参与分布式事务的所有节点（包括故障的节点）上对于“执行Commit还是Abort”达成共识。\n故障的节点可能什么都做不了，如何参与达成共识呢？这里的意思是，等故障节点恢复之后，它的决策（Commit或是Abort）必须与其它所有节点保持一致。那么，这是不是意味着，只要有节点发生故障，原子提交协议就一定会阻塞呢？这里有点让人奇怪，答案是「不一定」。根源就在于Abort和Commit并不是对等的决策。假设有一个节点宕机了，其它节点大可以选择Abort决策（注意不能选择Commit），从而让整个事务Abort掉（没有被阻塞住，等待宕机的节点恢复）。等宕机的那个节点恢复了，它会发现相应的事务已经执行Abort了，那么它也按照Abort处理就好了。在这个过程中，参与分布式事务的所有节点（包括宕机的这个节点）对于“执行Commit还是Abort”也是达成了共识的（这个共识是Abort）。正是这些细微却至关重要的细节，让2PC和3PC这种看似简单的协议实现起来没有那么容易。\n论文[5]进一步澄清了这一问题，原子提交问题被抽象成一个新的一致性问题，称为uniform consensus问题，它是与通常的共识问题（consensus problem）不同的问题，而且是更难的问题。uniform consensus，要求所有节点（包括故障节点）都要达成共识；而consensus问题只关注没有发生故障的节点达成共识。\n至此，我们总结一下本章节的结论：\n\n共识问题（consensus problem），解决的是如何在分布式系统中的多个节点之间就某个提议达成共识。它只关注没有发生故障的节点达成共识就可以了。\n在分布式事务中，ACID中的原子性，引出了原子提交问题，它解决的是参与分布式事务的所有节点在“执行Commit还是Abort”这一点上达成共识。原子提交问题属于uniform consensus问题，要求所有节点（包括故障节点）都要达成共识，是比consensus问题更难的一类问题。\nPaxos和解决拜占庭将军问题的算法，解决的是consensus问题；2PC/3PC，解决的是一个特定的uniform consensus问题。\n\nCAP与线性一致性CAP的三个字母分别代表了分布式系统的三个特性：一致性（Consistency）、可用性（Availability）和分区容错性（Partition-tolerance）。而CAP定理指出：任何一个分布式系统只能同时满足三个特性中的两个。但是，这一描述曾经引发了非常多的误解。\n为了进一步展开讨论，我们先关注CAP中的C，也就是一致性。它是什么意思呢？在证明CAP定理的原始论文中[6]，C指的是linearizable consistency，也就是「线性一致性」。更精简的英文表达则是linearizability。\n这听起来可能稍微有点奇怪，但事实就是这样。线性一致性（linearizability）是CAP中的C的原始定义。而很多人在谈到CAP时，则会把这个C看成是强一致性（strong consistency）。这其实也没错，因为线性一致性的另一个名字，就是强一致性[1]。只不过，相比「线性一致性」来说，「强一致性」并不是一个好名字。因为，从这个名字你看不出来它真实的含义（到底「强」在哪？）。在下面的讨论中，我们统一使用线性一致性(linearizability)这个词汇。\n那线性一致性是什么意思呢？它精确的形式化定义[7]非常抽象，且难以理解。大体上是说，在一个并发执行的环境中，不同的操作之间可能是有严格的先后关系的（一个操作执行结束之后另一个操作才开始执行），也可能是并发执行的（一个操作还没执行结束，另一个操作就开始执行了）；如果能够把所有操作排列成一个「合法」的全局线性顺序，那么这些操作就是满足线性一致性的。当然，在这个重新排列的过程中，原来就存在的严格的先后关系，必须得以保持。\n但是，怎么才算合法呢？我们具体到一个存储系统中通过例子来说明。假如我们先往某个数据对象中写入了一个值（假设是1），然后在写入操作结束之后，我们再把这个数据对象读出来（在写入和读取操作之间没有其它的写入操作了）。如果我们发现读取到的值是1，那么就是合法的；而如果读出来的值不是1，那么就是非法的。再假设，我们又执行了一次读取操作，发现读出来的值仍然是1，那么就是合法的；否则就是非法的。也就是说，如果一个读操作已经读到了某个值，那么下一个对于同一个数据对象的读操作就必须读取到同样的值（除非在两次读操作之间还存在别的写入操作）。\n这些例子都比较容易理解，因为站在观察者的角度它们是符合逻辑的。因此，对于一个分布式存储系统来说，线性一致性的含义可以用一个具体的描述来取代：对于任何一个数据对象来说，系统表现得就像它只有一个副本一样[1]。“表现得像只有一个副本”，也就相当于满足了前面的「合法」条件。显然，如果系统对于每个数据对象真的只存一个副本，那么肯定是满足线性一致性的。但是单一副本不具有容错性，所以分布式存储系统一般都会对数据进行复制（replication），也就是保存多个副本。这时，在一个分布式多副本的存储系统中，要提供线性一致性的保证，就需要付出额外的成本了。\n网上对于CAP的一致性的通俗解释，通常有两种：\n\n一致性是指：在分布式系统完成某写操作后的任何读操作，都应该获取到该写操作写入的那个最新的值。显然，如果系统“表现得像只有一个副本”一样，这个描述是成立的。不过这只是描述了线性一致性的一个特例而已，有以偏概全的嫌疑。\n一致性是指：保持所有节点在同一个时刻具有相同的、逻辑一致的数据。显然这种解释并不是从观察者的角度来描述的，而是试图从系统内部的行为（内部实现）来描述的。「所有节点」，可能指的是「所有副本」；至于“在同一个时刻具有相同的、逻辑一致的数据”这个说法，则似乎离线性一致性的本来含义偏离太远了。从逻辑上说，“表现得像只有一个副本”，并不一定需要系统“在同一个时刻具有相同的、逻辑一致的数据”。线性一致性可能有很多种实现方式，而这种解释规定了一种具体的系统实现，同样有以偏概全的嫌疑。\n\n我们前面提到，线性一致性，也被称为强一致性。之所以这么说，大概是因为线性一致性要求多个副本上的数据必须保持如此之「强」的一致性，以至于“让系统表现得就像只有一个副本”。\n另外，网上的资料提到强一致性的时候，还有可能会关联到分布式事务上面，比如2PC/3PC这些原子提交协议。但把它们关联到一起的说法，其深层次含义到底是什么，只能靠猜测。分布式事务处理的并不是同一个数据对象的多个副本的问题，而指的是将针对多个数据对象的各种操作组合起来，提供ACID的特性。将分布式事务看成是强一致性的保证，猜测可能实际上指的就是ACID的原子性。总之，「强一致性」这个词很容易产生误解，所以建议谨慎使用。\n在历史上，CAP定理具有巨大的知名度，但它实际的影响力却没有想象的那么大。随着分布式理论的发展，我们逐渐认识到，CAP并不是一个「大一统」的理论，远不能涵盖分布式系统设计中方方面面的问题。相反，CAP引发了很多误解和理解上的混乱（细节不讨论了）。因此，可以预见到，未来CAP定理的影响将会进一步被削弱。\n","categories":["分布式"]},{"title":"浅析强弱一致性","url":"/posts/54250/","content":"当前这篇文章至少比计划拖后了两个月。在上一篇文章《条分缕析分布式：到底什么是一致性？》中，我们仔细辨析了「一致性」相关的几个容易混淆的概念。而在本文中，我们会沿着逐步深入的思路，跟大家继续讨论顺序一致性、线性一致性、最终一致性等几个概念。\n为了避免产生歧义，我们先明确一下这几个概念的英文表达：\n\n顺序一致性的英文是：sequential consistency。\n线性一致性的英文是：linearizability。实际上，它就是CAP定理中的C，我们在上一篇文章中已经提到过。\n最终一致性的英文是：eventual consistency。\n\n在进行详细的技术性讨论之前，我们先把本文要讨论的几个重点问题和结论列出如下：\n\n线性一致性和顺序一致性，属于分布式系统的一致性模型 (consistency model)。这代表了分布式系统的一个非常非常重要的方面。\n通常人们把线性一致性称为「强一致性」，把最终一致性称为「弱一致性」，但线性一致性和最终一致性其实存在本质的区别。严格来说，它们并不是一个范畴的概念。\n一致性模型之间的「强弱」比较，是一个相对的概念。比如，线性一致性是比顺序一致性更强的一致性模型。当然，除了线性一致性和顺序一致性，也存在其它一些一致性模型（其中很多都比顺序一致性要弱）。\n满足线性一致性的系统，也必定满足顺序一致性，但反过来不一定。这是由一致性模型之间的强弱关系决定的。\n\n下面，我们就开始详细的解析。\n一致性模型的来历我们之所以使用分布式系统，无非是因为分布式系统能带来一些「好处」，比如容错性、可扩展性等等。为了获得这些「好处」，分布式系统实现上常用的方法是复制 (replication) 和分片 (sharding)。而我们将要讨论的一致性模型 (consistency model)，主要是与复制有关。因此这里我们先关注一下复制的机制。\n复制指的是将同一份数据保存在多个网络节点上。而保存同一份数据拷贝的节点，被称为副本 (replica)。复制带来的具体「好处」主要是体现在两个方面：\n\n容错 (fault tolerance)。即使某些网络节点发生故障，由于原本保存着在故障节点上的数据在正常节点上还有备份，所以整个系统仍然可能是可用的。这也是我们期待分布式系统能够提供的「高可用」特性。\n提升吞吐量。将一份数据复制多份并保存在多个副本节点上，还顺便带来一个好处：对于同一个数据对象的访问请求（至少是读请求）可以由多个副本节点分担，从而使得整个系统可以随着请求量的增加不断扩展。\n\n一方面，复制带来了诸多好处；另一方面，它也带来了很多挑战，其中最重要的一个就是数据的一致性问题。由于同一份数据保存在了多个副本节点上，它们之间就存在数据不一致的风险。我们当然希望同一份数据的不同副本总是保持一致。换句话说，我们希望在其中一个副本上所做的修改，在其它副本上也能随时观察到（即读取到）。\n当然我们心里都清楚，让所有副本在任何时刻都保持一致，是不可能的。因为副本之间的数据同步即使速度再快，也是需要时间的。不过幸运的是，我们其实并不关心所有时刻的数据一致性情况。只要系统能够保证，每当我们去「观察」的时候（即读取数据副本的时候），系统表现出来的行为是一致的，就可以了。换句话说，即使在两次「观察」之间，系统内部出现了短暂的数据不一致的情况，只要系统保证外部用户无论如何都发现不了，我们也是可以满意的。\n这意味着，我们应该从系统用户（使用系统的开发者）的角度，来对数据一致性的要求进行定义。\n实际上，早期的分布式系统设计者们对系统设计的要求，也是按照类似的思路进行的。在理想情况下，系统应该维持类似SSI (single-system image)[1]或distribution transparency[2]的特性。这两个概念要表达的核心意思是，系统内部有关分布式实现的复杂性应该对系统的外部用户透明；也就是说，对于系统的外部用户来说，系统应该表现得就好像只有一个单一的副本一样。如果系统能够提供这种「单一系统视图」或「透明性」，那么系统的使用者就能以比较简单的方式来使用系统；否则就可能带来很大的负担。\n系统“表现得就好像只有一个单一的副本”，这是一个相当「笼统」的说法。在此我们讨论3个具体的例子：\n\n我们先向一个副本节点写入x=42，然后读取数据对象x的值。显然，不管我们从哪个副本节点上进行读取，我们都希望读到最新写入的值（也就是42）。只有这样才合理。\n两个系统用户分别在两个副本节点上同时执行写操作。其中，用户A在第1个副本上执行x=42；用户B在第2个副本上执行x=43。然后用户C读取x的值。虽然两个写操作是「同时」进行的，但为了让系统“表现得像只有一个副本”，我们还是需要对它们进行一个先后排序。又因为它们是「同时」执行的，所以谁先谁后都有可能是合理的。如果我们认为x=42在x=43之前先执行，那么读取到的x的值就应该是43；反过来，如果我们认为x=43在x=42之前先执行，那么读取到的x的值就应该是42。\n用户A先在第1个副本上执行x=42，然后用户B再在第2个副本上执行x=43，最后用户C在第3个副本上读取x的值。仍然为了让系统“表现得像只有一个副本”，直觉上看，用户C读取到的x的值似乎应该是43。但是，也不一定非要如此。因为两个写操作是分别由用户A和用户B发起的，他们并不知道彼此谁先谁后（虽然从时间上看用户A的写操作确实在先）。所以，我们也可以选择认为用户B执行x=43在用户A执行x=42之前。这样的话，用户C读取到的x的值就应该是42。当然，根据本文后面的讨论，这种排序就不满足线性一致性了，但却满足顺序一致性。\n\n从这些例子不难看出，一个系统在数据一致性上的具体表现如何，取决于系统对关键事件（读写操作）的排序和执行采取什么样的规则和限制。比如在上面第3个例子中，出现了两种对于读写操作的排序。前一种排序是：\n\n用户A执行x=42。\n用户B执行x=43。\n用户C读取到x的值是43。\n\n而第3个例子中的后一种排序是：\n\n用户B执行x=43。\n用户A执行x=42。\n用户C读取到x的值是42。\n\n虽然这两种排序结果不同，但它们都做到了让系统“表现得像只有一个副本”。它们的不同在于，前一种排序遵循了不同用户的操作的时间先后顺序，而后一种排序没有。实际上，如果我们要求系统满足线性一致性，就只能得到前一种排序结果；而如果只要求系统满足顺序一致性，就有可能得到后一种排序结果（等看完本文后面的讨论，你就能自己得到这些结论）。\n可以这么说，一个分布式系统对于读写操作的某种排序和执行规则，就定义了一种一致性模型 (consistency model)。当一个系统选定了某种特定的一致性模型（比如线性一致性或顺序一致性），那么你就只能看到这种一致性模型所允许的那些操作序列。还是拿前面第3个例子来说明：如果你选定了线性一致性模型，那么系统就不会向你呈现后一种排序，你只能看到前一种排序。\n另外，在前面的三个例子中，不管系统最终给出了哪种排序结果，所有系统的用户其实都对那种操作序列达成了一致的看法。还有一些一致性模型，并不要求所有用户对操作排序的结果达成唯一的一种看法。这样的一致性模型稍显复杂，我们会放在下一篇文章中再讨论（比如因果一致性）。\n接下来，为了更清晰地认识一致性模型，我们来深入到线性一致性和顺序一致性的一些细节中去。\n线性一致性和顺序一致性在讨论之前，我们先把组成分布式系统的一些关键概念定义清楚：\n\n整个系统可以看成由多个进程和一个共享的数据存储组成。对于数据存储的读写操作由进程发起。这里的进程，相当于本文前面提到的系统用户或系统使用者。\n同一个进程发起的读写操作是先后顺序执行的。注意，这里的「进程」概念跟我们平常编程时用到的进程有所不同，进程里面不再分多个线程了。\n数据存储可能有多个副本，但我们在讨论一致性模型的时候，把它看成一个整体来看待，不区分读写操作提交到了具体哪个副本上。\n每个操作的执行，从开始调用到执行结束，都需要花一定的时间。因此，一个进程发起的操作还没有执行完的时候，另一个进程的操作可能就已经开始了。\n\n可见，系统的多个进程是并发执行的。下面我们通过一个例子来说明这种并发执行的情况，进而解释顺序一致性的概念。\n\n上面是一个类似「时空图」的图像，表达了3个进程（P1、P2和P3）对于数据存储的读写执行过程。在这个图中，横向从左到右表示时间递增，黑色的线段表示每个操作的执行起止。线段上面的符号表示具体的读写操作：\n\nA –&gt; w**i(x)，表示一个写操作：第i个进程向数据对象x写入了值A。\nr**i(x) –&gt; A，表示一个读操作：第i个进程从数据对象x中读到了值A。\n\n现在我们要考察的问题是：上图的这样一个执行过程，是否满足顺序一致性？要回答这个问题，我们首先得知道顺序一致性的定义是什么。\n顺序一致性定义[3,4]：如果一个并发执行过程所包含的所有读写操作能够重排成一个全局线性有序的序列，并且这个序列满足以下两个条件，那么这个并发执行过程就是满足顺序一致性的：\n\n条件I：重排后的序列中每一个读操作返回的值，必须等于前面对同一个数据对象的最近一次写操作所写入的值。\n条件II：原来每个进程中各个操作的执行先后顺序，在这个重排后的序列中必须保持一致。\n\n以上图的执行过程为例，我们重排所有的6个读写操作，可以得到如下的有序序列：\n\nA –&gt; w**1(x)\nr**3(x) –&gt; A\nC –&gt; w**2(x)\nr**3(x) –&gt; C\nB –&gt; w**1(x)\nr**3(x) –&gt; B\n\n很容易看出，这个序列是满足前面顺序一致性定义中的两个条件的：\n\n条件I：在这个重排后的序列中，每个读操作都返回了前面最近一次写入的值，比如第2个操作读到的值A，是前面第1个操作写入的；第4个操作读到的值C，是前面第3个操作写入的。\n条件II：原来进程P1中的两个写操作，A –&gt; w**1(x)和B –&gt; w**1(x)，在这个重排后的序列中仍然保持了先后顺序。与此类似，原来进程P3中的3个读操作，在这个重排后的序列中也保持了原来的先后顺序。\n\n所以现在我们可以回答前面的问题了：上图中的执行过程，是满足顺序一致性的。\n你可能会问，顺序一致性为什么会这样定义呢？这个定义的初衷是什么？\n我们可以试着这样理解：首先，重排成一个全局线性有序的序列，相当于系统对外表现出了一种「假象」，原本多进程并发执行的操作，好像是顺序执行的一样。本文前面提到过，理想情况下，分布式系统应该“表现得像只有一个副本”一样。顺序一致性正是遵循了这种「系统假象」，系统对外表现就好像在操作一个单一的副本，执行顺序也必然是可以看做顺序执行的。而条件I规定了系统的表现是合理的（即合乎逻辑的）；条件II则保证了以任何进程的视角来看，它所发起的操作执行顺序都是符合它原本的预期的。总之，一个满足顺序一致性的系统，对外表现就好像总是在操作一个副本一样。\n我们再通过一个例子来看一看这个问题的反面——不满足顺序一致性的执行过程是怎样的。\n\n这个图中的执行过程，与前面第一个图的执行过程非常相似，只是进程P3的几个操作的执行顺序稍有变化。\n我们根据前面顺序一致性的定义再来试着对这个执行过程中的所有操作进行重排：首先根据条件II和进程P1的执行顺序，我们知道，A –&gt; w**1(x)一定要排在B –&gt; w**1(x)前面；再根据条件I，进程P1的B –&gt; w**1(x)一定要排在进程P3的r**3(x) –&gt; B前面。最后，再结合条件II和进程P3的执行顺序，我们能够得出结论，进程P1和进程P3的所有操作，在最终重排后的完整序列中，必然保持以下的顺序：\n\nA –&gt; w**1(x)\nB –&gt; w**1(x)\nr**3(x) –&gt; B\nr**3(x) –&gt; C\nr**3(x) –&gt; A\n\n我们会发现，上面的序列有两个地方不满足条件I：\n\n第4个操作读到了值C，而前面最近一次写操作（第2个操作）所写入的值是B。\n第5个操作读到了值A，而前面最近一次写操作（也是第2个操作）所写入的值是B。\n\n我们还剩一个进程P2的写操作，即C –&gt; w**2(x)，没有放到最后这个序列中。也许我们可以试着将它放置到第3和第4个操作之间，这样就能把前面第一个不满足条件I的地方修复掉。但无论如何，也无法得到一个完全符合条件I和条件II的完整序列了。因此，前面第二个图中的执行过程，是不满足顺序一致性的。进一步说，如果一个系统的执行呈现出了这样的一种执行过程（如前面第二个图所示），那我们可以肯定地说，这个系统是没有遵守顺序一致性的。\n我们再来考察一下线性一致性的概念。线性一致性的定义[5]，与顺序一致性非常相似，也是试图把所有读写操作重排成一个全局线性有序的序列，但除了满足前面的条件I和条件II之外，还要同时满足一个条件：\n\n条件III：不同进程的操作，如果在时间上不重叠，那么它们的执行先后顺序，在这个重排后的序列中必须保持一致。\n\n根据最新定义的条件III，我们来重新评判一下前面第一个图所展现出来的执行过程是不是满足它。为了阅读和讨论方便，我们把第一个图重新展示在下面：\n\n针对条件III，我们分析一下各个操作之间的先后顺序：\n\n进程P1的B –&gt; w**1(x)和进程P2的C –&gt; w**2(x)，在执行时间上是重叠的，所以它们的排序不受条件III的约束。即，在重排后的序列中，这两个操作谁先谁后都可以。同样，进程P1的B –&gt; w**1(x)和进程P3的r**3(x) –&gt; A，也是如此。\n进程P1的A –&gt; w**1(x)和进程P2的C –&gt; w**2(x)，在执行时间上是不重叠的，即前一个操作都执行完了，后一个操作才开始执行。那么，这两个操作就必须满足条件III了：在重排后的序列中，A –&gt; w**1(x)必须排在C –&gt; w**2(x)前面。\n与上面同样的道理，在重排后的序列中，进程P2的C –&gt; w**2(x)必须排在进程P3的r**3(x) –&gt; A之前。\n\n容易看出，在遵守这样的先后关系约束的前提下，不管怎么重排，都无法得到一个满足条件I的完整序列了。所以说，前面第一个图所示的满足顺序一致性的执行过程，是不满足线性一致性的。\n下面我们举一个满足线性一致性的例子：\n\n上图的执行过程，所有操作重排后，可以得到如下的有序序列：\n\nA –&gt; w**1(x)\nr**3(x) –&gt; A\nC –&gt; w**2(x)\nr**3(x) –&gt; C\nB –&gt; w**1(x)\nr**3(x) –&gt; B\n\n不难看出，这个序列是满足所有的条件I、条件II和条件III这三个条件的。因此，这个执行过程满足线性一致性。\n细心的你可能已经发现了，最后这个线性一致性的例子，得到的重排后的序列，与开始第一个顺序一致性的例子重排后的序列，完全相同。当然，这两个例子中原始的多进程并发执行过程，是不同的。这是符合预期的（没有什么可奇怪的）。\n现在我们可以仔细分析一下条件II和条件III，它们囊括了任意两个操作之间所有可能的先后关系：\n\n进程内的任意两个操作之间，总是先后顺序执行的（执行时间上不可能重叠）；而根据条件II，它们的先后顺序在最后重排后的序列中也会保持。\n不同进程的不同操作之间，在执行时间上可能重叠（并发执行），也可能不重叠。根据条件III，不重叠的两个操作，它们在时间上的先后顺序，在最后重排后的序列中会得以保持。而对于执行时间上重叠的两个操作，它们在最后重排后的序列中的先后顺序没有规定。\n\n最后，我们比较一下顺序一致性和线性一致性：\n\n它们都试图让系统“表现得像只有一个副本”一样。\n它们都保证了程序执行顺序不会被打乱。体现在条件II对于进程内各个操作的排序保持上。\n线性一致性考虑了时间先后顺序，而顺序一致性没有。\n满足线性一致性的执行过程，肯定都满足顺序一致性；反之不一定。\n\n注意一下上面第3点两者在时间先后顺序上的不同。这意味着：\n\n线性一致性隐含了时效性保证（recency guarantee）。它保证我们总是能读到数据最新的值。\n在顺序一致性中，我们有可能读到旧版本的数据。比如，在本文第一个顺序一致性的例子中，在进程P2将数据对象x的值写成了C之后，进程P3仍然读到了旧的值（A）。\n\n最终一致性和它的特殊性我们在上一篇文章中提到过，CAP定理[6]中的C，指的就是线性一致性 (linearizability)。它也经常被称为「强一致性」。\n根据CAP定理，当存在网络分区的时候，我们必须在可用性 (availability) 和强一致性之间进行取舍。\n另外，即使在没有网络分区存在的情况下，我们也必须在延迟 (latency) 和强一致性之间进行取舍[7]。这是因为，系统维持强一致性是有成本的。想要维持越强的一致性，就需要在副本节点之间做更多的通信和协调工作，因此会增加操作的总延迟，进而降低整个系统的性能。\n从20世纪90年代中期开始，互联网开始蓬勃发展，系统的规模也变得越来越大。人们设计大型分布式系统的指导思想，也逐步开始更倾向于系统的高可用性和高性能。取舍的结果就是，降低系统提供的一致性保障。这其中非常重要的一条思路就是最终一致性[2]。\n最终一致性的设计思路，不再试图提供单一系统视图 (SSI)，即不再试图让系统“表现得像只有一个副本”一样。它允许读到旧版本的数据。最终一致性的原始出处是论文[2]，作者在论文中给出的最终一致性的定义如下：\n\nEventual consistency. This is a specific form of weak consistency; the storage system guarantees that if no new updates are made to the object, eventually all accesses will return the last updated value.(译文：最终一致性是弱一致性的一种特殊形式；存储系统保证，如果对象没有新的修改操作，那么所有的访问最终都会返回最新写入的值。)\n\n我们发现，虽然最终一致性和本文前面讨论的线性一致性或顺序一致性在命名上非常相似，但它的定义却与后两者存在非常大的差别。深层的原因在于，它们其实属于不同类别的系统属性 (property)。线性一致性和顺序一致性属于safety property（安全性）；而最终一致性属于liveness property（活性）[8]。\n一个并发程序或者一个分布式系统，它们的执行所展现出来的系统属性，可以分为两大类：\n\n***safety**：它表示「坏事」永远不会发生。比如，一个系统如果遵守线性一致性或顺序一致性，那么就永远不会出现违反三个（对于顺序一致性来说是两个）条件的执行过程。而一旦系统出现问题，safety*被违反了，我们也能明确指出是在哪个时间点上出现意外的。\n**liveness**：它表示「好事」最终会发生。这种属性听起来会比较神奇：在任何一个时间点，你都无法判定liveness*被违反了。因为，即使你期望的「好事」还没有发生，也不代表它未来不会发生。就像最终一致性一样，即使当前系统处于不一致的状态，也不代表未来系统就不会达到一致的状态。而只要系统存在“在未来某个时刻达到一致状态”的可能性，最终一致性就没有被违反。另外，可用性 (availability) 也属于liveness*属性。\n\n由此可见，我们在前一小节之所以能够将线性一致性和顺序一致性放在一起讨论和比较，是因为它们都属于safety属性。而最终一致性属于liveness属性，跟这两者存在本质的区别。实际上，最终一致性有点名不副实，它更好的名字可能是收敛性 (convergence)，表示所有副本最终都会收敛到相同的值[9]。\n通常来说，只有当safety和liveness这两种属性被同时考虑时，一个系统才能提供有意义的系统保证[1]。而当系统设计者遵循最终一致性的设计思路时，相当于放弃了所有的safety属性。这意味着，对于系统使用者来说，你必须针对数据不一致的可能性做好补偿措施 (compensation)。这也是最终一致性系统难用的地方。但不管怎么说，最终一致性仍然被认为是系统提供数据一致性的最低要求[1]。\n一致性的强弱关系在本文开头，我们提到过，通常人们把线性一致性称为「强一致性」，把最终一致性称为「弱一致性」。但对于指代特定的一种一致性模型来说，「强一致性」和「弱一致性」都不是一个好名字。因为强和弱，是个相对的概念。\n根据本文前面的讨论，从线性一致性，到顺序一致性，再到最终一致性，一致性的强度依次减弱。但是，一致性模型的强弱关系，其实是有更严格的定义的：\n\n当且仅当一个一致性模型所能接受的执行过程，都能被另一个一致性模型所接受时（前者的集合是后者集合的子集），我们就说前者是比后者「更强」(stronger) 的一致性模型。\n\n按照这个更严格的强弱关系定义，线性一致性是比顺序一致性更强的一致性模型。这是因为，线性一致性比顺序一致性多了一个条件III，所以凡是满足线性一致性的执行过程，肯定也满足顺序一致性。\n我们仔细分析一下也能知道，一致性模型的强弱关系定义，是基于safety属性定义的。所以，将线性一致性或顺序一致性与最终一致性比较强弱，这并不是一个严格的做法。实际上，就像我们前一小节所讨论的，最终一致性在safety方面提供的保证为零，它是属于liveness的概念。一个系统可以在提供最终一致性的同时，也提供另外一种更强一点的带有safety属性的一致性（比如因果一致性）。\n小结就如同我在之前另外一篇文章《漫谈分布式系统、拜占庭将军问题与区块链》中所指出的，理解问题本身比知道问题的答案要重要的多。本文中，我们辨析了线性一致性、顺序一致性、最终一致性这些概念，以及他们的关系和区别。由此我们了解到了分布式系统的一些核心问题，但我们并未讨论怎么解决这些问题。比如，采用什么算法才能提供线性一致性；面对最终一致性的系统，应该怎样编程，包括怎样处理边界情况，等等。相对于理解问题本身而言，这些反而都是细节。\n","categories":["分布式"]},{"title":"独立消息服务分布式事务","url":"/posts/48330/","content":"一、什么是独立消息服务&emsp;&emsp;独立消息服务就是把本地消息表独立成一个数据库，并独立成一个服务。这样做就是为了高内聚低耦合。而且解决了重复利用性的问题。\n二、为什么要这么做&emsp;&emsp;本地消息表分布式事务于2008年eBay提出来，得到了业界的广泛使用，但该方案的缺陷非常明显：为了达到本地事务的效果，消息数据和业务数据必须同一个数据库；这种问题比较大，耦合度高，不可重复利用。\n&emsp;&emsp;为此，经过10年的发展，业界在这基础上做了升级，即升级为 独立消息服务分布式\n三、独立消息服务与主动者之间的通信\n\n主动者发送“待发送”消息给消息服务。\n消息服务收到消息后，保持“待发送”消息，并返回。\n支付服务，直接执行本地业务。\n当本地业务执行成功后，通知消息服务，告诉消息服务，可以发送消息了。\n消息服务收到支付服务的请求，将该消息改为“已发送”，同时发送MQ消息。\n以上5个步骤都成功后，把消息投递给消息中间件MQ。\n\n四、独立消息服务与被动者之间的通信\n7.被动者（积分服务），监听MQ消息，并获取消息内容。\n8.被动者（积分服务），按MQ消息内容，处理本地业务，mq client 主动向消息中间件MQ返回ACK消息。\n9.被动者（积分服务），消费成功后，通知消息服务，我已经成功消费了；消息服务把消息状态改为“已完成”或直接删除即可。\n五、独立消息服务的通信异常分析第1、2步骤发生异常：无影响，因为当前消息服务的消息未落地。\n第3步骤发生异常：有影响，当前消息服务的消息为“待发送”，主动者（支付服务）没有执行业务，导致该条数据为脏数据。\n第4、5步骤发生异常：严重，将导致消息服务的消息为“待发送”，但主动者（支付服务）执行了业务，最终消息发不出去，导致数据不一致。\n第6步骤发生异常：严重，消息发不出去，导致数据不一致。\n第7步骤发生异常：严重，当前消息服务的“已发送”消息，无法消费。\n第8步骤发生异常：严重，当前消息服务的“已发送”消息，无法消费。\n第9步骤发生异常：严重，被动者（积分服务）已经消费成功后，但是消息服务不知道。\n五、如何保证消息100%发送成功\n投递不成功的业务场景，主要是针对3、4、5步骤发送异常导致。故只要解决3、4、5步骤的异常问题，就能保证消息的发送成功。\n保证消息不丢失发送成功的方案：\n采用定时器轮询确认机制，由主动者（支付服务）提供一个可供消息服务查询消息追溯业务执行状态的接口；\n如果业务执行成功，则更改消息的状态为“已发送”；\n否则删除此条消息确保数据一致性。\n具体细节：\n\n定时器轮询过期“待发送”状态的消息（过期消息一般是根据业务规则自行调整，例如2分钟）。\n消息服务向支付服务发起状态查询，并且支付服务返回业务执行状态。\n消息服务对消息进行确认：如果业务执行成功，则发送MQ消息并更改消息状态为“已发送”，否则删除此条消息确保数据一致性。\n\n如何保证消息不丢失，100%发送成功，核心点就是保证“待发送”变为“已发送”。\n六、如何保证消息100%消费成功\n消费不成功的业务场景，主要6、7、8、9步骤发生异常是，会导致消费不成功；故只需要解决步骤6、7、8、9就能保证消息消费成功。\n解决方案：\n采用定时器轮询过期的“已发送”数据，在保证幂等性的条件下，被动者（积分服务）监听到消息后，重新执行业务，并通知消息服务把该消息更改为“已完成”，最终保证主动者和被动者的数据一致性。\n具体细节：\n\n消息服务定期轮询，过期的“已发送”消息（过期一般是根据业务规则来自行调整，例如2分钟）\n消息服务重新将过期的“已发送”消息，发送到消息中间件MQ中。\n被动者（积分服务）监听到消息后，在保证幂等性的情况下重新执行业务。\n被动者通知消息服务，消费服务将该消息设置为“已完成”或直接删除。最终保证了主动者和被动者的数据一致性。\n\n七、独立消息服务分布式事务优缺点优点：\n解耦，降低了业务系统和消息系统之间的耦合度。\n消息服务更加灵活了，伸缩性强，独立部署，独立维护。\n相比本地消息表，降低了开发成本，消息服务可以共用，不用每次都建表。\n\n缺点：\n主动者必须实现消息状态回查的接口。\n需要2个定时器，对过期的状态进行重试操作。\n主动者通知被动者，需要发送2次消息，过于复杂。\n\n面试题1.为什么要设计一个“待发送”消息机制？为什么不业务成功后直接发送？\n如果业务执行成功后，再发送消息，如果发送失败采用重试机制来解决，当业务系统宕机或者重启，就会导致消息丢失，最终造成数据不一致。\n把 业务执行成功，发送消息 放在同一个事务中，看起来没问题，但发送超时其实是成功的，就会导致业务回滚，最终不一致。\n\n2.如果被动者消费失败了怎么办，是否需要主动者做回滚？主动者不会因为被动者的失败而回滚，因为基于消息的分布式事务，该技术方案的目的是高可用和最终一致性，故，它不会因为被动者失败了就做回滚操作，一般的做法是采用MQ负责重试，直到被动者成功为止。\n3.如果被动者因为业务异常必须回滚（例如库存不足），该怎么办？基于消息的分布式事务，必须保证被动者的业务没有异常，它 只允许系统异常，不允许业务异常。\n例如下单减库存、扣优惠券的问题，一般采用 TCC事务 来解决。\n","categories":["分布式"]},{"title":"Aviator 规则引擎","url":"/posts/15100/","content":"Github：https://github.com/killme2008/aviatorscript\n文档：https://www.yuque.com/boyan-avfmj/aviatorscript\n依赖&lt;dependency&gt;  &lt;groupId&gt;com.googlecode.aviator&lt;/groupId&gt;  &lt;artifactId&gt;aviator&lt;/artifactId&gt;  &lt;version&gt;&#123;version&#125;&lt;/version&gt;&lt;/dependency&gt;","categories":["实用类库"]},{"title":"Bean Searcher SQL 查询工具","url":"/posts/24978/","content":"GitHub 仓库\n文档\n依赖implementation &#x27;cn.zhxu:bean-searcher-boot-starter:4.1.2&#x27;\n\n","categories":["实用类库"]},{"title":"Dubbo3","url":"/posts/3153/","content":"Dubbo3 官方文档\n一、依赖implementation &#x27;org.apache.dubbo:dubbo-spring-boot-starter:3.0.7&#x27;\n\n若使用 Nacos 作为注册中心需添加依赖\nimplementation &#x27;org.apache.dubbo:dubbo-registry-nacos:3.0.7&#x27;\n\n二、配置\nSpring Boot 2.6.6\n\n应用级地址发现迁移指南\n1. 生产者添加注解生产者需要在启动类上添加 @EnableDubbo 注解。\n配置文件可在 application.yaml 配置文件中添加 dubbo.application.register-mode 配置应用发现方式。\n可选值：\n\ninterface：只注册接口级\ninstance：只注册应用级\nall：即接口级地址、应用级地址都注册\n\ndubbo:  application:    name: $&#123;spring.application.name&#125;    # 可选值 interface、instance、all，默认是 all，即接口级地址、应用级地址都注册    register-mode: instance  protocol:    name: dubbo    port: 18002  registry:    address: nacos://localhost:8848    version: 1.0.0#  provider:#    group: dubbo-demo\n\n2. 消费者配置文件可在 application.yaml 配置文件中添加 dubbo.application.service-discovery.migration 配置消费者获取生产者地址列表的方式。\n可选值：\n\nFORCE_INTERFACE：只消费接口级地址，如无地址则报错，单订阅 2.x 地址；\nAPPLICATION_FIRST：智能决策接口级/应用级地址，双订阅；\nFORCE_APPLICATION：只消费应用级地址，如无地址则报错，单订阅 3.x 地址。\n\ndubbo:  application:    name: $&#123;spring.application.name&#125;    # 采用应用级服务发现    service-discovery:      migration: APPLICATION_FIRST  registry:    address: nacos://localhost:8848    version: 1.0.0#  consumer:#    group: dubbo-demo\n\n","categories":["实用类库"],"tags":["SpringBoot","RPC"]},{"title":"Guice 轻量级 DI 框架","url":"/posts/34144/","content":"其他资料：https://iowiki.com/guice/guice_quick_guide.html\n背景在日常写一些小工具或者小项目的时候，有依赖管理和依赖注入的需求，但是Spring(Boot)体系作为DI框架过于重量级，于是需要调研一款微型的DI框架。Guice是Google出品的一款轻量级的依赖注入框架，使用它有助于解决项目中的依赖注入问题，提高了可维护性和灵活性。相对于重量级的Spring(Boot)体系，Guice项目只有一个小于1MB的核心模块，如果核心需求是DI（其实Guice也提供了很低层次的AOP实现），那么Guice应该会是一个合适的候选方案。\n\n在查找Guice相关资料的时候，见到不少介绍文章吐槽Guice过于简陋，需要在Module中注册接口和实现的链接关系，显得十分简陋。原因是：Guice是极度精简的DI实现，没有提供Class扫描和自动注册的功能。下文会提供一些思路去实现 ClassPath 下的Bean自动扫描方案\n\n依赖引入与入门示例Guice在5.x版本后整合了低版本的扩展类库，目前使用其所有功能只需要引入一个依赖即可，当前（2022-02前后）最新版本依赖为：\n&lt;dependency&gt;    &lt;groupId&gt;com.google.inject&lt;/groupId&gt;    &lt;artifactId&gt;guice&lt;/artifactId&gt;    &lt;version&gt;5.1.0&lt;/version&gt;&lt;/dependency&gt;\n\n一个入门例子如下：\npublic class GuiceDemo &#123;    public static void main(String[] args) throws Exception &#123;        Injector injector = Guice.createInjector(new DemoModule());        Greeter first = injector.getInstance(Greeter.class);        Greeter second = injector.getInstance(Greeter.class);        System.out.printf(&quot;first hashcode =&gt; %s\\n&quot;, first.hashCode());        first.sayHello();        System.out.printf(&quot;second hashcode =&gt; %s\\n&quot;, second.hashCode());        second.sayHello();    &#125;    @Retention(RUNTIME)    public @interface Count &#123;    &#125;    @Retention(RUNTIME)    public @interface Message &#123;    &#125;    @Singleton    public static class Greeter &#123;        private final String message;        private final Integer count;        @Inject        public Greeter(@Message String message,                       @Count Integer count) &#123;            this.message = message;            this.count = count;        &#125;        public void sayHello() &#123;            for (int i = 1; i &lt;= count; i++) &#123;                System.out.printf(&quot;%s,count =&gt; %d\\n&quot;, message, i);            &#125;        &#125;    &#125;    public static class DemoModule extends AbstractModule &#123;        @Override        public void configure() &#123;//            bind(Greeter.class).in(Scopes.SINGLETON);        &#125;        @Provides        @Count        public static Integer count() &#123;            return 2;        &#125;        @Provides        @Count        public static String message() &#123;            return &quot;vlts.cn&quot;;        &#125;    &#125;&#125;\n\n执行main方法控制台输出：\nfirst hashcode =&gt; 914507705vlts.cn,count =&gt; 1vlts.cn,count =&gt; 2second hashcode =&gt; 914507705vlts.cn,count =&gt; 1vlts.cn,count =&gt; 2\n\nGreeter类需要注册为单例，Guice中注册的实例如果不显式指定为单例，默认都是原型（Prototype，每次重新构造一个新的实例）。Guice注册一个单例目前来看主要有三种方式：\n\n方式一：在类中使用注解@Singleton（使用Injector#getInstance()会懒加载单例）\n\n@Singletonpublic static class Greeter &#123;    ......&#125;\n\n\n方式二：注册绑定关系的时候显式指定Scope为Scopes.SINGLETON\n\npublic static class DemoModule extends AbstractModule &#123;    @Override    public void configure() &#123;        bind(Greeter.class).in(Scopes.SINGLETON);        // 如果Greeter已经使用了注解@Singleton可以无需指定in(Scopes.SINGLETON)，仅bind(Greeter.class)即可    &#125;    &#125;\n\n\n方式三：组合使用注解@Provides和@Singleton，效果类似于Spring中的@Bean注解\n\npublic static class SecondModule extends AbstractModule &#123;    @Override    public void configure() &#123;        // config module    &#125;    @Provides    @Singleton    public Foo foo() &#123;        return new Foo();    &#125;&#125;public static class Foo &#123;&#125;\n\n上面的例子中，如果Greeter类不使用@Singleton，同时注释掉bind(Greeter.class).in(Scopes.SINGLETON);，那么执行main方法会发现两次从注入器中获取到的实例的hashCode不一致，也就是两次从注入器中获取到的都是重新创建的实例（hashCode不相同）：\nGuice中所有单例默认是懒加载的，理解为单例初始化使用了「懒汉模式」，可以通过ScopedBindingBuilder#asEagerSingleton()标记单例为饥饿加载模式，可以理解为切换单例加载模式为「饿汉模式」。\nGuice注入器初始化Guice注入器接口Injector是其核心API，类比为Spring中的BeanFactory。Injector初始化依赖于一或多个模块（com.google.inject.Module）的实现。初始化Injector的示例如下：\npublic class GuiceInjectorDemo &#123;    public static void main(String[] args) throws Exception &#123;        Injector injector = Guice.createInjector(                new FirstModule(),                new SecondModule()        );        //  injector.getInstance(Foo.class);    &#125;    public static class FirstModule extends AbstractModule &#123;        @Override        public void configure() &#123;            // config module        &#125;    &#125;    public static class SecondModule extends AbstractModule &#123;        @Override        public void configure() &#123;            // config module        &#125;    &#125;&#125;\n\nInjector支持基于当前实例创建子Injector实例，类比于Spring中的父子IOC容器：\npublic class GuiceChildInjectorDemo &#123;    public static void main(String[] args) throws Exception &#123;        Injector parent = Guice.createInjector(                new FirstModule()        );        Injector childInjector = parent.createChildInjector(new SecondModule());    &#125;    public static class FirstModule extends AbstractModule &#123;        @Override        public void configure() &#123;            // config module        &#125;    &#125;    public static class SecondModule extends AbstractModule &#123;        @Override        public void configure() &#123;            // config module        &#125;    &#125;&#125;\n\n子Injector实例会继承父Injector实例的所有状态（所有绑定、Scope、拦截器和转换器等）。\nGuice心智模型\n心智模型（Mental Model）的概念来自于认知心理学，心智模型指的是指认知主体运用概念对自身体验进行判断与分类的一种惯性化的心理机制或既定的认知框架\n\nGuice在认知上可以理解为一个map（文档中表示为map[^guice-map]），应用程序代码可以通过这个map声明和获取应用程序内的依赖组件。这个Guice Map每一个Map.Entry有两个部分：\n\nGuice Key：Guice Map中的键，用于获取该map中特定的值\nProvider：Guice Map中的值，用于创建应用于应用程序内的（组件）对象\n\n这个抽象的Guice Map有点像下面这样的结构：\n// com.google.inject.Key =&gt; com.google.inject.Providerprivate final ConcurrentMap&lt;Key&lt;?&gt;, Provider&lt;?&gt;&gt; guiceMap = new ConcurrentHashMap&lt;&gt;();\n\nGuice Key用于标识Guice Map中的一个依赖组件，这个键是全局唯一的，由com.google.inject.Key定义。鉴于Java里面没有形参（也就是方法的入参列表或者返回值只有顺序和类型，没有名称），所以很多时候在构建Guice Key的时候既需要依赖组件的类型，无法唯一确定组件类型的时候（例如一些定义常量的场景，只要满足常量的场景，对于类实例也是可行的），需要额外增加一个自定义注解用于生成组合的唯一标识Type + Annotation(Type)。例如：\n\n@Message String相当于Key&lt;String&gt;\n@Count int相当于Key&lt;Integer&gt;\n\npublic class GuiceMentalModelDemo &#123;    public static void main(String[] args) &#123;        Injector injector = Guice.createInjector(new EchoModule());        EchoService echoService = injector.getInstance(EchoService.class);    &#125;    @Qualifier    @Retention(RUNTIME)    public @interface Count &#123;    &#125;    @Qualifier    @Retention(RUNTIME)    public @interface Message &#123;    &#125;    public static class EchoModule extends AbstractModule &#123;        @Override        public void configure() &#123;            bind(EchoService.class).in(Scopes.SINGLETON);        &#125;        @Provides        @Message        public String messageProvider() &#123;            return &quot;foo&quot;;        &#125;        @Provides        @Count        public Integer countProvider() &#123;            return 10087;        &#125;    &#125;    public static class EchoService &#123;        private final String messageValue;        private final Integer countValue;        @Inject        public EchoService(@Message String messageValue, @Count Integer countValue) &#123;            this.messageValue = messageValue;            this.countValue = countValue;        &#125;    &#125;&#125;\n\nGuice注入器创建单例的处理逻辑类似于：\nString messageValue = injector.getInstance(Key.get(String.class, Message.class));Integer countValue = injector.getInstance(Key.get(Integer.class, Count.class));EchoService echoService = new EchoService(messageValue, countValue);\n\n这里的注解@Provides在Guice中的实现对应于Provider接口，该接口的定义十分简单：\ninterface Provider&lt;T&gt; &#123;    /** Provides an instance of T.**/    T get();&#125;\n\nGuice Map中所有的值都可以理解为一个Provider的实现，例如上面的例子可以理解为：\n// messageProvider.get() =&gt; &#x27;foo&#x27;Provider&lt;String&gt; messageProvider = () -&gt; EchoModule.messageProvider();// countProvider.get() =&gt; 10087Provider&lt;Integer&gt; countProvider = () -&gt; EchoModule.countProvider();\n\n依赖搜索和创建的过程也是根据条件创建Key实例，然后在Guice Map中定位唯一的于Provider，然后通过该Provider完成依赖组件的实例化，接着完成后续的依赖注入动作。这个过程在Guice文档中使用了一个具体的表格进行说明，这里贴一下这个表格：\n\n\n\nGuice DSL语法\n对应的模型\n\n\n\nbind(key).toInstance(value)\n【instance binding】map.put(key,() -&gt; value)\n\n\nbind(key).toProvider(provider)\n【provider binding】map.put(key, provider)\n\n\nbind(key).to(anotherKey)\n【linked binding】map.put(key, map.get(anotherKey))\n\n\n@Provides Foo provideFoo()&#123;...&#125;\n【provider method binding】map.put(Key.get(Foo.class), module::provideFoo)\n\n\nKey实例的创建有很多衍生方法，可以满足单具体类型、具体类型加注解等多种实例化方式。依赖注入使用@Inject注解，支持成员变量和构造注入，一个接口由多个实现的场景可以通过内建@Named注解或者自定义注解指定具体注入的实现，但是需要在构建绑定的时候通过@Named注解或者自定义注解标记具体的实现。例如：\npublic class GuiceMentalModelDemo &#123;    public static void main(String[] args) &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;                bind(MessageProcessor.class)                        .annotatedWith(Names.named(&quot;firstMessageProcessor&quot;))                        .to(FirstMessageProcessor.class)                        .in(Scopes.SINGLETON);                bind(MessageProcessor.class)                        .annotatedWith(Names.named(&quot;secondMessageProcessor&quot;))                        .to(SecondMessageProcessor.class)                        .in(Scopes.SINGLETON);            &#125;        &#125;);        MessageClient messageClient = injector.getInstance(MessageClient.class);        messageClient.invoke(&quot;hello world&quot;);    &#125;    interface MessageProcessor &#123;        void process(String message);    &#125;    public static class FirstMessageProcessor implements MessageProcessor &#123;        @Override        public void process(String message) &#123;            System.out.println(&quot;FirstMessageProcessor process message =&gt; &quot; + message);        &#125;    &#125;    public static class SecondMessageProcessor implements MessageProcessor &#123;        @Override        public void process(String message) &#123;            System.out.println(&quot;SecondMessageProcessor process message =&gt; &quot; + message);        &#125;    &#125;    @Singleton    public static class MessageClient &#123;        @Inject        @Named(&quot;secondMessageProcessor&quot;)        private MessageProcessor messageProcessor;        public void invoke(String message) &#123;            messageProcessor.process(message);        &#125;    &#125;&#125;// 控制台输出：SecondMessageProcessor process message =&gt; hello world\n\n@Named注解这里可以换成任意的自定义注解实现，不过注意自定义注解需要添加元注解@javax.inject.Qualifier，最终的效果是一致的，内置的@Named就能满足大部分的场景。最后，每个组件注册到Guice中，该组件的所有依赖会形成一个有向图，注入该组件的时候会递归注入该组件自身的所有依赖，这个遍历注入流程遵循「深度优先」。Guice会校验组件的依赖有向图的合法性，如果该有向图是非法的，会抛出CreationException异常。\nGuice支持的绑定Guice提供AbstractModule抽象模块类给使用者继承，覆盖configure()方法，通过bind()相关API创建绑定。\n\n❝\nGuice中的Binding其实就是前面提到的Mental Model中Guice Map中的键和值的映射关系，Guice提供多种注册这个绑定关系的API\n❞\n\n这里仅介绍最常用的绑定类型：\n\nLinked Binding\nInstance Binding\nProvider Binding\nConstructor Binding\nUntargeted Binding\nMulti Binding\nJIT Binding\n\nLinked BindingLinked Binding用于映射一个类型和此类型的实现类型，使用起来如下：\nbind(接口类型.class).to(实现类型.class);\n\n具体例子：\npublic class GuiceLinkedBindingDemo &#123;    public static void main(String[] args) &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;                bind(Foo.class).to(Bar.class).in(Scopes.SINGLETON);            &#125;        &#125;);        Foo foo = injector.getInstance(Foo.class);    &#125;    interface Foo &#123;    &#125;    public static class Bar implements Foo &#123;    &#125;&#125;\n\nLinked Binding常用于这种一个接口一个实现的场景。目标类型上添加了@Singleton注解，那么编程式注册绑定时候可以无需调用in(Scopes.SINGLETON)。\nInstance BindingInstance Binding用于映射一个类型和此类型的实现类型「实例」，也包括常量的绑定。以前一小节的例子稍微改造成Instance Binding的模式如下：\nfinal Bar bar = new Bar();bind(Foo.class).toInstance(bar);# 或者添加Named注解bind(Foo.class).annotatedWith(Names.named(&quot;bar&quot;)).toInstance(bar);# 常量绑定bindConstant().annotatedWith(Names.named(&quot;key&quot;)).to(value);\n\n可以基于这种方式进行常量的绑定，例如：\npublic class GuiceInstanceBindingDemo &#123;    public static void main(String[] args) &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;                bind(String.class).annotatedWith(Names.named(&quot;host&quot;)).toInstance(&quot;localhost&quot;);                bind(Integer.class).annotatedWith(Names.named(&quot;port&quot;)).toInstance(8080);                bindConstant().annotatedWith(Protocol.class).to(&quot;HTTPS&quot;);                bind(HttpClient.class).to(DefaultHttpClient.class).in(Scopes.SINGLETON);            &#125;        &#125;);        HttpClient httpClient = injector.getInstance(HttpClient.class);        httpClient.print();    &#125;    @Qualifier    @Retention(RUNTIME)    public @interface Protocol &#123;    &#125;    interface HttpClient &#123;        void print();    &#125;    public static class DefaultHttpClient implements HttpClient &#123;        @Inject        @Named(&quot;host&quot;)        private String host;        @Inject        @Named(&quot;port&quot;)        private Integer port;        @Inject        @Protocol        private String protocol;        @Override        public void print() &#123;            System.out.printf(&quot;host =&gt; %s, port =&gt; %d, protocol =&gt; %s\\n&quot;, host, port, protocol);        &#125;    &#125;&#125;// 输出结果：host =&gt; localhost, port =&gt; 8080, protocol =&gt; HTTPS\n\nProvider BindingProvider Binding，可以指定某个类型和该类型的Provider实现类型进行绑定，有点像设计模式中的简单工厂模式，可以类比为Spring中的FactoryBean接口。举个例子：\npublic class GuiceProviderBindingDemo &#123;    public static void main(String[] args) throws Exception &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;                bind(Key.get(Foo.class)).toProvider(FooProvider.class).in(Scopes.SINGLETON);            &#125;        &#125;);        Foo s1 = injector.getInstance(Key.get(Foo.class));        Foo s2 = injector.getInstance(Key.get(Foo.class));    &#125;    public static class Foo &#123;    &#125;    public static class FooProvider implements Provider&lt;Foo&gt; &#123;        private final Foo foo = new Foo();        @Override        public Foo get() &#123;            System.out.println(&quot;Get Foo from FooProvider...&quot;);            return foo;        &#125;    &#125;&#125;// Get Foo from FooProvider...\n\n\n这里也要注意，如果标记Provider为单例，那么在Injector中获取创建的实例，只会调用一次get()方法，也就是懒加载\n\n@Provides注解是Provider Binding一种特化模式，可以在自定义的Module实现中添加使用了@Provides注解的返回对应类型实例的方法，这个用法跟Spring里面的@Bean注解十分相似。一个例子如下：\npublic class GuiceAnnotationProviderBindingDemo &#123;    public static void main(String[] args) throws Exception &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;            &#125;            @Singleton            @Provides            public Foo fooProvider() &#123;                System.out.println(&quot;init Foo from method fooProvider()...&quot;);                return new Foo();            &#125;        &#125;);        Foo s1 = injector.getInstance(Key.get(Foo.class));        Foo s2 = injector.getInstance(Key.get(Foo.class));    &#125;    public static class Foo &#123;    &#125;&#125;// init Foo from method fooProvider()...\n\nConstructor BindingConstructor Binding需要显式绑定某个类型到其实现类型的一个明确入参类型的构造函数，目标构造函数不需要使用@Inject注解。例如：\npublic class GuiceConstructorBindingDemo &#123;    public static void main(String[] args) throws Exception &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;                try &#123;                    bind(Key.get(JdbcTemplate.class))                            .toConstructor(DefaultJdbcTemplate.class.getConstructor(DataSource.class))                            .in(Scopes.SINGLETON);                &#125; catch (NoSuchMethodException e) &#123;                    addError(e);                &#125;            &#125;        &#125;);        JdbcTemplate instance = injector.getInstance(JdbcTemplate.class);    &#125;    interface JdbcTemplate &#123;    &#125;    public static class DefaultJdbcTemplate implements JdbcTemplate &#123;        public DefaultJdbcTemplate(DataSource dataSource) &#123;            System.out.println(&quot;init JdbcTemplate,ds =&gt; &quot; + dataSource.hashCode());        &#125;    &#125;    public static class DataSource &#123;    &#125;&#125;// init JdbcTemplate,ds =&gt; 1420232606\n\n这里需要使用者捕获和处理获取构造函数失败抛出的NoSuchMethodException异常。\nUntargeted BindingUntargeted Binding用于注册绑定没有目标（实现）类型的特化场景，一般是没有实现接口的普通类型，在没有使用@Named注解或者自定义注解绑定的前提下可以忽略to()调用。但是如果使用了@Named注解或者自定义注解进行绑定，to()调用一定不能忽略。例如：\npublic class GuiceUnTargetedBindingDemo &#123;    public static void main(String[] args) throws Exception &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;                bind(Foo.class).in(Scopes.SINGLETON);                bind(Bar.class).annotatedWith(Names.named(&quot;bar&quot;)).to(Bar.class).in(Scopes.SINGLETON);            &#125;        &#125;);    &#125;    public static class Foo &#123;    &#125;    public static class Bar &#123;    &#125;&#125;\n\nMulti BindingMulti Binding也就是多（实例）绑定，使用特化的Binder代理完成，这三种Binder代理分别是：\n\nMultibinder：可以简单理解为Type =&gt; Set&lt;TypeImpl&gt;，注入类型为Set&lt;Type&gt;\nMapBinder：可以简单理解为(KeyType, ValueType) =&gt; Map&lt;KeyType, ValueTypeImpl&gt;，注入类型为Map&lt;KeyType, ValueType&gt;\nOptionalBinder：可以简单理解为Type =&gt; Optional.ofNullable(GuiceMap.get(Type)).or(DefaultImpl)，注入类型为Optional&lt;Type&gt;\n\nMultibinder的使用例子：\npublic class GuiceMultiBinderDemo &#123;    public static void main(String[] args) &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;                Multibinder&lt;Processor&gt; multiBinder = Multibinder.newSetBinder(binder(), Processor.class);                multiBinder.permitDuplicates().addBinding().to(FirstProcessor.class).in(Scopes.SINGLETON);                multiBinder.permitDuplicates().addBinding().to(SecondProcessor.class).in(Scopes.SINGLETON);            &#125;        &#125;);        injector.getInstance(Client.class).process();    &#125;    @Singleton    public static class Client &#123;        @Inject        private Set&lt;Processor&gt; processors;        public void process() &#123;            Optional.ofNullable(processors).ifPresent(ps -&gt; ps.forEach(Processor::process));        &#125;    &#125;    interface Processor &#123;        void process();    &#125;    public static class FirstProcessor implements Processor &#123;        @Override        public void process() &#123;            System.out.println(&quot;FirstProcessor process...&quot;);        &#125;    &#125;    public static class SecondProcessor implements Processor &#123;        @Override        public void process() &#123;            System.out.println(&quot;SecondProcessor process...&quot;);        &#125;    &#125;&#125;// 输出结果FirstProcessor process...SecondProcessor process...\n\nMapBinder的使用例子：\npublic class GuiceMapBinderDemo &#123;    public static void main(String[] args) &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;                MapBinder&lt;Type, Processor&gt; mapBinder = MapBinder.newMapBinder(binder(), Type.class, Processor.class);                mapBinder.addBinding(Type.SMS).to(SmsProcessor.class).in(Scopes.SINGLETON);                mapBinder.addBinding(Type.MESSAGE_TEMPLATE).to(MessageTemplateProcessor.class).in(Scopes.SINGLETON);            &#125;        &#125;);        injector.getInstance(Client.class).process();    &#125;    @Singleton    public static class Client &#123;        @Inject        private Map&lt;Type, Processor&gt; processors;        public void process() &#123;            Optional.ofNullable(processors).ifPresent(ps -&gt; ps.forEach(((type, processor) -&gt; processor.process())));        &#125;    &#125;    public enum Type &#123;        /**         * 短信         */        SMS,        /**         * 消息模板         */        MESSAGE_TEMPLATE    &#125;    interface Processor &#123;        void process();    &#125;    public static class SmsProcessor implements Processor &#123;        @Override        public void process() &#123;            System.out.println(&quot;SmsProcessor process...&quot;);        &#125;    &#125;    public static class MessageTemplateProcessor implements Processor &#123;        @Override        public void process() &#123;            System.out.println(&quot;MessageTemplateProcessor process...&quot;);        &#125;    &#125;&#125;// 输出结果SmsProcessor process...MessageTemplateProcessor process...\n\nOptionalBinder的使用例子：\npublic class GuiceOptionalBinderDemo &#123;    public static void main(String[] args) &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;//                bind(Logger.class).to(LogbackLogger.class).in(Scopes.SINGLETON);                OptionalBinder.newOptionalBinder(binder(), Logger.class)                        .setDefault()                        .to(StdLogger.class)                        .in(Scopes.SINGLETON);            &#125;        &#125;);        injector.getInstance(Client.class).log(&quot;Hello World&quot;);    &#125;    @Singleton    public static class Client &#123;        @Inject        private Optional&lt;Logger&gt; logger;        public void log(String content) &#123;            logger.ifPresent(l -&gt; l.log(content));        &#125;    &#125;    interface Logger &#123;        void log(String content);    &#125;    public static class StdLogger implements Logger &#123;        @Override        public void log(String content) &#123;            System.out.println(content);        &#125;    &#125;&#125;\n\nJIT BindingJIT Binding也就是Just-In-Time Binding，也可以称为隐式绑定（Implicit Binding）。隐式绑定需要满足：\n\n构造函数必须无参，并且非private修饰\n没有在Module实现中激活Binder#requireAtInjectRequired()\n\n调用Binder#requireAtInjectRequired()方法可以强制声明Guice只使用带有@Inject注解的构造器。调用Binder#requireExplicitBindings()方法可以声明Module内必须显式声明所有绑定，也就是禁用隐式绑定，所有绑定必须在Module的实现中声明。下面是一个隐式绑定的例子：\npublic class GuiceJustInTimeBindingDemo &#123;    public static void main(String[] args) throws Exception &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;            &#125;        &#125;);        Foo instance = injector.getInstance(Key.get(Foo.class));    &#125;    public static class Foo &#123;        public Foo() &#123;            System.out.println(&quot;init Foo...&quot;);        &#125;    &#125;&#125;// init Foo...\n\n此外还有两个运行时绑定注解：\n\n@ImplementedBy：特化的Linked Binding，用于运行时绑定对应的目标类型\n\n@ImplementedBy(MessageProcessor.class)public interface Processor &#123;  &#125;\n\n\n@ProvidedBy：特化的Provider Binding，用于运行时绑定对应的目标类型的Provider实现\n\n@ProvidedBy(DruidDataSource.class)public interface DataSource &#123;  &#125;\n\nAOP特性Guice提供了相对底层的AOP特性，使用者需要自行实现org.aopalliance.intercept.MethodInterceptor接口在方法执行点的前后插入自定义代码，并且通过Binder#bindInterceptor()注册方法拦截器。这里只通过一个简单的例子进行演示，模拟的场景是方法执行前和方法执行完成后分别打印日志，并且计算目标方法调用耗时：\npublic class GuiceAopDemo &#123;    public static void main(String[] args) &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;                bindInterceptor(Matchers.only(EchoService.class), Matchers.any(), new EchoMethodInterceptor());            &#125;        &#125;);        EchoService instance = injector.getInstance(Key.get(EchoService.class));        instance.echo(&quot;throwable&quot;);    &#125;    public static class EchoService &#123;        public void echo(String name) &#123;            System.out.println(name + &quot; echo&quot;);        &#125;    &#125;    public static class EchoMethodInterceptor implements MethodInterceptor &#123;        @Override        public Object invoke(MethodInvocation methodInvocation) throws Throwable &#123;            Method method = methodInvocation.getMethod();            String methodName = method.getName();            long start = System.nanoTime();            System.out.printf(&quot;Before invoke method =&gt; [%s]\\n&quot;, methodName);            Object result = methodInvocation.proceed();            long end = System.nanoTime();            System.out.printf(&quot;After invoke method =&gt; [%s], cost =&gt; %d ns\\n&quot;, methodName, (end - start));            return result;        &#125;    &#125;&#125;// 输出结果Before invoke method =&gt; [echo]throwable echoAfter invoke method =&gt; [echo], cost =&gt; 16013700 ns\n\n自定义注入通过TypeListener和MembersInjector可以实现目标类型实例的成员属性自定义注入扩展。例如可以通过下面的方式实现目标实例的org.slf4j.Logger属性的自动注入：\npublic class GuiceCustomInjectionDemo &#123;    public static void main(String[] args) throws Exception &#123;        Injector injector = Guice.createInjector(new AbstractModule() &#123;            @Override            public void configure() &#123;                bindListener(Matchers.any(), new LoggingListener());            &#125;        &#125;);        injector.getInstance(LoggingClient.class).doLogging(&quot;Hello World&quot;);    &#125;    public static class LoggingClient &#123;        @Logging        private Logger logger;        public void doLogging(String content) &#123;            Optional.ofNullable(logger).ifPresent(l -&gt; l.info(content));        &#125;    &#125;    @Qualifier    @Retention(RUNTIME)    @interface Logging &#123;    &#125;    public static class LoggingMembersInjector&lt;T&gt; implements MembersInjector&lt;T&gt; &#123;        private final Field field;        private final Logger logger;        public LoggingMembersInjector(Field field) &#123;            this.field = field;            this.logger = LoggerFactory.getLogger(field.getDeclaringClass());            field.setAccessible(true);        &#125;        @Override        public void injectMembers(T instance) &#123;            try &#123;                field.set(instance, logger);            &#125; catch (IllegalAccessException e) &#123;                throw new IllegalStateException(e);            &#125; finally &#123;                field.setAccessible(false);            &#125;        &#125;    &#125;    public static class LoggingListener implements TypeListener &#123;        @Override        public &lt;I&gt; void hear(TypeLiteral&lt;I&gt; typeLiteral, TypeEncounter&lt;I&gt; typeEncounter) &#123;            Class&lt;?&gt; clazz = typeLiteral.getRawType();            while (Objects.nonNull(clazz)) &#123;                for (Field field : clazz.getDeclaredFields()) &#123;                    if (field.getType() == Logger.class &amp;&amp; field.isAnnotationPresent(Logging.class)) &#123;                        typeEncounter.register(new LoggingMembersInjector&lt;&gt;(field));                    &#125;                &#125;                clazz = clazz.getSuperclass();            &#125;        &#125;    &#125;&#125;// 输出结果[2022-02-22 00:51:33,516] [INFO] cn.vlts.guice.GuiceCustomInjectionDemo$LoggingClient [main] [] - Hello World\n\n基于ClassGraph扫描和全自动注册绑定Guice本身不提供类路径或者Jar文件的类扫描功能，要实现类路径下的所有Bean全自动注册绑定，需要依赖第三方类扫描框架，这里选用了一个性能比较高社区比较活跃的类库io.github.classgraph:classgraph。引入ClassGraph的最新依赖：\n&lt;dependency&gt;    &lt;groupId&gt;io.github.classgraph&lt;/groupId&gt;    &lt;artifactId&gt;classgraph&lt;/artifactId&gt;    &lt;version&gt;4.8.138&lt;/version&gt;&lt;/dependency&gt;\n\n编写自动扫描Module：\nimport com.google.inject.AbstractModule;import com.google.inject.Guice;import com.google.inject.Injector;import com.jz.demo.guice.ConfigModule;import io.github.classgraph.ClassGraph;import io.github.classgraph.ClassInfo;import io.github.classgraph.ScanResult;import java.util.LinkedList;import java.util.List;import lombok.SneakyThrows;/** * @Auther jd */public class AutoModuleScanner &#123;  private static String[] acceptPackages = &#123;&quot;com.jz.demo.guice&quot;&#125;;  private static String[] rejectClasses = &#123;&quot;*Main&quot;&#125;;  private volatile static Injector injector;  @SneakyThrows  public static synchronized void init() &#123;    if (injector != null) &#123;      return;    &#125;    ClassGraph classGraph = new ClassGraph();    ScanResult scanResult = classGraph        .enableClassInfo()        .acceptPackages(acceptPackages)        .rejectClasses(rejectClasses)        .scan();    List&lt;AbstractModule&gt; modules = new LinkedList&lt;&gt;();    for (ClassInfo classInfo : scanResult.getSubclasses(AbstractModule.class)) &#123;      if (classInfo.getName().equals(AutoModuleScanner.class.getName())) &#123;        continue;      &#125;      modules.add(((Class&lt;AbstractModule&gt;) classInfo.loadClass()).getConstructor().newInstance());    &#125;    injector = Guice.createInjector(modules.toArray(new AbstractModule[0]));  &#125;  public static Injector injector() &#123;    return injector;  &#125;&#125;","categories":["实用类库"]},{"title":"Kryo 序列化","url":"/posts/30029/","content":"官方文档：https://github.com/EsotericSoftware/kryo/blob/master/README.md\n一、Maven 依赖&lt;dependency&gt;    &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt;    &lt;artifactId&gt;kryo&lt;/artifactId&gt;    &lt;version&gt;5.3.0&lt;/version&gt;&lt;/dependency&gt;\n\n二、工具类Kryo 线程不安全。\nimport com.esotericsoftware.kryo.Kryo;import com.esotericsoftware.kryo.io.Input;import com.esotericsoftware.kryo.io.Output;import com.esotericsoftware.kryo.util.DefaultInstantiatorStrategy;import java.io.ByteArrayInputStream;import java.io.ByteArrayOutputStream;import java.util.ArrayList;import java.util.List;import org.objenesis.strategy.StdInstantiatorStrategy;/** * @Auther jd */public class KryoUtils &#123;  private static final ThreadLocal&lt;Kryo&gt; KRYO = new ThreadLocal&lt;&gt;() &#123;    @Override    protected Kryo initialValue() &#123;      /**       * 不要轻易改变这里的配置,更改之后，序列化的格式就会发生变化，       * 上线的同时就必须清除 Redis 里的所有缓存，       * 否则那些缓存再回来反序列化的时候，就会报错       */      Kryo kryo = new Kryo();      kryo.setReferences(true);      kryo.setRegistrationRequired(false);      //Fix the NPE bug when deserializing Collections.      ((DefaultInstantiatorStrategy) kryo.getInstantiatorStrategy())          .setFallbackInstantiatorStrategy(new StdInstantiatorStrategy());      return kryo;    &#125;  &#125;;  public static &lt;T&gt; byte[] serializeObject(T obj) &#123;    if (obj == null) &#123;      return null;    &#125;    ByteArrayOutputStream os = null;    try (Output output = new Output(os = new ByteArrayOutputStream())) &#123;      Kryo kryo = KRYO.get();      kryo.writeObject(output, obj);      return os.toByteArray();    &#125; catch (Exception e) &#123;      e.printStackTrace();      return null;    &#125;  &#125;  public static &lt;T&gt; T deserialize(byte[] bytes, Class&lt;T&gt; clazz) &#123;    if (bytes == null || bytes.length == 0) &#123;      return null;    &#125;    ByteArrayInputStream is = null;    try (Input input = new Input(is = new ByteArrayInputStream(bytes))) &#123;      Kryo kryo = KRYO.get();      return kryo.readObject(input, clazz);    &#125; catch (Exception e) &#123;      e.printStackTrace();      return null;    &#125;  &#125;  public static &lt;T&gt; List&lt;T&gt; deserialize(byte[] bytes) &#123;    if (bytes == null || bytes.length == 0) &#123;      return null;    &#125;    ByteArrayInputStream is = null;    try (Input input = new Input(is = new ByteArrayInputStream(bytes))) &#123;      Kryo kryo = KRYO.get();      return kryo.readObject(input, ArrayList.class);    &#125; catch (Exception e) &#123;      e.printStackTrace();      return null;    &#125;  &#125;&#125;\n\n","categories":["实用类库"]},{"title":"Mapstruct","url":"/posts/3398/","content":"文档：https://mapstruct.org/\n依赖implementation &#x27;org.mapstruct:mapstruct:1.4.2.Final&#x27;implementation &#x27;org.mapstruct:mapstruct-jdk8:1.4.2.Final&#x27;annotationProcessor &#x27;org.mapstruct:mapstruct-processor:1.4.2.Final&#x27;","categories":["实用类库"]},{"title":"Micrometer 实现度量和监控","url":"/posts/11448/","content":"Micrometer提供的度量类库Meter是指一组用于收集应用中的度量数据的接口，Meter单词可以翻译为”米”或者”千分尺”，但是显然听起来都不是很合理，因此下文直接叫Meter，直接当成一个专有名词，理解它为度量接口即可。Meter是由MeterRegistry创建和保存的，可以理解MeterRegistry是Meter的工厂和缓存中心，一般而言每个JVM应用在使用Micrometer的时候必须创建一个MeterRegistry的具体实现。Micrometer中，Meter的具体类型包括：Timer，Counter，Gauge，DistributionSummary，LongTaskTimer，FunctionCounter，FunctionTimer和TimeGauge。下面分节详细介绍这些类型的使用方法和实战使用场景。而一个Meter具体类型需要通过名字和Tag(这里指的是Micrometer提供的Tag接口)作为它的唯一标识，这样做的好处是可以使用名字进行标记，通过不同的Tag去区分多种维度进行数据统计。\nMeterRegistryMeterRegistry在Micrometer是一个抽象类，主要实现包括：\n\n1、SimpleMeterRegistry：每个Meter的最新数据可以收集到SimpleMeterRegistry实例中，但是这些数据不会发布到其他系统，也就是数据是位于应用的内存中的。\n2、CompositeMeterRegistry：多个MeterRegistry聚合，内部维护了一个MeterRegistry的列表。\n3、全局的MeterRegistry：工厂类io.micrometer.core.instrument.Metrics中持有一个静态final的CompositeMeterRegistry实例globalRegistry。\n\n当然，使用者也可以自行继承MeterRegistry去实现自定义的MeterRegistry。SimpleMeterRegistry适合做调试的时候使用，它的简单使用方式如下：\nMeterRegistry registry = new SimpleMeterRegistry();Counter counter = registry.counter(&quot;counter&quot;);counter.increment();\n\nCompositeMeterRegistry实例初始化的时候，内部持有的MeterRegistry列表是空的，如果此时用它新增一个Meter实例，Meter实例的操作是无效的：\nCompositeMeterRegistry composite = new CompositeMeterRegistry();Counter compositeCounter = composite.counter(&quot;counter&quot;);compositeCounter.increment(); // &lt;- 实际上这一步操作是无效的,但是不会报错SimpleMeterRegistry simple = new SimpleMeterRegistry();composite.add(simple);  // &lt;- 向CompositeMeterRegistry实例中添加SimpleMeterRegistry实例compositeCounter.increment();  // &lt;-计数成功\n\n全局的MeterRegistry的使用方式更加简单便捷，因为一切只需要操作工厂类Metrics的静态方法：\nMetrics.addRegistry(new SimpleMeterRegistry());Counter counter = Metrics.counter(&quot;counter&quot;, &quot;tag-1&quot;, &quot;tag-2&quot;);counter.increment();\n\nTag与Meter的命名Micrometer中，Meter的命名约定使用英文逗号(dot，也就是”.”)分隔单词。但是对于不同的监控系统，对命名的规约可能并不相同，如果命名规约不一致，在做监控系统迁移或者切换的时候，可能会对新的系统造成破坏。Micrometer中使用英文逗号分隔单词的命名规则，再通过底层的命名转换接口NamingConvention进行转换，最终可以适配不同的监控系统，同时可以消除监控系统不允许的特殊字符的名称和标记等。开发者也可以覆盖NamingConvention实现自定义的命名转换规则：registry.config().namingConvention(myCustomNamingConvention);。在Micrometer中，对一些主流的监控系统或者存储系统的命名规则提供了默认的转换方式，例如当我们使用下面的命名时候：\nMeterRegistry registry = ...registry.timer(&quot;http.server.requests&quot;);\n\n对于不同的监控系统或者存储系统，命名会自动转换如下：\n\n1、Prometheus - http_server_requests_duration_seconds。\n2、Atlas - httpServerRequests。\n3、Graphite - http.server.requests。\n4、InfluxDB - http_server_requests。\n\n其实NamingConvention已经提供了5种默认的转换规则：dot、snakeCase、camelCase、upperCamelCase和slashes。\n另外，Tag（标签）是Micrometer的一个重要的功能，严格来说，一个度量框架只有实现了标签的功能，才能真正地多维度进行度量数据收集。Tag的命名一般需要是有意义的，所谓有意义就是可以根据Tag的命名可以推断出它指向的数据到底代表什么维度或者什么类型的度量指标。假设我们需要监控数据库的调用和Http请求调用统计，一般推荐的做法是：\nMeterRegistry registry = ...registry.counter(&quot;database.calls&quot;, &quot;db&quot;, &quot;users&quot;)registry.counter(&quot;http.requests&quot;, &quot;uri&quot;, &quot;/api/users&quot;)\n\n这样，当我们选择命名为”database.calls”的计数器，我们可以进一步选择分组”db”或者”users”分别统计不同分组对总调用数的贡献或者组成。一个反例如下：\nMeterRegistry registry = ...registry.counter(&quot;calls&quot;, &quot;class&quot;, &quot;database&quot;, &quot;db&quot;, &quot;users&quot;);registry.counter(&quot;calls&quot;, &quot;class&quot;, &quot;http&quot;, &quot;uri&quot;, &quot;/api/users&quot;);\n\n通过命名”calls”得到的计数器，由于标签混乱，数据是基本无法分组统计分析，这个时候可以认为得到的时间序列的统计数据是没有意义的。可以定义全局的Tag，也就是全局的Tag定义之后，会附加到所有的使用到的Meter上(只要是使用同一个MeterRegistry)，全局的Tag可以这样定义：\nMeterRegistry registry = ...registry.config().commonTags(&quot;stack&quot;, &quot;prod&quot;, &quot;region&quot;, &quot;us-east-1&quot;);// 和上面的意义是一样的registry.config().commonTags(Arrays.asList(Tag.of(&quot;stack&quot;, &quot;prod&quot;), Tag.of(&quot;region&quot;, &quot;us-east-1&quot;))); \n\n像上面这样子使用，就能通过主机，实例，区域，堆栈等操作环境进行多维度深入分析。\n还有两点点需要注意：\n\n1、Tag的值必须不为NULL。\n2、Micrometer中，Tag必须成对出现，也就是Tag必须设置为偶数个，实际上它们以Key=Value的形式存在，具体可以看io.micrometer.core.instrument.Tag接口：\n\npublic interface Tag extends Comparable&lt;Tag&gt; &#123;    String getKey();    String getValue();    static Tag of(String key, String value) &#123;        return new ImmutableTag(key, value);    &#125;    default int compareTo(Tag o) &#123;        return this.getKey().compareTo(o.getKey());    &#125;&#125;\n\n当然，有些时候，我们需要过滤一些必要的标签或者名称进行统计，或者为Meter的名称添加白名单，这个时候可以使用MeterFilter。MeterFilter本身提供一些列的静态方法，多个MeterFilter可以叠加或者组成链实现用户最终的过滤策略。例如：\nMeterRegistry registry = ...registry.config()    .meterFilter(MeterFilter.ignoreTags(&quot;http&quot;))    .meterFilter(MeterFilter.denyNameStartsWith(&quot;jvm&quot;));\n\n表示忽略”http”标签，拒绝名称以”jvm”字符串开头的Meter。更多用法可以参详一下MeterFilter这个类。\nMeter的命名和Meter的Tag相互结合，以命名为轴心，以Tag为多维度要素，可以使度量数据的维度更加丰富，便于统计和分析。\nMeters前面提到Meter主要包括：Timer，Counter，Gauge，DistributionSummary，LongTaskTimer，FunctionCounter，FunctionTimer和TimeGauge。下面逐一分析它们的作用和个人理解的实际使用场景（应该说是生产环境）。\nCounterCounter是一种比较简单的Meter，它是一种单值的度量类型，或者说是一个单值计数器。Counter接口允许使用者使用一个固定值（必须为正数）进行计数。准确来说：Counter就是一个增量为正数的单值计数器。这个举个很简单的使用例子：\nMeterRegistry meterRegistry = new SimpleMeterRegistry();Counter counter = meterRegistry.counter(&quot;http.request&quot;, &quot;createOrder&quot;, &quot;/order/create&quot;);counter.increment();System.out.println(counter.measure()); // [Measurement&#123;statistic=&#x27;COUNT&#x27;, value=1.0&#125;]\n\n使用场景：\nCounter的作用是记录XXX的总量或者计数值，适用于一些增长类型的统计，例如下单、支付次数、HTTP请求总量记录等等，通过Tag可以区分不同的场景，对于下单，可以使用不同的Tag标记不同的业务来源或者是按日期划分，对于HTTP请求总量记录，可以使用Tag区分不同的URL。用下单业务举个例子：\n//实体@Datapublic class Order &#123;    private String orderId;    private Integer amount;    private String channel;    private LocalDateTime createTime;&#125;public class CounterMain &#123;    private static final DateTimeFormatter FORMATTER = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd&quot;);    static &#123;        Metrics.addRegistry(new SimpleMeterRegistry());    &#125;    public static void main(String[] args) throws Exception &#123;        Order order1 = new Order();        order1.setOrderId(&quot;ORDER_ID_1&quot;);        order1.setAmount(100);        order1.setChannel(&quot;CHANNEL_A&quot;);        order1.setCreateTime(LocalDateTime.now());        createOrder(order1);        Order order2 = new Order();        order2.setOrderId(&quot;ORDER_ID_2&quot;);        order2.setAmount(200);        order2.setChannel(&quot;CHANNEL_B&quot;);        order2.setCreateTime(LocalDateTime.now());        createOrder(order2);        Search.in(Metrics.globalRegistry).meters().forEach(each -&gt; &#123;            StringBuilder builder = new StringBuilder();            builder.append(&quot;name:&quot;)                    .append(each.getId().getName())                    .append(&quot;,tags:&quot;)                    .append(each.getId().getTags())                    .append(&quot;,type:&quot;).append(each.getId().getType())                    .append(&quot;,value:&quot;).append(each.measure());            System.out.println(builder.toString());        &#125;);    &#125;    private static void createOrder(Order order) &#123;        //忽略订单入库等操作        Metrics.counter(&quot;order.create&quot;,                &quot;channel&quot;, order.getChannel(),                &quot;createTime&quot;, FORMATTER.format(order.getCreateTime())).increment();    &#125;&#125;\n\n控制台输出：\nname:order.create,tags:[tag(channel=CHANNEL_A), tag(createTime=2018-11-10)],type:COUNTER,value:[Measurement&#123;statistic=&#x27;COUNT&#x27;, value=1.0&#125;]name:order.create,tags:[tag(channel=CHANNEL_B), tag(createTime=2018-11-10)],type:COUNTER,value:[Measurement&#123;statistic=&#x27;COUNT&#x27;, value=1.0&#125;]\n\n上面的例子是使用全局静态方法工厂类Metrics去构造Counter实例，实际上，io.micrometer.core.instrument.Counter接口提供了一个内部建造器类Counter.Builder去实例化Counter，Counter.Builder的使用方式如下：\npublic class CounterBuilderMain &#123;\t\tpublic static void main(String[] args) throws Exception&#123;\t\tCounter counter = Counter.builder(&quot;name&quot;)  //名称\t\t\t\t.baseUnit(&quot;unit&quot;) //基础单位\t\t\t\t.description(&quot;desc&quot;) //描述\t\t\t\t.tag(&quot;tagKey&quot;, &quot;tagValue&quot;)  //标签\t\t\t\t.register(new SimpleMeterRegistry());//绑定的MeterRegistry\t\tcounter.increment();\t&#125;&#125;\n\nFunctionCounterFunctionCounter是Counter的特化类型，它把计数器数值增加的动作抽象成接口类型ToDoubleFunction，这个接口JDK1.8中对于Function的特化类型接口。FunctionCounter的使用场景和Counter是一致的，这里介绍一下它的用法：\npublic class FunctionCounterMain &#123;\tpublic static void main(String[] args) throws Exception &#123;\t\tMeterRegistry registry = new SimpleMeterRegistry();\t\tAtomicInteger n = new AtomicInteger(0);\t\t//这里ToDoubleFunction匿名实现其实可以使用Lambda表达式简化为AtomicInteger::get\t\tFunctionCounter.builder(&quot;functionCounter&quot;, n, new ToDoubleFunction&lt;AtomicInteger&gt;() &#123;\t\t\t@Override\t\t\tpublic double applyAsDouble(AtomicInteger value) &#123;\t\t\t\treturn value.get();\t\t\t&#125;\t\t&#125;).baseUnit(&quot;function&quot;)\t\t\t\t.description(&quot;functionCounter&quot;)\t\t\t\t.tag(&quot;createOrder&quot;, &quot;CHANNEL-A&quot;)\t\t\t\t.register(registry);\t\t//下面模拟三次计数\t\t\t\tn.incrementAndGet();\t\tn.incrementAndGet();\t\tn.incrementAndGet();\t&#125;&#125;\n\nFunctionCounter使用的一个明显的好处是，我们不需要感知FunctionCounter实例的存在，实际上我们只需要操作作为FunctionCounter实例构建元素之一的AtomicInteger实例即可，这种接口的设计方式在很多主流框架里面可以看到。\nTimerTimer（计时器）适用于记录耗时比较短的事件的执行时间，通过时间分布展示事件的序列和发生频率。所有的Timer的实现至少记录了发生的事件的数量和这些事件的总耗时，从而生成一个时间序列。Timer的基本单位基于服务端的指标而定，但是实际上我们不需要过于关注Timer的基本单位，因为Micrometer在存储生成的时间序列的时候会自动选择适当的基本单位。Timer接口提供的常用方法如下：\npublic interface Timer extends Meter &#123;    ...    void record(long var1, TimeUnit var3);    default void record(Duration duration) &#123;        this.record(duration.toNanos(), TimeUnit.NANOSECONDS);    &#125;    &lt;T&gt; T record(Supplier&lt;T&gt; var1);    &lt;T&gt; T recordCallable(Callable&lt;T&gt; var1) throws Exception;    void record(Runnable var1);    default Runnable wrap(Runnable f) &#123;        return () -&gt; &#123;            this.record(f);        &#125;;    &#125;    default &lt;T&gt; Callable&lt;T&gt; wrap(Callable&lt;T&gt; f) &#123;        return () -&gt; &#123;            return this.recordCallable(f);        &#125;;    &#125;    long count();    double totalTime(TimeUnit var1);    default double mean(TimeUnit unit) &#123;        return this.count() == 0L ? 0.0D : this.totalTime(unit) / (double)this.count();    &#125;    double max(TimeUnit var1);\t...&#125;\n\n实际上，比较常用和方便的方法是几个函数式接口入参的方法：\nTimer timer = ...timer.record(() -&gt; dontCareAboutReturnValue());timer.recordCallable(() -&gt; returnValue());Runnable r = timer.wrap(() -&gt; dontCareAboutReturnValue());Callable c = timer.wrap(() -&gt; returnValue());\n\n使用场景：\n根据个人经验和实践，总结如下：\n\n1、记录指定方法的执行时间用于展示。\n2、记录一些任务的执行时间，从而确定某些数据来源的速率，例如消息队列消息的消费速率等。\n\n这里举个实际的例子，要对系统做一个功能，记录指定方法的执行时间，还是用下单方法做例子：\npublic class TimerMain &#123;\tprivate static final Random R = new Random();\tstatic &#123;\t\tMetrics.addRegistry(new SimpleMeterRegistry());\t&#125;\tpublic static void main(String[] args) throws Exception &#123;\t\tOrder order1 = new Order();\t\torder1.setOrderId(&quot;ORDER_ID_1&quot;);\t\torder1.setAmount(100);\t\torder1.setChannel(&quot;CHANNEL_A&quot;);\t\torder1.setCreateTime(LocalDateTime.now());\t\tTimer timer = Metrics.timer(&quot;timer&quot;, &quot;createOrder&quot;, &quot;cost&quot;);\t\ttimer.record(() -&gt; createOrder(order1));\t&#125;\tprivate static void createOrder(Order order) &#123;\t\ttry &#123;\t\t\tTimeUnit.SECONDS.sleep(R.nextInt(5)); //模拟方法耗时\t\t&#125; catch (InterruptedException e) &#123;\t\t\t//no-op\t\t&#125;\t&#125;&#125;\n\n在实际生产环境中，可以通过spring-aop把记录方法耗时的逻辑抽象到一个切面中，这样就能减少不必要的冗余的模板代码。上面的例子是通过Mertics构造Timer实例，实际上也可以使用Builder构造：\nMeterRegistry registry = ...Timer timer = Timer    .builder(&quot;my.timer&quot;)    .description(&quot;a description of what this timer does&quot;) // 可选    .tags(&quot;region&quot;, &quot;test&quot;) // 可选    .register(registry);\n\n另外，Timer的使用还可以基于它的内部类Timer.Sample，通过start和stop两个方法记录两者之间的逻辑的执行耗时。例如：\nTimer.Sample sample = Timer.start(registry);// 这里做业务逻辑Response response = ...sample.stop(registry.timer(&quot;my.timer&quot;, &quot;response&quot;, response.status()));\n\nFunctionTimerFunctionTimer是Timer的特化类型，它主要提供两个单调递增的函数（其实并不是单调递增，只是在使用中一般需要随着时间最少保持不变或者说不减少）：一个用于计数的函数和一个用于记录总调用耗时的函数，它的建造器的入参如下：\npublic interface FunctionTimer extends Meter &#123;    static &lt;T&gt; Builder&lt;T&gt; builder(String name, T obj, ToLongFunction&lt;T&gt; countFunction,                                  ToDoubleFunction&lt;T&gt; totalTimeFunction,                                  TimeUnit totalTimeFunctionUnit) &#123;        return new Builder&lt;&gt;(name, obj, countFunction, totalTimeFunction, totalTimeFunctionUnit);    &#125;\t...&#125;\t\n\n官方文档中的例子如下：\nIMap&lt;?, ?&gt; cache = ...; // 假设使用了Hazelcast缓存registry.more().timer(&quot;cache.gets.latency&quot;, Tags.of(&quot;name&quot;, cache.getName()), cache,    c -&gt; c.getLocalMapStats().getGetOperationCount(),  //实际上就是cache的一个方法，记录缓存生命周期初始化的增量(个数)    c -&gt; c.getLocalMapStats().getTotalGetLatency(),  // Get操作的延迟时间总量，可以理解为耗时    TimeUnit.NANOSECONDS);\n\n按照个人理解，ToDoubleFunction用于统计事件个数，ToDoubleFunction用于记录执行总时间，实际上两个函数都只是Function函数的变体，还有一个比较重要的是总时间的单位totalTimeFunctionUnit。简单的使用方式如下：\npublic class FunctionTimerMain &#123;\tpublic static void main(String[] args) throws Exception &#123;\t\t//这个是为了满足参数,暂时不需要理会\t\tObject holder = new Object();\t\tAtomicLong totalTimeNanos = new AtomicLong(0);\t\tAtomicLong totalCount = new AtomicLong(0);\t\tFunctionTimer.builder(&quot;functionTimer&quot;, holder, p -&gt; totalCount.get(), \t\t\t\tp -&gt; totalTimeNanos.get(), TimeUnit.NANOSECONDS)\t\t\t\t.register(new SimpleMeterRegistry());\t\ttotalTimeNanos.addAndGet(10000000);\t\ttotalCount.incrementAndGet();\t&#125;&#125;\n\nLongTaskTimerLongTaskTimer是Timer的特化类型，主要用于记录长时间执行的任务的持续时间，在任务完成之前，被监测的事件或者任务仍然处于运行状态，任务完成的时候，任务执行的总耗时才会被记录下来。LongTaskTimer适合用于长时间持续运行的事件耗时的记录，例如相对耗时的定时任务。在Spring(Boot)应用中，可以简单地使用@Scheduled和@Timed注解，基于spring-aop完成定时调度任务的总耗时记录：\n@Timed(value = &quot;aws.scrape&quot;, longTask = true)@Scheduled(fixedDelay = 360000)void scrapeResources() &#123;    //这里做相对耗时的业务逻辑&#125;\n\n当然，在非Spring体系中也能方便地使用LongTaskTimer：\npublic class LongTaskTimerMain &#123;    public static void main(String[] args) throws Exception&#123;        MeterRegistry meterRegistry = new SimpleMeterRegistry();        LongTaskTimer longTaskTimer = meterRegistry.more().longTaskTimer(&quot;longTaskTimer&quot;);        longTaskTimer.record(() -&gt; &#123;             //这里编写Task的逻辑        &#125;);             //或者这样        Metrics.more().longTaskTimer(&quot;longTaskTimer&quot;).record(()-&gt; &#123;             //这里编写Task的逻辑        &#125;);    &#125;&#125;\n\nGaugeGauge（仪表）是获取当前度量记录值的句柄，也就是它表示一个可以任意上下浮动的单数值度量Meter。Gauge通常用于变动的测量值，测量值用ToDoubleFunction参数的返回值设置，如当前的内存使用情况，同时也可以测量上下移动的”计数”，比如队列中的消息数量。官网文档中提到Gauge的典型使用场景是用于测量集合或映射的大小或运行状态中的线程数。一般情况下，Gauge适合用于监测有自然上界的事件或者任务，而Counter一般使用于无自然上界的事件或者任务的监测，所以像HTTP请求总量计数应该使用Counter而非Gauge。MeterRegistry中提供了一些便于构建用于观察数值、函数、集合和映射的Gauge相关的方法：\nList&lt;String&gt; list = registry.gauge(&quot;listGauge&quot;, Collections.emptyList(), new ArrayList&lt;&gt;(), List::size); List&lt;String&gt; list2 = registry.gaugeCollectionSize(&quot;listSize2&quot;, Tags.empty(), new ArrayList&lt;&gt;()); Map&lt;String, Integer&gt; map = registry.gaugeMapSize(&quot;mapGauge&quot;, Tags.empty(), new HashMap&lt;&gt;());\n\n上面的三个方法通过MeterRegistry构建Gauge并且返回了集合或者映射实例，使用这些集合或者映射实例就能在其size变化过程中记录这个变更值。更重要的优点是，我们不需要感知Gauge接口的存在，只需要像平时一样使用集合或者映射实例就可以了。此外，Gauge还支持java.lang.Number的子类，java.util.concurrent.atomic包中的AtomicInteger和AtomicLong，还有Guava提供的AtomicDouble：\nAtomicInteger n = registry.gauge(&quot;numberGauge&quot;, new AtomicInteger(0));n.set(1);n.set(2);\n\n除了使用MeterRegistry创建Gauge之外，还可以使用建造器流式创建：\n//一般我们不需要操作Gauge实例Gauge gauge = Gauge    .builder(&quot;gauge&quot;, myObj, myObj::gaugeValue)    .description(&quot;a description of what this gauge does&quot;) // 可选    .tags(&quot;region&quot;, &quot;test&quot;) // 可选    .register(registry);\n\n使用场景：\n根据个人经验和实践，总结如下：\n\n1、有自然(物理)上界的浮动值的监测，例如物理内存、集合、映射、数值等。\n2、有逻辑上界的浮动值的监测，例如积压的消息、（线程池中）积压的任务等，其实本质也是集合或者映射的监测。\n\n举个相对实际的例子，假设我们需要对登录后的用户发送一条短信或者推送，做法是消息先投放到一个阻塞队列，再由一个线程消费消息进行其他操作：\npublic class GaugeMain &#123;    private static final MeterRegistry MR = new SimpleMeterRegistry();    private static final BlockingQueue&lt;Message&gt; QUEUE = new ArrayBlockingQueue&lt;&gt;(500);    private static BlockingQueue&lt;Message&gt; REAL_QUEUE;    static &#123;        REAL_QUEUE = MR.gauge(&quot;messageGauge&quot;, QUEUE, Collection::size);    &#125;    public static void main(String[] args) throws Exception &#123;        consume();        Message message = new Message();        message.setUserId(1L);        message.setContent(&quot;content&quot;);        REAL_QUEUE.put(message);    &#125;    private static void consume() throws Exception &#123;        new Thread(() -&gt; &#123;            while (true) &#123;                try &#123;                    Message message = REAL_QUEUE.take();                    //handle message                    System.out.println(message);                &#125; catch (InterruptedException e) &#123;                    //no-op                &#125;            &#125;        &#125;).start();    &#125;&#125;\n\n上面的例子代码写得比较糟糕，只为了演示相关使用方式，切勿用于生产环境。\nTimeGaugeTimeGauge是Gauge的特化类型，相比Gauge，它的构建器中多了一个TimeUnit类型的参数，用于指定ToDoubleFunction入参的基础时间单位。这里简单举个使用例子：\npublic class TimeGaugeMain &#123;    private static final SimpleMeterRegistry R = new SimpleMeterRegistry();    public static void main(String[] args) throws Exception &#123;        AtomicInteger count = new AtomicInteger();        TimeGauge.Builder&lt;AtomicInteger&gt; timeGauge = TimeGauge.builder(&quot;timeGauge&quot;, count,                TimeUnit.SECONDS, AtomicInteger::get);        timeGauge.register(R);        count.addAndGet(10086);        print();        count.set(1);        print();    &#125;    private static void print() throws Exception &#123;        Search.in(R).meters().forEach(each -&gt; &#123;            StringBuilder builder = new StringBuilder();            builder.append(&quot;name:&quot;)                    .append(each.getId().getName())                    .append(&quot;,tags:&quot;)                    .append(each.getId().getTags())                    .append(&quot;,type:&quot;).append(each.getId().getType())                    .append(&quot;,value:&quot;).append(each.measure());            System.out.println(builder.toString());        &#125;);    &#125;&#125;//输出name:timeGauge,tags:[],type:GAUGE,value:[Measurement&#123;statistic=&#x27;VALUE&#x27;, value=10086.0&#125;]name:timeGauge,tags:[],type:GAUGE,value:[Measurement&#123;statistic=&#x27;VALUE&#x27;, value=1.0&#125;]\n\nDistributionSummarySummary（摘要）主要用于跟踪事件的分布，在Micrometer中，对应的类是DistributionSummary（分布式摘要）。它的使用方式和Timer十分相似，但是它的记录值并不依赖于时间单位。常见的使用场景：使用DistributionSummary测量命中服务器的请求的有效负载大小。使用MeterRegistry创建DistributionSummary实例如下：\nDistributionSummary summary = registry.summary(&quot;response.size&quot;);\n\n通过建造器流式创建如下：\nDistributionSummary summary = DistributionSummary    .builder(&quot;response.size&quot;)    .description(&quot;a description of what this summary does&quot;) // 可选    .baseUnit(&quot;bytes&quot;) // 可选    .tags(&quot;region&quot;, &quot;test&quot;) // 可选    .scale(100) // 可选    .register(registry);\n\n使用场景：\n根据个人经验和实践，总结如下：\n\n1、不依赖于时间单位的记录值的测量，例如服务器有效负载值，缓存的命中率等。\n\n举个相对具体的例子：\npublic class DistributionSummaryMain &#123;    private static final DistributionSummary DS = DistributionSummary.builder(&quot;cacheHitPercent&quot;)            .register(new SimpleMeterRegistry());    private static final LoadingCache&lt;String, String&gt; CACHE = CacheBuilder.newBuilder()            .maximumSize(1000)            .recordStats()            .expireAfterWrite(60, TimeUnit.SECONDS)            .build(new CacheLoader&lt;String, String&gt;() &#123;                @Override                public String load(String s) throws Exception &#123;                     return selectFromDatabase();                &#125;            &#125;);    public static void main(String[] args) throws Exception &#123;        String key = &quot;doge&quot;;        String value = CACHE.get(key);        record();    &#125;    private static void record() throws Exception &#123;        CacheStats stats = CACHE.stats();        BigDecimal hitCount = new BigDecimal(stats.hitCount());        BigDecimal requestCount = new BigDecimal(stats.requestCount());        DS.record(hitCount.divide(requestCount, 2, BigDecimal.ROUND_HALF_DOWN).doubleValue());    &#125;&#125;","categories":["实用类库"],"tags":["监控"]},{"title":"Resilience4j","url":"/posts/20321/","content":"Resilience4j 官方文档\n一、依赖implementation(&#x27;org.springframework.boot:spring-boot-starter-aop&#x27;)implementation(&#x27;org.springframework.boot:spring-boot-starter-actuator&#x27;)implementation(&quot;io.github.resilience4j:resilience4j-spring-boot2:$&#123;resilience4jVersion&#125;&quot;)\n\n二、CircuitBreakerCircuitBreaker 是通过三个正常状态（CLOSED、OPEN 和 HALF_OPEN）以及两个特殊状态（DISABLED 和 FORCED_OPEN） 的有限状态机实现的。\n\nCircuitBreaker 使用滑动窗口来存储和汇总调用结果。\n支持基于计数的滑动窗口和基于时间的滑动窗口。基于计数的滑动窗口聚合最后 N 次调用的结果。基于时间的滑动窗口聚合了最后 N 秒的调用结果。\n失败率和慢调用率阈值当失败率大于或等于配置的阈值，CircuitBreaker 会从 CLOSED 状态变为 OPEN 状态。\n默认所有异常都被视为失败。\n可以定义应被视为失败的异常列表，然后其他所有的异常都被视为成功，除非它们被忽略。异常也可以被忽略，这样它们既不能算作失败也不能算成功。\n当慢速调用的百分比等于或大于可配置的阈值时，CircuitBreaker 也会从 CLOSED 变为 OPEN。\n只有记录了最少的调用次数，才能计算失败率和慢速调用率。例如，如果需要调用的最小数量为 10，则必须至少记录 10 个调用，然后才能计算失败率。\n在 OPEN 状态下，CircuitBreaker 会拒绝调用并抛出 CallNotPermittedException 异常。\n在等待时间一段时间后，CircuitBreaker 状态从 OPEN 变为 HALF _ OPEN，并允许可配置的调用数量，以查看后端是否仍然不可用或已重新可用。\n在所有允许的调用完成之前，进一步的调用将被 CallNotPermittedException 拒绝。\n如果失败率或慢调用率随后大于或等于配置的阈值，则状态将更改为 OPEN。如果失败率或慢调用率低于阈值，则状态将更改为 CLOSED 。\nCircuitBreaker 支持另外两个特殊的状态，DISABLED (总是允许访问)和 FORCED_OPEN (总是拒绝访问)。在这两种状态下，不会产生任何断路器事件(除了状态转换) ，也不会记录任何指标。退出这些状态的唯一方法是触发状态转换或重置断路器。\n配置\n\n\nConfig property\nDefault Value\nDescription\n\n\n\nfailureRateThreshold\n50\n以百分比为单位配置故障率阈值。当故障率等于或大于阈值时，CircuitBreaker 转换为打开并开始短路调用。\n\n\nslowCallRateThreshold\n100\n以百分比为单位配置阈值。当调用持续时间大于 slowCallDurationThreshold 时，CircuitBreaker 认为调用是慢的，当慢调用的百分比等于或大于阈值时，CircuitBreaker 转换为打开并开始短路调用。\n\n\nslowCallDurationThreshold\n60000 [ms]\n持续时间高于该值的调用被视为慢调用，并提高慢调用率，即slowCallRateThreshold的值。\n\n\npermittedNumberOfCalls InHalfOpenState\n10\n断路器半开时允许的调用数。\n\n\nmaxWaitDurationInHalfOpenState\n0 [ms]\n该等待时间控制 CircuitBreaker 在切换到 OPEN 状态之前可以保持 HALF_OPEN 状态的最长时间。值0意味着断路器将在 HALF_OPEN 状态下无限等待，直到所有允许的调用完成。\n\n\nslidingWindowType\nCOUNT_BASED\n滑动窗口类型，可以是基于计数的或基于时间的。  如果滑动窗口是 COUNT_BASED，则记录并聚合最后的 slidingWindowSize 次调用。如果滑动窗口是 time_based，则记录并聚合最后一次 slidingWindowSize 秒的调用。\n\n\nslidingWindowSize\n100\n配置滑动窗口的大小，该窗口用于在关闭 CircuitBreaker 时记录调用的结果。\n\n\nminimumNumberOfCalls\n100\nConfigures the minimum number of calls which are required (per sliding window period) before the CircuitBreaker can calculate the error rate or slow call rate. For example, if minimumNumberOfCalls is 10, then at least 10 calls must be recorded, before the failure rate can be calculated. If only 9 calls have been recorded the CircuitBreaker will not transition to open even if all 9 calls have failed.\n\n\nwaitDurationInOpenState\n60000 [ms]\nThe time that the CircuitBreaker should wait before transitioning from open to half-open.\n\n\nautomaticTransition FromOpenToHalfOpenEnabled\nfalse\nIf set to true it means that the CircuitBreaker will automatically transition from open to half-open state and no call is needed to trigger the transition. A thread is created to monitor all the instances of CircuitBreakers to transition them to HALF_OPEN once waitDurationInOpenState passes. Whereas, if set to false the transition to HALF_OPEN only happens if a call is made, even after waitDurationInOpenState is passed. The advantage here is no thread monitors the state of all CircuitBreakers.\n\n\nrecordExceptions\nempty\nA list of exceptions that are recorded as a failure and thus increase the failure rate. Any exception matching or inheriting from one of the list counts as a failure, unless explicitly ignored via ignoreExceptions. If you specify a list of exceptions, all other exceptions count as a success, unless they are explicitly ignored by ignoreExceptions.\n\n\nignoreExceptions\nempty\nA list of exceptions that are ignored and neither count as a failure nor success. Any exception matching or inheriting from one of the list will not count as a failure nor success, even if the exceptions is part of recordExceptions.\n\n\nrecordFailurePredicate\nthrowable -&gt; true  By default all exceptions are recored as failures.\nA custom Predicate which evaluates if an exception should be recorded as a failure. The Predicate must return true if the exception should count as a failure. The Predicate must return false, if the exception should count as a success, unless the exception is explicitly ignored by ignoreExceptions.\n\n\nignoreExceptionPredicate\nthrowable -&gt; false  By default no exception is ignored.\nA custom Predicate which evaluates if an exception should be ignored and neither count as a failure nor success. The Predicate must return true if the exception should be ignored. The Predicate must return false, if the exception should count as a failure.\n\n\n三、BulkheadResilience4j 提供了信号量隔离和线程池隔离，两种资源隔离方式。\n信号量隔离配置\n\n\nConfig property\nDefault value\nDescription\n\n\n\nmaxConcurrentCalls\n25\n最大并行数量\n\n\nmaxWaitDuration\n0\n当达到最大并行数量后，线程最大等待时间\n\n\n线程池隔离配置\n\n\nConfig property\nDefault value\nDescription\n\n\n\nmaxThreadPoolSize\nRuntime.getRuntime() .availableProcessors()\n线程池最大线程数\n\n\ncoreThreadPoolSize\nRuntime.getRuntime() .availableProcessors() - 1\n核心线程数\n\n\nqueueCapacity\n100\n阻塞队列容量\n\n\nkeepAliveDuration\n20 [ms]\n线程最大空闲时间\n\n\n四、RateLimiter配置\n\n\nConfig property\nDefault value\nDescription\n\n\n\ntimeoutDuration\n5 [s]\n线程等待许可的时间\n\n\nlimitRefreshPeriod\n500 [ns]\n许可刷新的时间周期。在每个周期之后，速率限制器将其权限计数设置为 limitForPeriod 值\n\n\nlimitForPeriod\n50\n每个周期内可用的许可数量\n\n\n","categories":["实用类库"],"tags":["限流"]},{"title":"RoaringBitmap","url":"/posts/55464/","content":"一、依赖&lt;dependency&gt;    &lt;groupId&gt;org.roaringbitmap&lt;/groupId&gt;    &lt;artifactId&gt;RoaringBitmap&lt;/artifactId&gt;    &lt;version&gt;0.9.30&lt;/version&gt;&lt;/dependency&gt;\n\n二、官方文档源码及文档\n","categories":["实用类库"]},{"title":"logback-spring.xml详解","url":"/posts/9760/","content":"在讲解 logback-spring.xml 之前我们先来了解三个单词：Logger, Appenders and Layouts（记录器、附加器、布局）\nLogback基于三个主要类：Logger、Appender 和 Layout。这三种类型的组件协同工作，使开发人员能够根据消息类型和级别记录消息，并在运行时控制这些消息的格式以及报告的位置。首先给出一个基本的xml配置如下：\n&lt;configuration&gt;   &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;    &lt;!-- encoders are assigned the type         ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --&gt;    &lt;encoder&gt;      &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt;    &lt;/encoder&gt;  &lt;/appender&gt;   &lt;logger name=&quot;chapters.configuration&quot; level=&quot;INFO&quot;/&gt;   &lt;!-- Strictly speaking, the level attribute is not necessary since --&gt;  &lt;!-- the level of the root level is set to DEBUG by default. --&gt;  &lt;root level=&quot;DEBUG&quot;&gt;              &lt;appender-ref ref=&quot;STDOUT&quot; /&gt;  &lt;/root&gt;    &lt;/configuration&gt;\n\n\n\n一、&lt;configuration&gt;元素logback.xml配置文件的基本结构可以描述为&lt;configuration&gt;元素，包含零个或多个&lt;appender&gt;元素，后跟零个或多个&lt;logger&gt;元素，后跟最多一个&lt;root&gt;元素(也可以没有)。下图说明了这种基本结构：\n\n二、&lt;logger&gt;元素&lt;logger&gt; 元素只接受一个必需的 name 属性，一个可选的 level 属性和一个可选的 additivity 属性，允许值为true或false。level属性的值允许一个不区分大小写的字符串值TRACE，DEBUG，INFO，WARN，ERROR，ALL或OFF。特殊于大小写不敏感的值INHERITED或其同义词NULL将强制记录器的级别从层次结构中的较高级别继承，&lt;logger&gt; 元素可以包含零个或多个 &lt;appender-ref&gt; 元素; 这样引用的每个 appender 都被添加到指定的logger中，(注：additivity属性下面详说)，logger元素级别具有继承性。\n例1：示例中，仅为根记录器分配了级别。此级别值DEBUG由其他记录器X，X.Y和X.Y.Z继承\n\n\n\nLogger name\nAssigned level\nEffective level\n\n\n\nroot\nDEBUG\nDEBUG\n\n\nX\nnone\nDEBUG\n\n\nX.Y\nnone\nDEBUG\n\n\nX.Y.Z\nnone\nDEBUG\n\n\n例2：所有记录器都有一个指定的级别值。级别继承不起作用\n\n\n\nLogger name\nAssigned level\nEffective level\n\n\n\nroot\nERROR\nERROR\n\n\nX\nINFO\nINFO\n\n\nX.Y\nDEBUG\nDEBUG\n\n\nX.Y.Z\nWARN\nWARN\n\n\n例3：记录器root，X和X.Y.Z分别被分配了DEBUG，INFO和ERROR级别。Logger X.Y从其父X继承其级别值。\n\n\n\nLogger name\nAssigned level\nEffective level\n\n\n\nroot\nDEBUG\nDEBUG\n\n\nX\nINFO\nINFO\n\n\nX.Y\nnone\nINFO\n\n\nX.Y.Z\nERROR\nERROR\n\n\n例4：在示例4中，记录器root和X分别被分配了DEBUG和INFO级别。记录器X.Y和X.Y.Z从其最近的父X继承其级别值，该父级具有指定的级别。\n\n\n\nLogger name\nAssigned level\nEffective level\n\n\n\nroot\nDEBUG\nDEBUG\n\n\nX\nINFO\nINFO\n\n\nX.Y\nnone\nINFO\n\n\nX.Y.Z\nnone\nINFO\n\n\n三、&lt;root&gt;元素&lt;root&gt; 元素配置根记录器。它支持单个属性，即level属性。它不允许任何其他属性，因为additivity标志不适用于根记录器。此外，由于根记录器已被命名为“ROOT”，因此它也不允许使用name属性。level属性的值可以是不区分大小写的字符串TRACE，DEBUG，INFO，WARN，ERROR，ALL或OFF之一&lt;root&gt;元素可以包含零个或多个&lt;appender-ref&gt;元素; 这样引用的每个appender都被添加到根记录器中(注：additivity属性下面详说)。\n四、&lt;appender&gt;元素appender使用&lt;appender&gt;元素配置，该元素采用两个必需属性name和class。name属性指定appender的名称，而class属性指定要实例化的appender类的完全限定名称。&lt;appender&gt;元素可以包含零个或一个&lt;layout&gt;元素，零个或多个&lt;encoder&gt;元素以及零个或多个&lt;filter&gt;元素，下图说明了常见的结构：\n\n重要：在logback中，输出目标称为appender，addAppender方法将appender添加到给定的记录器logger。给定记录器的每个启用的日志记录请求都将转发到该记录器中的所有appender以及层次结构中较高的appender。换句话说，appender是从记录器层次结构中附加地继承的。例如，如果将控制台appender添加到根记录器，则所有启用的日志记录请求将至少在控制台上打印。如果另外将文件追加器添加到记录器（例如L），则对L和L的子项启用的记录请求将打印在文件和控制台上。通过将记录器的additivity标志设置为false，可以覆盖此默认行为，以便不再添加appender累积。\nAppender是一个接口，它有许多子接口和实现类，具体如下图所示：\n\n其中最重要的两个Appender为：ConsoleAppender 、RollingFileAppender。\n1.ConsoleAppenderConsoleAppender，如名称所示，将日志输出到控制台上。\n2.RollingFileAppenderRollingFileAppender，是FileAppender的一个子类，扩展了FileAppender，具有翻转日志文件的功能。例如，RollingFileAppender 可以记录到名为log.txt文件的文件，并且一旦满足某个条件，就将其日志记录目标更改为另一个文件。\n有两个与RollingFileAppender交互的重要子组件。第一个RollingFileAppender子组件，即 RollingPolicy 负责执行翻转所需的操作。RollingFileAppender的第二个子组件，即 TriggeringPolicy 将确定是否以及何时发生翻转。因此，RollingPolicy 负责什么和TriggeringPolicy 负责什么时候。\n作为任何用途，RollingFileAppender 必须同时设置 RollingPolicy 和 TriggeringPolicy。但是，如果其 RollingPolicy 也实现了TriggeringPolicy 接口，则只需要显式指定前者。\n3.滚动策略TimeBasedRollingPolicy：可能是最受欢迎的滚动策略。它根据时间定义翻转策略，例如按天或按月。TimeBasedRollingPolicy承担滚动和触发所述翻转的责任。实际上，TimeBasedTriggeringPolicy实现了RollingPolicy和TriggeringPolicy接口。\nSizeAndTimeBasedRollingPolicy：有时您可能希望按日期归档文件，但同时限制每个日志文件的大小，特别是如果后处理工具对日志文件施加大小限制。为了满足此要求，logback 提供了 SizeAndTimeBasedRollingPolicy ，它是TimeBasedRollingPolicy的一个子类，实现了基于时间和日志文件大小的翻滚策略。\n4.&lt;encoder&gt;元素 encoder中最重要就是pattern属性，它负责控制输出日志的格式，这里给出一个我自己写的示例：\n&lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %highlight(%-5level) --- [%15.15(%thread)] %cyan(%-40.40(%logger&#123;40&#125;)) : %msg%n&lt;/pattern&gt;\n\n\n\n使用后的输出格式如下图所示\n\n其中：\n\n%d{yyyy-MM-dd HH:mm:ss.SSS}：日期\n%-5level：日志级别\n%highlight()：颜色，info为蓝色，warn为浅红，error为加粗红，debug为黑色\n%thread：打印日志的线程\n%15.15():如果记录的线程字符长度小于15(第一个)则用空格在左侧补齐,如果字符长度大于15(第二个),则从开头开始截断多余的字符 \n%logger：日志输出的类名\n%-40.40()：如果记录的logger字符长度小于40(第一个)则用空格在右侧补齐,如果字符长度大于40(第二个),则从开头开始截断多余的字符\n%cyan：颜色\n%msg：日志输出内容\n%n：换行符\n\n5.&lt;filter&gt;元素filter中最重要的两个过滤器为：LevelFilter、ThresholdFilter。\nLevelFilter 根据精确的级别匹配过滤事件。如果事件的级别等于配置的级别，则筛选器接受或拒绝该事件，具体取决于onMatch和onMismatch属性的配置。例如下面配置将只打印INFO级别的日志，其余的全部禁止打印输出：\n&lt;configuration&gt;  &lt;appender name=&quot;CONSOLE&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;    &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;      &lt;level&gt;INFO&lt;/level&gt;      &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;      &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;    &lt;/filter&gt;    &lt;encoder&gt;      &lt;pattern&gt;        %-4relative [%thread] %-5level %logger&#123;30&#125; - %msg%n      &lt;/pattern&gt;    &lt;/encoder&gt;  &lt;/appender&gt;  &lt;root level=&quot;DEBUG&quot;&gt;    &lt;appender-ref ref=&quot;CONSOLE&quot; /&gt;  &lt;/root&gt;&lt;/configuration&gt;\n\n\n\nThresholdFilter 过滤低于指定阈值的事件。对于等于或高于阈值的事件，ThresholdFilter将在调用其decision（）方法时响应NEUTRAL。但是，将拒绝级别低于阈值的事件，例如下面的配置将拒绝所有低于INFO级别的日志，只输出INFO以及以上级别的日志：\n&lt;configuration&gt;  &lt;appender name=&quot;CONSOLE&quot;    class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;    &lt;!-- deny all events with a level below INFO, that is TRACE and DEBUG --&gt;    &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt;      &lt;level&gt;INFO&lt;/level&gt;    &lt;/filter&gt;    &lt;encoder&gt;      &lt;pattern&gt;        %-4relative [%thread] %-5level %logger&#123;30&#125; - %msg%n      &lt;/pattern&gt;    &lt;/encoder&gt;  &lt;/appender&gt;  &lt;root level=&quot;DEBUG&quot;&gt;    &lt;appender-ref ref=&quot;CONSOLE&quot; /&gt;  &lt;/root&gt;&lt;/configuration&gt;\n\n五、详细的logback-spring.xml示例：以上介绍了xml中重要的几个元素，下面将我配置的xml贴出来以供参考（实现了基于日期和大小翻滚的策略，以及经INFO和ERROR日志区分输出，还有规范日志输出格式等）：\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration debug=&quot;true&quot;&gt;     &lt;!-- appender是configuration的子节点，是负责写日志的组件。--&gt;    &lt;!-- ConsoleAppender：把日志输出到控制台 --&gt;    &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;        &lt;!-- 默认情况下，每个日志事件都会立即刷新到基础输出流。这种默认方法更安全，因为如果应用程序在没有正确关闭appender的情况下退出，则日志事件不会丢失。         但是，为了显着增加日志记录吞吐量，您可能希望将immediateFlush属性设置为false --&gt;        &lt;!--&lt;immediateFlush&gt;true&lt;/immediateFlush&gt;--&gt;        &lt;encoder&gt;            &lt;!-- %37():如果字符没有37个字符长度,则左侧用空格补齐 --&gt;            &lt;!-- %-37():如果字符没有37个字符长度,则右侧用空格补齐 --&gt;            &lt;!-- %15.15():如果记录的线程字符长度小于15(第一个)则用空格在左侧补齐,如果字符长度大于15(第二个),则从开头开始截断多余的字符 --&gt;            &lt;!-- %-40.40():如果记录的logger字符长度小于40(第一个)则用空格在右侧补齐,如果字符长度大于40(第二个),则从开头开始截断多余的字符 --&gt;            &lt;!-- %msg：日志打印详情 --&gt;            &lt;!-- %n:换行符 --&gt;            &lt;!-- %highlight():转换说明符以粗体红色显示其级别为ERROR的事件，红色为WARN，BLUE为INFO，以及其他级别的默认颜色。--&gt;            &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %highlight(%-5level) --- [%15.15(%thread)] %cyan(%-40.40(%logger&#123;40&#125;)) : %msg%n&lt;/pattern&gt;            &lt;!-- 控制台也要使用UTF-8，不要使用GBK，否则会中文乱码 --&gt;            &lt;charset&gt;UTF-8&lt;/charset&gt;        &lt;/encoder&gt;    &lt;/appender&gt;     &lt;!-- info 日志--&gt;    &lt;!-- RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件 --&gt;    &lt;!-- 以下的大概意思是：1.先按日期存日志，日期变了，将前一天的日志文件名重命名为XXX%日期%索引，新的日志仍然是project_info.log --&gt;    &lt;!-- 2.如果日期没有发生变化，但是当前日志的文件大小超过10MB时，对当前日志进行分割 重命名--&gt;    &lt;appender name=&quot;info_log&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;        &lt;!--日志文件路径和名称--&gt;        &lt;File&gt;logs/project_info.log&lt;/File&gt;        &lt;!--是否追加到文件末尾,默认为true--&gt;        &lt;append&gt;true&lt;/append&gt;        &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;            &lt;level&gt;ERROR&lt;/level&gt;            &lt;onMatch&gt;DENY&lt;/onMatch&gt;&lt;!-- 如果命中ERROR就禁止这条日志 --&gt;            &lt;onMismatch&gt;ACCEPT&lt;/onMismatch&gt;&lt;!-- 如果没有命中就使用这条规则 --&gt;        &lt;/filter&gt;        &lt;!--有两个与RollingFileAppender交互的重要子组件。第一个RollingFileAppender子组件，即RollingPolicy:负责执行翻转所需的操作。         RollingFileAppender的第二个子组件，即TriggeringPolicy:将确定是否以及何时发生翻转。因此，RollingPolicy负责什么和TriggeringPolicy负责什么时候.        作为任何用途，RollingFileAppender必须同时设置RollingPolicy和TriggeringPolicy,但是，如果其RollingPolicy也实现了TriggeringPolicy接口，则只需要显式指定前者。--&gt;        &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt;            &lt;!-- 日志文件的名字会根据fileNamePattern的值，每隔一段时间改变一次 --&gt;            &lt;!-- 文件名：logs/project_info.2017-12-05.0.log --&gt;            &lt;!-- 注意：SizeAndTimeBasedRollingPolicy中 ％i和％d令牌都是强制性的，必须存在，要不会报错 --&gt;            &lt;fileNamePattern&gt;logs/project_info.%d.%i.log&lt;/fileNamePattern&gt;            &lt;!-- 每产生一个日志文件，该日志文件的保存期限为30天, ps:maxHistory的单位是根据fileNamePattern中的翻转策略自动推算出来的,例如上面选用了yyyy-MM-dd,则单位为天            如果上面选用了yyyy-MM,则单位为月,另外上面的单位默认为yyyy-MM-dd--&gt;            &lt;maxHistory&gt;30&lt;/maxHistory&gt;            &lt;!-- 每个日志文件到10mb的时候开始切分，最多保留30天，但最大到20GB，哪怕没到30天也要删除多余的日志 --&gt;            &lt;totalSizeCap&gt;20GB&lt;/totalSizeCap&gt;            &lt;!-- maxFileSize:这是活动文件的大小，默认值是10MB，测试时可改成5KB看效果 --&gt;            &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt;        &lt;/rollingPolicy&gt;        &lt;!--编码器--&gt;        &lt;encoder&gt;            &lt;!-- pattern节点，用来设置日志的输入格式 ps:日志文件中没有设置颜色,否则颜色部分会有ESC[0:39em等乱码--&gt;            &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %-5level --- [%15.15(%thread)] %-40.40(%logger&#123;40&#125;) : %msg%n&lt;/pattern&gt;            &lt;!-- 记录日志的编码:此处设置字符集 - --&gt;            &lt;charset&gt;UTF-8&lt;/charset&gt;        &lt;/encoder&gt;    &lt;/appender&gt;     &lt;!-- error 日志--&gt;    &lt;!-- RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件 --&gt;    &lt;!-- 以下的大概意思是：1.先按日期存日志，日期变了，将前一天的日志文件名重命名为XXX%日期%索引，新的日志仍然是project_error.log --&gt;    &lt;!-- 2.如果日期没有发生变化，但是当前日志的文件大小超过10MB时，对当前日志进行分割 重命名--&gt;    &lt;appender name=&quot;error_log&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;        &lt;!--日志文件路径和名称--&gt;        &lt;File&gt;logs/project_error.log&lt;/File&gt;        &lt;!--是否追加到文件末尾,默认为true--&gt;        &lt;append&gt;true&lt;/append&gt;        &lt;!-- ThresholdFilter过滤低于指定阈值的事件。对于等于或高于阈值的事件，ThresholdFilter将在调用其decision（）方法时响应NEUTRAL。但是，将拒绝级别低于阈值的事件 --&gt;        &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt;            &lt;level&gt;ERROR&lt;/level&gt;&lt;!-- 低于ERROR级别的日志（debug,info）将被拒绝，等于或者高于ERROR的级别将相应NEUTRAL --&gt;        &lt;/filter&gt;        &lt;!--有两个与RollingFileAppender交互的重要子组件。第一个RollingFileAppender子组件，即RollingPolicy:负责执行翻转所需的操作。        RollingFileAppender的第二个子组件，即TriggeringPolicy:将确定是否以及何时发生翻转。因此，RollingPolicy负责什么和TriggeringPolicy负责什么时候.       作为任何用途，RollingFileAppender必须同时设置RollingPolicy和TriggeringPolicy,但是，如果其RollingPolicy也实现了TriggeringPolicy接口，则只需要显式指定前者。--&gt;        &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt;            &lt;!-- 活动文件的名字会根据fileNamePattern的值，每隔一段时间改变一次 --&gt;            &lt;!-- 文件名：logs/project_error.2017-12-05.0.log --&gt;            &lt;!-- 注意：SizeAndTimeBasedRollingPolicy中 ％i和％d令牌都是强制性的，必须存在，要不会报错 --&gt;            &lt;fileNamePattern&gt;logs/project_error.%d.%i.log&lt;/fileNamePattern&gt;            &lt;!-- 每产生一个日志文件，该日志文件的保存期限为30天, ps:maxHistory的单位是根据fileNamePattern中的翻转策略自动推算出来的,例如上面选用了yyyy-MM-dd,则单位为天            如果上面选用了yyyy-MM,则单位为月,另外上面的单位默认为yyyy-MM-dd--&gt;            &lt;maxHistory&gt;30&lt;/maxHistory&gt;            &lt;!-- 每个日志文件到10mb的时候开始切分，最多保留30天，但最大到20GB，哪怕没到30天也要删除多余的日志 --&gt;            &lt;totalSizeCap&gt;20GB&lt;/totalSizeCap&gt;            &lt;!-- maxFileSize:这是活动文件的大小，默认值是10MB，测试时可改成5KB看效果 --&gt;            &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt;        &lt;/rollingPolicy&gt;        &lt;!--编码器--&gt;        &lt;encoder&gt;            &lt;!-- pattern节点，用来设置日志的输入格式 ps:日志文件中没有设置颜色,否则颜色部分会有ESC[0:39em等乱码--&gt;            &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %-5level --- [%15.15(%thread)] %-40.40(%logger&#123;40&#125;) : %msg%n&lt;/pattern&gt;            &lt;!-- 记录日志的编码:此处设置字符集 - --&gt;            &lt;charset&gt;UTF-8&lt;/charset&gt;        &lt;/encoder&gt;    &lt;/appender&gt;     &lt;!--给定记录器的每个启用的日志记录请求都将转发到该记录器中的所有appender以及层次结构中较高的appender（不用在意level值）。    换句话说，appender是从记录器层次结构中附加地继承的。    例如，如果将控制台appender添加到根记录器，则所有启用的日志记录请求将至少在控制台上打印。    如果另外将文件追加器添加到记录器（例如L），则对L和L&#x27;子项启用的记录请求将打印在文件和控制台上。    通过将记录器的additivity标志设置为false，可以覆盖此默认行为，以便不再添加appender累积--&gt;    &lt;!-- configuration中最多允许一个root，别的logger如果没有设置级别则从父级别root继承 --&gt;    &lt;root level=&quot;INFO&quot;&gt;        &lt;appender-ref ref=&quot;STDOUT&quot; /&gt;    &lt;/root&gt;     &lt;!-- 指定项目中某个包，当有日志操作行为时的日志记录级别 --&gt;    &lt;!-- 级别依次为【从高到低】：FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE --&gt;    &lt;logger name=&quot;com.sailing.springbootmybatis&quot; level=&quot;INFO&quot;&gt;        &lt;appender-ref ref=&quot;info_log&quot; /&gt;        &lt;appender-ref ref=&quot;error_log&quot; /&gt;    &lt;/logger&gt;     &lt;!-- 利用logback输入mybatis的sql日志，    注意：如果不加 additivity=&quot;false&quot; 则此logger会将输出转发到自身以及祖先的logger中，就会出现日志文件中sql重复打印--&gt;    &lt;logger name=&quot;com.sailing.springbootmybatis.mapper&quot; level=&quot;DEBUG&quot; additivity=&quot;false&quot;&gt;        &lt;appender-ref ref=&quot;info_log&quot; /&gt;        &lt;appender-ref ref=&quot;error_log&quot; /&gt;    &lt;/logger&gt;     &lt;!-- additivity=false代表禁止默认累计的行为，即com.atomikos中的日志只会记录到日志文件中，不会输出层次级别更高的任何appender--&gt;    &lt;logger name=&quot;com.atomikos&quot; level=&quot;INFO&quot; additivity=&quot;false&quot;&gt;        &lt;appender-ref ref=&quot;info_log&quot; /&gt;        &lt;appender-ref ref=&quot;error_log&quot; /&gt;    &lt;/logger&gt; &lt;/configuration&gt;\n\n\n\n六、附加内容6.1：这里再说下log日志输出代码，一般有人可能在代码中使用如下方式输出：\nObject entry = new SomeObject();logger.debug(&quot;The entry is &quot; + entry);\n\n6.2：上面看起来没什么问题，但是会存在构造消息参数的成本，即将entry转换成字符串相加。并且无论是否记录消息，都是如此，即：那怕日志级别为INFO，也会执行括号里面的操作，但是日志不会输出，下面是优化后的写法：\nif(logger.isDebugEnabled()) &#123;    Object entry = new SomeObject();    logger.debug(&quot;The entry is &quot; + entry);&#125;\n\n6.3：6.2的写法，首先对设置的日志级别进行了判断，如果为debug模式，才进行参数的构造，对第一种写法进行了改善。不过还有最好的写法，使用占位符：\nObject entry = new SomeObject();logger.debug(&quot;The entry is &#123;&#125;.&quot;, entry);\n\n只有在评估是否记录之后，并且只有在决策是肯定的情况下，记录器实现才会格式化消息并将“{}”对替换为条目的字符串值。换句话说，当禁用日志语句时，此表单不会产生参数构造的成本。\nlogback作者进行测试得出：第一种和第三种写法将产生完全相同的输出。但是，在禁用日志记录语句的情况下，第三个变体将比第一个变体优于至少30倍。\n如果有多个参数，写法如下：\nlogger.debug(&quot;The new entry is &#123;&#125;. It replaces &#123;&#125;.&quot;, entry, oldEntry);\n\n如果需要传递三个或更多参数，则还可以使用Object []变体：\nObject[] paramArray = &#123;newVal, below, above&#125;;logger.debug(&quot;Value &#123;&#125; was inserted between &#123;&#125; and &#123;&#125;.&quot;, paramArray);\n\n6.4：记录日志的时候我们可能需要在文件中记录下异常的堆栈信息，经过测试，logger.error(e) 不会打印出堆栈信息，正确的写法是：\nlogger.error(&quot;程序异常, 详细信息:&#123;&#125;&quot;, e.getLocalizedMessage() , e);\n\n","categories":["实用类库"]},{"title":"maxmind-db 根据 IP 获取地理位置信息","url":"/posts/15671/","content":"依赖&lt;dependency&gt;    &lt;groupId&gt;com.maxmind.db&lt;/groupId&gt;    &lt;artifactId&gt;maxmind-db&lt;/artifactId&gt;    &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;\n\n","categories":["实用类库"]},{"title":"zstd 压缩","url":"/posts/24925/","content":"zstd 官网\nJava 使用 zstdJava 类库\n依赖：\n&lt;dependency&gt;    &lt;groupId&gt;com.github.luben&lt;/groupId&gt;    &lt;artifactId&gt;zstd-jni&lt;/artifactId&gt;    &lt;version&gt;1.5.4-2&lt;/version&gt;&lt;/dependency&gt;\n\n压缩：\nbyte[] array=...;byte[] compressArray = Zstd.compress(array);\n\n解压缩：\nbyte[] compressArray=...int size = (int) Zstd.decompressedSize(compressArray);byte[] array = new byte[size];Zstd.decompress(array, compressArray);\n\n","categories":["实用类库"]},{"title":"Github 实用技巧","url":"/posts/48924/","content":"一、高级搜索页面地址：https://github.com/search/advanced\n二、WEB 编辑器官方文档\nshift + . 快捷键打开编辑器，或者直接将仓库地址的 github.com 域名换的 gitHub.dev。\n三、在线运行项目在仓库域名前加 gitpod.io/#/ ，例如：https://gitpod.io/#/github.com/xxx/blog\n","categories":["开发实用技巧"]},{"title":"Mac 配置终端","url":"/posts/29893/","content":"Oh My ZshOh My Zsh 是这么介绍自己的。\n\nOh My Zsh is a delightful, open source, community-driven framework for managing your Zsh configuration. It comes bundled with thousands of helpful functions, helpers, plugins, themes, and a few things that make you shout…\n\n简单来说，利用 Oh My Zsh 我们可以轻松管理 zsh 的配置，可以做非常多的定制化功能，比如主题，字体，插件等。\nOh My Zsh 支持 curl、wget 安装，命令如下：\n\ncurl：\nsh -c &quot;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;\nwget：\nsh -c &quot;$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)&quot;\n\n安装完成后，Oh My Zsh 会加载默认的主题。\nzsh-syntax-highlightingzsh-syntax-highlighting 终端命令语法高亮插件。\n执行以下命令，安装 zsh-syntax-highlighting。\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlighting\n\n在 zsh 的配置文件 ~/.zshrc 中的 plugins 中加入 zsh-syntax-highlighting。\nplugins=(  git  autojump  zsh-syntax-highlighting)\n\nzsh-autosuggestionszsh-autosuggestions 终端命令自动推荐插件，会记录之前使用过的命令，当你输入开头时，会暗色提示之前的历史命令供你选择，可直接按右方向键选中该命令。\n执行以下命令，安装 zsh-autosuggestions。\ngit clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions\n\n在 zsh 的配置文件 ~/.zshrc 中的 plugins 中加入 zsh-autosuggestions。\nplugins=(  git  autojump  zsh-syntax-highlighting  zsh-autosuggestions)\n\n参考链接：https://makeoptim.com/tool/terminal/\n","categories":["开发实用技巧"]},{"title":"Wireshark 抓包","url":"/posts/25831/","content":"一、Wireshakr抓包界面介绍\n 说明：数据包列表区中不同的协议使用了不同的颜色区分。协议颜色标识定位在菜单栏View –&gt; Coloring Rules。如下所示\n\nWireShark 主要分为这几个界面\n\nDisplay Filter(显示过滤器)， 用于设置过滤条件进行数据包列表过滤。菜单路径：Analyze –&gt; Display Filters。\n\n\n\nPacket List Pane(数据包列表)， 显示捕获到的数据包，每个数据包包含编号，时间戳，源地址，目标地址，协议，长度，以及数据包信息。 不同协议的数据包使用了不同的颜色区分显示。\n\n\n\nPacket Details Pane(数据包详细信息), 在数据包列表中选择指定数据包，在数据包详细信息中会显示数据包的所有详细信息内容。数据包详细信息面板是最重要的，用来查看协议中的每一个字段。各行信息分别为\n\n （1）Frame:  物理层的数据帧概况\n （2）Ethernet II: 数据链路层以太网帧头部信息\n （3）Internet Protocol Version 4: 互联网层IP包头部信息\n （4）Transmission Control Protocol: 传输层T的数据段头部信息，此处是TCP\n （5）Hypertext Transfer Protocol: 应用层的信息，此处是HTTP协议\n\nTCP包的具体内容\n 从下图可以看到wireshark捕获到的TCP包中的每个字段。\n\n\nDissector Pane(数据包字节区)。\n\n二、Wireshark过滤器设置  初学者使用wireshark时，将会得到大量的冗余数据包列表，以至于很难找到自己需要抓取的数据包部分。wireshark工具中自带了两种类型的过滤器，学会使用这两种过滤器会帮助我们在大量的数据中迅速找到我们需要的信息。\n（1）抓包过滤器  捕获过滤器的菜单栏路径为Capture –&gt; Capture Filters。用于在抓取数据包前设置。\n**\n 如何使用？可以在抓取数据包前设置如下。\n\nip host 60.207.246.216 and icmp表示只捕获主机IP为60.207.246.216的ICMP数据包。获取结果如下：\n\n（2）显示过滤器 显示过滤器是用于在抓取数据包后设置过滤条件进行过滤数据包。通常是在抓取数据包时设置条件相对宽泛或者没有设置导致抓取的数据包内容较多时使用显示过滤器设置条件过滤以方便分析。同样上述场景，在捕获时未设置抓包过滤规则直接通过网卡进行抓取所有数据包，如下\n\n执行ping www.huawei.com获取的数据包列表如下\n\n观察上述获取的数据包列表，含有大量的无效数据。这时可以通过设置显示器过滤条件进行提取分析信息。ip.addr == 211.162.2.183 and icmp。并进行过滤。\n\n 上述介绍了抓包过滤器和显示过滤器的基本使用方法。在组网不复杂或者流量不大情况下，使用显示器过滤器进行抓包后处理就可以满足我们使用。下面介绍一下两者间的语法以及它们的区别。\n三、wireshark过滤器表达式的规则1、抓包过滤器语法和实例  抓包过滤器类型Type（host、net、port）、方向Dir（src、dst）、协议Proto（ether、ip、tcp、udp、http、icmp、ftp等）、逻辑运算符（&amp;&amp; 与、|| 或、！非）\n（1）协议过滤\n 比较简单，直接在抓包过滤框中直接输入协议名即可。\n\ntcp，只显示TCP协议的数据包列表\nhttp，只查看HTTP协议的数据包列表\nicmp，只显示ICMP协议的数据包列表\n\n（2）IP过滤\n\nhost 192.168.1.104\nsrc host 192.168.1.104\ndst host 192.168.1.104\n\n（3）端口过滤\n\nport 80\nsrc port 80\ndst port 80\n\n（4）逻辑运算符&amp;&amp; 与、|| 或、！非\n src host 192.168.1.104 &amp;&amp; dst port 80 抓取主机地址为192.168.1.80、目的端口为80的数据包\n host 192.168.1.104 || host 192.168.1.102 抓取主机为192.168.1.104或者192.168.1.102的数据包\n ！broadcast 不抓取广播数据包\n2、显示过滤器语法和实例（1）比较操作符\n 比较操作符有== 等于、！= 不等于、&gt; 大于、&lt; 小于、&gt;= 大于等于、&lt;=小于等于。\n（2）协议过滤\n 比较简单，直接在Filter框中直接输入协议名即可。注意：协议名称需要输入小写。\n tcp，只显示TCP协议的数据包列表\n http，只查看HTTP协议的数据包列表\n icmp，只显示ICMP协议的数据包列表\n\n（3） ip过滤\n  ip.src ==192.168.1.104 显示源地址为192.168.1.104的数据包列表\n  ip.dst==192.168.1.104, 显示目标地址为192.168.1.104的数据包列表\n  ip.addr == 192.168.1.104 显示源IP地址或目标IP地址为192.168.1.104的数据包列表\n\n（4）端口过滤\n tcp.port ==80, 显示源主机或者目的主机端口为80的数据包列表。\n tcp.srcport == 80, 只显示TCP协议的源主机端口为80的数据包列表。\n tcp.dstport == 80，只显示TCP协议的目的主机端口为80的数据包列表。\n\n（5） Http模式过滤\n http.request.method==”GET”,  只显示HTTP GET方法的。\n（6）逻辑运算符为 and/or/not\n 过滤多个条件组合时，使用and/or。比如获取IP地址为192.168.1.104的ICMP数据包表达式为ip.addr == 192.168.1.104 and icmp\n\n（7）按照数据包内容过滤。假设我要以IMCP层中的内容进行过滤，可以单击选中界面中的码流，在下方进行选中数据。如下\n\n右键单击选中后出现如下界面\n\n选中Select后在过滤器中显示如下\n\n后面条件表达式就需要自己填写。如下我想过滤出data数据包中包含”abcd”内容的数据流。包含的关键词是contains 后面跟上内容。\n\n看到这， 基本上对wireshak有了初步了解。\n四、Wireshark抓包分析TCP三次握手（1）TCP三次握手连接建立过程Step1：客户端发送一个SYN=1，ACK=0标志的数据包给服务端，请求进行连接，这是第一次握手；\nStep2：服务端收到请求并且允许连接的话，就会发送一个SYN=1，ACK=1标志的数据包给发送端，告诉它，可以通讯了，并且让客户端发送一个确认数据包，这是第二次握手；\nStep3：服务端发送一个SYN=0，ACK=1的数据包给客户端端，告诉它连接已被确认，这就是第三次握手。TCP连接建立，开始通讯。\n\n（2）wireshark抓包获取访问指定服务端数据包Step1：启动wireshark抓包，打开浏览器输入www.huawei.com。\nStep2：使用ping www.huawei.com获取IP。\n\nStep3：输入过滤条件获取待分析数据包列表 ip.addr == 211.162.2.183\n\n 图中可以看到wireshark截获到了三次握手的三个数据包。第四个包才是HTTP的， 这说明HTTP的确是使用TCP建立连接的。\n第一次握手数据包\n客户端发送一个TCP，标志位为SYN，序列号为0， 代表客户端请求建立连接。 如下图。\n\n数据包的关键属性如下：\n SYN ：标志位，表示请求建立连接\n Seq = 0 ：初始建立连接值为0，数据包的相对序列号从0开始，表示当前还没有发送数据\n Ack =0：初始建立连接值为0，已经收到包的数量，表示当前没有接收到数据\n第二次握手的数据包\n服务器发回确认包, 标志位为 SYN,ACK. 将确认序号(Acknowledgement Number)设置为客户的I S N加1以.即0+1=1, 如下图\n\n 数据包的关键属性如下：\n Seq = 0 ：初始建立值为0，表示当前还没有发送数据\n Ack = 1：表示当前端成功接收的数据位数，虽然客户端没有发送任何有效数据，确认号还是被加1，因为包含SYN或FIN标志位。（并不会对有效数据的计数产生影响，因为含有SYN或FIN标志位的包并不携带有效数据）\n第三次握手的数据包\n 客户端再次发送确认包(ACK) SYN标志位为0,ACK标志位为1.并且把服务器发来ACK的序号字段+1,放在确定字段中发送给对方.并且在数据段放写ISN的+1, 如下图:\n\n数据包的关键属性如下：\n ACK ：标志位，表示已经收到记录\n Seq = 1 ：表示当前已经发送1个数据\n Ack = 1 : 表示当前端成功接收的数据位数，虽然服务端没有发送任何有效数据，确认号还是被加1，因为包含SYN或FIN标志位（并不会对有效数据的计数产生影响，因为含有SYN或FIN标志位的包并不携带有效数据)。\n 就这样通过了TCP三次握手，建立了连接。开始进行数据交互\n\n下面针对数据交互过程的数据包进行一些说明：\n\n数据包的关键属性说明\n Seq: 1\n Ack: 1: 说明现在共收到1字节数据\n\n Seq: 1 Ack: 951: 说明现在服务端共收到951字节数据\n 在TCP层，有个FLAGS字段，这个字段有以下几个标识：SYN, FIN, ACK, PSH, RST, URG。如下\n\n  其中，对于我们日常的分析有用的就是前面的五个字段。它们的含义是：SYN表示建立连接，FIN表示关闭连接，ACK表示响应，PSH表示有DATA数据传输，RST表示连接重置。\n五、Wireshark分析常用操作 调整数据包列表中时间戳显示格式。调整方法为View –&gt;Time Display Format –&gt; Date and Time of Day。调整后格式如下：\n\n","categories":["开发实用技巧"]},{"title":"brew 常用命令","url":"/posts/13254/","content":"常用命令brew -v # 查看brew版本号brew install xxx # 安装包brew search xxx # 搜索包brew info xxx # 查看包信息brew uninstall xxx # 卸载包brew list # 显示已安装的包brew update # 更新 Homebrew 在服务器端上的包目录brew outdated # 检查过时（是否有新版本），这会列出所有安装的包里，哪些可以升级brew outdated xxx # 检查包brew upgrade # 升级所有可以升级的软件们brew upgrade xxx # 升级某个包brew cleanup # 清理不需要的版本及安装包缓存brew cleanup xxx # 清理包brew –help # 查看brew的帮助brew pin xxx # 禁止指定软件升级brew unpin # 取消禁止指定软件升级\n\n彻底卸载brew tap beeftornado/rmtreebrew rmtree xxxbrew cleanup\n\n服务相关命令brew services list # 查看 services 列表brew services run mysql # 启动 mysql 服务brew services start mysql # 启动 mysql 服务，并注册开机自启brew services stop mysql # 停止 mysql 服务，并取消开机自启brew services restart mysql # 重启 mysql 服务，并注册开机自启brew services cleanup # 清除已卸载应用的无用配置\n\n","categories":["开发实用技巧"]},{"title":"加密数据模糊查询","url":"/posts/35095/","content":"一、前言对加密的数据模糊查询大致分为三类做法，如下所示：\n\n沙雕做法（不动脑思考直男的思路，只管实现功能从不深入思考问题）\n常规做法（思考了查询性能问题，也会使用一些存储空间换性能等做法）\n超神做法（比较高端的做法从算法层面上思考）\n\n我们就对这三种实现方法一一来讲讲实现思路和优劣性，首先我们先看沙雕做法。\n二、沙雕做法\n将所有数据加载到内存中进行解密，解密后通过程序算法来模糊匹配\n将密文数据映射一份明文映射表，俗称tag表，然后模糊查询tag来关联密文数据\n\n沙雕一我们先来看看第一个做法，将所有数据加载到内存中进行解密，这个如果数据量小的话可以使用这个方式来做，这样做既简单又实惠，如果数据量大的话那就是灾难，我们来大致算一下。\n一个英文字母(不分大小写)占一个字节的空间，一个中文汉字占两个字节的空间，用DES来举例，13800138000加密后的串HE9T75xNx6c5yLmS5l4r6Q==占24个字节。\n\n\n\n条数\nBytes\nMB\n\n\n\n100w\n2400万\n22.89\n\n\n1000w\n2.4亿\n228.89\n\n\n1亿\n24亿\n2288.89\n\n\n轻则上百兆，重则上千兆，这样分分钟给应用程序整成Out of memory，这样做如果数据少只有几百、几千、几万条时是完全可以这样做的，但是数据量大就强烈不建议了。\n沙雕二我们再来看第二个做法，将密文数据映射一份明文映射表，然后模糊查询映射表来关联密文数据，what？？？！！！那我们为什么要对数据加密呢，直接不加密不是更好么！\n我们既然对数据加密肯定是有安全诉求才会这样做，增加一个明文的映射表就违背了安全诉求，这样做既不安全也不方便完全是脱裤子放x，多此一举，强且不推荐。\n三、常规做法我们接下来看看常规的做法，也是最广泛使用的方法，此类方法及满足的数据安全性，又对查询友好。\n\n在数据库实现加密算法函数，在模糊查询的时候使用decode(key) like &#39;%partial%\n对密文数据进行分词组合，将分词组合的结果集分别进行加密，然后存储到扩展列，查询时通过key like &#39;%partial%&#39;\n\n常规一在数据库中实现与程序一致的加解密算法，修改模糊查询条件，使用数据库加解密函数先解密再模糊查找，这样做的优点是实现成本低，开发使用成本低，只需要将以往的模糊查找稍微修改一下就可以实现，但是缺点也很明显，这样做无法利用数据库的索引来优化查询，甚至有一些数据库可能无法保证与程序实现一致的加解密算法，但是对于常规的加解密算法都可以保证与应用程序一致。\n如果对查询性能要求不是特别高、对数据安全性要求一般，可以使用常见的加解密算法比如说AES、DES之类的也是一个不错的选择。\n如果公司有自己的算法实现，并且没有提供多端的算法实现，要么找个算法好的人去研究吃透补全多端实现，要么放弃使用这个办法。\n常规二对密文数据进行分词组合，将分词组合的结果集分别进行加密，然后存储到扩展列，查询时通过key like ‘%partial%’，这是一个比较划算的实现方法，我们先来分析一下它的实现思路。\n先对字符进行固定长度的分组，将一个字段拆分为多个，比如说根据4位英文字符（半角），2个中文字符（全角）为一个检索条件，举个例子：\n\nningyu1使用4个字符为一组的加密方式，第一组ning ，第二组ingy ，第三组ngyu ，第四组gyu1 … 依次类推。\n\n如果需要检索所有包含检索条件4个字符的数据比如：ingy ，加密字符后通过 key like “%partial%” 查库。\n我们都知道加密后长度会增长，增长的这部分长度存储就是我们要花费的额外成本，典型的使用成本来换取速度，密文增长的幅度随着算法不同而不同以DES举例，13800138000加密前占11个字节，加密后的串HE9T75xNx6c5yLmS5l4r6Q==占24个字节，增长是2.18倍，所以一个优秀的算法是多么的重要，能为公司节省不少成本，但是话又说回来算法工程师的工资也不低，所以我也不知道是节省成本还是增加成本，哈哈哈…你们自己算吧。\n回到主题，这个方法虽然可以实现加密数据的模糊查询，但是对模糊查询的字符长度是有要求的，以我上面举的例子模糊查询字符原文长度必须大于等于4个英文/数字，或者2个汉字，再短的长度不建议支持，因为分词组合会增多从而导致存储的成本增加，反而安全性降低。\n大家是否都对接过 淘宝、拼多多、JD他们的api，他们对平台订单数据中的用户敏感数据就是加密的同时支持模糊查询，使用就是这个方法，下面我整理了几家电商平台的密文字段检索方案的说明，感兴趣的可以查看下面链接。\n\n淘宝密文字段检索方案：https://open.taobao.com/docV3.htm?docId=106213&amp;docType=1\n阿里巴巴文字段检索方案：https://jaq-doc.alibaba.com/docs/doc.htm?treeId=1&amp;articleId=106213&amp;docType=1\n拼多多密文字段检索方案：https://open.pinduoduo.com/application/document/browse?idStr=3407B605226E77F2\n京东密文字段检索方案：https://jos.jd.com/commondoc?listId=345\n\n\nps. 基本上都是一样的，果然都是互相抄袭，连加密后的数据格式都一致。\n\n这个方法优点就是实现起来不算复杂，使用起来也较为简单，算是一个折中的做法，因为会有扩展字段存储成本会有升高，但是可利用数据库索引优化查询速度，推荐使用这个方法。\n四、超神做法我们接下来看看优秀的做法，此类做法难度较高，都是从算法层面来考虑，有些甚至会设计一个新算法，虽然已有一些现成的算法参考，但是大多都是半成品无法拿来直接使用，所以还是要有人去深入研究和整合到自己的应用中去。\n从算法层面思考，甚至会设计一个新算法来支持模糊查找\n这个层面大多是专业算法工程师的研究领域，想要设计一个有序的、非不可逆的、密文长度不能增长过快的算法不是一件简单的事情，大致的思路是这样的，使用译码的方式进行加解密，保留密文和原文一样的顺序，从而支持密文模糊匹配，说的比较笼统因为我也不是这方面的专家没有更深一步的研究过，所以我从网上找了一些资料可以参考一下。\n\n数据库中字符数据的模糊匹配加密方法：https://www.jiamisoft.com/blog/6542-zifushujumohupipeijiamifangfa.html\n\n这里提到的Hill密码处理和模糊匹配加密方法FMES可以重点看看.\n\n一种基于BloomFilter的改进型加密文本模糊搜索机制研究：http://kzyjc.cnjournals.com/html/2019/1/20190112.htm\n支持快速查询的数据库如何加密：https://www.jiamisoft.com/blog/5961-kuaisuchaxunshujukujiami.html\n基于Lucene的云端搜索与密文基础上的模糊查询：https://www.cnblogs.com/arthurqin/p/6307153.html\n\n基于Lucene的思路就跟我们上面介绍的常规做法二类似，对字符进行等长度分词，将分词后的结果集加密后存储，只不过存储的db不一样，一个是关系型数据库，一个是es搜索引擎。\n\n云存储中一种支持可验证的模糊查询加密方案：http://jeit.ie.ac.cn/fileDZYXXXB/journal/article/dzyxxxb/2017/7/PDF/160971.pdf\n\n","categories":["技术方案"]},{"title":"红黑树","url":"/posts/28769/","content":"美团博客：https://tech.meituan.com/2016/12/02/redblack-tree.html\n一、定义\n节点是黑色或者红色；\n根节点是黑色；\n所有叶子节点，即叶子是 null 的节点被认为是黑色；\n每个红色节点必须有两个黑色的子节点（从每个叶子到根的所有路径上不能有两个连续的红色节点）；\n从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。\n\n这些约束确保了红黑树的关键特性：从根到叶子的最长的可能路径不多于最短的可能路径的两倍长。\n结果是这个树大致上是平衡的。因为操作比如插入、删除和查找某个值的最坏情况时间都要求与树的高度成比例，这个在高度上的理论上限允许红黑树在最坏情况下都是高效的，而不同于普通的二叉查找树。\n要知道为什么这些性质确保了这个结果，注意到性质4导致了路径不能有两个毗连的红色节点就足够了。\n最短的可能路径都是黑色节点，最长的可能路径有交替的红色和黑色节点。\n因为根据性质5所有最长的路径都有相同数目的黑色节点，这就表明了没有路径能多于任何其他路径的两倍长。\n二、插入操作演示地址：https://www.cs.usfca.edu/~galles/visualization/RedBlack.html\n\n类型\n左左：插入节点的父节点是祖父节点的左子树，插入节点是父节点的左子树\n右右：插入节点的父节点是祖父节点的右子树，插入节点是父节点的右子树\n左右：插入节点的父节点是祖父节点的左子树，插入节点是父节点的右子树\n右左：插入节点的父节点是祖父节点的右子树，插入节点是父节点的左子树\n\n对于 左右 和 右左，其实是通过先进行左旋或右旋，变成 左左 和 右右 的情况进行处理。\n变色父节点变黑，祖父节点变红\n","categories":["数据结构"]},{"title":"Jmeter 安装QPS插件","url":"/posts/20220/","content":"插件下载https://jmeter-plugins.org/wiki/Start/\n\nResponse Times Over Time\nTransactions per Second\n\n","categories":["测试"]},{"title":"wrk 压测工具","url":"/posts/14340/","content":"Github\n其他资料：https://www.cnblogs.com/quanxiaoha/p/10661650.html\n一、安装Mac 安装brew install wrk\n\nDocker 运行https://hub.docker.com/r/williamyeh/wrk\ndocker run --rm  -v `pwd`:/data  \\      williamyeh/wrk  \\      -s script.lua  http://www.google.com/\n\n二、案例演示# 启用100个连接，每个线程并发30个测试线程，压测3分钟$ wrk -c100 -t30 -d3m --latency http://baidu.comRunning 3m test @ http://baidu.com (压测时间3分钟)  30 threads and 100 connections (共30个测试线程，100个连接)  Thread Stats   Avg      Stdev     Max   +/- Stdev             (平均值)    (标准差) (最大值)   (正负一个标准差所占比例)    Latency   116.04ms   97.56ms   1.81s    58.20% (延迟)[主要关注]    Req/Sec     9.48      8.80   310.00     90.34% (每秒处理中的请求数)[主要关注]  Latency Distribution (延迟分布)[主要关注]   50%    0.00us   75%    0.00us   90%    0.00us   99%    0.00us  158766 requests in 3.00m, 65.86MB read  (再3分钟之内共处理完成了158766个请求，读取了65.86MB数据)  Socket errors: connect 0, read 0, write 0, timeout 2  (Socket错误数统计，0个连接错误，0个读取错误，0个写入错误，2个超时)[主要关注]Requests/sec:    881.54   (平均每秒262.22个请求)[主要关注]Transfer/sec:    374.44KB (平均每秒读取数据554.27KB)\n\n三、Lua 支持\n使用 Lua 脚本个性化 wrk 压测\n\n[1] 使用扩展功能wrk 有个更牛叉的特性就是支持自定义的 Lua 脚本，像往常的压力测试工具，我们只是发起固定的请求，请求体是无法改变的。 但是 wrk 的 Lua 脚本不仅是可以做鉴权签名，可以改变请求体，模拟延迟，通过请求的返回值做些收尾wrk 支持在三个阶段(即 wrk 的生命周期)对压测进行个性化分别是启动阶段、运行阶段和结束阶段。每个测试线程，都拥有独立的 Lua 运行环境。\n# 使用Lua脚本的请求方式wrk -t1 -c100 -d10s -s post_test.lua --latency http://test.com\n\n[2] wrk 的生命周期wrk 可以在 Lua 脚本里添加下面的 Hook 函数，你可以想象成生命周期，每个生命周期做的事情都不一样，但是生命周期是有时间顺序的，而我们常用一般是 request 和 delay 周期。\n# setup函数在目标域名或IP地址已经解析完，并且所有thread已经生成但还没有开始时被调用时，每个线程执行一次这个函数，进行初始化可以通过thread:get(name)/thread:set(name, value)来设置线程级别的变量# init每次请求发送之前被调用，可以接受wrk命令行的额外参数# delay这个函数返回一个数值，在这次请求执行完以后延迟多长时间执行下一个请求可以对应 thinking time 的场景，做请求访问间隔的设定，防止请求被禁用# request通过这个函数可以每次请求之前修改本次请求体和Header属性，返回一个字符串这个函数要慎用其会影响测试端性能，我们可以在这里写一些要压力测试的逻辑# response每次请求返回以后被调用，可以根据响应内容做特殊处理，比如遇到特殊响应停止执行测试，或输出到控制台等等。\n\n\n\n# delay.lua# 实现的是每个请求前会有随机的延迟-- example script that demonstrates adding a random-- 10-50ms delay before each requestfunction delay()   return math.random(10, 50)end\n\n[3] POST 请求脚本# post.lua-- example HTTP POST script which demonstrates setting the-- HTTP method, body, and adding a headerwrk.method = &quot;POST&quot;wrk.body = &quot;foo=bar&amp;baz=quux&quot;wrk.headers[&quot;Content-Type&quot;] = &quot;application/x-www-form-urlencoded&quot;# 调用方式$ ./wrk -t4 -c100 -d30s -T30s --script=post.lua --latency http://www.douban.com\n\n\n\n# post.lua-- wrk 全局变量，改动之后会影响所有的请求wrk = &#123;    scheme  = &quot;http&quot;,    host    = &quot;localhost&quot;,    port    = nil,    method  = &quot;GET&quot;,    path    = &quot;/&quot;,    headers = &#123;&#125;,    body    = nil,    thread  = &lt;userdata&gt;,&#125;# 调用方式$ ./wrk -t1 -c10 -d20s -s post.lua --latency http://www.douban.com\n\n\n\n# post.lua - json-- wrk 全局变量，改动之后会影响所有的请求wrk = &#123;    scheme  = &quot;http&quot;,    host    = &quot;localhost&quot;,    port    = nil,    method  = &quot;GET&quot;,    path    = &quot;/&quot;,    headers = &#123;&#125;,    body    = nil,    thread  = &lt;userdata&gt;,&#125;# 调用方式$ ./wrk -t1 -c10 -d20s -s post.lua --latency http://www.douban.com\n\n[4] 上传文件# 实现上传文件测试与json类似# 同样是设置wrk.body和wrk.headers的值, 只是body较麻烦一些wrk.method = &quot;POST&quot;wrk.headers[&quot;Content-Type&quot;] = &quot;multipart/form-data;boundary=------WebKitFormBoundaryX3bY6PBMcxB1vCan&quot;file = io.open(&quot;path/to/fake.jpg&quot;, &quot;rb&quot;)-- 拼装form-dataform = &quot;------WebKitFormBoundaryX3bY6PBMcxB1vCan\\r\\n&quot;form = form .. &quot;Content-Disposition: form-data; name=&quot;file&quot;; filename=&quot;fake.jpg&quot;\\r\\n&quot;form = form .. &quot;Content-Type: image/jpeg\\r\\n\\r\\n&quot;form = form .. file:read(&quot;*a&quot;)form = form .. &quot;\\r\\n------WebKitFormBoundaryX3bY6PBMcxB1vCan--&quot;wrk.body  = form","categories":["测试"]},{"title":"混沌测试工具","url":"/posts/59508/","content":"为什么需要混沌测试？随着计算机技术的发展，系统架构从集中式演进到分布式。分布式系统相对于单台机器来说提供了更好的可扩展性，容错性以及更低的延迟，但在单台计算机上运行软件和分布式系统上运行软件却有着根本的区别，其中一点便是单台计算机上运行软件，错误是可预测的。当硬件没有故障时，运行在单台计算机的软件总是产生同样的结果；而硬件如果出现问题，那么后果往往是整个系统的故障。因此，对于单体系统来说，要么功能完好且正确，要么完全失效，而不是介于两者之间。\n而分布式系统则复杂的多。分布式系统涉及到多个节点和网络，因而存在部分失效的问题。分布式系统中不可靠的网络会导致数据包可能会丢失或任意延迟，不可靠的时钟导致某节点可能会与其他节点不同步 ，甚至一个节点上的进程可能会在任意时候暂停一段相当长的时间（比如由于垃圾收集器导致）而被宣告死亡，这些都给分布式系统带来了不确定性和不可预测性。事实上，这些问题在分布式系统中是无法避免的，就像著名的 CAP 理论中提出的，P（网络分区）是永远存在的，而不是可选的。\n既然分布式系统中故障是无法避免的，那么处理故障最简单的方法便是让整个服务失效，让应用“正确地死去”，但这并不是所有应用都能接受。故障转移企图解决该问题，当故障发生时将其中一个从库提升为主库，使新主库仍然对外提供服务。但是主从数据不一致、脑裂等问题可能会让应用“错误地活着”。代码托管网站 Github 在一场事故中，就因为一个过时的 MySQL 从库被提升为主库 ，造成 MySQL 和 Redis 中数据产生不一致，最后导致一些私有数据泄漏到错误的用户手中 。为了减轻故障带来的影响，我们需要通过某种手段来确保数据的一致性，而如何验证大规模分布式系统在故障下依然正确和稳定（可靠性）成为了新的难题。\n常用混沌测试工具目前比较常用的有如下框架：\n\n\n\n框架名称\n支持平台\n支持自动部署被测试服务\n故障类型是否丰富\n可视化界面\n是否支持断言\n官方文档\n\n\n\nChaos Mesh\nKubernetes\n不支持\n丰富\n有\n否\nhttps://chaos-mesh.org/zh/docs/\n\n\nChaosBlade\nDocker、Kubernetes、OS\n不支持\n丰富\n否\n是\nhttps://chaosblade.io/docs/\n\n\njepsen\nDocker、OS\n支持\n丰富\n否\n是\n\n\n\nopenchaos\nDocker、OS\n支持\n丰富\n否\n是\n\n\n\n","categories":["测试"]},{"title":"LVS","url":"/posts/19134/","content":"一、LVS介绍简介  LVS是Linux Virtual Server的简称，即Linux虚拟服务器，创始人前阿里云首席科学家章文嵩博士(现已经在滴滴)，官方网站：www.linuxvirtualserver.org。从内核版本2.4开始，已经完全内置了LVS的各个功能模块，无需给内核打任何补丁，可以直接使用LVS提供的各种功能。通过LVS提供的负载均衡技术和Linux操作系统可实现一个高性能、高可用的服务器群集，它具有良好可靠性、可扩展性和可操作性，以低廉的成本实现最优的服务性能。\n\n\n\n通用体系结构　　LVS集群采用IP负载均衡技术和基于内容请求分发技术。调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服 务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器。整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序，以下是体系结构图（来源http://www.linuxvirtualserver.org/architecture.html）：\n\n\n负载调度器（load balancer），它是整个集群对外面的前端机，负责将客户的请求发送到一组服务器上执行。\n服务器池（server pool），是一组真正执行客户请求的服务器，可以是WEB、MAIL、FTP和DNS服务器等。\n共享存储（shared storage），它为服务器池提供一个共享的存储区，这样很容易使得服务器池拥有相同的内容，提供相同的服务，例如数据库、分布式文件系统、网络存储等。\n\n优缺点\n高并发连接：LVS基于内核网络层面工作，有超强的承载能力和并发处理能力。单台LVS负载均衡器，可支持上万并发连接。稳定性强：是工作在网络4层之上仅作分发之用，这个特点也决定了它在负载均衡软件里的性能最强，稳定性最好，对内存和cpu资源消耗极低。\n成本低廉：硬件负载均衡器少则十几万，多则几十万上百万，LVS只需一台服务器和就能免费部署使用，性价比极高。\n配置简单：LVS配置非常简单，仅需几行命令即可完成配置，也可写成脚本进行管理。\n支持多种算法：支持8种负载均衡算法，可根据业务场景灵活调配进行使用。\n支持多种工作模型：可根据业务场景，使用不同的工作模式来解决生产环境请求处理问题。\n应用范围广：因为LVS工作在4层，所以它几乎可以对所有应用做负载均衡，包括http、数据库、DNS、ftp服务等等。\n缺点：工作在4层，不支持7层规则修改，机制过于庞大，不适合小规模应用。\n\n组件和专业术语组件：\n\nipvsadm：用户空间的客户端工具，用于管理集群服务及集群服务上的RS等；\nipvs：工作于内核上的netfilter INPUT钩子之上的程序，可根据用户定义的集群实现请求转发；\n\n专业术语：\n\nVS：Virtual Server ，虚拟服务\nDirector： Balancer ，也叫DS(Director Server)负载均衡器、分发器\nRS：Real Server ，后端请求处理服务器，真实服务器\nCIP: Client IP ，客户端IP\nVIP：Director Virtual IP ，负载均衡器虚拟IP\nDIP：Director IP ，负载均衡器IP\nRIP：Real Server IP ，后端请求处理的服务器IP\n\n工作模型\nVS工作在内核空间，基于内核包处理框架Netfilter实现的一种负责均衡技术，其在工作模式如上图,大致过程：\n\n当客户端的请求到达负载均衡器的内核空间时，首先会到达PREROUTING链。\n当内核发现请求数据包的目的地址是本机时，将数据包送往INPUT链。\nLVS由用户空间的ipvsadm和内核空间的IPVS组成，ipvsadm用来定义规则，IPVS利用ipvsadm定义的规则工作，IPVS工作在INPUT链上,当数据包到达INPUT链时，首先会被IPVS检查，如果数据包里面的目的地址及端口没有在规则里面，那么这条数据包将被放行至用户空间。\n如果数据包里面的目的地址及端口在规则里面，那么这条数据报文将被修改目的地址为事先定义好的后端服务器，并送往POSTROUTING链。\n最后经由POSTROUTING链发往后端服务器。\n\n二、负载均衡模式LVS-NAT模式简介\n　　NAT模式称为全称Virtualserver via Network address translation(VS/NAT)，是通过网络地址转换的方法来实现调度的。首先调度器(Director)接收到客户的请求数据包时（请求的目的IP为VIP），根据调度算法决定将请求发送给哪个后端的真实服务器（RS）。然后调度就把客户端发送的请求数据包的目标IP地址及端口改成后端真实服务器的IP地址（RIP）,这样真实服务器（RS）就能够接收到客户的请求数据包了。真实服务器响应完请求后，查看默认路由（NAT模式下我们需要把RS的默认路由设置为DS服务器。）把响应后的数据包发送给DS,DS再接收到响应包后，把包的源地址改成虚拟地址（VIP）然后发送回给客户端。\n具体工作流程：\n\n说明：\n(1)当用户请求到达DirectorServer，此时请求的数据报文会先到内核空间的PREROUTING链。 此时报文的源IP为CIP，目标IP为VIP。\n(2) PREROUTING检查发现数据包的目标IP是本机，将数据包送至INPUT链。\n(3) IPVS比对数据包请求的服务是否为集群服务，若是，修改数据包的目标IP地址为后端服务器IP，然后将数据包发至POSTROUTING链。 此时报文的源IP为CIP，目标IP为RIP ，在这个过程完成了目标IP的转换（DNAT）。\n(4) POSTROUTING链通过选路，将数据包发送给Real Server。\n(5) Real Server比对发现目标为自己的IP，开始构建响应报文发回给Director Server。 此时报文的源IP为RIP，目标IP为CIP 。\n(6) Director Server在响应客户端前，此时会将源IP地址修改为自己的VIP地址（SNAT），然后响应给客户端。 此时报文的源IP为VIP，目标IP为CIP。 \nNAT模式优缺点：\n\nNAT技术将请求的报文和响应的报文都需要通过DS进行地址改写，因此网站访问量比较大的时候DS负载均衡调度器有比较大的瓶颈，一般要求最多之能10-20台节点。\n节省IP，只需要在DS上配置一个公网IP地址就可以了。\n每台内部的节点服务器的网关地址必须是调度器LB的内网地址。\nNAT模式支持对IP地址和端口进行转换。即用户请求的端口和真实服务器的端口可以不一致。 \n\n地址变化过程：\n\nLVS-DR模式简介\n　　全称：Virtual Server via Direct Routing(VS-DR)，也叫直接路由模式，用直接路由技术实现虚拟服务器｡当参与集群的计算机和作为控制管理的计算机在同一个网段时可以用此方法,控制管理的计算机接收到请求包时直接送到参与集群的节点｡直接路由模式比较特别，很难说和什么方面相似，前种模式基本上都是工作在网络层上（三层），而直接路由模式则应该是工作在数据链路层上（二层）。\n工作原理 \n　　DS和RS都使用同一个IP对外服务。但只有DS对ARP请求进行响应，所有RS对本身这个IP的ARP请求保持静默（对ARP请求不做响应），也就是说，网关会把对这个服务IP的请求全部定向给DS，而DS收到数据包后根据调度算法，找出对应的 RS，把目的MAC地址改为RS的MAC并发给这台RS。这时RS收到这个数据包，则等于直接从客户端收到这个数据包无异，处理后直接返回给客户端。由于DS要对二层包头进行改换，所以DS和RS之间必须在一个广播域，也可以简单的理解为在同一台交换机上。\n工作流程\n\n说明：\n\n当用户请求到达Director Server，此时请求的数据报文会先到内核空间的PREROUTING链。 此时报文的源IP为CIP，目标IP为VIP；\nPREROUTING检查发现数据包的目标IP是本机，将数据包送至INPUT链；\nIPVS比对数据包请求的服务是否为集群服务，若是，将请求报文中的源MAC地址修改为DIP的MAC地址，将目标MAC地址修改RIP的MAC地址，然后将数据包发至POSTROUTING链。 此时的源IP和目的IP均未修改，仅修改了源MAC地址为DIP的MAC地址，目标MAC地址为RIP的MAC地址；\n由于DS和RS在同一个网络中，所以是通过二层，数据链路层来传输。POSTROUTING链检查目标MAC地址为RIP的MAC地址，那么此时数据包将会发至Real Server；\nRS发现请求报文的MAC地址是自己的MAC地址，就接收此报文。处理完成之后，将响应报文通过lo接口传送给eth0网卡然后向外发出。 此时的源IP地址为VIP，目标IP为CIP；\n响应报文最终送达至客户端。\n\n地址变化过程\n\n DR模式特点以及注意事项：\n\n在前端路由器做静态地址路由绑定，将对于VIP的地址仅路由到Director Server\n在arp的层次上实现在ARP解析时做防火墙规则，过滤RS响应ARP请求。修改RS上内核参数（arp_ignore和arp_announce）将RS上的VIP配置在网卡接口的别名上，并限制其不能响应对VIP地址解析请求。\nRS可以使用私有地址；但也可以使用公网地址，此时可以直接通过互联网连入RS以实现配置、监控等；\nRS的网关一定不能指向DIP；\n因为DR模式是通过MAC地址改写机制实现转发,RS跟Dirctory要在同一物理网络内（不能由路由器分隔）；\n请求报文经过Directory，但响应报文一定不经过Director\n不支持端口映射；\nRS可以使用大多数的操作系统；\nRS上的lo接口配置VIP的IP地址; \n\nLVS- UN模式介绍  在VS/NAT 的集群系统中，请求和响应的数据报文都需要通过负载调度器，当真实服务器的数目在10台和20台之间时，负载调度器将成为整个集群系统的新瓶颈。大多数 Internet服务都有这样的特点：请求报文较短而响应报文往往包含大量的数据。如果能将请求和响应分开处理，即在负载调度器中只负责调度请求而响应直 接返回给客户，将极大地提高整个集群系统的吞吐量。\n   IP隧道（IP tunneling）是将一个IP报文封装在另一个IP报文的技术，这可以使得目标为一个IP地址的数据报文能被封装和转发到另一个IP地址。IP隧道技 术亦称为IP封装技术（IP encapsulation）。IP隧道主要用于移动主机和虚拟私有网络（Virtual Private Network），在其中隧道都是静态建立的，隧道一端有一个IP地址，另一端也有唯一的IP地址。\n  在TUN模式下，利用IP隧道技术将请求报文封装转发给后端服务器，响应报文能从后端服务器直接返回给客户。但在这里，后端服务器有一组而非一个，所以我们不可能静态地建立一一对应的隧道，而是动态地选择 一台服务器，将请求报文封装和转发给选出的服务器。\n工作流程\n客户端将请求发往前端的负载均衡器，请求报文源地址是CIP，目标地址为VIP。\n负载均衡器收到报文后，发现请求的是在规则里面存在的地址，那么它将在客户端请求报文的首部再封装一层IP报文,将源地址改为DIP，目标地址改为RIP,并将此包发送给RS。\nRS收到请求报文后，会首先拆开第一层封装,然后发现里面还有一层IP首部的目标地址是自己lo接口上的VIP，所以会处理次请求报文，并将响应报文通过lo接口送给eth0网卡直接发送给客户端。注意：需要设置lo接口的VIP不能在共网上出现\n\n地址变化过程\nFULL-NAT模式介绍  FULL-NAT模式可以实际上是根据LVS-NAT模式的一种扩展。在NAT模式下DS需要先对请求进行目的地址转换(DNAT)，然后对响应包进行源地址转换（SNAT），先后进行两次NAT，而 FULL-NAT则分别对请求进行和响应进行DNAT和SNAT，进行4次NAT，当然这样多次数的NAT会对性能大大削减，但是由于对请求报文的目的地址和源地址都进行了转换，后端的RS可以不在同一个VLAN下。 \n工作流程\n说明：\n\n首先client 发送请求package给VIP;\nVIP 收到package后，会根据LVS设置的LB算法选择一个合适的RS，然后把package 的目地址修改为RS的ip地址，把源地址改成DS的ip地址；\nRS收到这个package后发现目标地址是自己，就处理这个package ，处理完后把这个包发送给DS；\nDS收到这个package 后把源地址改成VIP的IP，目的地址改成CIP(客户端ip)，然后发送给客户端；\n\n优缺点：\n\nRIP，DIP可以使用私有地址；\nRIP和DIP可以不再同一个网络中，且RIP的网关未必需要指向DIP；\n支持端口映射；\nRS的OS可以使用任意类型；\n请求报文经由Director，响应报文也经由Director；\nFULL-NAT因为要经过4次NAT，所以性能比NAT还要低；\n由于做了源地址转换，RS无法获取到客户端的真实IP； \n\n各个模式的区别lvs-nat与lvs-fullnat：请求和响应报文都经由Director\n  　　lvs-nat：RIP的网关要指向DIP\n 　　 lvs-fullnat：RIP和DIP未必在同一IP网络，但要能通信\nlvs-dr与lvs-tun：请求报文要经由Director，但响应报文由RS直接发往Client\n 　　 lvs-dr：通过封装新的MAC首部实现，通过MAC网络转发\n 　　 lvs-tun：通过在原IP报文外封装新IP头实现转发，支持远距离通信\n三、调度算法LVS在内核中的负载均衡调度是以连接为粒度的。在HTTP协议（非持久）中，每个对象从WEB服务器上获取都需要建立一个TCP连接，同一用户 的不同请求会被调度到不同的服务器上，所以这种细粒度的调度在一定程度上可以避免单个用户访问的突发性引起服务器间的负载不平衡。在内核中的连接调度算法上，IPVS已实现了以下八种调度算法：\n\n轮叫调度rr（Round-Robin Scheduling）\n加权轮叫调度wrr（Weighted Round-Robin Scheduling）\n最小连接调度lc（Least-Connection Scheduling）\n加权最小连接调度wlc（Weighted Least-Connection Scheduling）\n基于局部性的最少链接LBLC（Locality-Based Least Connections Scheduling）\n带复制的基于局部性最少链接LBLCR（Locality-Based Least Connections with Replication Scheduling）\n目标地址散列调度DH（Destination Hashing Scheduling）\n源地址散列调度SH（Source Hashing Scheduling）\n\nrr（轮询）　　轮询调度：这种是最简单的调度算法，调度器把用户请求按顺序1:1的分配到集群中的每个Real Server上，这种算法平等地对待每一台Real Server，而不管服务器的压力和负载状况。\nwrr（权重, 即加权轮询）   加权轮叫调度（Weighted Round-Robin Scheduling）算法可以解决服务器间性能不一的情况，它用相应的权值表示服务器的处理性能，服务器的缺省权值为1。假设服务器A的权值为1，B的 权值为2，则表示服务器B的处理性能是A的两倍。加权轮叫调度算法是按权值的高低和轮叫方式分配请求到各服务器。权值高的服务器先收到的连接，权值高的服 务器比权值低的服务器处理更多的连接，相同权值的服务器处理相同数目的连接数\nsh（源地址哈希）　　源地址散列：主要是实现将此前的session（会话）绑定。将此前客户的源地址作为散列键，从静态的散列表中找出对应的服务器，只要目标服务器是没有超负荷的就将请求发送过去。就是说某客户访问过A,现在这个客户又来了，所以客户请求会被发送到服务过他的A主机。\ndh（目的地址哈希）　　目标地址散列调度（Destination Hashing Scheduling）算法也是针对目标IP地址的负载均衡，但它是一种静态映射算法，通过一个散列（Hash）函数将一个目标IP地址映射到一台服务器。\nlc（最少链接）　　最小连接调度（Least-Connection Scheduling）算法是把新的连接请求分配到当前连接数最小的服务器。最小连接调度是一种动态调度算法，它通过服务器当前所活跃的连接数来估计服务 器的负载情况。调度器需要记录各个服务器已建立连接的数目，当一个请求被调度到某台服务器，其连接数加1；当连接中止或超时，其连接数减一。\nwlc（加权最少链接）LVS的理想算法   加权最小连接调度（Weighted Least-Connection Scheduling）算法是最小连接调度的超集，各个服务器用相应的权值表示其处理性能。服务器的缺省权值为1，系统管理员可以动态地设置服务器的权 值。加权最小连接调度在调度新连接时尽可能使服务器的已建立连接数和其权值成比例。 \nLBLC（基于局部性的最少连接）　　这个算法主要用于Cache集群系统，因为Cache集群的中客户请求报文的目标IP地址的变化，将相同的目标URL地址请求调度到同一台服务器，来提高服务器的访问的局部性和Cache命中率。从而调整整个集群的系统处理能力。但是，如果realserver的负载处于一半负载，就用最少链接算法，将请求发送给活动链接少的主机。 \nLBLCR（带复制的基于局部性的最少链接）　　该算法首先是基于最少链接的，当一个新请求收到后，一定会将请求发给最少连接的那台主机的。但这样又破坏了cache命中率。但这个算法中，集群服务是cache共享的，假设A的PHP跑了一遍，得到缓存。但其他realserver可以去A那里拿缓存，这是种缓存复制机制。\n四、管理工具ipvsadm使用　　 ipvsadm是LVS的管理工具，ipvsadm工作在用户空间，用户通过ipvsadm命令编写负载均衡规则。\n安装yum install ipvsadm -y ###文件说明Unit 文件: ipvsadm.service主程序：/usr/sbin/ipvsadm规则保存工具：/usr/sbin/ipvsadm-save规则重载工具：/usr/sbin/ipvsadm-restore配置文件：/etc/sysconfig/ipvsadm-config\n\n用法以及参数ipvsadm --help #查看使用方法及参数命令：-A, --add-service： #添加一个集群服务. 即为ipvs虚拟服务器添加一个虚拟服务，也就是添加一个需要被负载均衡的虚拟地址。虚拟地址需要是ip地址，端口号，协议的形式。-E, --edit-service： #修改一个虚拟服务。-D, --delete-service： #删除一个虚拟服务。即删除指定的集群服务;-C, --clear： #清除所有虚拟服务。-R, --restore： #从标准输入获取ipvsadm命令。一般结合下边的-S使用。-S, --save： #从标准输出输出虚拟服务器的规则。可以将虚拟服务器的规则保存，在以后通过-R直接读入，以实现自动化配置。-a, --add-server： #为虚拟服务添加一个real server（RS）-e, --edit-server： #修改RS-d, --delete-server： #删除-L, -l, --list： #列出虚拟服务表中的所有虚拟服务。可以指定地址。添加-c显示连接表。-Z, --zero： #将所有数据相关的记录清零。这些记录一般用于调度策略。--set tcp tcpfin udp： #修改协议的超时时间。--start-daemon state： #设置虚拟服务器的备服务器，用来实现主备服务器冗余。（注：该功能只支持ipv4）--stop-daemon： #停止备服务器。 参数：以下参数可以接在上边的命令后边。-t, --tcp-service service-address： #指定虚拟服务为tcp服务。service-address要是host[:port]的形式。端口是0表示任意端口。如果需要将端口设置为0，还需要加上-p选项（持久连接）。-u, --udp-service service-address： #使用udp服务，其他同上。-f, --fwmark-service integer： #用firewall mark取代虚拟地址来指定要被负载均衡的数据包，可以通过这个命令实现把不同地址、端口的虚拟地址整合成一个虚拟服务，可以让虚拟服务器同时截获处理去往多个不同地址的数据包。fwmark可以通过iptables命令指定。如果用在ipv6需要加上-6。-s, --scheduler scheduling-method： #指定调度算法,默认是wlc。调度算法可以指定以下8种：rr（轮询），wrr（权重），lc（最后连接），wlc（权重），lblc（本地最后连接），lblcr（带复制的本地最后连接），dh（目的地址哈希），sh（源地址哈希），sed（最小期望延迟），nq（永不排队）-p, --persistent [timeout]： #设置持久连接，这个模式可以使来自客户的多个请求被送到同一个真实服务器，通常用于ftp或者ssl中。-M, --netmask netmask： #指定客户地址的子网掩码。用于将同属一个子网的客户的请求转发到相同服务器。-r, --real-server server-address： #为虚拟服务指定数据可以转发到的真实服务器的地址。可以添加端口号。如果没有指定端口号，则等效于使用虚拟地址的端口号。[packet-forwarding-method]： #此选项指定某个真实服务器所使用的数据转发模式。需要对每个真实服务器分别指定模式。-g, --gatewaying： #使用网关（即直接路由），此模式是默认模式。-i, --ipip： #使用ipip隧道模式。-m, --masquerading： #使用NAT模式。-w, --weight weight:  #设置权重。权重是0~65535的整数。如果将某个真实服务器的权重设置为0，那么它不会收到新的连接，但是已有连接还会继续维持（这点和直接把某个真实服务器删除时不同的）。-x, --u-threshold uthreshold： #设置一个服务器可以维持的连接上限。0~65535。设置为0表示没有上限。-y, --l-threshold lthreshold： #设置一个服务器的连接下限。当服务器的连接数低于此值的时候服务器才可以重新接收连接。如果此值未设置，则当服务器的连接数连续三次低于uthreshold时服务器才可以接收到新的连接。--mcast-interface interface： #指定使用备服务器时候的广播接口。--syncid syncid：#指定syncid， 同样用于主备服务器的同步。 #以下选项用于list(-l)命令：-c, --connection： #列出当前的IPVS连接。--timeout： #列出超时--stats： #状态信息--rate： #传输速率--thresholds： #列出阈值--persistent-conn： #持久连接--sor： #把列表排序--nosort： #不排序-n, --numeric： #不对ip地址进行dns查询--exact： #单位-6： 如#果fwmark用的是ipv6地址需要指定此选项。         #如果使用IPv6地址，需要在地址两端加上&quot;[]&quot;。例如：ipvsadm -A -t [2001:db8::80]:80 -s rr\n\nLVS集群管理示例####管理LVS集群中的RealServer举例1) 添加RS : -a# ipvsadm -a -t|u|f service-address -r server-address [-g|i|m] [-w weight]  #举例1: 往VIP资源为10.1.210.58的集群服务里添加1个realserveripvsadm -a -t 10.1.210.58 -r 10.1.210.52 –g -w 5  2) 修改RS : -e# ipvsadm -e -t|u|f service-address -r server-address [-g|i|m] [-w weight]  #举例2: 修改10.1.210.58集群服务里10.1.210.52这个realserver的权重为3ipvsadm -e -t 10.1.210.58:80 -r 10.1.210.52 –g -w 3  3) 删除RS : -d# ipvsadm -d -t|u|f service-address -r server-address  #举例3: 删除10.1.210.58集群服务里10.1.210.52这个realserveripvsadm -d -t 10.1.210.58:80 -r 10.1.210.524) 清除规则 (删除所有集群服务), 该命令与iptables的-F功能类似，执行后会清除所有规则:# ipvsadm -C  5) 保存及读取规则：# ipvsadm -S &gt; /path/to/somefile# ipvsadm-save &gt; /path/to/somefile# ipvsadm-restore &lt; /path/to/somefile####管理LVS集群服务的查看# ipvsadm -L|l [options]   options可以为：   -n：数字格式显示   --stats 统计信息   --rate：统计速率   --timeout：显示tcp、tcpinfo、udp的会话超时时长   -c：连接客户端数量  #查看lvs集群转发情况# ipvsadm -Ln  #查看lvs集群的连接状态# ipvsadm -l --stats  说明：Conns    (connections scheduled)  已经转发过的连接数InPkts   (incoming packets)       入包个数OutPkts  (outgoing packets)       出包个数InBytes  (incoming bytes)         入流量（字节）OutBytes (outgoing bytes)         出流量（字节）  #查看lvs集群的速率ipvsadm -l --rate  说明：CPS      (current connection rate)   每秒连接数InPPS    (current in packet rate)    每秒的入包个数OutPPS   (current out packet rate)   每秒的出包个数InBPS    (current in byte rate)      每秒入流量（字节）OutBPS   (current out byte rate)      每秒入流量（字节）\n\n五、案例篇环境服务器系统：centos7.4\n调度服务器DS：10.1.210.51\n两台真实服务RS：10.1.210.52、10.1.210.53\nVIP:10.1.210.58 \nLVS-DR模式案例　　 centos7默认已经将ipvs编译进内核模块，名称为ip_vs,使用时候需要先加载该内核模块。\n以下步骤需要在DS上进行：\n1.加载ip_vs模块\nmodprobe ip_vs #加载ip_vs模块cat /proc/net/ip_vs #查看是否加载成功lsmod | grep ip_vs   #查看加载的模块yum install ipvsadm # 安装管理工具\n\n2.配置调度脚本dr.sh\n#!/bin/bashVIP=10.1.210.58  #虚拟IPRIP1=10.1.210.52 #真实服务器IP1RIP2=10.1.210.53 #真实服务器IP2PORT=80 #端口ifconfig ens192:1 $VIP broadcast $VIP netmask 255.255.255.255 up #添加VIP,注意网卡名称echo 1 &gt; /proc/sys/net/ipv4/ip_forward    #开启转发 route add -host $VIP dev ens192:1    #添加VIP路由/sbin/ipvsadm -C      #清空ipvs中的规则/sbin/ipvsadm -A -t $VIP:80 -s wrr  #添加调度器/sbin/ipvsadm -a -t $VIP:80 -r $RIP1 -g -w 1 #添加RS/sbin/ipvsadm -a -t $VIP:80 -r $RIP2 -g -w 1 #添加RS/sbin/ipvsadm -ln  #查看规则\n\n3.执行脚本\n[root@app51 ~]# sh dr.shIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConnTCP  10.1.210.58:80 wrr  -&gt; 10.1.210.52:80               Route   1      0          0           -&gt; 10.1.210.53:80               Route   1      0          0 \n\n以下步骤需要在RS上执行：\n1.真实服务RS配置脚本rs.sh\n#!/bin/bashVIP=10.1.210.58  #RS上VIP地址#关闭内核arp响应，永久修改配置参数到/etc/sysctl.conf，目的是为了让rs顺利发送mac地址给客户端 echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore      echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announce echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announceifconfig lo:0 $VIP broadcast $VIP netmask 255.255.255.255 up  #绑定VIP到RS服务器上/sbin/route add -host $VIP dev lo:0  #添加VIP路由\n\n2.执行脚本\nsh rs.sh\n3.配置测试web服务（以一台为示例）\nsystemctl stop firewalld  #关闭防火墙systemctl disable firewalld #禁止开机启动yum install httpd #安装httpd###RS1虚拟主机配置vi /etc/httpd/conf/httpd.confServerName 10.1.210.52:80echo &quot;RS 10.1.210.52&quot; &gt; /var/www/html/index.html###RS2虚拟主机配置vi /etc/httpd/conf/httpd.confServerName 10.1.210.53:80echo &quot;RS 10.1.210.53&quot; &gt; /var/www/html/index.html#启动httpd服务systemctl start httpd\n\n测试\n调度算法是轮训，所以结果是交替出现 。\n[root@node1 ~]# for i in &#123;1..10&#125; ;do curl http://10.1.210.58 ;doneRS 10.1.210.53RS 10.1.210.52RS 10.1.210.53RS 10.1.210.52RS 10.1.210.53RS 10.1.210.52RS 10.1.210.53RS 10.1.210.52RS 10.1.210.53RS 10.1.210.52\n\nLVS-NAT案例　　LVS-NAT模式和DR区别要做nat，并且请求和响应都要经过DS，所有需要将RS网关指向DS,由于之前测试过DR模式，在测试NAT模式时候需要将RS环境恢复，RS恢复步骤如下：\necho 0 &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore     echo 0 &gt;/proc/sys/net/ipv4/conf/lo/arp_announceecho 0 &gt;/proc/sys/net/ipv4/conf/all/arp_ignoreecho 0 &gt;/proc/sys/net/ipv4/conf/all/arp_announceifconfig lo:0 down\n\n调度服务DS配置\n#!/bin/bashVIP=10.1.210.58  #虚拟IPRIP1=10.1.210.52 #真实服务器IP1RIP2=10.1.210.53 #真实服务器IP2PORT=80 #端口ifconfig ens192:1 $VIP broadcast $VIP netmask 255.255.255.255 up #添加VIPecho 1 &gt; /proc/sys/net/ipv4/ip_forward    #开启转发 route add -host $VIP dev ens192:1    #添加VIP路由/sbin/ipvsadm -C      #清空ipvs中的规则/sbin/ipvsadm -A -t $VIP:80 -s wlc  #添加调度器/sbin/ipvsadm -a -t $VIP:80 -r $RIP1 -m -w 1 #添加RS/sbin/ipvsadm -a -t $VIP:80 -r $RIP2 -m -w 1 #添加RS/sbin/ipvsadm -ln  #查看规则\n\nRS配置\n　　nat模式RS配置很简单，只需要将RS路由指向DS\nvi /etc/sysconfig/network-scripts/ifcfg-ens192 GATEWAY=10.1.210.58 #修改网关至RS地址systemctl restart network #重启网络\n\n测试\n　　 由于这里的环境DS和RS在同一个网段下，NAT模式下如果客户端是同网段情况下，RS响应的时候直接响应给同网段的服务器了并不经过DS，这样就导致客户端会丢弃该请求。如果想要同网段的想要访问到DS则需要添加路由，这里需要RS在响应同网段服务器时候网关指向DS，这样同网段就能访问到DS了，示例：\nroute add -net 10.1.210.0/24 gw 10.1.210.58\n\n测试结果：\n[root@app36 ~]# for i in &#123;1..10&#125; ; do curl http://10.1.210.58 ;doneRS 10.1.210.53RS 10.1.210.52RS 10.1.210.53RS 10.1.210.52RS 10.1.210.53RS 10.1.210.52RS 10.1.210.53RS 10.1.210.52RS 10.1.210.53RS 10.1.210.52\n\n六、持久连接什么是持久连接　　在LVS中，持久连接是为了用来保证当来自同一个用户的请求时能够定位到同一台服务器，目的是为了会话保持，而通常使用的会话保持技术手段就是cookie与session。\ncookie与session简述　　 在Web服务通信中，HTTP本身是无状态协议，不能标识用户来源，当用户在访问A网页，再从A网页访问其他资源跳转到了B网页，这对于服务器来说又是一个新的请求，之前的登陆信息都没有了，怎么办？为了记录用户的身份信息，开发者在浏览器和服务器之间提供了cookie和session技术，简单说来在你浏览网页时候，服务器建立session用于保存你的身份信息，并将与之对应的cookie信息发送给浏览器，浏览器保存cookie，当你再次浏览该网页时候，服务器检查你的浏览器中的cookie并获取与之对应的session数据，这样一来你上次浏览网页的数据依然存在。\n4层均衡负载导致的问题　　由于cookie和session技术是基于应用层（七层）,而LVS工作在4层，只能根据IP地址和端口进行转发，不能根据应用层信息进行转发，所以就存在了问题。比如LVS集群RS是三台，A用户登陆后请求在由第一台处理，而A用户跳转到了另一个页面请求经过DS转发到第二台服务器，但是此时这台服务器上并没有session，用户的信息就没有了，显然这是不能接受的。为了避免上述的问题，一般的解决方案有三种：\n\n将来自于同一个用户的请求发往同一个服务器(例如nginx的ip_hash算法)；\n将session信息在服务器集群内共享，每个服务器都保存整个集群的session信息；\n建立一个session存，所有session信息都保存到存储池中 ；\n\nLVS会话保持实现方式就是通过将来自于同一个用户的请求发往同一个服务器，具体实现分为sh算法和持久连接：\nsh算法：使用SH算法，SH算法在内核中会自动维护一个哈希表，此哈希表中用每一个请求的源IP地址经过哈希计算得出的值作为键，把请求所到达的RS的地址作为值。在后面的请求中，每一个请求会先经过此哈希表，如果请求在此哈希表中有键值，那么直接定向至特定RS，如没有，则会新生成一个键值，以便后续请求的定向。但是此种方法在时间的记录上比较模糊（依据TCP的连接时长计算），而且其是算法本身，所以无法与算法分离，并不是特别理想的方法。\n 持久连接：此种方法实现了无论使用哪一种调度方法，持久连接功能都能保证在指定时间范围之内，来自于同一个IP的请求将始终被定向至同一个RS，还可以把多种服务绑定后统一进行调度。 \n　　详细一点说来：当用户请求到达director时。无论使用什么调度方法，都可以实现对同一个服务的请求在指定时间范围内始终定向为同一个RS。在director内有一个LVS持久连接模板，模板中记录了每一个请求的来源、调度至的RS、维护时长等等，所以，在新的请求进入时，首先在此模板中检查是否有记录（有内置的时间限制，比如限制是300秒，当在到达300秒时依然有用户访问，那么持久连接模板就会将时间增加两分钟，再计数，依次类推，每次只延长2分钟），如果该记录未超时，则使用该记录所指向的RS，如果是超时记录或者是新请求，则会根据调度算法先调度至特定RS，再将调度的记录添加至此表中。这并不与SH算法冲突，lvs持久连接会在新请求达到时，检查后端RS的负载状况，这就是比较精细的调度和会话保持方法。\nLVS的三种持久连接方式PCC：每客户端持久；将来自于同一个客户端的所有请求统统定向至此前选定的RS；也就是只要IP相同，分配的服务器始终相同。\nPPC：每端口持久；将来自于同一个客户端对同一个服务(端口)的请求，始终定向至此前选定的RS。例如：来自同一个IP的用户第一次访问集群的80端口分配到A服务器，25号端口分配到B服务器。当之后这个用户继续访问80端口仍然分配到A服务器，25号端口仍然分配到B服务器。\nPFMC：持久防火墙标记连接；将来自于同一客户端对指定服务(端口)的请求，始终定向至此选定的RS；不过它可以将两个毫不相干的端口定义为一个集群服务，例如：合并http的80端口和https的443端口定义为同一个集群服务，当用户第一次访问80端口分配到A服务器，第二次访问443端口时仍然分配到A服务器。 \n示例　　LVS的持久连接功能需要定义在集群服务上面，使用-p timeout选项。\nPPC：\n[root@localhost ~]# ipvsadm -At 10.1.210.58:80 -s rr -p 300  #上面命令的意思是：添加一个集群服务为10.1.210.58:80，使用的调度算法为rr，持久连接的保持时间是300秒。当超过300秒都没有请求时，则清空LVS的持久连接模板。\n\nPCC：\n# ipvsadm -A -t 10.1.210.58:0 -s rr -p 600# ipvsadm -a -t 10.1.210.58:0 -r 10.1.210.52 -g -w 2# ipvsadm -a -t 10.1.210.58:0 -r 0.1.210.53 -g -w 1\n\nPFMC：\n######PNMPP是通过路由前给数据包打标记来实现的# iptables -t mangle -A PREROUTING -d 10.1.210.58 -ens192 -p tcp --dport 80 -j MARK --set-mark 3# iptables -t mangle -A PREROUTING -d 10.1.210.58 -ens192 -p tcp --dport 443 -j MARK --set-mark 3# ipvsadm -A -f 3 -s rr -p 600# ipvsadm -a -f 3 -r 10.1.210.52 -g -w 2# ipvsadm -a -f 3 -r 10.1.210.52 -g -w 2\n","categories":["知识库"]},{"title":"OAuth2","url":"/posts/13255/","content":"一、OAuth 2.0 的一个简单解释1. 快递员问题我住在一个大型的居民小区。\n小区有门禁系统。\n进入的时候需要输入密码。\n我经常网购和外卖，每天都有快递员来送货。我必须找到一个办法，让快递员通过门禁系统，进入小区。\n如果我把自己的密码，告诉快递员，他就拥有了与我同样的权限，这样好像不太合适。万一我想取消他进入小区的权力，也很麻烦，我自己的密码也得跟着改了，还得通知其他的快递员。\n有没有一种办法，让快递员能够自由进入小区，又不必知道小区居民的密码，而且他的唯一权限就是送货，其他需要密码的场合，他都没有权限？\n2. 授权机制的设计于是，我设计了一套授权机制。\n第一步，门禁系统的密码输入器下面，增加一个按钮，叫做”获取授权”。快递员需要首先按这个按钮，去申请授权。\n第二步，他按下按钮以后，屋主（也就是我）的手机就会跳出对话框：有人正在要求授权。系统还会显示该快递员的姓名、工号和所属的快递公司。\n我确认请求属实，就点击按钮，告诉门禁系统，我同意给予他进入小区的授权。\n第三步，门禁系统得到我的确认以后，向快递员显示一个进入小区的令牌（access token）。令牌就是类似密码的一串数字，只在短期内（比如七天）有效。\n第四步，快递员向门禁系统输入令牌，进入小区。\n有人可能会问，为什么不是远程为快递员开门，而要为他单独生成一个令牌？这是因为快递员可能每天都会来送货，第二天他还可以复用这个令牌。另外，有的小区有多重门禁，快递员可以使用同一个令牌通过它们。\n3. 互联网场景我们把上面的例子搬到互联网，就是 OAuth 的设计了。\n首先，居民小区就是储存用户数据的网络服务。比如，微信储存了我的好友信息，获取这些信息，就必须经过微信的”门禁系统”。\n其次，快递员（或者说快递公司）就是第三方应用，想要穿过门禁系统，进入小区。\n最后，我就是用户本人，同意授权第三方应用进入小区，获取我的数据。\n简单说，OAuth 就是一种授权机制。数据的所有者告诉系统，同意授权第三方应用进入系统，获取这些数据。系统从而产生一个短期的进入令牌（token），用来代替密码，供第三方应用使用。\n4. 令牌与密码令牌（token）与密码（password）的作用是一样的，都可以进入系统，但是有三点差异。\n（1）令牌是短期的，到期会自动失效，用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。\n（2）令牌可以被数据所有者撤销，会立即失效。以上例而言，屋主可以随时取消快递员的令牌。密码一般不允许被他人撤销。\n（3）令牌有权限范围（scope），比如只能进小区的二号门。对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。\n上面这些设计，保证了令牌既可以让第三方应用获得权限，同时又随时可控，不会危及系统安全。这就是 OAuth 2.0 的优点。\n注意，只要知道了令牌，就能进入系统。系统一般不会再次确认身份，所以令牌必须保密，泄漏令牌与泄漏密码的后果是一样的。 这也是为什么令牌的有效期，一般都设置得很短的原因。\n二、OAuth 2.0 的四种授权类型1. RFC 6749OAuth 2.0 的标准是 RFC 6749 文件。该文件先解释了 OAuth 是什么。\n\nOAuth 引入了一个授权层，用来分离两种不同的角色：客户端和资源所有者。……资源所有者同意以后，资源服务器可以向客户端颁发令牌。客户端通过令牌，去请求数据。\n\n这段话的意思就是，OAuth 的核心就是向第三方应用颁发令牌。然后，RFC 6749 接着写道：\n\n（由于互联网有多种场景，）本标准定义了获得令牌的四种授权方式（authorization grant ）。\n\n也就是说，OAuth 2.0 规定了四种获得令牌的流程。你可以选择最适合自己的那一种，向第三方应用颁发令牌。下面就是这四种授权方式。\n\n授权码（authorization-code）\n隐藏式（implicit）\n密码式（password）：\n客户端凭证（client credentials）\n\n注意，不管哪一种授权方式，第三方应用申请令牌之前，都必须先到系统备案，说明自己的身份，然后会拿到两个身份识别码：客户端 ID（client ID）和客户端密钥（client secret）。这是为了防止令牌被滥用，没有备案过的第三方应用，是不会拿到令牌的。\n2. 第一种授权方式：授权码授权码（authorization code）方式，指的是第三方应用先申请一个授权码，然后再用该码获取令牌。\n这种方式是最常用的流程，安全性也最高，它适用于那些有后端的 Web 应用。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。\n第一步，A 网站提供一个链接，用户点击后就会跳转到 B 网站，授权用户数据给 A 网站使用。下面就是 A 网站跳转 B 网站的一个示意链接。\nhttps://b.com/oauth/authorize?  response_type=code&amp;  client_id=CLIENT_ID&amp;  redirect_uri=CALLBACK_URL&amp;  scope=read\n\n上面 URL 中，response_type 参数表示要求返回授权码（code），client_id 参数让 B 知道是谁在请求，redirect_uri 参数是 B 接受或拒绝请求后的跳转网址，scope 参数表示要求的授权范围（这里是只读）。\n\n第二步，用户跳转后，B 网站会要求用户登录，然后询问是否同意给予 A 网站授权。用户表示同意，这时 B 网站就会跳回redirect_uri参数指定的网址。跳转时，会传回一个授权码，就像下面这样。\nhttps://a.com/callback?code=AUTHORIZATION_CODE\n\n上面 URL 中，code 参数就是授权码。\n\n第三步，A 网站拿到授权码以后，就可以在后端，向 B 网站请求令牌。\nhttps://b.com/oauth/token? client_id=CLIENT_ID&amp; client_secret=CLIENT_SECRET&amp; grant_type=authorization_code&amp; code=AUTHORIZATION_CODE&amp; redirect_uri=CALLBACK_URL\n\n上面 URL 中，client_id 参数和 client_secret 参数用来让 B 确认 A 的身份（client_secret参数是保密的，因此只能在后端发请求），grant_type 参数的值是 AUTHORIZATION_CODE，表示采用的授权方式是授权码，code 参数是上一步拿到的授权码，redirect_uri 参数是令牌颁发后的回调网址。\n\n第四步，B 网站收到请求以后，就会颁发令牌。具体做法是向 redirect_uri 指定的网址，发送一段 JSON 数据。\n&#123;      &quot;access_token&quot;:&quot;ACCESS_TOKEN&quot;,  &quot;token_type&quot;:&quot;bearer&quot;,  &quot;expires_in&quot;:2592000,  &quot;refresh_token&quot;:&quot;REFRESH_TOKEN&quot;,  &quot;scope&quot;:&quot;read&quot;,  &quot;uid&quot;:100101,  &quot;info&quot;:&#123;...&#125;&#125;\n\n上面 JSON 数据中，access_token 字段就是令牌，A 网站在后端拿到了。\n\n3. 第二种方式：隐藏式有些 Web 应用是纯前端应用，没有后端。这时就不能用上面的方式了，必须将令牌储存在前端。RFC 6749 就规定了第二种方式，允许直接向前端颁发令牌。这种方式没有授权码这个中间步骤，所以称为（授权码）”隐藏式”（implicit）。\n第一步，A 网站提供一个链接，要求用户跳转到 B 网站，授权用户数据给 A 网站使用。\nhttps://b.com/oauth/authorize?  response_type=token&amp;  client_id=CLIENT_ID&amp;  redirect_uri=CALLBACK_URL&amp;  scope=read\n\n上面 URL 中，response_type 参数为 token，表示要求直接返回令牌。\n第二步，用户跳转到 B 网站，登录后同意给予 A 网站授权。这时，B 网站就会跳回 redirect_uri 参数指定的跳转网址，并且把令牌作为 URL 参数，传给 A 网站。\nhttps://a.com/callback#token=ACCESS_TOKEN\n\n上面 URL 中，token 参数就是令牌，A 网站因此直接在前端拿到令牌。\n注意，令牌的位置是 URL 锚点（fragment），而不是查询字符串（querystring），这是因为 OAuth 2.0 允许跳转网址是 HTTP 协议，因此存在”中间人攻击”的风险，而浏览器跳转时，锚点不会发到服务器，就减少了泄漏令牌的风险。\n\n这种方式把令牌直接传给前端，是很不安全的。因此，只能用于一些安全要求不高的场景，并且令牌的有效期必须非常短，通常就是会话期间（session）有效，浏览器关掉，令牌就失效了。\n4. 第三种方式：密码式如果你高度信任某个应用，RFC 6749 也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为”密码式”（password）。\n第一步，A 网站要求用户提供 B 网站的用户名和密码。拿到以后，A 就直接向 B 请求令牌。\nhttps://oauth.b.com/token?  grant_type=password&amp;  username=USERNAME&amp;  password=PASSWORD&amp;  client_id=CLIENT_ID\n\n上面 URL 中，grant_type 参数是授权方式，这里的password表示”密码式”，username 和 password 是 B 的用户名和密码。\n第二步，B 网站验证身份通过后，直接给出令牌。注意，这时不需要跳转，而是把令牌放在 JSON 数据里面，作为 HTTP 回应，A 因此拿到令牌。\n这种方式需要用户给出自己的用户名/密码，显然风险很大，因此只适用于其他授权方式都无法采用的情况，而且必须是用户高度信任的应用。\n5. 第四种方式：凭证式最后一种方式是凭证式（client credentials），适用于没有前端的命令行应用，即在命令行下请求令牌。\n第一步，A 应用在命令行向 B 发出请求。\nhttps://oauth.b.com/token?  grant_type=client_credentials&amp;  client_id=CLIENT_ID&amp;  client_secret=CLIENT_SECRET\n\n上面 URL 中，grant_type 参数等于 client_credentials 表示采用凭证式，client_id 和 client_secret 用来让 B 确认 A 的身份。\n第二步，B 网站验证通过以后，直接返回令牌。\n这种方式给出的令牌，是针对第三方应用的，而不是针对用户的，即有可能多个用户共享同一个令牌。\n6. 令牌的使用A 网站拿到令牌以后，就可以向 B 网站的 API 请求数据了。\n此时，每个发到 API 的请求，都必须带有令牌。具体做法是在请求的头信息，加上一个 Authorization 字段，令牌就放在这个字段里面。\ncurl -H &quot;Authorization: Bearer ACCESS_TOKEN&quot; \\&quot;https://api.b.com&quot;\n\n上面命令中，ACCESS_TOKEN 就是拿到的令牌。\n7. 更新令牌令牌的有效期到了，如果让用户重新走一遍上面的流程，再申请一个新的令牌，很可能体验不好，而且也没有必要。OAuth 2.0 允许用户自动更新令牌。\n具体方法是，B 网站颁发令牌的时候，一次性颁发两个令牌，一个用于获取数据，另一个用于获取新的令牌（refresh token 字段）。令牌到期前，用户使用 refresh token 发一个请求，去更新令牌。\nhttps://b.com/oauth/token?  grant_type=refresh_token&amp;  client_id=CLIENT_ID&amp;  client_secret=CLIENT_SECRET&amp;  refresh_token=REFRESH_TOKEN\n\n上面 URL 中，grant_type 参数为 refresh_token 表示要求更新令牌，client_id 参数和 client_secret 参数用于确认身份，refresh_token 参数就是用于更新令牌的令牌。\nB 网站验证通过以后，就会颁发新的令牌。\n","categories":["知识库"]},{"title":"PaaS、IaaS 和 SaaS 的区别","url":"/posts/50925/","content":"一、概述“即服务（aaS）”通常是指由别人提供的服务，它可以让您专注于更重要的事务，例如写代码和客户关系。即服务选项还包括：基础架构即服务（IaaS）、平台即服务（PaaS）和 软件即服务（SaaS）。\n\n二、IaaS基础架构即服务（IaaS）也称为云基础架构服务，是一种经由互联网向最终用户提供 IT 基础架构的云计算形式。IaaS 通常与无服务器计算息息相关。\nIaaS 表示将由提供商通过云为您管理基础架构，包括实际的服务器、网络、虚拟化和存储。用户可通过应用编程接口（API） 或控制面板进行访问，并且基本上是租用基础架构。操作系统、应用和中间件等内容由用户管理，而提供商则提供硬件、网络、硬盘驱动器、存储和服务器，并负责处理中断、维修及硬件问题。\n三、PaaS平台即服务（PaaS）是一种由第三方提供硬件和应用软件平台的云计算形式。PaaS 主要面向开发人员和程序员，它允许用户开发、运行和管理自己的应用，而无需构建和维护通常与该流程相关联的基础架构或平台。\nPaaS 提供商会将硬件和软件托管在自己的基础架构上，并通过互联网以集成解决方案、解决方案堆栈或服务的形式将该平台交付给用户。\n举例而言，假设您构思好了自己下一步的大项目，已经写好了一个生活便利应用的代码。您对这个应用、它的目标和未来发展充满兴奋。为了避免因安装本地硬件、维护服务器、更新基础架构软件以及必须设置用于构建应用的自定义平台而带来的额外压力，您会选择由 PaaS 提供商来托管平台并提供运行代码所需的环境。\n四、SaaSSaaS 是指由提供商为您管理应用。提供商将负责处理软件更新、漏洞修复及其他常规软件维护工作，而您只用通过 Web 浏览器或 API 连接至软件。这样一来，您就无需在每台计算机上安装应用。\n","categories":["知识库"]},{"title":"代理详解","url":"/posts/53164/","content":"OSI七层网络模型\nTCP/IP 四层模型\n一、代理是什么代理（英语：Proxy）也称网络代理，是一种特殊的网络服务，允许一个网络终端（一般为客户端）通过这个服务与另一个网络终端（一般为服务器）进行非直接的连接。一些网关、路由器等网络设备具备网络代理功能。一般认为代理服务有利于保障网络终端的隐私或安全，防止攻击。\n二、正向代理与反向代理本质上来讲，代理都是存在于Client与Server之间的，但由于性质不同分为了两种。\n举个例子假设A、B、C三个人之间存在借钱的关系。\n正向代理：\n\nA需要钱，A知道C有很多钱，想向C借钱\n但是A和C有矛盾，于是A想通过B去借C的钱\nB向C借到了钱，C不知道A的存在\n最终B帮助A借到了C的钱\n\n这个过程中，B充当了代理的角色，代替A找C借钱，C对A是无感知的，这是正向代理。\n其中A是Client，C是Server。\n比如说翻墙，我们就是Client，通过代理，访问到了Google。\n反向代理：\n\nA需要钱，C有很多钱，但A不知道C有很多钱\nA找B借钱\nB知道C有很多钱\nB向C借钱，并把从C借到的钱给了A\nA不知道C的存在，以为钱是B的\n\n这个过程中，B充当了代理借钱借钱的角色，不过不是替A借的，而是找C借钱然后给A了，换言之就是代替C将钱借给了A，同时A和C相互无感知，这就是反向代理。\n其中A还是Client，C是Server。\n比如Nginx的代理就是反向代理。\n安全模型不同\n正向代理允许客户端通过它访问任意网站并且隐藏客户端自身，因此必须采取安全措施以确保仅为授权的客户端提供服务\n反向代理都对外都是透明的，访问者并不知道自己访问的是代理，访问者不知道服务节点的存在，认为处理请求的就是代理节点\n\n总而言之，正向代理是从客户端的角度出发，服务于局域网用户，以访问非特定的服务，其中最典型的例子就是翻墙。\n反向代理正好与此相反，从服务端的角度出发，服务于所有用户，隐藏实际的服务节点，服务节点的架构对用户透明，以代理节点统一对外服务。\n三、四层代理如果说网络层通信的粒度是物理终端设备的话，IP 就是标识不同物理设备的标识符。\n那么传输层通信的粒度就是进程，端口就是标识不同进程的标识符。\n四层代理的四层就是OSI七层网络模型中的 传输层。\n四层代理是基于 IP + 端口 做的代理。通过 虚拟IP + 端口 接收请求，然后再分配到真实的服务器。\n举个栗子以常见的TCP为例，从三次握手的第一次握手开始，代理设备在接收到第一个来自客户端的 SYN 请求时，即按照一定的策略选择一个被代理的后端服务器，并对报文中的 目标IP地址 进行修改（改为后端服务器IP），直接转发给该服务器。\nTCP 连接建立，即三次握手，实际上是客户端和后端服务器建立的，代理设备只是起到一个类似路由器的转发动作。\n在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的原地址进行修改。\n凤凰架构中对四层代理的描述现在所说的 “四层负载均衡” 其实是多种均衡器工作模式的统称，“四层”是说这些工作模式的共同特点是维持同一个 TCP 连接，而不是说它只工作在第四层。\n事实上，这些模式主要都工作在第二层（数据链路层，改写 MAC 地址）和第三层（网络层，改写 IP 地址）上，单纯只处理第四层（传输层，可以改写 TCP、UDP 等协议的内容和端口）的数据无法做到负载均衡的转发，因为 OSI 的下三层是媒体层（Media Layer），上四层是主机层（Host Layer），既然流量都已经到达目标主机上了，也就谈不上什么流量转发，最多只能做代理。\n四、七层代理七层代理中的七层就是OSI七层网络模型中的 应用层。\n七层代理是基于数据内容做的代理（应用层中的应用数据），最终的转发规则取决于内容的差异。\n鉴于七层应用协议非常广泛，现在的七层代理主要是指 HTTP 代理。\n与四层代理的不同七层代理必须要先和代理设备三次握手后，才能得到七层（HTTP层）的具体内容，然后再转发。\n也就是说 代理机必须要与客户端和后端服务器的机器都要建立连接。\n显然，七层代理对代理设备的性能要求要高于四层代理。\n七层代理的好处平常使用的Nginx，用作代理服务器的时候一般都是工作在第七层的。使用Nginx，我们可以代理静态文件、ajax后台接口、CDN重定向等，都是在传输层之上理解内容后做的工作。\n（1）使整个网络更智能化例如访问一个网站的用户流量，可以通过七层的方式，将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；\n将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术。\n当然这只是七层应用的一个小案例，从技术原理上，这种方式可以对客户端的请求和服务器的响应进行任意意义上的修改，极大的提升了应用系统在网络层的灵活性。\n很多在后台，例如Nginx或者Apache上部署的功能可以前移到负载均衡设备上，例如客户请求中的Header重写，服务器响应中的关键字过滤或者内容插入等功能。\n（2）安全性网络中最常见的SYN Flood攻击，即黑客控制众多源客户端，使用虚假IP地址对同一目标发送SYN攻击。\n通常这种攻击会大量发送SYN报文，耗尽服务器上的相关资源，以达到Denial of Service(DoS)的目的。\n从技术原理上也可以看出，四层模式下这些SYN攻击都会被转发到后端的服务器上。\n而七层模式下这些SYN攻击自然在负载均衡设备上就截止，不会影响后台服务器的正常运营。\n另外负载均衡设备可以在七层层面设定多种策略，过滤特定报文，例如SQL Injection等应用层面的特定攻击手段，从应用层面进一步提高系统整体安全。\n小结现在的七层代理，主要还是着重于应用HTTP协议，所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。\n四层代理则对应其他TCP应用，例如基于C/S开发的系统。\n","categories":["知识库"]},{"title":"最大QPS推算","url":"/posts/42351/","content":"一、RTRT = ThreadCPUTime + ThreadWaitTime\nRT（Response Time）即响应时间。\n可以理解为系统从输入到输出的时间间隔，系统是指一个网站或其他类型的软件应用，或者指某个设备，比如手机，手机界面也有响应时间。\n所以RT是一个比较广泛的概念，不过本章探讨的RT都是特指互联网应用。\n服务器端RT：指从服务器接收请求到该请求响应的全部数据被发往客户端的时间\n客户端RT：从客户端（例如浏览器）发起请求到客户端接收到请求响应的全部数据的时间间隔。\n服务端RT + 网络开销 ≈ 客户端RT\n也就是说一个差的网络环境会导致两个RT差距悬殊，比如从国外到国内的RT远大于国内网络环境中的RT。\n客户端的RT直接影响用户体验，要想降低客户端RT，需要考虑亮点：\n\n服务器端RT\n网络\n\n对于网络来说，常见的优化方式有CDN、AND和专线，分别适用于不同的场景。\n对于服务端的RT来说，主要看服务端的做法。\n从上边的公式可以看出，要想降低 RT，可以通过降低 ThreadCPUTime 和 ThreadWaitTime 的方式。\n后面将从 RT、ThreadCPUTime 和 ThreadWaitTime 来进行讨论。\n二、单线程QPS上一节中简单地定义了RT由 ThreadCPUTime 和 ThreadWaitTime 两部分组成。\n如果系统里只有一个线程或者一个进程（该进程中只有一个线程），那么最大QPS是多少？\n假设RT是199ms，其中ThreadCPUTime 为19ms，ThreadWaitTime 为 180ms，\n那么1000ms内系统可以接收的最大请求数就是 1000ms/(19ms+180ms) ≈ 5.025。\n所以单线程的 QPS = 1000ms / RT\n三、最佳线程数还是上边的例子，假设CPU核数是1.\n假设只有一个线程，这个线程在执行某个请求时，CPU真正花在该线程上的时间就是 ThreadCPUTime，即19ms。\n那么在整个RT生命周期中，还有180ms的 ThreadWaitTime，这个时间CPU在做什么？\n在理想情况下，抛开系统层面的问题，可以认为CPU在这180ms内什么都没做，至少对于业务来说什么都没做。\n1核的情况由于每个请求的接收，CPU只需要工作19ms，所以在180ms的时间内，可以认为系统还可以额外接收 180ms/19ms ≈ 9 个请求。\n由于在同步模型中，一个请求需要一个线程来处理，因此需要额外的9个线程来处理这些。\n所以1核的最佳线程数就是：** (180ms + 19ms) / 19ms ≈ 10 **\n多线程之后，ThreadCPUTime 从19ms变成了20ms，这1ms的差值表示多线程之后线程上下文切换、GC等带来的额外开销（JVM情况下），这里的1ms只是一个假设，可以把它看作n。\n2核的情况1核的情况下可以有10个线程，在理想情况下，2核的最佳线程数可以认为：\n2 * (180ms + 20ms) / 20ms = 20\nCPU利用率上面举的例子都是在 CPU满载 的情况下。\n有时由于某个瓶颈，导致CPU得不到有效利用。比如2核的CPU，因为某个资源，各自职能使用一半的效能，这样总的CPU利用率就变成了 50%。\n这种情况下，最佳线程数是：50% * 2 * (180ms + 20ms) / 20ms = 10\n最佳线程数公式根据上面的分析，最佳线程数公式如下：最佳线程数 = (RT / ThreadCPUTime) * CPU核数 * CPU利用率\n当然，最佳线程数公式不是任意推测的，在一些权威著作上都有论述，所以接下来，假设最佳线程数的公式是正确的。\n四、最大QPS最大QPS公式推导假设知道最佳线程数，也知道每个线程的QPS，那么线程数✖️每个线程的QPS，即这台机器在最佳线程数下的QPS。\n\n把分子、分母去约数\n\n化简后得\n\n从公式可可以看出，决定QPS的是 ThreadCPUTime、CPU核数、CPU利用率。\nCPU核数师由硬件决定的，ThreadCPUTime 和 CPU利用率与我们的代码息息相关。\n虽然宏观上是正确的，但是推导的过程还是由一点瑕疵，因为多线程下的 ThreadCPUTime （比如高并发下的GC次数增加、线程上下文切换等，导致ThreadCPUTime增加）和单线程的 ThreadCPUTime 是不一样的，所以导致推算出的结果存在误差。\nThreadCPUTimeThreadCPUTime 不只是业务逻辑所消耗的CPU时间，而是一次请求中所有环节上消耗的CPU时间之和。\n比如，在web应用中，一个请求过来的HTTP的解析所消耗的CPU时间、这个请求中RPC调用的encode和decode所消耗的CPU时间，这些都是 ThreadCPUTime 的一部分。\n所以影响 ThreadCPUTime 的因素有两个：\n\n数据结构\n算法\n\n例如：Hash问题、排序与查找问题、状态机问题、序列化问题等。\nCPU利用率CUP利用率不高的情况是经常发生的，以下因素都会影响CPU利用率，从而影响系统可以支持的最大QPS。\n（1）IO能力\n磁盘IO\n网络IO\n带宽：比如某大促压测时，由于某个应用放在Tair中的数据量大，导致Tair的机器网卡跑满\n网络链路：还是大促，借用了其他核心交换机下的机器，导致客户端RT明显增加\n\n\n\n（2）数据库连接池并发能力 = PoolWaitTime / RT(client) * PoolSize\n（3）内存不足GC大量占用CPU，导致给业务逻辑使用的CPU利用率下降，而且GC时还满足Amdahl定律所定义的场景。\n（4）共享资源的竞争比如各种锁策略（读写锁、锁分离等）、阻塞队列等。\n（5）依赖的服务如RPC调用其他服务、DB等。\n（6）线程数或进程数，乃至编程模型（同步模型、异步模型，某些场景适合同步模型，某些适合异步模型）在压测过程中，出现最多的是网络IO层面的问题、GC大量占用 ThreadCPUTime 之类的问题。\nCPU核数1.Amdahl定律（安达尔定律）Amdahl定律是用来描述可伸缩性的。\n可伸缩性指：当增加计算资源的时候，如CPU、内存、带宽等，QPS能够相应地进行改进。\nAmdahl在论文中指出，可伸缩性是指在一个系统中，基于可并行化和串行化的组件各自所占的比例，当程序获得额外的计算资源（如CPU、内存等）时，系统理论上能够获得的加速值（QPS或者其他指标可以翻几倍）。\n用公式来表示，如果F表示必须串行化执行的比例，那么在一个N核的CPU的机器中，\n加速比 = 1 / (F + (1 - F) / N)\n这个公式代表的意义比较广泛，在项目管理中有一句类似的话：\n一个女人生一个孩子，需要9个月，但是永远不可能让9女人在一个月内就生一个孩子。\n拿公式套用一下，这里F = 100%，9 个女人表示N = 9，于是就有 1 / (100% + (1 - 100%) / 9) = 1，所以加速比为1，等于没加速。\n\n 这里需要注意的是，这个公式描述的是，在增加资源的情况下系统的加速比，而不是在资源不变的情况下优化数据结构和算法之后带来的提升。\n优化数据结构和算法带来的提升要看前文最大QPS公式。\n不过这两个公式也不是完全没有联系，在增加资源的情况下，它们的联系还是比较紧密的。\n\n2.Gustafson定律（古斯塔夫森定律）该定律是对Amdahl定律的补充：S(P) = P - a * (P - 1)\nP是处理器核数，a是串行时间占总执行时间的比例。\n上边的案例套用公式，P为女人个数9，串行比例100%。\nSpeedup = 9 - 100% * (9 - 1) = 1。\n也就是无法加速。\n这两个定律有关系吗？有，它们是相辅相成的。\n前者从串行和并行执行时间的角度来推导，后者从串行和并行的计算量角度来推导，不管哪个角度，最终的结果是一样的。\n3.CPU核数与Amdahl定律关系通过最大QPS公式，笔者发现，在CPU Time和CPU利用率不变的情况下，核数越多，QPS就越大。比如核数从1到4，在CPU Time和CPU利用率不变的情况下，加速比应该是4，所以QPS应该增加4倍。\n这是资源增加（CPU核数增加）的情况下的加速比，也可以通过Amdahl定律来衡量，考虑串行和并行的比例在增加资源的情况下是否会改变。也就是要考虑在N增加的情况下，F受哪些因素的影响：\n只要F大于0，最大QPS就不会翻4倍。\n一个公式说要增加4倍，一个定理说没有4倍，互相矛盾？\n其实事情是这样的，通过最大QPS公式，我们得知，如果 ThreadCPUTime 和CPU利用率不变，核数从1增加到4，QPS会相应地增加4倍。但是在实际情况下，当核数增加时，ThreadCPUTime 和CPU利用率大部分时候是变化的，所以前面的假设不成立，即一般场景下最大QPS不能增加4倍。\n为什么增加计算资源时，最大QPS公式中的CPU Time和CPU利用率会变化，F也会变化呢？我们可以从宏观上分析一下，增加计算资源时，达到满载：\n\nQPS会更高，单位时间内产生的对象会更多。在同等条件下，minor GC被触发的次数增加，还有些场景发生过对象多到响应没返回它们就进了“老年代”，从而full GC被触发。宏观上，这是属于串行的部分，对于Amdahl公式来说F会受到影响，对于最大QPS公式来说，CPU Time和CPU利用率也受到影响。\n在同步模型下大量的线程在完成一次请求中，上下文被切换的次数大大增加。\n尤其是在有串行模块的时候，串行的执行和等待时间增加，F会变化，某些场景下CPU利用率也达不到理想效果，这取决于你的代码。这也是要做锁分离、为什么要缩小同步块的原因。当然还有锁自身的优化，比如偏向、自旋、读写分离等技术，都是为了不断地减少Amdahl定律中的F，也是为了减少ThreadCPUTime（锁本身的优化），提高CPU利用率（使用锁的方法的优化）。\n锁本身的优化最为津津乐道的是自旋、偏向、自适应，这些知识点请看网上的synchronized分析，还有reetrantLock的代码及AQS论文。\n使用锁的优化方法最常见的是缩小锁区间、锁分离、无锁化、volatile。\n\n\n\n所以在增加计算资源时，更高的并发产生，会引起最大QPS公式中两个参数的变化，也会引起Amdahl定律中F值的变化，同时公式和定律变化的趋势是相同的。Amdahl定律是得到广泛认可的，也是得到数据验证的。最大QPS公式好像没有人验证过，这里引用一个比较有名的测试结果。\n\n从图中可以看出\n\n当计算资源（处理器数量）增加时，在串行部分比例不变的情况下，CPU利用率下降。\n当计算资源（处理器数量）增加时，串行占的比例越大，CPU利用率下降得越多。\n\n原文连接：http://bjxiaoyu.com/newsde.html?id=77\n","categories":["知识库"]},{"title":"HTTPS 优化","url":"/posts/43954/","content":"由裸数据传输的 HTTP 协议转成加密数据传输的 HTTPS 协议，给应用数据套了个「保护伞」，提高安全性的同时也带来了性能消耗。\n因为 HTTPS 相比 HTTP 协议多一个 TLS 协议握手过程，目的是为了通过非对称加密握手协商或者交换出对称加密密钥，这个过程最长可以花费掉 2 RTT，接着后续传输的应用数据都得使用对称加密密钥来加密/解密。\n为了数据的安全性，我们不得不使用 HTTPS 协议，至今大部分网址都已从 HTTP 迁移至 HTTPS 协议，因此针对 HTTPS 的优化是非常重要的。\n这次，就从多个角度来优化 HTTPS。\n\n一、分析性能损耗既然要对 HTTPS 优化，那得清楚哪些步骤会产生性能消耗，再对症下药。\n产生性能消耗的两个环节：\n\n 第一个环节， TLS 协议握手过程；\n 第二个环节，握手后的对称加密报文传输。\n\n对于第二环节，现在主流的对称加密算法 AES、ChaCha20 性能都是不错的，而且一些 CPU 厂商还针对它们做了硬件级别的优化，因此这个环节的性能消耗可以说非常地小。\n而第一个环节，TLS 协议握手过程不仅增加了网络延时（最长可以花费掉 2 RTT），而且握手过程中的一些步骤也会产生性能损耗，比如：\n\n 对于 ECDHE 密钥协商算法，握手过程中会客户端和服务端都需要临时生成椭圆曲线公私钥；\n 客户端验证证书时，会访问 CA 获取 CRL 或者 OCSP，目的是验证服务器的证书是否有被吊销；\n 双方计算 Pre-Master，也就是会话密钥；\n\n为了大家更清楚这些步骤在 TLS 协议握手的哪一个阶段，我画出了这幅图：\n\n二、硬件优化玩游戏时，如果我们怎么都战胜不了对方，那么有一个最有效、最快的方式来变强，那就是「充钱」，如果还是不行，那说明你充的钱还不够多。\n对于计算机里也是一样，软件都是跑在物理硬件上，硬件越牛逼，软件跑的也越快，所以如果要优化 HTTPS 优化，最直接的方式就是花钱买性能参数更牛逼的硬件。\n但是花钱也要花对方向，HTTPS 协议是计算密集型，而不是 I/O 密集型，所以不能把钱花在网卡、硬盘等地方，应该花在 CPU 上。\n一个好的 CPU，可以提高计算性能，因为 HTTPS 连接过程中就有大量需要计算密钥的过程，所以这样可以加速 TLS 握手过程。\n另外，如果可以，应该选择可以支持 AES-NI 特性的 CPU，因为这种款式的 CPU 能在指令级别优化了 AES 算法，这样便加速了数据的加解密传输过程。\n如果你的服务器是 Linux 系统，那么你可以使用下面这行命令查看 CPU 是否支持 AES-NI 指令集：\n\n如果我们的 CPU 支持 AES-NI 特性，那么对于对称加密的算法应该选择 AES 算法。否则可以选择 ChaCha20 对称加密算法，因为 ChaCha20 算法的运算指令相比 AES 算法会对 CPU 更友好一点。\n三、软件优化如果公司预算充足对于新的服务器是可以考虑购买更好的 CPU，但是对于已经在使用的服务器，硬件优化的方式可能就不太适合了，于是就要从软件的方向来优化了。\n软件的优化方向可以分层两种，一个是软件升级，一个是协议优化。\n先说第一个软件升级，软件升级就是将正在使用的软件升级到最新版本，因为最新版本不仅提供了最新的特性，也优化了以前软件的问题或性能。比如：\n\n 将 Linux 内核从 2.x 升级到 4.x；\n 将 OpenSSL 从 1.0.1 升级到 1.1.1；\n …\n\n看似简单的软件升级，对于有成百上千服务器的公司来说，软件升级也跟硬件升级同样是一个棘手的问题，因为要实行软件升级，会花费时间和人力，同时也存在一定的风险，也可能会影响正常的线上服务。\n既然如此，我们把目光放到协议优化，也就是在现有的环节下，通过较小的改动，来进行优化。\n四、协议优化协议的优化就是对「密钥交换过程」进行优化。\n密钥交换算法优化TLS 1.2 版本如果使用的是 RSA 密钥交换算法，那么需要 4 次握手，也就是要花费 2 RTT，才可以进行应用数据的传输，而且 RSA 密钥交换算法不具备前向安全性。\n总之使用 RSA 密钥交换算法的 TLS 握手过程，不仅慢，而且安全性也不高。\n因此如果可以，尽量选用 ECDHE 密钥交换算法替换 RSA 算法，因为该算法由于支持「False Start」，它是“抢跑”的意思，客户端可以在 TLS 协议的第 3 次握手后，第 4 次握手前，发送加密的应用数据，以此将 TLS 握手的消息往返由 2 RTT 减少到 1 RTT，而且安全性也高，具备前向安全性。\nECDHE 算法是基于椭圆曲线实现的，不同的椭圆曲线性能也不同，应该尽量选择 x25519 曲线，该曲线是目前最快的椭圆曲线。\n比如在 Nginx 上，可以使用 ssl_ecdh_curve 指令配置想使用的椭圆曲线，把优先使用的放在前面：\n\n对于对称加密算法方面，如果对安全性不是特别高的要求，可以选用 AES_128_GCM，它比 AES_256_GCM 快一些，因为密钥的长度短一些。\n比如在 Nginx 上，可以使用 ssl_ciphers 指令配置想使用的非对称加密算法和对称加密算法，也就是密钥套件，而且把性能最快最安全的算法放在最前面：\n\nTLS 升级当然，如果可以，直接把 TLS 1.2 升级成 TLS 1.3，TLS 1.3 大幅度简化了握手的步骤，完成 TLS 握手只要 1 RTT，而且安全性更高。\n在 TLS 1.2 的握手中，一般是需要 4 次握手，先要通过 Client Hello （第 1 次握手）和 Server Hello（第 2 次握手） 消息协商出后续使用的加密算法，再互相交换公钥（第 3 和 第 4 次握手），然后计算出最终的会话密钥，下图的左边部分就是 TLS 1.2 的握手过程：\n\n上图的右边部分就是 TLS 1.3 的握手过程，可以发现 TLS 1.3 把 Hello 和公钥交换这两个消息合并成了一个消息，于是这样就减少到只需 1 RTT 就能完成 TLS 握手。\n怎么合并的呢？具体的做法是，客户端在 Client Hello 消息里带上了支持的椭圆曲线，以及这些椭圆曲线对应的公钥。\n服务端收到后，选定一个椭圆曲线等参数，然后返回消息时，带上服务端这边的公钥。经过这 1 个 RTT，双方手上已经有生成会话密钥的材料了，于是客户端计算出会话密钥，就可以进行应用数据的加密传输了。\n而且，TLS1.3 对密码套件进行“减肥”了，对于密钥交换算法，废除了不支持前向安全性的 RSA 和 DH 算法，只支持 ECDHE 算法。\n对于对称加密和签名算法，只支持目前最安全的几个密码套件，比如 openssl 中仅支持下面 5 种密码套件：\n\n TLS_AES_256_GCM_SHA384\n TLS_CHACHA20_POLY1305_SHA256\n TLS_AES_128_GCM_SHA256\n TLS_AES_128_CCM_8_SHA256\n TLS_AES_128_CCM_SHA256\n\n之所以 TLS1.3 仅支持这么少的密码套件，是因为 TLS1.2 由于支持各种古老且不安全的密码套件，中间人可以利用降级攻击，伪造客户端的 Client Hello 消息，替换客户端支持的密码套件为一些不安全的密码套件，使得服务器被迫使用这个密码套件进行 HTTPS 连接，从而破解密文。\n五、证书优化为了验证的服务器的身份，服务器会在 TSL 握手过程中，把自己的证书发给客户端，以此证明自己身份是可信的。\n对于证书的优化，可以有两个方向：\n\n 一个是证书传输，\n 一个是证书验证；\n\n证书传输优化要让证书更便于传输，那必然是减少证书的大小，这样可以节约带宽，也能减少客户端的运算量。所以，对于服务器的证书应该选择 椭圆曲线（ECDSA）证书，而不是 RSA 证书，因为在相同安全强度下， ECC 密钥长度比 RSA 短的多。 \n证书验证优化客户端在验证证书时，是个复杂的过程，会走证书链逐级验证，验证的过程不仅需要「用 CA 公钥解密证书」以及「用签名算法验证证书的完整性」，而且为了知道证书是否被 CA 吊销，客户端有时还会再去访问 CA， 下载 CRL 或者 OCSP 数据，以此确认证书的有效性。\n这个访问过程是 HTTP 访问，因此又会产生一系列网络通信的开销，如 DNS 查询、建立连接、收发数据等。\nCRLCRL 称为证书吊销列表（Certificate Revocation List），这个列表是由 CA 定期更新，列表内容都是被撤销信任的证书序号，如果服务器的证书在此列表，就认为证书已经失效，不在的话，则认为证书是有效的。\n\n但是 CRL 存在两个问题：\n\n 第一个问题，由于 CRL 列表是由 CA 维护的，定期更新，如果一个证书刚被吊销后，客户端在更新 CRL 之前还是会信任这个证书，实时性较差；\n 第二个问题，随着吊销证书的增多，列表会越来越大，下载的速度就会越慢，下载完客户端还得遍历这么大的列表，那么就会导致客户端在校验证书这一环节的延时很大，进而拖慢了 HTTPS 连接。\n\nOCSP因此，现在基本都是使用 OCSP ，名为在线证书状态协议（Online Certificate Status Protocol）来查询证书的有效性，它的工作方式是向 CA 发送查询请求，让 CA 返回证书的有效状态。\n\n不必像 CRL 方式客户端需要下载大大的列表，还要从列表查询，同时因为可以实时查询每一张证书的有效性，解决了 CRL 的实时性问题。\nOCSP 需要向 CA 查询，因此也是要发生网络请求，而且还得看 CA 服务器的“脸色”，如果网络状态不好，或者 CA 服务器繁忙，也会导致客户端在校验证书这一环节的延时变大。\nOCSP Stapling于是为了解决这一个网络开销，就出现了 OCSP Stapling，其原理是：服务器向 CA 周期性地查询证书状态，获得一个带有时间戳和签名的响应结果并缓存它。\n\n当有客户端发起连接请求时，服务器会把这个「响应结果」在 TLS 握手过程中发给客户端。由于有签名的存在，服务器无法篡改，因此客户端就能得知证书是否已被吊销了，这样客户端就不需要再去查询。\n六、会话复用TLS 握手的目的就是为了协商出会话密钥，也就是对称加密密钥，那我们如果我们把首次 TLS 握手协商的对称加密密钥缓存起来，待下次需要建立 HTTPS 连接时，直接「复用」这个密钥，不就减少 TLS 握手的性能损耗了吗？\n这种方式就是会话复用（TLS session resumption），会话复用分两种：\n\n 第一种叫 Session ID；\n 第二种叫 Session Ticket；\n\nSession ID\nSession ID 的工作原理是，客户端和服务器首次 TLS 握手连接后，双方会在内存缓存会话密钥，并用唯一的 Session ID 来标识，Session ID 和会话密钥相当于 key-value 的关系。\n当客户端再次连接时，hello 消息里会带上 Session ID，服务器收到后就会从内存找，如果找到就直接用该会话密钥恢复会话状态，跳过其余的过程，只用一个消息往返就可以建立安全通信。当然为了安全性，内存中的会话密钥会定期失效。\n\n但是它有两个缺点：\n\n 服务器必须保持每一个客户端的会话密钥，随着客户端的增多，服务器的内存压力也会越大。\n 现在网站服务一般是由多台服务器通过负载均衡提供服务的，客户端再次连接不一定会命中上次访问过的服务器，于是还要走完整的 TLS 握手过程；\n\nSession Ticket为了解决 Session ID 的问题，就出现了 Session Ticket，服务器不再缓存每个客户端的会话密钥，而是把缓存的工作交给了客户端，类似于 HTTP 的 Cookie。\n客户端与服务器首次建立连接时，服务器会加密「会话密钥」作为 Ticket 发给客户端，交给客户端缓存该 Ticket。\n客户端再次连接服务器时，客户端会发送 Ticket，服务器解密后就可以获取上一次的会话密钥，然后验证有效期，如果没问题，就可以恢复会话了，开始加密通信。\n\n对于集群服务器的话，要确保每台服务器加密 「会话密钥」的密钥是一致的，这样客户端携带 Ticket 访问任意一台服务器时，都能恢复会话。\nSession ID 和 Session Ticket 都不具备前向安全性，因为一旦加密「会话密钥」的密钥被破解或者服务器泄漏「会话密钥」，前面劫持的通信密文都会被破解。\n同时应对重放攻击也很困难，这里简单介绍下重放攻击工作的原理。\n\n假设 Alice 想向 Bob 证明自己的身份。Bob 要求 Alice 的密码作为身份证明，爱丽丝应尽全力提供（可能是在经过如哈希函数的转换之后）。与此同时，Eve 窃听了对话并保留了密码（或哈希)。\n交换结束后，Eve（冒充 Alice ）连接到 Bob。当被要求提供身份证明时，Eve 发送从 Bob 接受的最后一个会话中读取的 Alice 的密码（或哈希），从而授予 Eve 访问权限。\n重放攻击的危险之处在于，如果中间人截获了某个客户端的 Session ID 或 Session Ticket 以及 POST 报文，而一般 POST 请求会改变数据库的数据，中间人就可以利用此截获的报文，不断向服务器发送该报文，这样就会导致数据库的数据被中间人改变了，而客户是不知情的。\n避免重放攻击的方式就是需要对会话密钥设定一个合理的过期时间。\nPre-shared Key前面的 Session ID 和 Session Ticket 方式都需要在 1 RTT 才能恢复会话。\n而 TLS1.3 更为牛逼，对于重连 TLS1.3 只需要 0 RTT，原理和 Ticket 类似，只不过在重连时，客户端会把 Ticket 和 HTTP 请求一同发送给服务端，这种方式叫 Pre-shared Key。\n\n同样的，Pre-shared Key 也有重放攻击的危险。\n\n如上图，假设中间人通过某种方式，截获了客户端使用会话重用技术的 POST 请求，通常 POST 请求是会改变数据库的数据，然后中间人就可以把截获的这个报文发送给服务器，服务器收到后，也认为是合法的，于是就恢复会话，致使数据库的数据又被更改，但是此时用户是不知情的。\n所以，应对重放攻击可以给会话密钥设定一个合理的过期时间，以及只针对安全的 HTTP 请求如 GET/HEAD 使用会话重用。\n七、总结对于硬件优化的方向，因为 HTTPS 是属于计算密集型，应该选择计算力更强的 CPU，而且最好选择支持 AES-NI 特性的 CPU，这个特性可以在硬件级别优化 AES 对称加密算法，加快应用数据的加解密。\n对于软件优化的方向，如果可以，把软件升级成较新的版本，比如将 Linux 内核 2.X 升级成 4.X，将 openssl 1.0.1 升级到 1.1.1，因为新版本的软件不仅会提供新的特性，而且还会修复老版本的问题。\n对于协议优化的方向：\n\n密钥交换算法应该选择 ECDHE 算法，而不用 RSA 算法，因为 ECDHE 算法具备前向安全性，而且客户端可以在第三次握手之后，就发送加密应用数据，节省了 1 RTT。\n 将 TSL1.2 升级 TSL1.3，因为 TSL1.3 的握手过程只需要 1 RTT，而且安全性更强。\n\n对于证书优化的方向：\n\n 服务器应该选用 ECDSA 证书，而非 RSA 证书，因为在相同安全级别下，ECC 的密钥长度比 RSA 短很多，这样可以提高证书传输的效率；\n 服务器应该开启 OCSP Stapling 功能，由服务器预先获得 OCSP 的响应，并把响应结果缓存起来，这样 TLS 握手的时候就不用再访问 CA 服务器，减少了网络通信的开销，提高了证书验证的效率；\n\n对于重连 HTTPS 时，我们可以使用一些技术让客户端和服务端使用上一次 HTTPS 连接使用的会话密钥，直接恢复会话，而不用再重新走完整的 TLS 握手过程。\n常见的会话重用技术有 Session ID 和 Session Ticket，用了会话重用技术，当再次重连 HTTPS 时，只需要 1 RTT 就可以恢复会话。对于 TLS1.3 使用 Pre-shared Key 会话重用技术，只需要 0 RTT 就可以恢复会话。\n这些会话重用技术虽然好用，但是存在一定的安全风险，它们不仅不具备前向安全，而且有重放攻击的风险，所以应当对会话密钥设定一个合理的过期时间。 \n","categories":["经验文档"]},{"title":"总结 httpclient 资源释放和连接复用","url":"/posts/20915/","content":"最近修改同事代码时遇到一个问题，通过 httpclient 默认配置产生的 httpclient 如果不关闭，会导致连接无法释放，很快打满服务器连接（内嵌 Jetty 配置了 25 连接上限），主动关闭问题解决；后来优化为通过连接池生成 httpclient 后，如果关闭 httpclient 又会导致连接池关闭，后面新的 httpclient 也无法再请求，这里总结遇到的一些问题和疑问。\n\n官网示例中的以下三个 close 分别释放了什么资源，是否可以省略，以及在什么时机调用，使用连接池时有区别么？\n作为 RPC 通信客户端，如何复用 TCP 连接？\n\n一、资源释放CloseableHttpClient httpclient = HttpClients.createDefault();HttpGet httpget = new HttpGet(&quot;http://localhost/&quot;);CloseableHttpResponse response = httpclient.execute(httpget);try &#123;    HttpEntity entity = response.getEntity();    if (entity != null) &#123;        InputStream instream = entity.getContent();        try &#123;            // do something useful        &#125; finally &#123;            instream.close();        &#125;    &#125;&#125; finally &#123;    response.close();&#125;// httpclient.close();\n\n首先需要了解默认配置 createDefault 和使用了 custom 连接池（文章最后的 HttpClientUtil）两种情况的区别，通过源码可以看到前者也创建了连接池，最大连接20个，单个 host最大2个，但是区别在于每次创建的 httpclient 都自己维护了自己的连接池，而 custom 连接池时所有 httpclient 共用同一个连接池，这是在 api 使用方面需要注意的地方，要避免每次请求新建连接池、关闭连接池，造成性能问题。\n\nThe difference between closing the content stream and closing the response is that the former will attempt to keep the underlying connection alive by consuming the entity content while the latter immediately shuts down and discards the connection.\n\n第一个 close 是读取 http 正文的数据流，类似的还有响应写入流，都需要主动关闭，如果是使用 EntityUtils.toString(response.getEntity(), &quot;UTF-8&quot;); 的方式，其内部会进行关闭。如果还有要读/写的数据、或不主动关闭，相当于 http 请求事务未处理完成，这时通过其他方式关闭（第二个 close）相当于异常终止，会导致该连接无法被复用，对比下面两段日志。\n第一个 close 未调用时，第二个 close 调用，连接无法被复用，kept alive 0。\no.a.http.impl.execchain.MainClientExec   : Connection can be kept alive indefinitelyh.i.c.DefaultManagedHttpClientConnection : http-outgoing-0: Close connectiono.a.http.impl.execchain.MainClientExec   : Connection discardedh.i.c.PoolingHttpClientConnectionManager : Connection released: [id: 0][route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 0; route allocated: 0 of 2; total allocated: 0 of 20]\n\n第一个 close 正常调用时，第二个 close 调用，连接可以被复用，kept alive 1。\no.a.http.impl.execchain.MainClientExec   : Connection can be kept alive indefinitelyh.i.c.PoolingHttpClientConnectionManager : Connection [id: 0][route: &#123;&#125;-&gt;http://127.0.0.1:8080] can be kept alive indefinitelyh.i.c.DefaultManagedHttpClientConnection : http-outgoing-0: set socket timeout to 0h.i.c.PoolingHttpClientConnectionManager : Connection released: [id: 0][route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 1; route allocated: 1 of 2; total allocated: 1 of 20]\n\n第二个 close 是强行制止和释放连接到连接池，相当于对第一个 close 的保底操作（上面关闭了这个似乎没必要了？），结合上面引用的官方文档写到 immediately shuts down and discards the connection，这里如果判断需要 keep alive 实际也不会关闭 TCP 连接，因为通过 netstat 可以看到，第二段日志后在终端可以继续观察到连接：\n# netstat -n | grep tcp4 | grep 8080tcp4       0      0  127.0.0.1.8080         127.0.0.1.51003        ESTABLISHEDtcp4       0      0  127.0.0.1.51003        127.0.0.1.8080         ESTABLISHED\n\n在 SOF 上可以搜到这段话，但是感觉和上面观察到的并不相符？\n\nThe underlying HTTP connection is still held by the response object to allow the response content to be streamed directly from the network socket. In order to ensure correct deallocation of system resources, the user MUST call CloseableHttpResponse#close() from a finally clause. Please note that if response content is not fully consumed the underlying connection cannot be safely re-used and will be shut down and discarded by the connection manager.\n\n第三个 clsoe，也就是 httpclient.close 会彻底关闭连接池，以及其中所有连接，一般情况下，只有在关闭应用时调用以释放资源（补充：当 httpClientBuilder.setConnectionManagerShared(true) 时，并不会关闭连接池）。\n二、连接复用根据 http 协议 1.1 版本，各个 web 服务器都默认支持 keepalive，因此当 http 请求正常完成后，服务器不会主动关闭 tcp（直到空闲超时或数量达到上限），使连接会保留一段时间，前面我们也知道 httpclient 在判断可以 keepalive 后，即使调用了 close 也不会关闭 tcp 连接（可以认为 release 到连接池）。为了管理这些保留的连接，以及方便 api 调用，一般设置一个全局的连接池，并基于该连接池提供 httpclient 实例，这样就不需要考虑维护 httpclient 实例生命周期，随用随取（方便状态管理？），此外考虑到 http 的单路性，一个请求响应完成结束后，该连接才可以再次复用，因此连接池的最大连接数决定了并发处理量，该配置也是一种保护机制，超出上限的请求会被阻塞，也可以配合熔断组件使用，当服务方慢、或不健康时熔断降级。\n最后还有一个问题，观察到 keepalive 的 tcp 连接过一段时间后会变成如下状态：\n# netstat -n | grep tcp4 | grep 8080tcp4       0      0  127.0.0.1.8080         127.0.0.1.51866        FIN_WAIT_2tcp4       0      0  127.0.0.1.51866        127.0.0.1.8080         CLOSE_WAIT\n\n可以看出服务器经过一段时间，认为该连接空闲，因此主动关闭，收到对方响应后进入 FIN_WAIT_2 状态（等待对方也发起关闭），而客户端进入 CLOSE_WAIT 状态后却不再发起自己这一方的关闭请求，这时双方处于半关闭。官方文档解释如下：\n\nOne of the major shortcomings of the classic blocking I/O model is that the network socket can react to I/O events only when blocked in an I/O operation. When a connection is released back to the manager, it can be kept alive however it is unable to monitor the status of the socket and react to any I/O events. If the connection gets closed on the server side, the client side connection is unable to detect the change in the connection state (and react appropriately by closing the socket on its end).\n\n这需要有定期主动做一些检测和关闭动作，从这个角度考虑，默认配置产生的 HttpClient 没有这一功能，不应该用于生产环境，下面这个监控线程可以完成该工作，包含它的完整的 HttpUtil 从文章最后连接获取。\npublic static class IdleConnectionMonitorThread extends Thread &#123;  private final HttpClientConnectionManager connMgr;  private volatile boolean shutdown;  public IdleConnectionMonitorThread(HttpClientConnectionManager connMgr) &#123;    super();    this.connMgr = connMgr;  &#125;  @Override  public void run() &#123;    try &#123;      while (!shutdown) &#123;        synchronized (this) &#123;          wait(30 * 1000);          // Close expired connections          connMgr.closeExpiredConnections();          // Optionally, close connections          // that have been idle longer than 30 sec          connMgr.closeIdleConnections(30, TimeUnit.SECONDS);        &#125;      &#125;    &#125; catch (InterruptedException ex) &#123;      // terminate    &#125;  &#125;\n\n最后展示一个完整的示例，首先多线程发起两个请求，看到创建两个连接，30秒之后再发起一个请求，可以复用之前其中一个连接，另一个连接因空闲被关闭，随后最后等待 2 分钟后再发起一个请求，由于之前连接已过期失效，重新创建连接。\n\n并发两个请求\n16:54:44.504  [       Thread-4] : Connection request: [route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 0; route allocated: 0 of 150; total allocated: 0 of 150]16:54:44.504  [       Thread-5] : Connection request: [route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 0; route allocated: 0 of 150; total allocated: 0 of 150]16:54:44.515  [       Thread-5] : Connection leased: [id: 1][route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 0; route allocated: 2 of 150; total allocated: 2 of 150]16:54:44.515  [       Thread-4] : Connection leased: [id: 0][route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 0; route allocated: 2 of 150; total allocated: 2 of 150]16:54:44.517  [       Thread-5] : Opening connection &#123;&#125;-&gt;http://127.0.0.1:808016:54:44.517  [       Thread-4] : Opening connection &#123;&#125;-&gt;http://127.0.0.1:808016:54:44.519  [       Thread-4] : Connecting to /127.0.0.1:808016:54:44.519  [       Thread-5] : Connecting to /127.0.0.1:808016:54:44.521  [       Thread-5] : Connection established 127.0.0.1:52421&lt;-&gt;127.0.0.1:808016:54:44.521  [       Thread-4] : Connection established 127.0.0.1:52420&lt;-&gt;127.0.0.1:8080....16:54:49.486  [           main] : [leased: 2; pending: 0; available: 0; max: 150]16:54:49.630  [       Thread-4] : Connection can be kept alive indefinitely16:54:49.630  [       Thread-5] : Connection can be kept alive indefinitely16:54:49.633  [       Thread-4] : Connection [id: 0][route: &#123;&#125;-&gt;http://127.0.0.1:8080] can be kept alive indefinitely16:54:49.633  [       Thread-5] : Connection [id: 1][route: &#123;&#125;-&gt;http://127.0.0.1:8080] can be kept alive indefinitely16:54:49.633  [       Thread-4] : http-outgoing-0: set socket timeout to 016:54:49.633  [       Thread-5] : http-outgoing-1: set socket timeout to 016:54:49.633  [       Thread-4] : Connection released: [id: 0][route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 1; route allocated: 2 of 150; total allocated: 2 of 150]16:54:49.633  [       Thread-5] : Connection released: [id: 1][route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 2; route allocated: 2 of 150; total allocated: 2 of 150]16:54:54.488  [           main] : [leased: 0; pending: 0; available: 2; max: 150]\n\n#netstat -n | grep tcp4 | grep 8080tcp4       0      0  127.0.0.1.8080         127.0.0.1.52421        ESTABLISHEDtcp4       0      0  127.0.0.1.8080         127.0.0.1.52420        ESTABLISHEDtcp4       0      0  127.0.0.1.52421        127.0.0.1.8080         ESTABLISHEDtcp4       0      0  127.0.0.1.52420        127.0.0.1.8080         ESTABLISHED\n下一个请求\n16:55:14.489  [       Thread-6] : Connection request: [route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 2; route allocated: 2 of 150; total allocated: 2 of 150]16:55:14.491  [       Thread-6] : http-outgoing-1 &lt;&lt; &quot;[read] I/O error: Read timed out&quot;16:55:14.491  [       Thread-6] : Connection leased: [id: 1][route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 1; route allocated: 2 of 150; total allocated: 2 of 150]16:55:14.491  [       Thread-6] : http-outgoing-1: set socket timeout to 016:55:14.492  [       Thread-6] : http-outgoing-1: set socket timeout to 8000.....16:55:19.501  [           main] : [leased: 1; pending: 0; available: 1; max: 150]16:55:19.504  [       Thread-6] : Connection can be kept alive indefinitely16:55:19.504  [       Thread-6] : Connection [id: 1][route: &#123;&#125;-&gt;http://127.0.0.1:8080] can be kept alive indefinitely16:55:19.505  [       Thread-6] : http-outgoing-1: set socket timeout to 016:55:19.505  [       Thread-6] : Connection released: [id: 1][route: &#123;&#125;-&gt;http://127.0.0.1:8080][total kept alive: 2; route allocated: 2 of 150; total allocated: 2 of 150]16:55:24.504  [           main] : [leased: 0; pending: 0; available: 2; max: 150]\n\n#netstat -n | grep tcp4 | grep 8080tcp4       0      0  127.0.0.1.8080         127.0.0.1.52421        ESTABLISHEDtcp4       0      0  127.0.0.1.8080         127.0.0.1.52420        ESTABLISHEDtcp4       0      0  127.0.0.1.52421        127.0.0.1.8080         ESTABLISHEDtcp4       0      0  127.0.0.1.52420        127.0.0.1.8080         ESTABLISHED\n\n复用了上面的连接，下面是随后逐步超时的日志。\n16:55:39.513  [           main] : [leased: 0; pending: 0; available: 2; max: 150]16:55:44.491  [       Thread-8] : Closing expired connections16:55:44.492  [       Thread-8] : Closing connections idle longer than 30 SECONDS16:55:44.492  [       Thread-8] : http-outgoing-0: Close connection16:55:44.518  [           main] : [leased: 0; pending: 0; available: 1; max: 150]....16:56:09.535  [           main] : [leased: 0; pending: 0; available: 1; max: 150]16:56:14.499  [       Thread-8] : Closing expired connections16:56:14.499  [       Thread-8] : Closing connections idle longer than 30 SECONDS16:56:14.499  [       Thread-8] : http-outgoing-1: Close connection16:56:14.540  [           main] : [leased: 0; pending: 0; available: 0; max: 150]\n\n分别对应状态如下，可以看到复用了 52421，随后 52420 空闲超时被回收，以及最后 52421 也被回收。\n#netstat -n | grep tcp4 | grep 8080tcp4       0      0  127.0.0.1.8080         127.0.0.1.52421        ESTABLISHEDtcp4       0      0  127.0.0.1.52421        127.0.0.1.8080         ESTABLISHEDtcp4       0      0  127.0.0.1.52420        127.0.0.1.8080         TIME_WAIT...#netstat -n | grep tcp4 | grep 8080tcp4       0      0  127.0.0.1.52421        127.0.0.1.8080         TIME_WAIT\n最后一个请求后，日志省略，可以看到是新的连接 52443。\nnetstat -n | grep tcp4 | grep 8080tcp4       0      0  127.0.0.1.8080         127.0.0.1.52443        ESTABLISHEDtcp4       0      0  127.0.0.1.52443        127.0.0.1.8080         ESTABLISHED\n\n文章所有演示用例和封装类链接：https://github.com/JeffreyPeng/http-client-case/\n参考：\nhttps://hc.apache.org/httpcomponents-client-4.5.x/tutorial/html/fundamentals.html\nhttps://hc.apache.org/httpcomponents-client-ga/tutorial/html/connmgmt.html\nhttps://www.baeldung.com/httpclient-connection-management\nhttps://www.jianshu.com/p/56881801d02c\nhttps://zhuanlan.zhihu.com/p/61423830\n","categories":["经验文档"]},{"title":"每天100w次登陆请求, 8G 内存该如何设置JVM参数？","url":"/posts/44012/","content":"每天100w次登陆请求, 8G 内存该如何设置JVM参数，大概可以分为以下8个步骤。\nStep1：新系统上线如何规划容量？1.套路总结任何新的业务系统在上线以前都需要去估算服务器配置和JVM的内存参数，这个容量与资源规划并不仅仅是系统架构师的随意估算的，需要根据系统所在业务场景去估算，推断出来一个系统运行模型，评估JVM性能和GC频率等等指标。以下是我结合大牛经验以及自身实践来总结出来的一个建模步骤：\n\n计算业务系统每秒钟创建的对象会佔用多大的内存空间，然后计算集群下的每个系统每秒的内存佔用空间（对象创建速度）\n设置一个机器配置，估算新生代的空间，比较不同新生代大小之下，多久触发一次MinorGC。\n为了避免频繁GC，就可以重新估算需要多少机器配置，部署多少台机器，给JVM多大内存空间，新生代多大空间。\n根据这套配置，基本可以推算出整个系统的运行模型，每秒创建多少对象，1s以后成为垃圾，系统运行多久新生代会触发一次GC，频率多高。\n\n2.套路实战——以登录系统为例有些同学看到这些步骤还是发憷，说的好像是那么回事，一到实际项目中到底怎麽做我还是不知道！\n光说不练假把式，以登录系统为例模拟一下推演过程：\n\n假设每天100w次登陆请求，登陆峰值在早上，预估峰值时期每秒100次登陆请求。\n假设部署3台服务器，每台机器每秒处理30次登陆请求，假设一个登陆请求需要处理1秒钟，JVM新生代里每秒就要生成30个登陆对象，1s之后请求完毕这些对象成为了垃圾。\n一个登陆请求对象假设20个字段，一个对象估算500字节，30个登陆佔用大约15kb，考虑到RPC和DB操作，网络通信、写库、写缓存一顿操作下来，可以扩大到20-50倍，大约1s产生几百k-1M数据。\n假设2C4G机器部署，分配2G堆内存，新生代则只有几百M，按照1s1M的垃圾产生速度，几百秒就会触发一次MinorGC了。\n假设4C8G机器部署，分配4G堆内存，新生代分配2G，如此需要几个小时才会触发一次MinorGC。\n\n所以，可以粗略的推断出来一个每天100w次请求的登录系统，按照4C8G的3实例集群配置，分配4G堆内存、2G新生代的JVM，可以保障系统的一个正常负载。\n基本上把一个新系统的资源评估了出来，所以搭建新系统要每个实例需要多少容量多少配置，集群配置多少个实例等等这些，并不是拍拍脑袋和胸脯就可以决定的下来的。\nStep2：该如何进行垃圾回收器的选择？吞吐量还是响应时间首先引入两个概念：吞吐量和低延迟\n吞吐量 = CPU在用户应用程序运行的时间 / （CPU在用户应用程序运行的时间 + CPU垃圾回收的时间）\n响应时间 = 平均每次的GC的耗时\n通常，吞吐优先还是响应优先这个在JVM中是一个两难之选。\n堆内存增大，gc一次能处理的数量变大，吞吐量大；但是gc一次的时间会变长，导致后面排队的线程等待时间变长；相反，如果堆内存小，gc一次时间短，排队等待的线程等待时间变短，延迟减少，但一次请求的数量变小（并不绝对符合）。\n无法同时兼顾，是吞吐优先还是响应优先，这是一个需要权衡的问题。\n垃圾回收器设计上的考量\nJVM在GC时不允许一边垃圾回收，一边还创建新对象（就像不能一边打扫卫生，还在一边扔垃圾）。\nJVM需要一段Stop the world的暂停时间，而STW会造成系统短暂停顿不能处理任何请求；\n新生代收集频率高，性能优先，常用复制算法；老年代频次低，空间敏感，避免复制方式。\n所有垃圾回收器的涉及目标都是要让GC频率更少，时间更短，减少GC对系统影响！\n\nCMS和G1目前主流的垃圾回收器配置是新生代采用ParNew，老年代采用CMS组合的方式，或者是完全采用G1回收器，\n从未来的趋势来看，G1是官方维护和更为推崇的垃圾回收器。\n\n业务系统:\n\n延迟敏感的推荐CMS；\n大内存服务，要求高吞吐的，采用G1回收器！\n\nCMS垃圾回收器的工作机制CMS主要是针对老年代的回收器，老年代是标记-清除，默认会在一次FullGC算法后做整理算法，清理内存碎片。\n\n\n\nCMS GC\n描述\nStop the world\n速度\n\n\n\n1.开始标记\n初始标记仅标记GCRoots能直接关联到的对象，速度很快\nYes\n很快\n\n\n2.并发标记\n并发标记阶段就是进行GCRoots Tracing的过程\nNo\n慢\n\n\n3.重新标记\n重新标记阶段则是为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录。\nYes\n很快\n\n\n4.垃圾回收\n并发清理垃圾对象(标记清除算法)\nNo\n慢\n\n\n\n优点：并发收集、主打“低延时” 。在最耗时的两个阶段都没有发生STW，而需要STW的阶段都以很快速度完成。\n缺点：1、消耗CPU；2、浮动垃圾；3、内存碎片\n适用场景：重视服务器响应速度，要求系统停顿时间最短。\n\n总之：\n业务系统，延迟敏感的推荐CMS；\n大内存服务，要求高吞吐的，采用G1回收器！\nStep3：如何对各个分区的比例、大小进行规划一般的思路为:\n首先，JVM最重要最核心的参数是去评估内存和分配，第一步需要指定堆内存的大小，这个是系统上线必须要做的，-Xms 初始堆大小，-Xmx 最大堆大小，后台Java服务中一般都指定为系统内存的一半，过大会佔用服务器的系统资源，过小则无法发挥JVM的最佳性能。\n其次，需要指定-Xmn新生代的大小，这个参数非常关键，灵活度很大，虽然sun官方推荐为3/8大小，但是要根据业务场景来定，针对于无状态或者轻状态服务（现在最常见的业务系统如Web应用）来说，一般新生代甚至可以给到堆内存的3/4大小；而对于有状态服务（常见如IM服务、网关接入层等系统）新生代可以按照默认比例1/3来设置。服务有状态，则意味著会有更多的本地缓存和会话状态信息常驻内存，应为要给老年代设置更大的空间来存放这些对象。\n最后，是设置-Xss栈内存大小，设置单个线程栈大小，默认值和JDK版本、系统有关，一般默认512~1024kb。一个后台服务如果常驻线程有几百个，那麽栈内存这边也会佔用了几百M的大小。\n\n\n\nJVM参数\n描述\n默认\n推荐\n\n\n\n-Xms\nJava堆内存的大小\nOS内存64/1\nOS内存一半\n\n\n-Xmx\nJava堆内存的最大大小\nOS内存4/1\nOS内存一半\n\n\n-Xmn\nJava堆内存中的新生代大小，扣除新生代剩下的就是老年代的内存大小了\n跌认堆的1/3\nsun推荐3/8\n\n\n-Xss\n每个线程的栈内存大小\n和idk有关\nsun\n\n\n对于8G内存，一般分配一半的最大内存就可以了,因为机器本上还要占用一定内存，一般是分配4G内存给JVM，\n引入性能压测环节，测试同学对登录接口压至1s内60M的对象生成速度，采用ParNew+CMS的组合回收器，\n正常的JVM参数配置如下：\n-Xms3072M -Xmx3072M -Xss1M -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M -XX:SurvivorRatio=8 \n\n这样设置可能会由于动态对象年龄判断原则导致频繁full gc。为啥呢？\n压测过程中，短时间（比如20S后）Eden区就满了，此时再运行的时候对象已经无法分配，会触发MinorGC，\n假设在这次GC后S1装入100M，马上过20S又会触发一次MinorGC，多出来的100M存活对象+S1区的100M已经无法顺利放入到S2区，此时就会触发JVM的动态年龄机制，将一批100M左右的对象推到老年代保存，持续运行一段时间，系统可能一个小时候内就会触发一次FullGC。\n按照默认8:1:1的比例来分配时,  survivor区只有 1G的 10%左右，也就是几十到100M，\n如果  每次minor GC垃圾回收过后进入survivor对象很多，并且survivor对象大小很快超过 Survivor 的 50% ，  那么会触发动态年龄判定规则，让部分对象进入老年代.\n而一个GC过程中，可能部分WEB请求未处理完毕,  几十兆对象，进入survivor的概率，是非常大的，甚至是一定会发生的.\n如何解决这个问题呢？为了让对象尽可能的在新生代的eden区和survivor区, 尽可能的让survivor区内存多一点,达到200兆左右,\n于是我们可以更新下JVM参数设置：\n-Xms3072M -Xmx3072M -Xmn2048M -Xss1M -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M  -XX:SurvivorRatio=8  说明：‐Xmn2048M ‐XX:SurvivorRatio=8 年轻代大小2g，eden与survivor的比例为8:1:1，也就是1.6g:0.2g:0.2g\n\n\nsurvivor达到200m，如果几十兆对象到底survivor， survivor 也不一定超过 50%\n这样可以防止每次垃圾回收过后，survivor对象太早超过 50% ,\n这样就降低了因为对象动态年龄判断原则导致的对象频繁进入老年代的问题，\n什么是JVM动态年龄判断规则呢？对象进入老年代的动态年龄判断规则（动态晋升年龄计算阈值）：Minor GC 时，Survivor 中年龄 1 到 N 的对象大小超过 Survivor 的 50% 时，则将大于等于年龄 N 的对象放入老年代。\n核心的优化策略是：是让短期存活的对象尽量都留在survivor里，不要进入老年代，这样在minor gc的时候这些对象都会被回收，不会进到老年代从而导致full gc。\n应该如何去评估新生代内存和分配合适？这里特别说一下，JVM最重要最核心的参数是去评估内存和分配，\n第一步需要指定堆内存的大小，这个是系统上线必须要做的，-Xms 初始堆大小，-Xmx 最大堆大小，\n后台Java服务中一般都指定为系统内存的一半，过大会佔用服务器的系统资源，过小则无法发挥JVM的最佳性能。\n其次需要指定-Xmn新生代的大小，这个参数非常关键，灵活度很大，虽然sun官方推荐为3/8大小，但是要根据业务场景来定：\n\n针对于无状态或者轻状态服务（现在最常见的业务系统如Web应用）来说，一般新生代甚至可以给到堆内存的3/4大小；\n而对于有状态服务（常见如IM服务、网关接入层等系统）新生代可以按照默认比例1/3来设置。\n\n服务有状态，则意味著会有更多的本地缓存和会话状态信息常驻内存，应为要给老年代设置更大的空间来存放这些对象。\nstep4：栈内存大小多少比较合适？-Xss栈内存大小，设置单个线程栈大小，默认值和JDK版本、系统有关，一般默认512~1024kb。一个后台服务如果常驻线程有几百个，那麽栈内存这边也会佔用了几百M的大小。\nstep5：对象年龄应该为多少才移动到老年代比较合适？假设一次minor gc要间隔二三十秒，并且，大多数对象一般在几秒内就会变为垃圾，\n如果对象这么长时间都没被回收，比如2分钟没有回收，可以认为这些对象是会存活的比较长的对象，从而移动到老年代，而不是继续一直占用survivor区空间。\n所以，可以将默认的15岁改小一点，比如改为5，\n那么意味着对象要经过5次minor gc才会进入老年代，整个时间也有一两分钟了（5*30s= 150s），和几秒的时间相比，对象已经存活了足够长时间了。\n所以：可以适当调整JVM参数如下：\n‐Xms3072M ‐Xmx3072M ‐Xmn2048M ‐Xss1M ‐XX:MetaspaceSize=256M ‐XX:MaxMetaspaceSize=256M ‐XX:SurvivorRatio=8 ‐XX:MaxTenuringThreshold=5 \n\nstep6：多大的对象，可以直接到老年代比较合适？对于多大的对象直接进入老年代(参数-XX:PretenureSizeThreshold)，一般可以结合自己系统看下有没有什么大对象 生成，预估下大对象的大小，一般来说设置为1M就差不多了，很少有超过1M的大对象，\n所以：可以适当调整JVM参数如下：\n‐Xms3072M ‐Xmx3072M ‐Xmn2048M ‐Xss1M ‐XX:MetaspaceSize=256M ‐XX:MaxMetaspaceSize=256M ‐XX:SurvivorRatio=8 ‐XX:MaxTenuringThreshold=5 ‐XX:PretenureSizeThreshold=1M\n\nstep7：垃圾回收器CMS老年代的参数优化JDK8默认的垃圾回收器是-XX:+UseParallelGC(年轻代)和-XX:+UseParallelOldGC(老年代)，\n如果内存较大(超过4个G，只是经验 值)，还是建议使用G1.\n这里是4G以内，又是主打“低延时” 的业务系统，可以使用下面的组合：\nParNew+CMS(-XX:+UseParNewGC -XX:+UseConcMarkSweepGC)\n\n新生代的采用ParNew回收器，工作流程就是经典复制算法，在三块区中进行流转回收，只不过采用多线程并行的方式加快了MinorGC速度。\n老生代的采用CMS。再去优化老年代参数：比如老年代默认在标记清除以后会做整理，还可以在CMS的增加GC频次还是增加GC时长上做些取舍，\n如下是响应优先的参数调优：\nXX:CMSInitiatingOccupancyFraction=70\n\n设定CMS在对内存占用率达到70%的时候开始GC(因为CMS会有浮动垃圾,所以一般都较早启动GC)\nXX:+UseCMSInitiatinpOccupancyOnly\n\n和上面搭配使用，否则只生效一次\n-XX:+AlwaysPreTouch\n\n强制操作系统把内存真正分配给IVM，而不是用时才分配。\n综上，只要年轻代参数设置合理，老年代CMS的参数设置基本都可以用默认值，如下所示：\n‐Xms3072M ‐Xmx3072M ‐Xmn2048M ‐Xss1M ‐XX:MetaspaceSize=256M ‐XX:MaxMetaspaceSize=256M ‐XX:SurvivorRatio=8  ‐XX:MaxTenuringThreshold=5 ‐XX:PretenureSizeThreshold=1M ‐XX:+UseParNewGC ‐XX:+UseConcMarkSweepGC ‐XX:CMSInitiatingOccupancyFraction=70 ‐XX:+UseCMSInitiatingOccupancyOnly ‐XX:+AlwaysPreTouch\n\n参数解释\n1.‐Xms3072M ‐Xmx3072M 最小最大堆设置为3g，最大最小设置为一致防止内存抖动\n2.‐Xss1M 线程栈1m\n3.‐Xmn2048M ‐XX:SurvivorRatio=8 年轻代大小2g，eden与survivor的比例为8:1:1，也就是1.6g:0.2g:0.2g\n4.-XX:MaxTenuringThreshold=5 年龄为5进入老年代 5.‐XX:PretenureSizeThreshold=1M 大于1m的大对象直接在老年代生成\n6.‐XX:+UseParNewGC ‐XX:+UseConcMarkSweepGC 使用ParNew+cms垃圾回收器组合\n7.‐XX:CMSInitiatingOccupancyFraction=70 老年代中对象达到这个比例后触发fullgc\n8.‐XX:+UseCMSInitiatinpOccupancyOnly 老年代中对象达到这个比例后触发fullgc，每次\n9.‐XX:+AlwaysPreTouch 强制操作系统把内存真正分配给IVM，而不是用时才分配。\nstep8：配置OOM时候的内存dump文件和GC日志额外增加了GC日志打印、OOM自动dump等配置内容，帮助进行问题排查\n-XX:+HeapDumpOnOutOfMemoryError\n\n在Out Of Memory，JVM快死掉的时候，输出Heap Dump到指定文件。\n不然开发很多时候还真不知道怎么重现错误。\n路径只指向目录，JVM会保持文件名的唯一性，叫java_pid${pid}.hprof。\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$&#123;LOGDIR&#125;/\n\n因为如果指向特定的文件，而文件已存在，反而不能写入。\n输出4G的HeapDump，会导致IO性能问题，在普通硬盘上，会造成20秒以上的硬盘IO跑满，\n需要注意一下，但在容器环境下，这个也会影响同一宿主机上的其他容器。\nGC的日志的输出也很重要：\n-Xloggc:/dev/xxx/gc.log -XX:+PrintGCDateStamps -XX:+PrintGCDetails\n\nGC的日志实际上对系统性能影响不大，打日志对排查GC问题很重要。\n一份通用的JVM参数模板\n一般来说，大企业或者架构师团队，都会为项目的业务系统定制一份较为通用的JVM参数模板，但是许多小企业和团队可能就疏于这一块的设计，如果老板某一天突然让你负责定制一个新系统的JVM参数，你上网去搜大量的JVM调优文章或博客，结果发现都是零零散散的、不成体系的JVM参数讲解，根本下不了手，这个时候你就需要一份较为通用的JVM参数模板了，不能保证性能最佳，但是至少能让JVM这一层是稳定可控的，\n在这里给大家总结了一份模板：\n\n基于4C8G系统的ParNew+CMS回收器模板（响应优先），新生代大小根据业务灵活调整！\n-Xms4g-Xmx4g-Xmn2g-Xss1m-XX:SurvivorRatio=8-XX:MaxTenuringThreshold=10-XX:+UseConcMarkSweepGC-XX:CMSInitiatingOccupancyFraction=70-XX:+UseCMSInitiatingOccupancyOnly-XX:+AlwaysPreTouch-XX:+HeapDumpOnOutOfMemoryError-verbose:gc-XX:+PrintGCDetails-XX:+PrintGCDateStamps-XX:+PrintGCTimeStamps-Xloggc:gc.log\n\n如果是GC的吞吐优先，推荐使用G1，基于8C16G系统的G1回收器模板：G1收集器自身已经有一套预测和调整机制了，因此我们首先的选择是相信它，\n即调整-XX:MaxGCPauseMillis=N参数，这也符合G1的目的——让GC调优尽量简单！\n同时也不要自己显式设置新生代的大小（用-Xmn或-XX:NewRatio参数），\n如果人为干预新生代的大小，会导致目标时间这个参数失效。\n-Xms8g-Xmx8g-Xss1m-XX:+UseG1GC-XX:MaxGCPauseMillis=150-XX:InitiatingHeapOccupancyPercent=40-XX:+HeapDumpOnOutOfMemoryError-verbose:gc-XX:+PrintGCDetails-XX:+PrintGCDateStamps-XX:+PrintGCTimeStamps-Xloggc:gc.log\n\n\n\n\nG1参数\n描述\n默认值\n\n\n\nXX:MaxGCPauseMillis=N\n最大GC停顿时间。柔性目标，JVM满足90%，不保证100%。\n200\n\n\n-XX:nitiatingHeapOccupancyPercent=n\n当整个堆的空间使用百分比超过这个值时，就会融发MixGC\n45\n\n\n针对-XX:MaxGCPauseMillis来说，参数的设置带有明显的倾向性：调低↓：延迟更低，但MinorGC频繁，MixGC回收老年代区减少，增大Full GC的风险。调高↑：单次回收更多的对象，但系统整体响应时间也会被拉长。\n针对InitiatingHeapOccupancyPercent来说，调参大小的效果也不一样：调低↓：更早触发MixGC，浪费cpu。调高↑：堆积过多代回收region，增大FullGC的风险。\n调优总结系统在上线前的综合调优思路：\n1、业务预估：根据预期的并发量、平均每个任务的内存需求大小，然后评估需要几台机器来承载，每台机器需要什么样的配置。\n2、容量预估：根据系统的任务处理速度，然后合理分配Eden、Surivior区大小，老年代的内存大小。\n3、回收器选型：响应优先的系统，建议采用ParNew+CMS回收器；吞吐优先、多核大内存(heap size≥8G)服务，建议采用G1回收器。\n4、优化思路：让短命对象在MinorGC阶段就被回收（同时回收后的存活对象&lt;Survivor区域50%，可控制保留在新生代），长命对象尽早进入老年代，不要在新生代来回复制；尽量减少Full GC的频率，避免FGC系统的影响。\n5、到目前为止，总结到的调优的过程主要基于上线前的测试验证阶段，所以我们尽量在上线之前，就将机器的JVM参数设置到最优！\nJVM调优只是一个手段，但并不一定所有问题都可以通过JVM进行调优解决，大多数的Java应用不需要进行JVM优化，我们可以遵循以下的一些原则：\n\n上线之前，应先考虑将机器的JVM参数设置到最优；\n减少创建对象的数量（代码层面）；\n减少使用全局变量和大对象（代码层面）；\n优先架构调优和代码调优，JVM优化是不得已的手段（代码、架构层面）；\n分析GC情况优化代码比优化JVM参数更好（代码层面）；\n\n通过以上原则，我们发现，其实最有效的优化手段是架构和代码层面的优化，而JVM优化则是最后不得已的手段，也可以说是对服务器配置的最后一次“压榨”。\n什么是ZGC？ZGC （Z Garbage Collector）是一款由Oracle公司研发的，以低延迟为首要目标的一款垃圾收集器。\n它是基于动态Region内存布局，（暂时）不设年龄分代，使用了读屏障、染色指针和内存多重映射等技术来实现可并发的标记-整理算法的收集器。\n在 JDK 11 新加入，还在实验阶段，\n主要特点是：回收TB级内存（最大4T），停顿时间不超过10ms。\n优点：低停顿，高吞吐量， ZGC 收集过程中额外耗费的内存小\n缺点：浮动垃圾\n目前使用的非常少，真正普及还是需要写时间的。\n如何选择垃圾收集器？在真实场景中应该如何去选择呢，下面给出几种建议，希望对你有帮助：\n1、如果你的堆大小不是很大（比如 100MB ），选择串行收集器一般是效率最高的。参数：-XX:+UseSerialGC 。\n2、如果你的应用运行在单核的机器上，或者你的虚拟机核数只有 单核，选择串行收集器依然是合适的，这时候启用一些并行收集器没有任何收益。参数：-XX:+UseSerialGC 。\n3、如果你的应用是“吞吐量”优先的，并且对较长时间的停顿没有什么特别的要求。选择并行收集器是比较好的。参数：-XX:+UseParallelGC 。\n4、如果你的应用对响应时间要求较高，想要较少的停顿。甚至 1 秒的停顿都会引起大量的请求失败，那么选择 G1 、 ZGC 、 CMS 都是合理的。虽然这些收集器的 GC 停顿通常都比较短，但它需要一些额外的资源去处理这些工作，通常吞吐量会低一些。参数：-XX:+UseConcMarkSweepGC 、 -XX:+UseG1GC 、 -XX:+UseZGC 等。从上面这些出发点来看，我们平常的 Web 服务器，都是对响应性要求非常高的。\n选择性其实就集中在 CMS、G1、ZGC 上。而对于某些定时任务，使用并行收集器，是一个比较好的选择。\nHotspot为什么使用元空间替换了永久代？什么是元空间？什么是永久代？为什么用元空间代替永久代？\n我们先回顾一下方法区吧,看看虚拟机运行时数据内存图，如下:\n\n方法区和堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据。\n什么是永久代？它和方法区有什么关系呢？\n如果在HotSpot虚拟机上开发、部署，很多程序员都把方法区称作永久代。\n可以说方法区是规范，永久代是Hotspot针对该规范进行的实现。\n在Java7及以前的版本，方法区都是永久代实现的。\n什么是元空间？它和方法区有什么关系呢？\n对于Java8，HotSpots取消了永久代，取而代之的是元空间(Metaspace)。\n换句话说，就是方法区还是在的，只是实现变了，从永久代变为元空间了。\n为什么使用元空间替换了永久代？\n永久代的方法区，和堆使用的物理内存是连续的。\n永久代是通过以下这两个参数配置大小的~\n\n-XX:PremSize：设置永久代的初始大小\n-XX:MaxPermSize: 设置永久代的最大值，默认是64M\n\n对于永久代，如果动态生成很多class的话，就很可能出现java.lang.OutOfMemoryError:PermGen space错误，因为永久代空间配置有限嘛。最典型的场景是，在web开发比较多jsp页面的时候。\nJDK8之后，方法区存在于元空间(Metaspace)。\n物理内存不再与堆连续，而是直接存在于本地内存中，理论上机器内存有多大，元空间就有多大。\n可以通过以下的参数来设置元空间的大小：\n\n-XX:MetaspaceSize，初始空间大小，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize时，适当提高该值。\n-XX:MaxMetaspaceSize，最大空间，默认是没有限制的。\n-XX:MinMetaspaceFreeRatio，在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集\n-XX:MaxMetaspaceFreeRatio，在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集\n\n所以，为什么使用元空间替换永久代？\n表面上看是为了避免OOM异常。\n因为通常使用PermSize和MaxPermSize设置永久代的大小就决定了永久代的上限，但是不是总能知道应该设置为多大合适, 如果使用默认值很容易遇到OOM错误。\n当使用元空间时，可以加载多少类的元数据就不再由MaxPermSize控制, 而由系统的实际可用空间来控制啦。\n什么是Stop The World ? 什么是OopMap？什么是安全点？进行垃圾回收的过程中，会涉及对象的移动。\n为了保证对象引用更新的正确性，必须暂停所有的用户线程，像这样的停顿，虚拟机设计者形象描述为Stop The World。也简称为STW。\n在HotSpot中，有个数据结构（映射表）称为OopMap。\n一旦类加载动作完成的时候，HotSpot就会把对象内什么偏移量上是什么类型的数据计算出来，记录到OopMap。\n在即时编译过程中，也会在特定的位置生成 OopMap，记录下栈上和寄存器里哪些位置是引用。\n这些特定的位置主要在：1.循环的末尾（非 counted 循环）\n2.方法临返回前 / 调用方法的call指令后\n3.可能抛异常的位置\n这些位置就叫作安全点(safepoint)。\n用户程序执行时并非在代码指令流的任意位置都能够在停顿下来开始垃圾收集，而是必须是执行到安全点才能够暂停。\n","categories":["经验文档"]},{"title":"Leaf——美团点评分布式ID生成系统","url":"/posts/53061/","content":"美团博客：https://tech.meituan.com/2017/04/21/mt-leaf.html\nGithub：https://github.com/Meituan-Dianping/Leaf\n","categories":["运维"]},{"title":"Sentinel","url":"/posts/38559/","content":"Spring Cloud Alibaba组件版本对应说明\nSentinel 官方文档\nSentinel 集群限流\n本文依赖版本：\n\nSpring Boot：2.6.6\n\n一、Sentinel 依赖&lt;dependencyManagement&gt;    &lt;dependency&gt;        &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;        &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt;        &lt;version&gt;2021.0.1.0&lt;/version&gt;    &lt;/dependency&gt;&lt;/dependencyManagement&gt;&lt;dependency&gt;    &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt;    &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt;\n\n本文采用 Nacos 作为数据源\n&lt;dependency&gt;    &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt;    &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt;    &lt;version&gt;1.8.4&lt;/version&gt;&lt;/dependency&gt;\n\n二、安装 Nacoshttp://jzcupid.cn/posts/36267/\n三、安装 Sentinel 控制台Sentinel  控制台配置\n1. 基于Docker镜像拉取镜像docker pull bladex/sentinel-dashboard:1.8.0\n\n官方没有提供镜像\n运行控制台docker run -d --name sentinel -p 8858:8858 \\   -e auth.enabled=&quot;true&quot; \\   -e sentinel.dashboard.auth.username=admin \\   -e sentinel.dashboard.auth.password=admin \\   -e server.servlet.session.timeout=7200 \\   bladex/sentinel-dashboard:1.8.0\n\n访问控制台：http://localhost:8858/\n2. 基于源码源码仓库\n控制台源码位于 sentinel-dashboard 模块。\n默认在控制台修改规则后，无法持久化到数据源（例如：Nacos），对源码进行改造，使用 push 模式，使规则修改后自动同步到数据源，这里以 Nacos 为例。\n1）修改 POM将 POM 中下边依赖的 &lt;scope&gt;test&lt;/scope&gt; 去掉\n&lt;dependency&gt;  &lt;groupId&gt;com.alibaba.csp&lt;/groupId&gt;  &lt;artifactId&gt;sentinel-datasource-nacos&lt;/artifactId&gt;  &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;\n\n将 sentinel-dashboard/src/test/java/com/alibaba/csp/sentinel/dashboard/rule/nacos 目录整个拷贝到 sentinel-dashboard/src/main/java/com/alibaba/csp/sentinel/dashboard/rule 目录下。\n⚠️ nacos 最终是在 rule 目录下的。\n2）修改配置NacosConfig 添加如下配置，同时由于该类默认加载 Nacos 配置时用的 nacosConfigService() 方法比较简单，可以自行封装该方法。\n@Beanpublic Converter&lt;List&lt;DegradeRuleEntity&gt;, String&gt; degradeRuleEntityEncoder() &#123;  return JSON::toJSONString;&#125;@Beanpublic Converter&lt;String, List&lt;DegradeRuleEntity&gt;&gt; degradeRuleEntityDecoder() &#123;  return s -&gt; JSON.parseArray(s, DegradeRuleEntity.class);&#125;@Beanpublic Converter&lt;List&lt;ParamFlowRuleEntity&gt;, String&gt; paramFlowRuleEntityEncoder() &#123;  return JSON::toJSONString;&#125;@Beanpublic Converter&lt;String, List&lt;ParamFlowRuleEntity&gt;&gt; paramFlowRuleEntityDecoder() &#123;  return s -&gt; JSON.parseArray(s, ParamFlowRuleEntity.class);&#125;@Beanpublic Converter&lt;List&lt;AuthorityRuleEntity&gt;, String&gt; authorityRuleEntityEncoder() &#123;  return JSON::toJSONString;&#125;@Beanpublic Converter&lt;String, List&lt;AuthorityRuleEntity&gt;&gt; authorityRuleEntityDecoder() &#123;  return s -&gt; JSON.parseArray(s, AuthorityRuleEntity.class);&#125;\n\nNacosConfigUtil 类添加\npublic static final String DEGRADE_DATA_ID_POSTFIX = &quot;-degrade-rules&quot;;public static final String AUTHORITY_DATA_ID_POSTFIX = &quot;-authority-rules&quot;;\n\n3）自动推送 Flow 规则修改 com.alibaba.csp.sentinel.dashboard.controller.v2.FlowControllerV2 类。\n找到如下代码\n@Autowired@Qualifier(&quot;flowRuleDefaultProvider&quot;)private DynamicRuleProvider&lt;List&lt;FlowRuleEntity&gt;&gt; ruleProvider;@Autowired@Qualifier(&quot;flowRuleDefaultPublisher&quot;)private DynamicRulePublisher&lt;List&lt;FlowRuleEntity&gt;&gt; rulePublisher;\n\n修改为\n@Autowired@Qualifier(&quot;flowRuleNacosProvider&quot;)private DynamicRuleProvider&lt;List&lt;FlowRuleEntity&gt;&gt; ruleProvider;@Autowired@Qualifier(&quot;flowRuleNacosPublisher&quot;)private DynamicRulePublisher&lt;List&lt;FlowRuleEntity&gt;&gt; rulePublisher;\n\n修改的 Bean Name 可在上边拷贝的类中找到。\n前端页面修改\n将 src/main/webapp/resources/app/scripts/directives/sidebar/sidebar.html 中如下代码的注释去掉。\n&lt;!--&lt;li ui-sref-active=&quot;active&quot; ng-if=&quot;entry.appType==0&quot;&gt;--&gt;  &lt;!--&lt;a ui-sref=&quot;dashboard.flow(&#123;app: entry.app&#125;)&quot;&gt;--&gt;    &lt;!--&lt;i class=&quot;glyphicon glyphicon-filter&quot;&gt;&lt;/i&gt;&amp;nbsp;&amp;nbsp;流控规则 V1&lt;/a&gt;--&gt;&lt;!--&lt;/li&gt;--&gt;\n\n4）自动推送 Degrade 规则新增 Publisher\nimport com.alibaba.csp.sentinel.dashboard.datasource.entity.rule.DegradeRuleEntity;import com.alibaba.csp.sentinel.dashboard.rule.DynamicRulePublisher;import com.alibaba.csp.sentinel.datasource.Converter;import com.alibaba.csp.sentinel.util.AssertUtil;import com.alibaba.nacos.api.config.ConfigService;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * @Auther jd */@Component(&quot;degradeRuleNacosPublisher&quot;)public class DegradeRuleNacosPublisher implements DynamicRulePublisher&lt;List&lt;DegradeRuleEntity&gt;&gt; &#123;  @Autowired  private ConfigService configService;  @Autowired  private Converter&lt;List&lt;DegradeRuleEntity&gt;, String&gt; converter;  @Override  public void publish(String app, List&lt;DegradeRuleEntity&gt; rules) throws Exception &#123;    AssertUtil.notEmpty(app, &quot;app name cannot be empty&quot;);    if (rules == null) &#123;      return;    &#125;    boolean successed = configService.publishConfig(app + NacosConfigUtil.DEGRADE_DATA_ID_POSTFIX,        NacosConfigUtil.GROUP_ID, converter.convert(rules));    if (!successed) &#123;      throw new RuntimeException(&quot;publish degrade rule to nacos fail&quot;);    &#125;  &#125;&#125;\n\n改造 DegradeController\n@Autowiredprivate DegradeRuleNacosPublisher degradeRuleNacosPublisher;private boolean publishRules(String app, String ip, Integer port) &#123;  List&lt;DegradeRuleEntity&gt; rules = repository.findAllByMachine(MachineInfo.of(app, ip, port));  try &#123;    degradeRuleNacosPublisher.publish(app, rules);  &#125; catch (Exception e) &#123;    e.printStackTrace();  &#125;  return sentinelApiClient.setDegradeRuleOfMachine(app, ip, port, rules);&#125;\n\n5）自动推送 ParamFlow 规则新增 ParamFlowRulePublisher 实现将规则推送到 Nacos\nimport com.alibaba.csp.sentinel.dashboard.datasource.entity.rule.ParamFlowRuleEntity;import com.alibaba.csp.sentinel.dashboard.rule.DynamicRulePublisher;import com.alibaba.csp.sentinel.datasource.Converter;import com.alibaba.csp.sentinel.util.AssertUtil;import com.alibaba.nacos.api.config.ConfigService;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * @Auther jd */@Component(&quot;paramFlowRulePublisher&quot;)public class ParamFlowRulePublisher implements DynamicRulePublisher&lt;List&lt;ParamFlowRuleEntity&gt;&gt; &#123;  @Autowired  private ConfigService configService;  @Autowired  private Converter&lt;List&lt;ParamFlowRuleEntity&gt;, String&gt; converter;  @Override  public void publish(String app, List&lt;ParamFlowRuleEntity&gt; rules) throws Exception &#123;    AssertUtil.notEmpty(app, &quot;app name cannot be empty&quot;);    if (rules == null) &#123;      return;    &#125;    boolean successed = configService.publishConfig(app + NacosConfigUtil.PARAM_FLOW_DATA_ID_POSTFIX,        NacosConfigUtil.GROUP_ID, converter.convert(rules));    if (!successed) &#123;      throw new RuntimeException(&quot;publish param flow rule to nacos fail&quot;);    &#125;  &#125;&#125;\n\n改造 ParamFlowRuleController\n@Autowiredprivate ParamFlowRulePublisher paramFlowRulePublisher;private CompletableFuture&lt;Void&gt; publishRules(String app, String ip, Integer port) &#123;  List&lt;ParamFlowRuleEntity&gt; rules = repository.findAllByMachine(MachineInfo.of(app, ip, port));  try &#123;    paramFlowRulePublisher.publish(app, rules);  &#125; catch (Exception e) &#123;    e.printStackTrace();  &#125;  return sentinelApiClient.setParamFlowRuleOfMachine(app, ip, port, rules);&#125;\n\n6）自动推送 Authority 规则新增 AuthorityRuleNacosPublisher 将规则推送到 Nacos\nimport com.alibaba.csp.sentinel.dashboard.datasource.entity.rule.AuthorityRuleEntity;import com.alibaba.csp.sentinel.dashboard.rule.DynamicRulePublisher;import com.alibaba.csp.sentinel.datasource.Converter;import com.alibaba.csp.sentinel.util.AssertUtil;import com.alibaba.nacos.api.config.ConfigService;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * @Auther jd */@Component(&quot;authorityRuleNacosPublisher&quot;)public class AuthorityRuleNacosPublisher implements DynamicRulePublisher&lt;List&lt;AuthorityRuleEntity&gt;&gt; &#123;  @Autowired  private ConfigService configService;  @Autowired  private Converter&lt;List&lt;AuthorityRuleEntity&gt;, String&gt; converter;  @Override  public void publish(String app, List&lt;AuthorityRuleEntity&gt; rules) throws Exception &#123;    AssertUtil.notEmpty(app, &quot;app name cannot be empty&quot;);    if (rules == null) &#123;      return;    &#125;    boolean successed = configService.publishConfig(app + NacosConfigUtil.AUTHORITY_DATA_ID_POSTFIX,        NacosConfigUtil.GROUP_ID, converter.convert(rules));    if (!successed) &#123;      throw new RuntimeException(&quot;publish authority rule to nacos fail&quot;);    &#125;  &#125;&#125;\n\n改造 AuthorityRuleController\n@Autowiredprivate AuthorityRuleNacosPublisher authorityRuleNacosPublisher;private boolean publishRules(String app, String ip, Integer port) &#123;  List&lt;AuthorityRuleEntity&gt; rules = repository.findAllByMachine(MachineInfo.of(app, ip, port));  try &#123;    authorityRuleNacosPublisher.publish(app, rules);  &#125; catch (Exception e) &#123;    e.printStackTrace();  &#125;  return sentinelApiClient.setAuthorityRuleOfMachine(app, ip, port, rules);&#125;\n\n7）使用控制台默认登陆用户名和密码在配置文件中 auth.username 和 auth.password\n在 流控规则 V1 菜单配置的流量规则将会同步到 Nacos，同时服务也会应用该规则，需要注意服务中配置的 Nacos 配置文件的 dataId 和 groupId 要与生成的一致。\n四、Spring Boot 添加配置spring:  application:    name: spring-cloud-demo-consumer  cloud:    # 使用 Sentinel 进行流量控制    sentinel:      eager: true      # 取消控制台懒加载      transport:        port: 8732     # 上报给控制台的端口        dashboard: 127.0.0.1:8080        # 控制台地址      datasource:        nacos-degrade:          nacos:            server-addr: 127.0.0.1:8848            namespace: public            username: nacos            password: nacos            dataId: $&#123;spring.application.name&#125;-degrade-rules            groupId: SENTINEL_GROUP        # 需要与 Sentinel 控制台的 GroupId 一致            data-type: json            rule-type: degrade        nacos-flow:          nacos:            server-addr: 127.0.0.1:8848            namespace: public            username: nacos            password: nacos            dataId: $&#123;spring.application.name&#125;-flow-rules            groupId: SENTINEL_GROUP        # 需要与 Sentinel 控制台的 GroupId 一致            data-type: json            rule-type: flow\n\n","categories":["运维"],"tags":["SpringBoot","Spring","Sentinel"]},{"title":"SkyWalking","url":"/posts/15856/","content":"一、安装 SkyWalking1. 安装 Elasticsearchdocker pull --platform linux/arm64/v8 elasticsearch:7.16.3docker run -d --name es \\    --platform linux/arm64/v8 \\    -p 9200:9200 -p 9300:9300 \\    -e &quot;discovery.type=single-node&quot; \\    -e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; \\    elasticsearch:7.16.3\n\n1. 安装 SkyWalking-OAP-Server这里提供 Elasticsearch 和 H2 作为 storage 的安装。\n使用 Elasticsearch 作为 storage安装 Elasticsearch\ndocker pull --platform linux/arm64/v8 elasticsearch:7.16.3docker run -d --name es \\    --platform linux/arm64/v8 \\    -p 9200:9200 -p 9300:9300 \\    -e &quot;discovery.type=single-node&quot; \\    -e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; \\    elasticsearch:7.16.3\n\n安装 SkyWalking-OAP-Server\ndocker pull --platform linux/arm64 apache/skywalking-oap-server:9.2.0docker run -d --name oap \\    -p 1234:1234 -p 11800:11800 -p 12800:12800 \\    --link es:elasticsearch \\    -e TZ=Asia/Shanghai \\    -e SW_STORAGE=elasticsearch \\    -e SW_STORAGE_ES_CLUSTER_NODES=elasticsearch:9200 \\    -e SW_NAMESPACE=oap \\    apache/skywalking-oap-server:9.2.0\n\n\nSW_STORAGE：选择 elasticsearch 配置，若不选默认为 H2\nSW_STORAGE_ES_CLUSTER_NODES：elasticsearch 地址\n\n使用 H2 作为 storagedocker run -d --name oap \\    -p 1234:1234 -p 11800:11800 -p 12800:12800 \\    -e TZ=Asia/Shanghai \\    apache/skywalking-oap-server:9.2.0\n\n3. 安装 SkyWalking-UIdocker pull --platform linux/arm64 apache/skywalking-ui:8.9.1docker run -d --name ui \\    -p 8080:8080 \\    --link oap:oap \\    -e TZ=Asia/Shanghai \\    -e SW_OAP_ADDRESS=http://oap:12800 \\    apache/skywalking-ui:9.2.0\n\n访问界面：http://localhost:8080/\n⚠️ SW_OAP_ADDRESS 必须添加协议类型，如：http 。\n二、服务引用 SkyWalkingSkeWalking Agents 下载地址：https://skywalking.apache.org/downloads/\n在项目启动时添加配置\n优先级：探针 &gt; JVM配置 &gt; 系统环境变量 &gt; agent.config\n探针格式配置如下：\n（1）-javaagent:/path/to/skywalking-agent.jar={config1}={value1},{config2}={value2}-javaagent:/Users/xx/Desktop/apache-skywalking-apm-bin-es7/agent/skywalking-agent.jar=agent.service_name=demo,collector.backend_service=localhost:11800\n\n（2）-Dskywalking.[option1]=[value2]-javaagent:/Users/xx/Desktop/apache-skywalking-apm-bin-es7/agent/skywalking-agent.jar -Dskywalking.agent.service_name=demo -Dskywalking.collector.backend_service=localhost:11800\n\n","categories":["运维"],"tags":["链路追踪","SkyWalking"]},{"title":"搭建 Minio","url":"/posts/18302/","content":"Docker 搭建 Miniodocker run -d -p 9000:9000 -p 9001:9001 --name minio\\  -e &quot;MINIO_ROOT_USER=admin&quot; \\  -e &quot;MINIO_ROOT_PASSWORD=12345678&quot; \\  minio/minio server /data --console-address &quot;:9001&quot;\n\n\n需要通过 --console-address &quot;:9001&quot; 指定控制台端口，不然访问页面会一直转发别的端口。\n9001 为控制台端口，需要与 servier 的端口不同。\n\n","categories":["运维"]},{"title":"搭建 Nacos","url":"/posts/36267/","content":"一、单机版拉取镜像docker pull --platform linux/arm64 nacos/nacos-server:2.1.0-BETA\n\n启动 Nacosdocker run -d --name nacos \\   -p 8848:8848 -p 9848:9848 \\   -e MODE=standalone \\   nacos/nacos-server:2.1.0-BETA\n\n访问 Nacos 页面 http://localhost:8848/nacos\n默认账号密码：nacos/nacos\n二、K8S 搭建 Nacos 集群Nacos 数据库初始化脚本，可以根据不同版本选择 tag。\n官方yaml\n","categories":["运维"]},{"title":"DDD & Microservices","url":"/posts/29601/","content":"Microservices（微服务架构）和DDD（领域驱动设计）是时下最炙手可热的两个技术词汇。在最近两年的咨询工作中总是会被不同的团队和角色询问，由此也促使我思考为什么这两个技术词汇被这么深入人心的绑定，它们之间的关系是什么呢？\n服务于更高的业务响应力首先从两个词汇的发明来看它们是没有因果关系的。DDD是Eric Evans于2003年出版的书名，同时也是这个架构设计方法名的起源。DDD的想法是让我们的软件实现和一个演进的架构模型保持一致，而这个演进的模型来自于我们的业务需求。这种演进式设计方法在当时看来还是比较挑战的，更为流行的解决架构设计复杂度的方法是分层：比如数据架构、服务架构、中间件架构等。MVC在互联网应用开发领域也基本成为了标配。\n时间很快过了10年，Martin Fowler和ThoughtWorks英国架构师James Lewis坐下来一起分析了好几个能够持续演进的大型复杂系统，总结出了9大核心特质，然后用Microservices来定义了拥有这些特质的架构。之后由于Google、Netflix、Amazon等一系列明星企业都对号入座，Microservices开始风靡整个软件业。这时候很多人会问微服务架构是怎么设计出来的，业界人士会说DDD是一个好方法，其中也包括微服务定义者Martin Fowler，毕竟DDD原书的序是他给著的；）于是乎DDD开始在被定义10年后火了。\n从我个人角度来看，如果真的需要找到因果关系的话，最根本的驱动力来自于科技时代对软件系统（数字化）响应力要求的不断提升，而系统的复杂度却随着业务的多元化而与日俱增。如何驾驭这样的高复杂度成了每个企业必须面对的挑战，以至于业界开始把这种模型总结为响应力企业（Responsive Enterprise），而模型中总结的大部分原则都是为了更好的适应环境不确定性带来的高复杂度。\n一、从业务视角分离复杂度每个人能够认知的复杂度都是有限的，在面对高复杂度的时候我们会做关注点分离，这是一个最基本的哲学原则。显然在针对复杂业务场景进行建模时，我们也会应用此原则。这个时候去分离关注点一般可以从两个维度出发：\n\n技术维度分离，类似MVC这样的分层思想是我们广泛接受的。\n业务维度分离，根据不同的业态划分系统，比如按售前、销售、售后划分。\n\n以上两个维度没有孰优孰劣之分，在处理复杂问题的时候一定都会用上，但为了能够高效响应业务的变化，微服务的架构更强调业务维度的关注点分离来应对高复杂度。这是显著区别于传统SOA架构的特质之一，比如诞生于传统SOA时代的ESB（工业服务总线）就是一个典型的技术关注点分离出来的中间件。随着业务的变化，我们也看到ESB成为了一个架构上的反模式，即大量的业务规则和流程被封装在了ESB里，让ESB成为了不可驾驭的复杂度之源，以至于破坏了SOA架构之前承诺的各种优势。当然Microservices架构并非是新一代SOA架构这么简单，已经有不少文章在讨论这个话题，本文就不在展开了。\n所以从本质上作为一种架构设计方法的DDD和作为一种架构风格的Microservices都是为着追求高响应力目标而从业务视角去分离复杂度的手段。\n如果这个时代你还觉得自己的架构不需要这种响应力，我建议你问问身边维护3年以上系统的朋友或同事们，他们会告诉你这是怎样的一种痛苦。实际上很多企业对这种响应力的追求已经很“疯狂”了，这也是微服务的两位定义者可能都始料未及的。\n他们在定义文章中带着很强警告语气让大家慎用，但在这个科技时代，微服务架构实施的可能风险对比高响应力在未来可能带来的市场机会几乎可以忽略不计。一个Netflix的成功就足以让大部分企业毫不犹豫的选择微服务作为自身的架构风格。\n二、业务和技术渐进统一的架构设计如果Microservices和DDD在目标上达成了上文的统一，那么在具体做法上和以前有什么不同呢？\n为了解释清楚这个问题让我们极简化架构设计为以下三个层面工作：\n\n业务架构：根据业务需求设计业务模块及交互关系。\n系统架构：根据业务需求设计系统和子系统的模块。\n技术架构：根据业务需求决定采用的技术及框架。\n\n显然这三者在具体一个架构设计活动中应该是有先后顺序的，但并非一定是孰先孰后，比如一个简单的web应用，很多人会说MVC是标配了（首先确定了系统架构），或者有人说用RoR快（首先确定了技术架构）。在给定的业务场景里，也许这样的顺序是合理的。\n\n架构设计工作分层及传统意义上的负责人\n这个时候咱们增加复杂业务需求和快速市场变化这两个环境变量，这个顺序就变得很有意思了。于是我们听到不少走出初创期的互联网服务平台开始“重写”他们的系统（从PHP到Java），很多文章开始反思MVC带来的僵化（臃肿的展现层）。经历了这样变迁的架构师们都会感同身受的出来为DDD站台，其原因就是“跳过”（或“后补”）业务架构显然表明设计出来的架构关注点并不在业务的响应力上，因为业务的可能变化点并没有被分析出来指导系统和技术架构的设计。\nDDD的核心诉求就是能够让业务架构和系统架构形成绑定关系，从而当我们去响应业务变化调整业务架构时，系统架构的改变是随之自发的。\n这个变化的结果有两个：\n\n业务架构的梳理和系统架构的梳理是同步渐进的，其结果是划分出的业务上下文和系统模块结构是绑定的。\n技术架构是解耦的，可以根据划分出来的业务上下文的系统架构选择最合适的实现技术。\n\n第一点显然也是我们产生微服务划分所必须遵循的，因为微服务追求的是业务层面的复用，所以设计出来的系统必须是跟业务一致的。第二点更是微服务架构的特质：“去中心化”的治理技术和数据管理。 作为架构设计的方法，DDD的各种实践，包括最近流行的Event Storming（事件风暴）实际上都是促进业务和系统架构梳理的渐进式认知。\n在一次DDD工作坊中，一位同事给出了“你们连业务故事都讲不清楚，还有必要继续做架构设计吗？”这样的经典评论。而DDD的整个方法也没有涉及具体的技术架构实现，这个选型的权利很多时候被“下放”给了真正的开发团队。\n值得一提的是采用DDD这种架构设计方法并不一定就产生Mircoservices这种架构风格，往往会推荐用大颗粒度的服务来包含业务分析过程中发现的不确定点，以避免拆分后变化过度频繁带来的双向修改成本。\n三、跨职能协作的架构设计业务和系统的渐进认知改变了很多之前的架构工作模式，在采用DDD的过程中，很容易感受到业务专家的重要性。而如果还有人寄希望让业务能够一次性给架构师讲清楚需求，那我建议抱有这样希望的同学去亲身参加一次自己不熟悉业务领域的架构设计讨论。你会很容易得出结论“原来业务也不懂他要什么”。当然业务人员听说要参加某种（软件）架构设计方法时心里也一定是抵触的。\nDDD成功运用的基础就是创造让业务和系统这两种不同认知模型逐步统一的环境。\n\n所以“不幸”的是如果你不能建立一个跨业务和技术的新型架构设计小组，你的DDD实践就没有成功的基础，继而采用微服务架构可能就会是一场灾难。幸运的是这种跨职能组织结构已经是前文中“采用”微服务架构企业（如Amazon）的标配，你不必再论证这件事情的可实施性。剩下的关键就是如何能够让不同背景的人们协作起来。这也是大家可以看到DDD领域的下一个热点，类似Event Storming这样的模式化协作工作坊会更多的出现在大家的视线里。\n四、永无终止的DDD和演进的MicroservicesDDD是容易上瘾的，当大家发现原来通过这个建模过程业务专家更了解服务划分（系统模块），架构设计更懂业务需求，这种协作会成为常态。在这个tech@core的时代，这样的融合将成为企业的核心竞争力。\n当然刚开始采用DDD方法的时候，请不要认为每个系统搞一次所谓的DDD工作坊就能够找到最佳的服务划分了。业务的变化是持续的，而每次业务架构变化必然牵动系统架构的变化。良好的领域架构绑定了业务和系统，让双方人员能够用统一语言交流，这件事情建立不易，而持续运作更难。\n成功的DDD方法运用是贯穿系统的整个生命周期的，这个过程中业务和技术的协作是持续发生的。\nMicroservices的最后一个特质：“演进式”设计 - 也明确了设计是一种持续的活动。DDD提供了一种符合这个微服务特质的工作方法，让演进能够落地。值得一提的是就笔者最近的经验，这个演进过程中最难认知到变化的就是DDD里最显而易见的“统一语言”。当大家形成了一个业务概念-“客户”后，少有团队能够持续审视这个“客户”是否随着市场的变化而发生了含义的变迁。\n对比传统的SOA，微服务的拆分也是动态的，禚娴静在自己的文章中描述一个系统采用微服务架构历程中服务拆分的演变。这里不会有一个ESB来以不变应万变，这种幻想在过去的10年里已经被数次打脸。DDD的好处是让业务和技术人员都能够在合作中理解这种变化，而不至于陷入业务人员抱怨技术架构不知所谓，技术人员觉得业务人员朝三暮四的尴尬。\n五、你需要成为那个高个子！Martin Fowler在Microservies的定义文章中画了下面的图，评论“你必须有那个高度”来隐喻微服务实施的能力要求。就架构建模方面来说我认为DDD应该是一个团队必须去掌握的，包括这个团队的业务人员和产品设计人员。\n\n微服务前置条件示意\n很有意思的是目前Service Design也是全球用户体验设计领域的一个热门话题，从用户视角出发去设计整个服务链条。比如时下热门的共享单车，一个成功的服务设计应该是从用户开始有用车需求触发到最后完成骑行缴费离开，而不仅仅是去设计一辆能够互联网解锁的自行车。\n我们可以找到很多Service Deisgn和DDD在原则上的相似之处，比如用户中心和协同设计。借用上面的高个子说法:\n在业务需求认知和跨职能协作方面你一定需要成为高个子！\n","categories":["领域驱动设计"]},{"title":"从“四色建模法”到“限界纸笔建模法”","url":"/posts/17671/","content":"对于领域专家、程序员和测试工程师来说，领域建模所构建的概念模型是撑起软件开发系统的骨架。领域建模的方法有很多种，ThoughtWorks的同事们经常使用经过徐昊改编的“四色建模法”[1]来进行建模。而本文所描述的“限界纸笔建模法”，在“四色建模法”的“时标对象”的基础上确定”限界上下文”[2]与“聚集”[3]的概念，再使用“纸和笔来管理”的方法，力图在建模过程中实现“分而治之”，增强数据的完整性，并避免过度设计。\n为了便于说明，下文将以“小画笔”绘画课外班的业务为例，首先描述同事亢江妹用四色建模法对其所建的模型，然后再描述如何在其“时标对象”的基础上，运用限界纸笔建模法进行建模。\n一、“小画笔”绘画课外班“小画笔”是一家面向2-10岁小朋友的绘画培训机构。乐乐老师是这个培训机构的主管。她毕业于北京师范学院，有一些认识的同学和老师愿意在业余时间授课来获得一定收入。\n“小画笔”目前有三个班：\n\n美术预科：适合2-3岁孩子，从看、摸、闻、听、尝培养艺术感；每期课程8次，每周一次，周末上；每班最少6个孩子开课，最多10个孩子。\n书法：适合3-6岁孩子，学习字体结构、笔画线条；每期课程8次，每周一次，周末或晚上上; 每班最少6个孩子开课，最多12个孩子。\n儿童绘本：适合年龄5-10岁，用文字和图画表达；每期课程8次，每周一次，周末或晚上上; 每班最少6个孩子开课，最多12个孩子。\n\n“小画笔”有三个教室：达芬奇，毕加索和梵高。\n每两个月开始新期课程，8次课结束后重新开始。\n乐乐老师的主要工作是：\n\n根据现有老师能够讲授的课程及时间，做好课程表安排，提前两个月把下期课程介绍做好，印成宣传彩页；\n在附近社区里给家长宣传，招募生源；\n接受家长报名（电话或面对面），记录和追踪报名情况；\n收取学费，学费支付采用现金付款，或银联卡刷卡支付；\n争取在每一期课程开始前都有足够学员报名，可以准时开班；\n保证课程质量和安全，保证营收，维持培训机构正常运转。\n\n乐乐老师需要处理的三个核心问题有：\n\n下一期的课程如何安排？ - 排课\n如何招生和管理报名？ - 管理报名\n如何保证课程质量？ - 管理课堂\n\n挑战：\n请为乐乐老师设计一个软件工具的概念模型，能让乐乐老师方便地管理排课、报名和学员考勤，维护“小画笔”的正常运营。\n在日常运营中，乐乐老师经常碰到下列问题：\n\n课程开始后发现有孩子没来上课，也没有请假，需要查找家长联系电话来询问。\n有时孩子请假会造成缺课，家长会要求在后期课程中补上。乐乐老师需要记录孩子的上课次数，在可以补上课程的时候通知孩子来参加。\n需要查看还有哪个家长没有缴费，通知他们缴费。\n需要查看某个孩子的缴费记录，以确认家长是否缴费。\n经常有家长要求先试一次课，再确定报名交钱。但每班只能在报名不满额的情况下才能试听，且最多只能允许3个孩子试听。所以乐乐需要追踪每班报名多少孩子、试学多少孩子、还有没有试学名额。\n需要经常查看每班是否报满，可否开课；如果不满，需要再去小区做宣传。\n需要提前跟上课老师确认，确保他们能按时授课。\n有些课程可能会有一位主任老师和另一位见习老师来共同授课，所以这些课程每门课需要管理多位老师的信息。\n需要跟踪教学质量，确保老师按时按质授课，及正常教学安全和秩序；\n\n请检查你所设计的概念模型能否帮乐乐老师解决上述问题。\n二、用“四色建模法”进行建模第一步：寻找要追溯的事件\n\n谁，在什么时候，为谁，报名了什么课程\n谁，在什么时候，为谁，支付了多少学费\n谁，在什么时候，上了什么课程\n\n第二步：识别“时标对象”[4]\n按时间发展的先后顺序，用红色所表示的起到“追溯单据”作用的“时标”概念，如下图所示：\n\n第三步：寻找时标对象周围的“人、地、物”[5]\n在“时标”对象周围的用绿色所表示的“人、地、物”概念，如下图所示：\n\n第四步：抽象“角色”[6]\n在上图中插入用黄色所表示的“角色”概念，如下图所示：\n\n第五步：补充“描述”信息[7]\n在上图中插入用蓝色所表示的“描述”概念，如下图所示：\n\n三、用“限界纸笔建模法”进行建模上面用四色建模法对“小画笔”绘画课外班的软件系统所做的建模过程，最大的亮点是按时间发展的先后顺序，识别出起到“追溯单据”作用的“时标”概念。这种识别方法直达业务核心数据，简便有效。\n此外，限界纸笔建模法可以继续进行以下三项建模工作：\n\n划分限界上下文，避免模型发展成“大泥球架构”。\n强调“聚集根”[8]的概念，更好地保证数据的完整性。\n寻找“恰好够用”的概念，避免过度设计，降低所建模型的复杂性。\n\n一个“聚集根”好比一个经理，这个“聚集”所汇聚的所有彼此相关的概念好比这个经理手下管理的所有员工。当另一个“聚集”的经理，要访问该“聚集”中的某个员工时，必须要通过该“聚集”的“聚集根”这个经理的同意和安排才行，不能直接去找。这样设计能够将管理数据的访问职责缩小到一些“经理”那里，既方便定位职责，也有助于增强数据的完整性。\n下面尝试用限界纸笔建模法来对“小画笔”绘画课外班的软件系统建模。\n第一步：根据“追溯单据”的价值识别核心领域[9]\n首先以“小画笔”绘画课外班的业务“追溯单据”为线索，列出这些追溯单据为乐乐老师所提供价值，并合并其中一些价值相同的单据。比如对于一个只有三间教室的小课外培训机构，只有交费成功的“报名”才视作有效报名。所以“报名登记表”和“交费纪录”可以合并。然后确定这些价值所对应的核心领域。如下表所示：\n\n 第二步：确定核心领域之间的依赖关系\n排课、报名与签到三个核心领域的依赖关系如下所示：\n排课 &lt;—— 报名 &lt;—— 签到\n上面的箭头表示：“报名”需要依赖“排课”所提供的信息，而“签到”需要依赖“报名”所提供的信息。\n第三步：用纸和笔画表格并写实例\n先选择一个核心领域，然后开始在其所对应的“限界上下文”中，开始对其建模。假设时光回退100年，那时没有电脑，只有纸和笔。用纸和笔画表格并写实例的方法，来管理该核心领域的“恰好够用”的数据，来达成乐乐老师在此核心领域所期望的价值。\n首先选择“排课”。下图是用纸和笔画出的表格，并有一条实例数据：\n\n第四步：确定“聚集根”\n给表中所有的列找一个“经理”来作为“聚集根”。这里选择“课程”作为“聚集根”，它是一种具有唯一标识（即有唯一的课程编号）的Entity[10]概念，然后把表中所有的列名都抄写到表下“聚集根”的右侧，并用括号括起来，如下图所示：\n\n第五步：以“人以群分”的原则抽取新的“聚集”\n观察所抄写的“聚集根”右侧所有聚集在一起的各个概念，提取出总是“一起玩儿”的多个概念，形成一个新的聚集，并确定这个新聚集的名字。比如，“上课时间”、“起始日期”、“次数”这三个概念总是一起出现，它们决定了这门课程的学时，所以就提取“学时”这个新聚集，并把这三个概念从“课程”右侧划掉。“学时”这个概念不需要有唯一标识，所以是Value Object[11]概念。它和“课程”是一对一的关系。用相同的方法可以提取“老师”这个新聚集，它是Entity概念，“课程”与它是一对多的关系。如下图所示：\n \n对于那些只“自己单独玩儿”的概念，如“学费”、“教室”等，就先暂时放到“课程”这个聚集根下面，等随着将来业务演进出现了新的“能一起玩儿的伙伴”后，再提取新的聚集不迟。\n至此，使用限界纸笔建模法对“排课”这个核心领域所进行的建模告一段落。下图是使用这种方法对“报名”这个核心领域所建的模型，“签到”核心领域的建模略去不讨论。\n\n这里有一个问题，“排课”这个限界上下文中有“课程”这个聚集根，而“报名”这个限界上下文中也有“课程”这个聚集根，这两者是同一个概念吗？\n从DDD的观点来看，这两者是不同的概念，而且在数据库中也应该是两张不同的表。前者是“排课.课程”，后者是“报名.课程”。两者的属性也各不相同，前者是：排课.课程（名称、介绍、适合年龄、学费、教室），后者是：报名.课程（名称）。当然，这两个概念的“课程名称”和课程编号都是一致的。当“报名”限界上下文需要访问“排课”限界上下文中的“课程”概念来获取“课程介绍”等信息时，需要通过两者之间的“翻译器”来根据上述一致的课程编号来进行翻译和转换。\n四、限界纸笔建模法的3点优势\n划分核心领域有助于“分而治之”：一旦确定了核心领域，限界上下文也就确定了，不同的限界上下文之间通过“翻译器”来彼此沟通并屏蔽干扰，这样就避免了“大泥球”的设计，并有助于演进到微服务架构。\n“聚集根”有助于数据完整性：每个限界上下文都有一个“聚集根”的概念，外界对其下属概念的访问都必须通过它来进行，这样既方便定位职责，也有助于增强数据的完整性。\n用“纸和笔”画恰好够用的概念有助于避免过度设计：每个限界上下文中要管理的概念，都是通过“倒退到没有电脑而用纸和笔的时代如何管理”来引导出来的，用纸和笔来记录，能促使人避免写过多的信息，而只写限界上下文中恰好够用的概念。\n\n五、总结四色建模法最大的亮点是按时间发展的先后顺序，识别起“追溯单据”作用的“时标”概念，从而能把握业务核心数据，简便有效。\n限界纸笔建模法，使用了DDD中的”限界上下文”与“聚集”的概念以及“纸和笔来管理”方法，来实现“分而治之”，增强数据的完整性，避免过度设计。\n\n注：\n[1] 四色建模法：经过徐昊改编的“四色建模法”参见：http://www.infoq.com/cn/articles/xh-four-color-modeling\n[2] 限界上下文：Bounded Context，DDD概念，与“核心领域”(Core Domain)一一对应，它被程序员所关注，包含其所对应的核心领域的概念模型。\n[3] 聚集：Aggregates，DDD概念，指某一簇彼此相关联的概念集合。\n[4] 时标对象：指四色建模法中的”Moment, Interval”概念，用红色表示。近似于DDD中的Repositories概念。\n[5] “人、地、物”：指四色建模法中的“Party, Place, Thing”概念，用绿色表示。近似于DDD中的Entities概念。\n[6] 角色：指四色建模法中的“Role”概念，用黄色表示。近似于DDD中的Services概念。\n[7] 描述信息：指四色建模法中的“Description”概念，用蓝色表示。近似于DDD中国的Value Objects概念。\n[8] 聚集根：Aggregate Root，DDD概念，指对其所属的聚集内的相关从属概念进行统一访问控制的那个概念。\n[9] 核心领域：Core Domain，DDD概念，与“限界上下文”(Bounded Context)一一对应，指某个业务专家所关注的、具有差异化竞争优势的、相对独立的业务领域。\n[10] Entity：DDD概念，表示具有唯一标识的概念。\n[11] Value Object: DDD概念，表示不必有唯一标识的概念。\n","categories":["领域驱动设计"]},{"title":"从三明治到六边形","url":"/posts/47281/","content":"软件项目的套路如果你平时的工作是做各种项目（而不是产品），而且你工作的时间足够长，那么自然见识过很多不同类型的项目。在切换过多次上下文之后，作为程序员的你，自然而然的会感到一定程度的重复：稍加抽象，你会发现所有的业务系统都几乎做着同样的事情：\n\n从某种渠道与用户交互，从而接受输入（Native App，Mobile Site，Web Site，桌面应用等等）\n将用户输入的数据按照一定规则进行转换，然后保存起来（通常是关系型数据库）\n将业务数据以某种形式展现（列表，卡片，地图上的Marker，时间线等）\n\n稍加简化，你会发现大部分业务系统其实就是对某种形式的资源进行管理。所谓管理，也无非是增删查改（CRUD）操作。比如知乎是对“问题”这种资源的管理，LinkedIn是对“Profile”的管理，Jenkins对构建任务的管理等等，粗略的看起来都是这一个套路（当然，每个系统管理的资源类型可能不止一种，比如知乎还有时间线，Live，动态等等资源的管理）。\n这些情况甚至会给开发者一种错觉：世界上所有的信息管理系统都是一样的，不同的仅仅是技术栈和操作的业务对象而已。如果写好一个模板，几乎都可以将开发过程自动化起来。事实上，有一些工具已经支持通过配置文件（比如yaml或者json/XML）的描述来生成对应的代码的功能。\n如果真是这样的话，软件开发就简单多了，只需要知道客户业务的资源，然后写写配置文件，最后执行了一个命令来生成应用程序就好了。不过如果你和我一样生活在现实世界的话，还是趁早放弃这种完全自动化的想法吧。\n复杂的业务现实世界的软件开发是复杂的，复杂性并不体现在具体的技术栈上。如Java，Spring，Docker，MySQL等等具体的技术是可以学习很快就熟练掌握的。软件真正复杂的部分，往往是业务本身，比如航空公司的超售策略，在超售之后Remove乘客的策略等；比如亚马逊的打折策略，物流策略等。\n用软件模型如何优雅而合理的反应复杂的业务（以便在未来业务发生变化时可以更快速，更低错误的作出响应）本身也是复杂的。要将复杂的业务规则转换成软件模型是软件活动中非常重要的一环，也是信息传递往往会失真的一环。业务人员说的A可能被软件开发者理解成Z，反过来也一样。\n举个例子，我给租来的房子买了1年的联通宽带。可是过了6个月后，房东想要卖房子把我赶了出来，在搬家之后，我需要通知联通公司帮我做移机服务。\n如果纯粹从开发者的角度出发，写出来的代码可能看起来是这样的：\npublic class Customer &#123;     private String address;     public void setAddress(String address) &#123;         this.address = address;     &#125;     public String getAddress() &#123;         return this.address;     &#125;&#125;\n\n中规中矩，一个简单的值对象。作为对比，通过与领域专家的交流之后，写出来的代码会是这样：\npublic class Customer &#123;     private String address;         public void movingHome(String address) &#123;         this.address = address;     &#125;&#125;\n\n通过引入业务场景中的概念movingHome，代码就变得有了业务含义，除了可读性变强之外，这样的代码也便于和领域专家进行交流和讨论。Eric在领域驱动设计（Domain Drvien Design）中将统一语言视为实施DDD的先决条件。\n层次架构（三明治）\nAll problems in computer science can be solved by another level of indirection, except of course for the problem of too many indirections. – David Wheeler\n\n上文提到，业务系统对外的呈现是对某种资源的管理，而且，现实世界里的业务系统往往要对多种资源进行管理。这些资源还会互相引用，互相交织。比如一个看板系统中的泳道、价值流、卡片等；LinkedIn中的公司，学校，个人，研究机构，项目，项目成员等，它们往往会有嵌套、依赖等关系。\n为了管理庞大的资源种类和繁复的引用关系，人们自然而然的将做同样事情的代码放在了统一的地方。将不同职责的事物分类是人类在处理复杂问题时自然使用的一种方式，将复杂的、庞大的问题分解、降级成可以解决的问题，然后分而治之。\n比如在实践中 ，展现部分的代码只负责将数据渲染出来，应用部分的代码只负责序列化/反序列化、组织并协调对业务服务的调用，数据访问层则负责屏蔽底层关系型数据库的差异，为上层提供数据。这就是层级架构的由来：上层的代码直接依赖于临近的下层，一般不对间接的下层产生依赖，层次之间通过精心设计的API来通信（依赖通常也是单向的）。\n以现代的眼光来看，层次架构的出现似乎理所应当、自然而然，其实它也是经过了很多次的演进而来的。以JavaEE世界为例，早期人们会把应用程序中负责请求处理、文件IO、业务逻辑、结果生成都放在servlet中；后来发明了可以被Web容器翻译成servlet的JSP，这样数据和展现可以得到比较好的分离（当然中间还有一些迂回，比如JSTL、taglib的滥用又导致很多逻辑被泄露到了展现层）；数据存储则从JDBC演化到了各种ORM框架，最后再到JPA的大一统。\n如果现在把一个Spring-Boot写的RESTful后端，和SSH（Spring-Struts-Hibernate）流行的年代的后端来做对比，除了代码量上会少很多以外，层次结构上基本上并无太大区别。不过当年在SSH中复杂的配置，比如大量的XML变成了代码中的注解，容器被内置到应用中，一些配置演变成了惯例，大致来看，应用的层次基本还是保留了：\n\n展现层\n应用层\n数据访问层\n\n在有些场景下，应用层内还可能划分出一个服务层。\n\n前后端分离随着智能设备的大爆发，移动端变成了展现层的主力，如何让应用程序很容易的适配新的展现层变成了新的挑战。这个新的挑战驱动出了前后端分离方式，即后端只提供数据（JSON或者XML），前端应用来展现这些数据。甚至很多时候，前端会成为一个独立的应用程序，有自己的MVC/MVP，只需要有一个HTTP后端就可以独立工作。\n\n前后端分离可以很好的解决多端消费者的问题，后端应用现在不区分前端的消费者到底是谁，它既可以是通过4G网络连接的iOS上的Native App，也可以是iMac桌面上的Chrome浏览器，还可以是Android上的猎豹浏览器。甚至它还可以是另一个后台的应用程序：总之，只要可以消费HTTP协议的文本就可以了！\n这不得不说是一个非常大的进步，一旦后端应用基本稳定，频繁改变的用户界面不会影响后端的发布计划，手机用户的体验改进也与后端的API设计没有任何关系，似乎一切都变的美好起来了。\n业务与基础设施分离不过，如果有一个消费者（一个业务系统），它根本不使用HTTP协议怎么办？比如使用消息队列，或者自定义的Socket协议来进行通信，应用程序如何处理这种场景？ 这种情况就好比你看到了这样一个函数：\nhttpService(request, response);\n\n作为程序员，自然会做一次抽象，将协议作为参数传入：\nservice(request, response, protocol);\n\n更进一步，protocol可以在service之外构造，并注入到应用中，这样代码就可以适配很多种协议（比如消息队列，或者其他自定义的Socket协议）。 比如：\npublic interface Protocol &#123;  void transform(Request request, Response response);&#125;public class HTTP implements Protocol &#123;&#125;public class MyProtocol implements Protocol &#123;&#125;public class Service &#123;        public Service(Protocol protocol) &#123;         this.protocol = protocol;        &#125;             public void service(request, response) &#123;         //business logic here         protocol.transfrom(request, response);        &#125;&#125;\n\n类似的，对于数据的持久化，也可以使用同样的原则。对于代码中诸如这样的代码：\npersisteToDatabase(data);\n\n在修改之后会变成： persistenceTo(data, repository);\n应用依赖倒置原则，我们会写出这样的形式：\npublic class DomainService &#123;     public BusinessLogic(Repository repository) &#123;           this.repository = repository     &#125;      public void perform() &#123;           //perform business logic           repository.save(record);     &#125;&#125;\n\n对于Repository可能会有多种实现。根据不同的需求，我们可以自由的在各种实现中切换：\npublic class InMemoryRepository implements Repository &#123;&#125;public class RDBMSRepository implements Repository &#123;&#125;\n\n这样业务逻辑和外围的传输协议、持久化机制、安全、审计等等都隔离开来了，应用程序不再依赖具体的传输细节，持久化细节，这些具体的实现细节反过来会依赖于应用程序。\n通过将传统内置在层次架构中的数据库访问层、通信机制等部分的剥离，应用程序可以简单的分为内部和外部两大部分。内部是业务的核心，也就是DDD（Domain Driven Design）中强调的领域模型（其中包含领域服务，对业务概念的建立的模型等）；外部则是类似RESTful API，SOAP，AMQP，或者数据库，内存，文件系统，以及自动化测试。\n这种架构风格被称为六边形架构，也叫端口适配器架构。\n六边形架构（端口适配器）六边形架构最早由Alistair Cockburn提出。在DDD社区得到了发展和推广，然后IDDD（《实现领域驱动设计》）一书中，作者进行了比较深入的讨论。\n\n（图片来自：slideshare.net ）\n简而言之，在六边形架构风格中，应用程序的内部（中间的橙色六边形）包含业务规则，基于业务规则的计算，领域对象，领域事件等。这部分是企业应用的核心：比如在线商店里什么样的商品可以打折，对那种类型的用户进行80%的折扣；取消一个正在执行的流水线会需要发生什么动作，删除一个已经被别的Job依赖的Stage又应该如何处理。\n而外部的，也是我们平时最熟悉的诸如REST，SOAP，NoSQL，SQL，Message Queue等，都通过一个端口接入，然后在内外之间有一个适配器组成的层，它负责将不同端口来的数据进行转换，翻译成领域内部可以识别的概念（领域对象，领域事件等）。\n内部不关心数据从何而来，不关心数据如何存储，不关心输出时JSON还是XML，事实上它对调用者一无所知，它可以处理的数据已经是经过适配器转换过的领域对象了。\n六边形架构的优点\n业务领域的边界更加清晰\n更好的可扩展性\n对测试的友好支持\n更容易实施DDD\n\n要新添加一种数据库的支持，或者需要将RESTful的应用扩展为支持SOAP，我们只需要定义一组端口-适配器即可，对于业务逻辑部分无需触碰，而且对既有的端口-适配器也不会有影响。\n由于业务之外的一切都属于外围，所以应用程序是真的跑在了Web容器中还是一个Java进程中其实是无所谓的，这时候自动化测试会容易很多，因为测试的重点：业务逻辑和复杂的计算都是简单对象，也无需容器，数据库之类的环境问题，单元级别的测试就可以覆盖大部分的业务场景。\n这种架构模式甚至可能影响到团队的组成，对业务有深入理解的业务专家和技术专家一起来完成核心业务领域的建模及编码，而外围的则可以交给新人或者干脆外包出去。\n在很多情况下，从开发者的角度进行的假设都会在事后被证明是错误的。人们在预测软件未来演进方向时往往会做很多错误的决定。比如对关系型数据库的选用，对前端框架的选用，对中间件的选用等等，六边形架构可以很好的帮助我们避免这一点。\n小结软件的核心复杂度在于业务本身，我们需要对业务本身非常熟悉才可能正确的为业务建模。通过统一的语言我们可以编写出表意而且易于和业务人员交流的模型。\n另一方面模型应该尽可能的和基础设施（比如JSON/XML的，数据库存储，通信机制等）分离开。这样一来可以很容易用mock的方式来解耦模型和基础设施，从而更容易测试和修改，二来我们的领域模型也更独立，更精简，在适应新的需求时修改也会更容易。\n","categories":["领域驱动设计"]},{"title":"在微服务中使用领域事件","url":"/posts/34323/","content":"稍微回想一下计算机硬件的工作原理我们便不难发现，整个计算机的工作过程其实就是一个对事件的处理过程。当你点击鼠标、敲击键盘或者插上U盘时，计算机便以中断的形式处理各种外部事件。在软件开发领域，事件驱动架构（Event Driven Architecture，EDA）早已被开发者用于各种实践，典型的应用场景比如浏览器对用户输入的处理、消息机制以及SOA。最近几年重新进入开发者视野的响应式编程（Reactive Programming）更是将事件作为该编程模型中的一等公民。可见，“事件”这个概念一直在计算机科学领域中扮演着重要的角色。\n一、认识领域事件领域事件（Domain Events）是领域驱动设计（Domain Driven Design，DDD）中的一个概念，用于捕获我们所建模的领域中所发生过的事情。领域事件本身也作为通用语言（Ubiquitous Language）的一部分成为包括领域专家在内的所有项目成员的交流用语。比如，在用户注册过程中，我们可能会说“当用户注册成功之后，发送一封欢迎邮件给客户。”，此时的“用户已经注册”便是一个领域事件。\n当然，并不是所有发生过的事情都可以成为领域事件。一个领域事件必须对业务有价值，有助于形成完整的业务闭环，也即一个领域事件将导致进一步的业务操作。举个咖啡厅建模的例子，当客户来到前台时将产生“客户已到达”的事件，如果你关注的是客户接待，比如需要为客户预留位置等，那么此时的“客户已到达”便是一个典型的领域事件，因为它将用于触发下一步——“预留位置”操作；但是如果你建模的是咖啡结账系统，那么此时的“客户已到达”便没有多大存在的必要——你不可能在用户到达时就立即向客户要钱对吧，而”客户已下单“才是对结账系统有用的事件。\n在微服务（Microservices）架构实践中，人们大量地借用了DDD中的概念和技术，比如一个微服务应该对应DDD中的一个限界上下文（Bounded Context）；在微服务设计中应该首先识别出DDD中的聚合根（Aggregate Root）；还有在微服务之间集成时采用DDD中的防腐层（Anti-Corruption Layer, ACL）；我们甚至可以说DDD和微服务有着天生的默契。更多有关DDD的内容，请参考笔者的另一篇文章或参考《领域驱动设计》及《实现领域驱动设计》。\n在DDD中有一条原则：一个业务用例对应一个事务，一个事务对应一个聚合根，也即在一次事务中，只能对一个聚合根进行操作。但是在实际应用中，我们经常发现一个用例需要修改多个聚合根的情况，并且不同的聚合根还处于不同的限界上下文中。比如，当你在电商网站上买了东西之后，你的积分会相应增加。这里的购买行为可能被建模为一个订单（Order）对象，而积分可以建模成账户（Account）对象的某个属性，订单和账户均为聚合根，并且分别属于订单系统和账户系统。显然，我们需要在订单和积分之间维护数据一致性，通常的做法是在同一个事务中同时更新两者，但是这会存在以下问题：\n\n违背DDD中”单个事务修改单个聚合根”的设计原则；\n需要在不同的系统之间采用重量级的分布式事务（Distributed Transactioin，也叫XA事务或者全局事务）；\n在不同系统之间产生强耦合。\n\n通过引入领域事件，我们可以很好地解决上述问题。 总的来说，领域事件给我们带来以下好处：\n\n解耦微服务（限界上下文）；\n帮助我们深入理解领域模型；\n提供审计和报告的数据来源；\n迈向事件溯源（Event Sourcing）和CQRS等。\n\n还是以上面的电商网站为例，当用户下单之后，订单系统将发出一个“用户已下单”的领域事件，并发布到消息系统中，此时下单便完成了。账户系统订阅了消息系统中的“用户已下单”事件，当事件到达时进行处理，提取事件中的订单信息，再调用自身的积分引擎（也有可能是另一个微服务）计算积分，最后更新用户积分。可以看到，此时的订单系统在发送了事件之后，整个用例操作便结束了，根本不用关心是谁收到了事件或者对事件做了什么处理。事件的消费方可以是账户系统，也可以是任何一个对事件感兴趣的第三方，比如物流系统。由此，各个微服务之间的耦合关系便解开了。值得注意的一点是，此时各个微服务之间不再是强一致性，而是基于事件的最终一致性。\n\n二、事件风暴（Event Storming）事件风暴是一项团队活动，旨在通过领域事件识别出聚合根，进而划分微服务的限界上下文。在活动中，团队先通过头脑风暴的形式罗列出领域中所有的领域事件，整合之后形成最终的领域事件集合，然后对于每一个事件，标注出导致该事件的命令（Command），再然后为每个事件标注出命令发起方的角色，命令可以是用户发起，也可以是第三方系统调用或者是定时器触发等。最后对事件进行分类整理出聚合根以及限界上下文。事件风暴还有一个额外的好处是可以加深参与人员对领域的认识。需要注意的是，在事件风暴活动中，领域专家是必须在场的。更多有关事件风暴的内容，请参考这里。\n创建领域事件领域事件应该回答“什么人什么时候做了什么事情”这样的问题，在实际编码中，可以考虑采用层超类型(Layer Supertype)来包含事件的某些共有属性：\npublic abstract class Event &#123;    private final UUID id;    private final DateTime createdTime;    public Event() &#123;        this.id = UUID.randomUUID();        this.createdTime = new DateTime();    &#125;&#125;\n\n可以看到，领域事件还包含了ID，但是该ID并不是实体（Entity）层面的ID概念，而是主要用于事件追溯和日志。另外，由于领域事件描述的是过去发生的事情，我们应该将领域事件建模成不可变的（Immutable）。从DDD概念上讲，领域事件更像一种特殊的值对象（Value Object）。对于上文中提到的咖啡厅例子，创建“客户已到达”事件如下：\npublic final class CustomerArrivedEvent extends Event &#123;    private final int customerNumber;    public CustomerArrivedEvent(int customerNumber) &#123;        super();        this.customerNumber = customerNumber;    &#125;&#125;\n\n在这个CustomerArrivedEvent事件中，除了继承自Event的属性外，还自定义了一个与该事件密切关联的业务属性——客户人数（customerNumber）——这样后续操作便可预留相应数目的座位了。另外，我们将所有属性以及CustomerArrivedEvent本身都声明成了final，并且不向外暴露任何可能修改这些属性的方法，这样便保证了事件的不变性。\n三、发布领域事件在使用领域事件时，我们通常采用“发布-订阅”的方式来集成不同的模块或系统。在单个微服务内部，我们可以使用领域事件来集成不同的功能组件，比如在上文中提到的“用户注册之后向用户发送欢迎邮件”的例子中，注册组件发出一个事件，邮件发送组件接收到该事件后向用户发送邮件。\n\n在微服务内部使用领域事件时，我们不一定非得引入消息中间件（比如ActiveMQ等）。还是以上面的“注册后发送欢迎邮件”为例，注册行为和发送邮件行为虽然通过领域事件集成，但是他们依然发生在同一个线程中，并且是同步的。另外需要注意的是，在限界上下文之内使用领域事件时，我们依然需要遵循“一个事务只更新一个聚合根”的原则，违反之往往意味着我们对聚合根的拆分是错的。即便确实存在这样的情况，也应该通过异步的方式（此时需要引入消息中间件）对不同的聚合根采用不同的事务，此时可以考虑使用后台任务。\n除了用于微服务的内部，领域事件更多的是被用于集成不同的微服务，如上文中的“电商订单”例子。\n\n通常，领域事件产生于领域对象中，或者更准确的说是产生于聚合根中。在具体编码实现时，有多种方式可用于发布领域事件。\n一种直接的方式是在聚合根中直接调用发布事件的Service对象。以上文中的“电商订单”为例，当创建订单时，发布“订单已创建”领域事件。此时可以考虑在订单对象的构造函数中发布事件：\npublic class Order &#123;    public Order(EventPublisher eventPublisher) &#123;        //create order                //…                eventPublisher.publish(new OrderPlacedEvent());            &#125;&#125;\n\n注：为了把焦点集中在事件发布上，我们对Order对象做了简化，Order对象本身在实际编码中不具备参考性。\n可以看到，为了发布OrderPlacedEvent事件，我们需要将Service对象EventPublisher传入，这显然是一种API污染，即Order作为一个领域对象只需要关注和业务相关的数据，而不是诸如EventPublisher这样的基础设施对象。另一种方法是由NServiceBus的创始人Udi Dahan提出来的，即在领域对象中通过调用EventPublisher上的静态方法发布领域事件：\n public class Order &#123;    public Order() &#123;        //create order        //...        EventPublisher.publish(new OrderPlacedEvent());    &#125;&#125;\n\n这种方法虽然避免了API污染，但是这里的publish()静态方法将产生副作用，对Order对象的测试带来了难处。此时，我们可以采用“在聚合根中临时保存领域事件”的方式予以改进：\npublic class Order &#123;    private List&lt;Event&gt; events;    public Order() &#123;        //create order        //...        events.add(new OrderPlacedEvent());    &#125;    public List&lt;Event&gt; getEvents() &#123;        return events;    &#125;    public void clearEvents() &#123;        events.clear();    &#125;&#125;\n\n在测试Order对象时，我们便你可以通过验证events集合保证Order对象在创建时的确发布了OrderPlacedEvent事件：\n@Testpublic void shouldPublishEventWhenCreateOrder() &#123;    Order order = new Order();    List&lt;Event&gt; events = order.getEvents();    assertEquals(1, events.size());    Event event = events.get(0);    assertTrue(event instanceof OrderPlacedEvent);&#125;\n\n在这种方式中，聚合根对领域事件的保存只能是临时的，在对该聚合根操作完成之后，我们应该将领域事件发布出去并及时清空events集合。可以考虑在持久化聚合根时进行这样的操作，在DDD中即为资源库（Repository）：\npublic class OrderRepository &#123;    private EventPublisher eventPublisher;    public void save(Order order) &#123;        List&lt;Event&gt; events = order.getEvents();        events.forEach(event -&gt; eventPublisher.publish(event));        order.clearEvents();        //save the order        //...    &#125;&#125;\n\n除此之外，还有一种与“临时保存领域事件”相似的做法是“在聚合根方法中直接返回领域事件”，然后在Repository中进行发布。这种方式依然有很好的可测性，并且开发人员不用手动清空先前的事件集合，不过还是得记住在Repository中将事件发布出去。另外，这种方式不适合创建聚合根的场景，因为此时的创建过程既要返回聚合根本身，又要返回领域事件。\n这种方式也有不好的地方，比如它要求开发人员在每次更新聚合根时都必须记得清空events集合，忘记这么做将为程序带来严重的bug。不过虽然如此，这依然是笔者比较推荐的方式。\n业务操作和事件发布的原子性虽然在不同聚合根之间我们采用了基于领域事件的最终一致性，但是在业务操作和事件发布之间我们依然需要采用强一致性，也即这两者的发生应该是原子的，要么全部成功，要么全部失败，否则最终一致性根本无从谈起。以上文中“订单积分”为例，如果客户下单成功，但是事件发送失败，下游的账户系统便拿不到事件，导致最终客户的积分并不增加。\n要保证业务操作和事件发布之间的原子性，最直接的方法便是采用XA事务，比如Java中的JTA，这种方式由于其重量级并不被人们所看好。但是，对于一些对性能要求不那么高的系统，这种方式未尝不是一个选择。一些开发框架已经能够支持独立于应用服务器的XA事务管理器（如Atomikos和Bitronix），比如Spring Boot作为一个微服务框架便提供了对Atomikos和Bitronix的支持。\n如果JTA不是你的选项，那么可以考虑采用事件表的方式。这种方式首先将事件保存到聚合根所在的数据库中，由于事件表和聚合根表同属一个数据库，整个过程只需要一个本地事务就能完成。然后，在一个单独的后台任务中读取事件表中未发布的事件，再将事件发布到消息中间件中。\n\n这种方式需要注意两个问题，第一个是由于发布了事件之后需要将表中的事件标记成“已发布”状态，即依然涉及到对数据库的操作，因此发布事件和标记“已发布”之间需要原子性。当然，此时依旧可以采用XA事务，但是这违背了采用事件表的初衷。一种解决方法是将事件的消费方创建成幂等的，即消费方可以多次消费同一个事件而不污染系统数据。这个过程大致为：整个过程中事件发送和数据库更新采用各自的事务管理，此时有可能发生的情况是事件发送成功而数据库更新失败，这样在下一次事件发布操作中，由于先前发布过的事件在数据库中依然是“未发布”状态，该事件将被重新发布到消息系统中，导致事件重复，但由于事件的消费方是幂等的，因此事件重复不会存在问题。\n另外一个需要注意的问题是持久化机制的选择。其实对于DDD中的聚合根来说，NoSQL是相比于关系型数据库更合适的选择，比如用MongoDB的Document保存聚合根便是种很自然的方式。但是多数NoSQL是不支持ACID的，也就是说不能保证聚合更新和事件发布之间的原子性。还好，关系型数据库也在向NoSQL方向发展，比如新版本的PostgreSQL(版本9.4)和MySQL（版本5.7）已经能够提供具备NoSQL特征的JSON存储和基于JSON的查询。此时，我们可以考虑将聚合根序列化成JSON格式的数据进行保存，从而避免了使用重量级的ORM工具，又可以在多个数据之间保证ACID，何乐而不为？\n五、总结领域事件主要用于解耦微服务，此时各个微服务之间将形成最终一致性。事件风暴活动有助于我们对微服务进行拆分，并且有助于我们深入了解某个领域。领域事件作为已经发生过的历史数据，在建模时应该将其创建为不可变的特殊值对象。存在多种方式用于发布领域事件，其中“在聚合中临时保存领域事件”的方式是值得推崇的。另外，我们需要考虑到聚合更新和事件发布之间的原子性，可以考虑使用XA事务或者采用单独的事件表。为了避免事件重复带来的问题，最好的方式是将事件的消费方创建为幂等的。\n","categories":["领域驱动设计"]},{"title":"如何划分限界上下文","url":"/posts/43572/","content":"一、聚合分组法和它的问题在事件风暴工作坊中，常用的划分限界上下文的方法是：\n\n对前一步（事件风暴）产生的聚合进行分组，通过业务的内聚性和关联度划分边界，结合限界上下文的定义进行判断，并给出上下文名称。\n[服务化设计阶段路径方案]\n\n我将其称之为“聚合分组法”。然而面对一堆聚合，要得出一套合理的分组是非常困难的：\n\n“相关性”全凭经验\n相关性是一个过于抽象的规则，非常依赖经验。\n举个例子。在一个活动运营系统中，有“注册奖励活动”、“注册奖励规则”、“任务奖励活动”、“任务奖励规则”等概念。是把所有的“活动”分为一组，所有“规则”分为一组，还是把“注册”相关的分为一组，把“任务”相关的分为一组？这是个让人头疼的问题。也许你会说需要业务人员的输入，但是业务人员很可能只会告诉你这些概念之间都有关系。\n\n不健康的聚合上下文\n聚合分组法很容易导向一种按照聚合划分的架构。服务围绕聚合建设，而非针对某个业务价值，也就无法提供正确的业务价值。围绕聚合建设的服务，看上去可以复用，但是会造成服务间的紧耦合，容易成为最糟糕的分布式单体架构：\n\n当架构是分布式单体时，往往需要同时修改多个服务，同时部署多个服务、服务之间调用非常频繁。\n[You’re Not Actually Building Microservices]\n\n聚合分组法也无法很好的识别“重复的概念”问题（[领域驱动设计]14.1，指某一个概念，应该被设计成多个模型，因为它们有不同的规则，甚至有不同的数据）。使用聚合分组法往往导致把带着这样的聚合简单的放到某个限界上下文中。\n\n隐藏的划分方案\n还很可能是这种情况：在使用聚合分组法时，架构师已经有一个隐藏在心里的模糊的划分方案，在划分限界上下文时都是往该方案上靠。但是由于这个划分方案只是模糊存在于架构师的脑中，并没有拿出来讨论，很可能经不起推敲，最终无法言说，沦为“by experience”。\n\n\n二、如何划分限界上下文如何划分限界上下文？在回答这个问题前，让我们先看看限界上下文到底是什么。\n在[领域驱动设计]第14章提出了著名的限界上下文。限界上下文是为了分解大型模型：\n\n然而在几乎所有这种规模的组织中，整个业务模型太大且过于复杂以至于难以管理，甚至很难把它作为一个整体来理解。我们必须把系统分解为较小的组成部分，无论在概念还是在实现上。\n有时，企业系统会集成各种不同来源的子系统，或者包含诸多属于完全不同领域的应用程序。要把这些不同部分中隐含的模型统一起来是不可能的。通过为每个模型显式地定义一个限界上下文，然后在必要的情况下定义它与其他上下文的关系，建模人员就可以避免模型变得混乱。\n[领域驱动设计 第四部分] (https://book.douban.com/subject/26819666/)\n\n\n限界上下文告诉我们，同一个概念，不必总是对应于一个单一模型，也可以对应于多个模型。用限界上下文明确模型要解决的问题，可以保持每个模型的清晰。限界上下文是领域模型的边界，也就是领域知识的边界。和上下文主题紧密相关的模型内聚在上下文内，而其他模型被会分到其他限界上下文中。限界上下文内的领域知识是高内聚低耦合的。\n\n限界上下文的主题是什么呢？我认为是子域。每个限界上下文专注于解决某个特定的子域的问题。每个子域都对应一个明确的问题，提供独立的价值，所以每个子域都相对独立。子域及其对应的限界上下文中的模型会因为其要解决的问题变化而变化，不会因为其他子域的变化而变化，即低耦合；当一个子域发生变化时，只需要修改其对应限界上下文中的模型，不需要变动其他子域的模型，即高内聚。\nEvans也谈论了限界上下文和子域的关系：\n\nOne confusion that Evans sometimes notices in teams is differentiating between bounded contexts and subdomains. In an ideal world they coincide, but in reality they are often misaligned.\nEvans有时会在团队中发现的一个困惑，就是如何区分限界上下文和子域。在理想的世界中它们是重合的，但在现实世界中它们常常是错位的。\n[Defining Bounded Contexts — Eric Evans at DDD Europe]\n\n当我们设计一个新系统或者设计遗留系统的目标架构时，我们往往会按照理想的方式进行设计。而在理想情况下，子域和限界上下文是重合的。\n[领域驱动设计精粹]中也讲述了一个通过寻找核心域相关的概念来识别限界上下文的方法。\n三、如何分解子域根据子域来识别限界上下文，那么子域如何得到呢？我们通过分解问题域的方式，将整个问题域分解成若干个更小、更简单、更容易解决的问题子域。\n我们需要某种方法，将领域分解成逻辑上相互独立且没有交叉的子域。在这里的方法是通过产品愿景，识别核心域，进而识别核心域周边的子域。\n识别核心域由于核心域是最明显、最容易识别出来的子域，所以我们先从核心域开始。\n每一个子域甚至每一个领域模型都是为了产品愿景而存在的。我们分解子域的第一步，就是从产品愿景中获取核心域。产品愿景包含“相对抽象的产品价值”，以及“实现该价值的主要功能”。其中，主要功能就是我们寻找核心域的依据。想象一下，如果要做MVP的话，我们会挑选最能够提供其核心价值的功能来开发，以验证产品价值。MVP往往就是核心域。\n以上述活动运营系统为例，其产品愿景是通过各种吸引用户的优惠活动，以帮助客户通过活动提升用户量和知名度。其核心域是给客户提供吸引用户的多样的灵活的活动，包括活动形式、活动规则和多种奖励。\n识别核心域周边的子域核心域识别出来了，接下来就是识别核心域周边的子域。核心域往往不会独立存在，会有其他子域同核心域一起才能达成业务目标。这里需要回答的问题是：\n\n有哪些子域是用来支撑核心域的？\n这些子域是帮助核心域更好的工作。例如提供审批流程以配置核心域，提供各种辅助功能更好的为核心域提供内容。\n\n有哪些子域是核心域衍生出来的？\n核心域经常会产生一些数据，这些数据也有其价值。比如产生各种报表，活动奖励的发放记录。\n\n有哪些子域是用来支撑或衍生自这些新识别出的子域的？\n用来支撑核心域的子域、以及核心域衍生的子域，也有各自的支撑子域和衍生子域。\n\n\n\n识别出来的每个子域只对应一个问题，子域之间是相互独立的，没有交叉，不是包含关系。所以子域加起来就是整个领域。\n也可以通过角色、时间等因素分解子域。解决不同角色的问题可能分属不同子域，比如用户参与活动、运营人员配置活动分属不同子域，两个子域的变化原因不同；不同时间使用的功能可能属于不到子域，比如先有运营人员配置活动，再有用户参与活动，配置活动和参与活动分属不同子域。\n如果按照聚合分组划分限界上下文，很可能出现“活动上下文”，同时活动模型，即承担运营人员配置的职责，又承担用户参与规则校验的职责，这会导致职责过多，违背了单一职责。另外活动规则校验的模块需要支持高并发，需要使用和配置模块不同的技术架构。如果这些相似的概念和不同的技术实现属于不同的上下文，就可以保持各自模型的完整，技术上也可以做到独立演进。\n子域的粒度理论上子域仍然可以被分解。例如活动子域可以分解为活动参与规则子域、奖励子域等。那么子域粒度多大是合适的呢？\n我们希望每个子域可以解决某个特定的问题，让这个问题的解决方案都内聚在子域对应的限界上下文内，所以如果问题的再分解没有的边界并不清晰，建议先不分解。随意的拆分会导致成为“分布式单体”。\n四、识别限界上下文\n一个限界上下文封装了一个相对独立子领域的领域模型和服务。\n子域subdomain和限界上下文某种意义上是互相印证的\nDDD战术篇：领域模型的应用\n\n这个时候我们通过事件风暴得到的领域模型就可以出场了。领域模型和子域都是从业务知识里分析得到的，将两者匹配起来可以再次验证我们对于业务的理解、子域的分解和领域模型是否合理。\n为每个子域创建一个解决其问题的限界上下文，然后为每个领域模型找到其归属的限界上下文。每个领域事件都是为了解决某个问题，它和它相关的领域模型就应该放在这个问题子域对应的限界上下文里。\n比如“活动已上线“这个事件，由运营人员在配置时触发，会导致用户可以开始参与活动。那么这个事件及其对应的“活动”概念应该被分为两个模型，分别归属于活动配置子域对应的“活动配置上下文”和活动子域对应的“活动上下文”。\n为领域模型寻找归属完成后，我们会发现这么几个情况。\n\n同一个概念可能会出现在多个限界上下文中。发生这种情况很正常，说明这多个子域都需要这个概念，而且很可能不同子域的领域模型不完全相同。\n比如刚才说到“活动”既存在于“活动上下文”中，又在“活动配置上下文”中。这里我们就很好的识别出了“重复的概念”问题。\n\n也有一些概念重复在多个限界上下文中，这些概念和该上下文的主题并没有紧密的关系。这些模型可以单独出一个限界上下文，用以同时支撑多个限界上下文，以减轻限界上下文的负担。\n\n有时候某个模型找不到合适的限界上下文，说明很可能是遗漏了一个子域，那就需要回到“分解子域”步骤，重新审视产品愿景。\n\n\n\n聚合分组法采用“相关性”来划分限界上下文，其问题在于缺少一个主题，而子域恰好可以用来提供这个主题。本文的“愿景”-“核心域”-“周边子域”方法，不是唯一分解问题域的方法，任何可以将领域分解成高内聚低耦合的子域的方法都是可行的方法。\n\n领域驱动设计\n实现领域驱动设计\n微服务设计\n微服务 | Martin Fowler\nPattern: Decompose by subdomain\nDDD &amp; Microservices\nDDD战术篇：领域模型的应用\n当Subdomain遇见Bounded Context\n【博客】使用 DDD 指导微服务拆分的逻辑\n领域驱动设计实践（战略篇）\nPROBLEM SPACE vs SOLUTION SPACE\nYou’re Not Actually Building Microservices\n利用事件风暴发现限界上下文\nPattern: Saga\n精益价值树\nComplicated\nDefining Bounded Contexts — Eric Evans at DDD Europe\n领域驱动设计精粹\n\n","categories":["领域驱动设计"]},{"title":"单服务器高性能模式：PPC与TPC","url":"/posts/34851/","content":"高性能是每个程序员的追求，无论我们是做一个系统还是写一行代码，都希望能够达到高性能的效果，而高性能又是最复杂的一环，磁盘、操作系统、CPU、内存、缓存、网络、编程语言、架构等，每个都有可能影响系统达到高性能，一行不恰当的 debug 日志，就可能将服务器的性能从 TPS 30000 降低到 8000；一个 tcp_nodelay 参数，就可能将响应时间从 2 毫秒延长到 40 毫秒。因此，要做到高性能计算是一件很复杂很有挑战的事情，软件系统开发过程中的不同阶段都关系着高性能最终是否能够实现。\n站在架构师的角度，当然需要特别关注高性能架构的设计。高性能架构设计主要集中在两方面：\n\n尽量提升单服务器的性能，将单服务器的性能发挥到极致。\n如果单服务器无法支撑性能，设计服务器集群方案。\n\n除了以上两点，最终系统能否实现高性能，还和具体的实现及编码相关。但架构设计是高性能的基础，如果架构设计没有做到高性能，则后面的具体实现和编码能提升的空间是有限的。形象地说，架构设计决定了系统性能的上限，实现细节决定了系统性能的下限。\n单服务器高性能的关键之一就是服务器采取的并发模型，并发模型有如下两个关键设计点：\n\n服务器如何管理连接。\n服务器如何处理请求。\n\n以上两个设计点最终都和操作系统的 I/O 模型及进程模型相关。\n\nI/O 模型：阻塞、非阻塞、同步、异步。\n进程模型：单进程、多进程、多线程。\n\n在下面详细介绍并发模型时会用到上面这些基础的知识点，所以我建议你先检测一下对这些基础知识的掌握情况，更多内容你可以参考《UNIX 网络编程》三卷本。今天，我们先来看看单服务器高性能模式：PPC 与 TPC。\n一、PPCPPC 是 Process Per Connection 的缩写，其含义是指每次有新的连接就新建一个进程去专门处理这个连接的请求，这是传统的 UNIX 网络服务器所采用的模型。基本的流程图是：\n\n\n父进程接受连接（图中 accept）。\n父进程“fork”子进程（图中 fork）。\n子进程处理连接的读写请求（图中子进程 read、业务处理、write）。\n子进程关闭连接（图中子进程中的 close）。\n\n注意，图中有一个小细节，父进程“fork”子进程后，直接调用了 close，看起来好像是关闭了连接，其实只是将连接的文件描述符引用计数减一，真正的关闭连接是等子进程也调用 close 后，连接对应的文件描述符引用计数变为 0 后，操作系统才会真正关闭连接，更多细节请参考《UNIX 网络编程：卷一》。\nPPC 模式实现简单，比较适合服务器的连接数没那么多的情况，例如数据库服务器。对于普通的业务服务器，在互联网兴起之前，由于服务器的访问量和并发量并没有那么大，这种模式其实运作得也挺好，世界上第一个 web 服务器 CERN httpd 就采用了这种模式（具体你可以参考https://en.wikipedia.org/wiki/CERN_httpd）。互联网兴起后，服务器的并发和访问量从几十剧增到成千上万，这种模式的弊端就凸显出来了，主要体现在这几个方面：\n\nfork 代价高：站在操作系统的角度，创建一个进程的代价是很高的，需要分配很多内核资源，需要将内存映像从父进程复制到子进程。即使现在的操作系统在复制内存映像时用到了 Copy on Write（写时复制）技术，总体来说创建进程的代价还是很大的。\n父子进程通信复杂：父进程“fork”子进程时，文件描述符可以通过内存映像复制从父进程传到子进程，但“fork”完成后，父子进程通信就比较麻烦了，需要采用 IPC（Interprocess Communication）之类的进程通信方案。例如，子进程需要在 close 之前告诉父进程自己处理了多少个请求以支撑父进程进行全局的统计，那么子进程和父进程必须采用 IPC 方案来传递信息。\n支持的并发连接数量有限：如果每个连接存活时间比较长，而且新的连接又源源不断的进来，则进程数量会越来越多，操作系统进程调度和切换的频率也越来越高，系统的压力也会越来越大。因此，一般情况下，PPC 方案能处理的并发连接数量最大也就几百。\n\n二、preforkPPC 模式中，当连接进来时才 fork 新进程来处理连接请求，由于 fork 进程代价高，用户访问时可能感觉比较慢，prefork 模式的出现就是为了解决这个问题。\n顾名思义，prefork 就是提前创建进程（pre-fork）。系统在启动的时候就预先创建好进程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去 fork 进程的操作，让用户访问更快、体验更好。prefork 的基本示意图是：\n\nprefork 的实现关键就是多个子进程都 accept 同一个 socket，当有新的连接进入时，操作系统保证只有一个进程能最后 accept 成功。但这里也存在一个小小的问题：“惊群”现象，就是指虽然只有一个子进程能 accept 成功，但所有阻塞在 accept 上的子进程都会被唤醒，这样就导致了不必要的进程调度和上下文切换了。幸运的是，操作系统可以解决这个问题，例如 Linux 2.6 版本后内核已经解决了 accept 惊群问题。\nprefork 模式和 PPC 一样，还是存在父子进程通信复杂、支持的并发连接数量有限的问题，因此目前实际应用也不多。Apache 服务器提供了 MPM prefork 模式，推荐在需要可靠性或者与旧软件兼容的站点时采用这种模式，默认情况下最大支持 256 个并发连接。\n三、TPCTPC 是 Thread Per Connection 的缩写，其含义是指每次有新的连接就新建一个线程去专门处理这个连接的请求。与进程相比，线程更轻量级，创建线程的消耗比进程要少得多；同时多线程是共享进程内存空间的，线程通信相比进程通信更简单。因此，TPC 实际上是解决或者弱化了 PPC fork 代价高的问题和父子进程通信复杂的问题。\nTPC 的基本流程是：\n\n\n父进程接受连接（图中 accept）。\n父进程创建子线程（图中 pthread）。\n子线程处理连接的读写请求（图中子线程 read、业务处理、write）。\n子线程关闭连接（图中子线程中的 close）。\n\n注意，和 PPC 相比，主进程不用“close”连接了。原因是在于子线程是共享主进程的进程空间的，连接的文件描述符并没有被复制，因此只需要一次 close 即可。\nTPC 虽然解决了 fork 代价高和进程通信复杂的问题，但是也引入了新的问题，具体表现在：\n\n创建线程虽然比创建进程代价低，但并不是没有代价，高并发时（例如每秒上万连接）还是有性能问题。\n无须进程间通信，但是线程间的互斥和共享又引入了复杂度，可能一不小心就导致了死锁问题。\n多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出（例如内存越界）。\n\n除了引入了新的问题，TPC 还是存在 CPU 线程调度和切换代价的问题。因此，TPC 方案本质上和 PPC 方案基本类似，在并发几百连接的场景下，反而更多地是采用 PPC 的方案，因为 PPC 方案不会有死锁的风险，也不会多进程互相影响，稳定性更高。\n四、prethreadTPC 模式中，当连接进来时才创建新的线程来处理连接请求，虽然创建线程比创建进程要更加轻量级，但还是有一定的代价，而 prethread 模式就是为了解决这个问题。\n和 prefork 类似，prethread 模式会预先创建线程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去创建线程的操作，让用户感觉更快、体验更好。\n由于多线程之间数据共享和通信比较方便，因此实际上 prethread 的实现方式相比 prefork 要灵活一些，常见的实现方式有下面几种：\n\n主进程 accept，然后将连接交给某个线程处理。\n子线程都尝试去 accept，最终只有一个线程 accept 成功，方案的基本示意图如下：\n\n\nApache 服务器的 MPM worker 模式本质上就是一种 prethread 方案，但稍微做了改进。Apache 服务器会首先创建多个进程，每个进程里面再创建多个线程，这样做主要是为了考虑稳定性，即：即使某个子进程里面的某个线程异常导致整个子进程退出，还会有其他子进程继续提供服务，不会导致整个服务器全部挂掉。\nprethread 理论上可以比 prefork 支持更多的并发连接，Apache 服务器 MPM worker 模式默认支持 16 × 25 = 400 个并发处理线程。\n","categories":["从0开始学架构"]},{"title":"单服务器高性能模式：Reactor与Proactor","url":"/posts/1102/","content":"一、ReactorPPC 模式最主要的问题就是每个连接都要创建进程（为了描述简洁，这里只以 PPC 和进程为例，实际上换成 TPC 和线程，原理是一样的），连接结束后进程就销毁了，这样做其实是很大的浪费。为了解决这个问题，一个自然而然的想法就是资源复用，即不再单独为每个连接创建进程，而是创建一个进程池，将连接分配给进程，一个进程可以处理多个连接的业务。\n引入资源池的处理方式后，会引出一个新的问题：进程如何才能高效地处理多个连接的业务？当一个连接一个进程时，进程可以采用“read -&gt; 业务处理 -&gt; write”的处理流程，如果当前连接没有数据可以读，则进程就阻塞在 read 操作上。这种阻塞的方式在一个连接一个进程的场景下没有问题，但如果一个进程处理多个连接，进程阻塞在某个连接的 read 操作上，此时即使其他连接有数据可读，进程也无法去处理，很显然这样是无法做到高性能的。\n解决这个问题的最简单的方式是将 read 操作改为非阻塞，然后进程不断地轮询多个连接。这种方式能够解决阻塞的问题，但解决的方式并不优雅。首先，轮询是要消耗 CPU 的；其次，如果一个进程处理几千上万的连接，则轮询的效率是很低的。\n为了能够更好地解决上述问题，很容易可以想到，只有当连接上有数据的时候进程才去处理，这就是 I/O 多路复用技术的来源。\nI/O 多路复用技术归纳起来有两个关键实现点：\n\n当多条连接共用一个阻塞对象后，进程只需要在一个阻塞对象上等待，而无须再轮询所有连接，常见的实现方式有 select、epoll、kqueue 等。\n当某条连接有新的数据可以处理时，操作系统会通知进程，进程从阻塞状态返回，开始进行业务处理。\n\nI/O 多路复用结合线程池，完美地解决了 PPC 和 TPC 的问题，而且“大神们”给它取了一个很牛的名字：Reactor，中文是“反应堆”。联想到“核反应堆”，听起来就很吓人，实际上这里的“反应”不是聚变、裂变反应的意思，而是“事件反应”的意思，可以通俗地理解为“来了一个事件我就有相应的反应”，这里的“我”就是 Reactor，具体的反应就是我们写的代码，Reactor 会根据事件类型来调用相应的代码进行处理。Reactor 模式也叫 Dispatcher 模式（在很多开源的系统里面会看到这个名称的类，其实就是实现 Reactor 模式的），更加贴近模式本身的含义，即 I/O 多路复用统一监听事件，收到事件后分配（Dispatch）给某个进程。\nReactor 模式的核心组成部分包括 Reactor 和处理资源池（进程池或线程池），其中 Reactor 负责监听和分配事件，处理资源池负责处理事件。初看 Reactor 的实现是比较简单的，但实际上结合不同的业务场景，Reactor 模式的具体实现方案灵活多变，主要体现在：\n\nReactor 的数量可以变化：可以是一个 Reactor，也可以是多个 Reactor。\n资源池的数量可以变化：以进程为例，可以是单个进程，也可以是多个进程（线程类似）。\n\n将上面两个因素排列组合一下，理论上可以有 4 种选择，但由于“多 Reactor 单进程”实现方案相比“单 Reactor 单进程”方案，既复杂又没有性能优势，因此“多 Reactor 单进程”方案仅仅是一个理论上的方案，实际没有应用。\n最终 Reactor 模式有这三种典型的实现方案：\n\n单 Reactor 单进程 / 线程。\n单 Reactor 多线程。\n多 Reactor 多进程 / 线程。\n\n以上方案具体选择进程还是线程，更多地是和编程语言及平台相关。例如，Java 语言一般使用线程（例如，Netty），C 语言使用进程和线程都可以。例如，Nginx 使用进程，Memcache 使用线程。\n1.单 Reactor 单进程 / 线程单 Reactor 单进程 / 线程的方案示意图如下（以进程为例）：\n\n注意，select、accept、read、send 是标准的网络编程 API，dispatch 和“业务处理”是需要完成的操作，其他方案示意图类似。\n详细说明一下这个方案：\n\nReactor 对象通过 select 监控连接事件，收到事件后通过 dispatch 进行分发。\n如果是连接建立的事件，则由 Acceptor 处理，Acceptor 通过 accept 接受连接，并创建一个 Handler 来处理连接后续的各种事件。\n如果不是连接建立事件，则 Reactor 会调用连接对应的 Handler（第 2 步中创建的 Handler）来进行响应。\nHandler 会完成 read-&gt; 业务处理 -&gt; send 的完整业务流程。\n\n单 Reactor 单进程的模式优点就是很简单，没有进程间通信，没有进程竞争，全部都在同一个进程内完成。但其缺点也是非常明显，具体表现有：\n\n只有一个进程，无法发挥多核 CPU 的性能；只能采取部署多个系统来利用多核 CPU，但这样会带来运维复杂度，本来只要维护一个系统，用这种方式需要在一台机器上维护多套系统。\nHandler 在处理某个连接上的业务时，整个进程无法处理其他连接的事件，很容易导致性能瓶颈。\n\n因此，单 Reactor 单进程的方案在实践中应用场景不多，只适用于业务处理非常快速的场景，目前比较著名的开源软件中使用单 Reactor 单进程的是 Redis。\n需要注意的是，C 语言编写系统的一般使用单 Reactor 单进程，因为没有必要在进程中再创建线程；而 Java 语言编写的一般使用单 Reactor 单线程，因为 Java 虚拟机是一个进程，虚拟机中有很多线程，业务线程只是其中的一个线程而已。\n2.单 Reactor 多线程为了克服单 Reactor 单进程 / 线程方案的缺点，引入多进程 / 多线程是显而易见的，这就产生了第 2 个方案：单 Reactor 多线程。\n单 Reactor 多线程方案示意图是：\n\n我来介绍一下这个方案：\n\n主线程中，Reactor 对象通过 select 监控连接事件，收到事件后通过 dispatch 进行分发。\n如果是连接建立的事件，则由 Acceptor 处理，Acceptor 通过 accept 接受连接，并创建一个 Handler 来处理连接后续的各种事件。\n如果不是连接建立事件，则 Reactor 会调用连接对应的 Handler（第 2 步中创建的 Handler）来进行响应。\nHandler 只负责响应事件，不进行业务处理；Handler 通过 read 读取到数据后，会发给 Processor 进行业务处理。\nProcessor 会在独立的子线程中完成真正的业务处理，然后将响应结果发给主进程的 Handler 处理；Handler 收到响应后通过 send 将响应结果返回给 client。\n\n单 Reator 多线程方案能够充分利用多核多 CPU 的处理能力，但同时也存在下面的问题：\n\n多线程数据共享和访问比较复杂。例如，子线程完成业务处理后，要把结果传递给主线程的 Reactor 进行发送，这里涉及共享数据的互斥和保护机制。以 Java 的 NIO 为例，Selector 是线程安全的，但是通过 Selector.selectKeys() 返回的键的集合是非线程安全的，对 selected keys 的处理必须单线程处理或者采取同步措施进行保护。\nReactor 承担所有事件的监听和响应，只在主线程中运行，瞬间高并发时会成为性能瓶颈。\n\n你可能会发现，我只列出了“单 Reactor 多线程”方案，没有列出“单 Reactor 多进程”方案，这是什么原因呢？主要原因在于如果采用多进程，子进程完成业务处理后，将结果返回给父进程，并通知父进程发送给哪个 client，这是很麻烦的事情。因为父进程只是通过 Reactor 监听各个连接上的事件然后进行分配，子进程与父进程通信时并不是一个连接。如果要将父进程和子进程之间的通信模拟为一个连接，并加入 Reactor 进行监听，则是比较复杂的。而采用多线程时，因为多线程是共享数据的，因此线程间通信是非常方便的。虽然要额外考虑线程间共享数据时的同步问题，但这个复杂度比进程间通信的复杂度要低很多。\n3.多 Reactor 多进程 / 线程为了解决单 Reactor 多线程的问题，最直观的方法就是将单 Reactor 改为多 Reactor，这就产生了第 3 个方案：多 Reactor 多进程 / 线程。\n多 Reactor 多进程 / 线程方案示意图是（以进程为例）：\n\n方案详细说明如下：\n\n父进程中 mainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 接收，将新的连接分配给某个子进程。\n子进程的 subReactor 将 mainReactor 分配的连接加入连接队列进行监听，并创建一个 Handler 用于处理连接的各种事件。\n当有新的事件发生时，subReactor 会调用连接对应的 Handler（即第 2 步中创建的 Handler）来进行响应。\nHandler 完成 read→业务处理→send 的完整业务流程。\n\n多 Reactor 多进程 / 线程的方案看起来比单 Reactor 多线程要复杂，但实际实现时反而更加简单，主要原因是：\n\n父进程和子进程的职责非常明确，父进程只负责接收新连接，子进程负责完成后续的业务处理。\n父进程和子进程的交互很简单，父进程只需要把新连接传给子进程，子进程无须返回数据。\n子进程之间是互相独立的，无须同步共享之类的处理（这里仅限于网络模型相关的 select、read、send 等无须同步共享，“业务处理”还是有可能需要同步共享的）。\n\n目前著名的开源系统 Nginx 采用的是多 Reactor 多进程，采用多 Reactor 多线程的实现有 Memcache 和 Netty。\n我多说一句，Nginx 采用的是多 Reactor 多进程的模式，但方案与标准的多 Reactor 多进程有差异。具体差异表现为主进程中仅仅创建了监听端口，并没有创建 mainReactor 来“accept”连接，而是由子进程的 Reactor 来“accept”连接，通过锁来控制一次只有一个子进程进行“accept”，子进程“accept”新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程，更多细节请查阅相关资料或阅读 Nginx 源码。\n二、ProactorReactor 是非阻塞同步网络模型，因为真正的 read 和 send 操作都需要用户进程同步操作。这里的“同步”指用户进程在执行 read 和 send 这类 I/O 操作的时候是同步的，如果把 I/O 操作改为异步就能够进一步提升性能，这就是异步网络模型 Proactor。\nProactor 中文翻译为“前摄器”比较难理解，与其类似的单词是 proactive，含义为“主动的”，因此我们照猫画虎翻译为“主动器”反而更好理解。Reactor 可以理解为“来了事件我通知你，你来处理”，而 Proactor 可以理解为“来了事件我来处理，处理完了我通知你”。这里的“我”就是操作系统内核，“事件”就是有新连接、有数据可读、有数据可写的这些 I/O 事件，“你”就是我们的程序代码。\nProactor 模型示意图是：\n\n详细介绍一下 Proactor 方案：\n\nProactor Initiator 负责创建 Proactor 和 Handler，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核。\nAsynchronous Operation Processor 负责处理注册请求，并完成 I/O 操作。\nAsynchronous Operation Processor 完成 I/O 操作后通知 Proactor。\nProactor 根据不同的事件类型回调不同的 Handler 进行业务处理。\nHandler 完成业务处理，Handler 也可以注册新的 Handler 到内核进程。\n\n理论上 Proactor 比 Reactor 效率要高一些，异步 I/O 能够充分利用 DMA 特性，让 I/O 操作与计算重叠，但要实现真正的异步 I/O，操作系统需要做大量的工作。目前 Windows 下通过 IOCP 实现了真正的异步 I/O，而在 Linux 系统下的 AIO 并不完善，因此在 Linux 下实现高并发网络编程时都是以 Reactor 模式为主。所以即使 Boost.Asio 号称实现了 Proactor 模型，其实它在 Windows 下采用 IOCP，而在 Linux 下是用 Reactor 模式（采用 epoll）模拟出来的异步模型。\n","categories":["从0开始学架构"]},{"title":"不可变基础设施【从微服务到云原生】","url":"/posts/34368/","content":"\n云原生定义（Cloud Native Definition）Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.\nThese techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.\n云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。\n这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。\n​                                                                                                                                                     —— Cloud Native Definition, CNCF，2018\n\n“不可变基础设施”这个概念由来已久。2012 年 Martin Fowler 设想的“凤凰服务器”与 2013 年 Chad Fowler 正式提出的“不可变基础设施，都阐明了基础设施不变性所能带来的益处。在云原生基金会定义的“云原生”概念中，“不可变基础设施”提升到了与微服务平级的重要程度，此时它的内涵已不再局限于方便运维、程序升级和部署的手段，而是升华为向应用代码隐藏分布式架构复杂度、让分布式架构得以成为一种可普遍推广的普适架构风格的必要前提。\n前一章以“分布式的基石”为题，介绍了微服务中关键的技术问题与解决方案，解决这些问题原本应该是架构师与程序员的本职工作。在本章，笔者会以容器、编排系统和服务网格的发展为主线，介绍虚拟化容器与服务网格是如何模糊掉软件与硬件之间的界限，如何在基础设施与通讯层面上帮助微服务隐藏复杂性，解决原本只能由程序员通过软件编程来解决的分布式问题。\n","categories":["凤凰架构"]},{"title":"不可变基础设施【虚拟化容器】","url":"/posts/30140/","content":"容器是云计算、微服务等诸多软件业界核心技术的共同基石，容器的首要目标是让软件分发部署过程从传统的发布安装包、靠人工部署转变为直接发布已经部署好的、包含整套运行环境的虚拟化镜像。在容器技术成熟之前，主流的软件部署过程是由系统管理员编译或下载好二进制安装包，根据软件的部署说明文档准备好正确的操作系统、第三方库、配置文件、资源权限等各种前置依赖以后，才能将程序正确地运行起来。Chad Fowler在提出“不可变基础设施”这个概念的文章《Trash Your Servers and Burn Your Code》里，开篇就直接吐槽：要把一个不知道打过多少个升级补丁，不知道经历了多少任管理员的系统迁移到其他机器上，毫无疑问会是一场灾难。\n让软件能够在任何环境、任何物理机器上达到“一次编译，到处运行”曾是 Java 早年的宣传口号，这并不是一个简单的目标，不设前提的“到处运行”，仅靠 Java 语言和 Java 虚拟机是不可能达成的，因为一个计算机软件要能够正确运行，需要有以下三方面的兼容性来共同保障（这里仅讨论软件兼容性，不去涉及“如果没有摄像头就无法运行照相程序”这类问题）：\n\nISA 兼容：目标机器指令集兼容性，譬如 ARM 架构的计算机无法直接运行面向 x86 架构编译的程序。\nABI 兼容：目标系统或者依赖库的二进制兼容性，譬如 Windows 系统环境中无法直接运行 Linux 的程序，又譬如 DirectX 12 的游戏无法运行在 DirectX 9 之上。\n环境兼容：目标环境的兼容性，譬如没有正确设置的配置文件、环境变量、注册中心、数据库地址、文件系统的权限等等，任何一个环境因素出现错误，都会让你的程序无法正常运行。\n\n额外知识：ISA 与 ABI\n指令集架构（Instruction Set Architecture，ISA）是计算机体系结构中与程序设计有关的部分，包含了基本数据类型，指令集，寄存器，寻址模式，存储体系，中断，异常处理以及外部 I/O。指令集架构包含一系列的 Opcode 操作码（即通常所说的机器语言），以及由特定处理器执行的基本命令。\n应用二进制接口（Application Binary Interface，ABI）是应用程序与操作系统之间或其他依赖库之间的低级接口。ABI 涵盖了各种底层细节，如数据类型的宽度大小、对象的布局、接口调用约定等等。ABI 不同于应用程序接口（Application Programming Interface，API），API 定义的是源代码和库之间的接口，因此同样的代码可以在支持这个 API 的任何系统中编译，而 ABI 允许编译好的目标代码在使用兼容 ABI 的系统中无需改动就能直接运行。\n笔者把使用仿真（Emulation）以及虚拟化（Virtualization）技术来解决以上三项兼容性问题的方法都统称为虚拟化技术。根据抽象目标与兼容性高低的不同，虚拟化技术又分为下列五类：\n\n指令集虚拟化（ISA Level Virtualization）。通过软件来模拟不同 ISA 架构的处理器工作过程，将虚拟机发出的指令转换为符合本机 ISA 的指令，代表为QEMU和Bochs。指令集虚拟化就是仿真，能提供了几乎完全不受局限的兼容性，甚至能做到直接在 Web 浏览器上运行完整操作系统这种令人惊讶的效果，但由于每条指令都要由软件来转换和模拟，它也是性能损失最大的虚拟化技术。\n硬件抽象层虚拟化（Hardware Abstraction Level Virtualization）。以软件或者直接通过硬件来模拟处理器、芯片组、内存、磁盘控制器、显卡等设备的工作过程。既可以使用纯软件的二进制翻译来模拟虚拟设备，也可以由硬件的Intel VT-d、AMD-Vi这类虚拟化技术，将某个物理设备直通（Passthrough）到虚拟机中使用，代表为VMware ESXi和Hyper-V。如果没有预设语境，一般人们所说的“虚拟机”就是指这一类虚拟化技术。\n操作系统层虚拟化（OS Level Virtualization）。无论是指令集虚拟化还是硬件抽象层虚拟化，都会运行一套完全真实的操作系统来解决 ABI 兼容性和环境兼容性问题，虽然 ISA 兼容性是虚拟出来的，但 ABI 兼容性和环境兼容性却是真实存在的。而操作系统层虚拟化则不会提供真实的操作系统，而是采用隔离手段，使得不同进程拥有独立的系统资源和资源配额，看起来仿佛是独享了整个操作系统一般，其实系统的内核仍然是被不同进程所共享的。操作系统层虚拟化的另一个名字就是本章的主角“容器化”（Containerization），由此可见，容器化仅仅是虚拟化的一个子集，只能提供操作系统内核以上的部分 ABI 兼容性与完整的环境兼容性。这意味着如果没有其他虚拟化手段的辅助，在 Windows 系统上是不可能运行 Linux 的 Docker 镜像的（现在可以，是因为有其他虚拟机或者 WSL2 的支持），反之亦然。也同样决定了如果 Docker 宿主机的内核版本是 Linux Kernel 5.6，那无论上面运行的镜像是 Ubuntu、RHEL、Fedora、Mint 或者任何发行版的镜像，看到的内核一定都是相同的 Linux Kernel 5.6。容器化牺牲了一定的隔离性与兼容性，换来的是比前两种虚拟化更高的启动速度、运行性能和更低的执行负担。\n运行库虚拟化（Library Level Virtualization）。与操作系统虚拟化采用隔离手段来模拟系统不同，运行库虚拟化选择使用软件翻译的方法来模拟系统，它以一个独立进程来代替操作系统内核来提供目标软件运行所需的全部能力，这种虚拟化方法获得的 ABI 兼容性高低，取决于软件是否能足够准确和全面地完成翻译工作，其代表为WINE（Wine Is Not an Emulator 的缩写，一款在 Linux 下运行 Windows 程序的软件）和WSL（特指 Windows Subsystem for Linux Version 1）。\n语言层虚拟化（Programming Language Level Virtualization）。由虚拟机将高级语言生成的中间代码转换为目标机器可以直接执行的指令，代表为 Java 的 JVM 和.NET 的 CLR。虽然厂商肯定会提供不同系统下都有相同接口的标准库，但本质上这种虚拟化并不直接解决任何 ABI 兼容性和环境兼容性问题。\n\n一、容器的崛起容器的最初的目的不是为了部署软件，而是为了隔离计算机中的各类资源，以便降低软件开发、测试阶段可能产生的误操作风险，或者专门充当蜜罐，吸引黑客的攻击，以便监视黑客的行为。下面，笔者将以容器发展历史为线索，介绍容器技术在不同历史阶段中的主要关注点。\n隔离文件：chroot容器的起点可以追溯到 1979 年Version 7 UNIX系统中提供的chroot命令，这个命令是英文单词“Change Root”的缩写，功能是当某个进程经过chroot操作之后，它的根目录就会被锁定在命令参数所指定的位置，以后它或者它的子进程将不能再访问和操作该目录之外的其他文件。\n1991 年，世界上第一个监控黑客行动的蜜罐程序就是使用chroot来实现的，那个参数指定的根目录当时被作者戏称为“Chroot 监狱”（Chroot Jail），黑客突破chroot限制的方法就称为 Jailbreak。后来，FreeBSD 4.0 系统重新实现了chroot命令，用它作为系统中进程沙箱隔离的基础，并将其命名为FreeBSD jail，再后来，苹果公司又以 FreeBSD 为基础研发出了举世闻名的 iOS 操作系统，此时，黑客们就将绕过 iOS 沙箱机制以 root 权限任意安装程序的方法称为“越狱”（Jailbreak），这些故事都是题外话了。\n2000 年，Linux Kernel 2.3.41 版内核引入了pivot_root技术来实现文件隔离，pivot_root直接切换了根文件系统（rootfs），有效地避免了chroot命令可能出现的安全性漏洞。本文后续提到的容器技术，如 LXC、Docker 等也都是优先使用pivot_root来实现根文件系统切换的。\n时至今日，chroot命令依然活跃在 UNIX 系统与几乎所有主流的 Linux 发行版中，同时以命令行工具（chroot(8)）或者系统调用（chroot(2)）的形式存在，但无论是chroot命令抑或是pivot_root，都并不能提供完美的隔离性。原本按照 UNIX 的设计哲学，一切资源都可以视为文件（In UNIX，Everything is a File），一切处理都可以视为对文件的操作，理论上，只要隔离了文件系统，一切资源都应该被自动隔离才对。可是哲学归哲学，现实归现实，从硬件层面暴露的低层次资源，如磁盘、网络、内存、处理器，到经操作系统层面封装的高层次资源，如 UNIX 分时（UNIX Time-Sharing，UTS）、进程 ID（Process ID，PID）、用户 ID（User ID，UID）、进程间通信（Inter-Process Communication，IPC）都存在大量以非文件形式暴露的操作入口，因此，以chroot为代表的文件隔离，仅仅是容器崛起之路的起点而已。\n隔离访问：namespaces2002 年，Linux Kernel 2.4.19 版内核引入了一种全新的隔离机制：Linux 名称空间（Linux Namespaces）。名称空间的概念在很多现代的高级程序语言中都存在，用于避免不同开发者提供的 API 相互冲突，相信作为一名开发人员的你肯定不陌生。\nLinux 的名称空间是一种由内核直接提供的全局资源封装，是内核针对进程设计的访问隔离机制。进程在一个独立的 Linux 名称空间中朝系统看去，会觉得自己仿佛就是这方天地的主人，拥有这台 Linux 主机上的一切资源，不仅文件系统是独立的，还有着独立的 PID 编号（譬如拥有自己的 0 号进程，即系统初始化的进程）、UID/GID 编号（譬如拥有自己独立的 root 用户）、网络（譬如完全独立的 IP 地址、网络栈、防火墙等设置），等等，此时进程的心情简直不能再好了。\nLinux 的名称空间是受“贝尔实验室九号项目”（一个分布式操作系统，“九号”项目并非代号，操作系统的名字就叫“Plan 9 from Bell Labs”，充满了赛博朋克风格）的启发而设计的，最初的目的依然只是为了隔离文件系统，而非为了什么容器化的实现。这点从 2002 年发布时只提供了 Mount 名称空间，并且其构造参数为“CLONE_NEWNS”（即 Clone New Namespace 的缩写）而非“CLONE_NEWMOUNT”便能看出一些端倪。后来，要求系统隔离其他访问操作的呼声愈发强烈，从 2006 年起，内核陆续添加了 UTS、IPC 等名称空间隔离，直到目前最新的 Linux Kernel 5.6 版内核为止，Linux 名称空间支持以下八种资源的隔离（内核的官网Kernel.org上仍然只列出了前六种，从 Linux 的 Man 命令能查到全部八种）。\n表 11-1 Linux 名称空间支持以下八种资源的隔离\n\n\n\n名称空间\n隔离内容\n内核版本\n\n\n\nMount\n隔离文件系统，功能上大致可以类比chroot\n2.4.19\n\n\nUTS\n隔离主机的Hostname、Domain names\n2.6.19\n\n\nIPC\n隔离进程间通信的渠道（详见“远程服务调用”中对 IPC 的介绍）\n2.6.19\n\n\nPID\n隔离进程编号，无法看到其他名称空间中的 PID，意味着无法对其他进程产生影响\n2.6.24\n\n\nNetwork\n隔离网络资源，如网卡、网络栈、IP 地址、端口，等等\n2.6.29\n\n\nUser\n隔离用户和用户组\n3.8\n\n\nCgroup\n隔离cgroups信息，进程有自己的cgroups的根目录视图（在/proc/self/cgroup 不会看到整个系统的信息）。cgroups的话题很重要，稍后笔者会安排一整节来介绍\n4.6\n\n\nTime\n隔离系统时间，2020 年 3 月最新的 5.6 内核开始支持进程独立设置系统时间\n5.6\n\n\n如今，对文件、进程、用户、网络等各类信息的访问，都被囊括在 Linux 的名称空间中，即使今天仍有一些没被隔离的访问（譬如syslog就还没被隔离，容器内可以看到容器外其他进程产生的内核 syslog），日后也可以随内核版本的更新纳入到这套框架之内，现在距离完美的隔离性就只差最后一步了：资源的隔离。\n隔离资源：cgroups如果要让一台物理计算机中的各个进程看起来像独享整台虚拟计算机的话，不仅要隔离各自进程的访问操作，还必须能独立控制分配给各个进程的资源使用配额，不然的话，一个进程发生了内存溢出或者占满了处理器，其他进程就莫名其妙地被牵连挂起，这样肯定算不上是完美的隔离。\nLinux 系统解决以上问题的方案是控制群组（Control Groups，目前常用的简写为cgroups），它与名称空间一样都是直接由内核提供的功能，用于隔离或者说分配并限制某个进程组能够使用的资源配额，资源配额包括处理器时间、内存大小、磁盘 I/O 速度，等等，具体可以参见表 11-2 所示。\n表 11-2 Linux 控制群组子系统\n\n\n\n控制组子系统\n功能\n\n\n\nblkio\n为块设备（如磁盘，固态硬盘，USB 等等）设定 I/O 限额。\n\n\ncpu\n控制cgroups中进程的处理器占用比率。\n\n\ncpuacct\n自动生成cgroups中进程所使用的处理器时间的报告。\n\n\ncpuset\n为cgroups中的进程分配独立的处理器（包括多路系统的处理器，多核系统的处理器核心）。\n\n\ndevices\n设置cgroups中的进程访问某个设备的权限（读、写、创建三种权限）。\n\n\nfreezer\n挂起或者恢复cgroups中的进程。\n\n\nmemory\n设定cgroups中进程使用内存的限制，并自动生成内存资源使用报告。\n\n\nnet_cls\n使用等级识别符标记网络数据包，可允许 Linux 流量控制程序识别从具体 cgroups中生成的数据包。\n\n\nnet_prio\n用来设置网络流量的优先级。\n\n\nhugetlb\n主要针对于 HugeTLB 系统进行限制。\n\n\nperf_event\n允许 Perf 工具基于cgroups分组做性能监测。\n\n\ncgroups项目最早是由 Google 的工程师（主要是 Paul Menage 和 Rohit Seth）在 2006 年发起的，当时取的名字就叫作“进程容器”（Process Containers），不过“容器”（Container）这个名词的定义在那时候尚不如今天清晰，不同场景中常有不同所指，为避免混乱，2007 年这个项目才被重命名为cgroups，在 2008 年合并到 2.6.24 版的内核后正式对外发布，这一阶段的cgroups被称为“第一代cgroups”。2016 年 3 月发布的 Linux Kernel 4.5 中，搭载了由 Facebook 工程师（主要是 Tejun Heo）重新编写的“第二代cgroups”，其关键改进是支持统一层级管理（Unified Hierarchy），使得管理员能更加清晰精确地控制资源的层级关系。目前这两个版本的cgroups在 Linux 内核代码中是并存的，稍后介绍的 Docker 暂时仅支持第一代的cgroups。\n封装系统：LXC当文件系统、访问、资源都可以被隔离后，容器已经有它降生所需的全部前置支撑条件，并且 Linux 的开发者们也已经明确地看到了这一点。为降低普通用户综合使用namespaces、cgroups这些低级特性的门槛，2008 年 Linux Kernel 2.6.24 内核刚刚开始提供cgroups的同一时间，就马上发布了名为Linux 容器（LinuX Containers，LXC）的系统级虚拟化功能。\n此前，在 Linux 上并不是没有系统级虚拟化的解决方案，譬如传统的OpenVZ和Linux-VServer都能够实现容器隔离，并且只会有很低的性能损失（按 OpenVZ 提供的数据，只会有 1-3%的损失），但它们都是非官方的技术，使用它们最大的阻碍是系统级虚拟化必须要有内核的支持，为此它们就只能通过非官方内核补丁的方式修改标准内核，才能获得那些原本在内核中不存在的能力。\nLXC 带着令人瞩目的光环登场，它的出现促使“容器”从一个阳春白雪的只流传于开发人员口中的技术词汇，逐渐向整个软件业的公共概念、共同语言发展，就如同今天的“服务器”、“客户端”和“互联网”一样。相信你现在肯定会好奇为什么现在一提到容器，大家首先联想到的是 Docker 而不是 LXC？为什么去问 10 个开发人员，至少有 9 个听过 Docker，但如果问 LXC，可能只有 1 个人会听说过？\nLXC 的出现肯定受到了 OpenVZ 和 Linux-VServer 的启发，摸着巨人的肩膀过河这并没有什么不对。可惜的是，LXC 在设定自己的发展目标时，也被前辈们的影响所局限住。LXC 眼中的容器的定义与 OpenVZ 和 Linux-VServer 并无差别，是一种封装系统的轻量级虚拟机，而 Docker 眼中的容器的定义则是一种封装应用的技术手段。这两种封装理念在技术层面并没有什么本质区别，但应用效果就差异巨大。举个具体例子，如果你要建设一个LAMP（Linux、Apache、MySQL、PHP）应用，按照 LXC 的思路，你应该先编写或者寻找到LAMP 的 template（可以暂且不准确地类比为 LXC 版本的 Dockerfile 吧），以此构造出一个安装了 LAMP 的虚拟系统。如果按部署虚拟机的角度来看，这还算挺方便的，作为那个时代（距今也就十年）的系统管理员，所有软件、补丁、配置都是自己搞定的，部署一台新虚拟机要花费一两天时间都很正常，有 LXC 的 template，一下子帮你把 LAMP 都安装好了，还想要啥自行车？但是，作为一名现代的系统管理员，这里问题就相当大，如果我想把 LAMP 改为 LNMP（Linux、Nginx、MySQL、PHP）该怎么办？如果我想把 LAMP 里的 MySQL 5 调整为 MySQL 8 该怎么办？都得找到或者自己编写新的 template 来解决。好吧，那这台机的软件、版本都配置对了，下一台机我要构建LYME或者MEAN，又该怎么办？以封装系统为出发点，仍是按照先装系统然再装软件的思路，就永远无法做到一两分钟甚至十几秒钟就构造出一个合乎要求的软件运行环境，也决定了 LXC 不可能形成今天的容器生态的，所以，接下来舞台的聚光灯终于落到了 Docker 身上。\n封装应用：Docker2013 年宣布开源的 Docker 毫无疑问是容器发展历史上里程碑式的发明，然而 Docker 的成功似乎没有太多技术驱动的成分。至少对开源早期的 Docker 而言，确实没有什么能构成壁垒的技术。它的容器化能力直接来源于 LXC，它镜像分层组合的文件系统直接来源于AUFS，Docker 开源后不久，就有人仅用了一百多行 Shell 脚本便实现了 Docker 的核心功能（名为Bocker，提供了docker build/pull/images/ps/run/exec/logs/commit/rm/rmi等功能）。\n那为何历史选择了 Docker，而不是 LXC 或者其他容器技术呢？对于这个问题，笔者引用（转述非直译，有所精简）DotCloud 公司（当年创造 Docker 的公司，已于 2016 年倒闭）创始人 Solomon Hykes 在Stackoverflow 上的一段问答：\n为什么要用 Docker 而不是 LXC？（Why would I use Docker over plain LXC？）\nDocker 除了包装来自 Linux 内核的特性之外，它的价值还在于：\n\n跨机器的绿色部署：Docker 定义了一种将应用及其所有的环境依赖都打包到一起的格式，仿佛它原本就是绿色软件一样。LXC 并没有提供这样的能力，使用 LXC 部署的新机器很多细节都依赖人的介入，虚拟机的环境几乎肯定会跟你原本部署程序的机器有所差别。\n以应用为中心的封装：Docker 封装应用而非封装机器的理念贯穿了它的设计、API、界面、文档等多个方面。相比之下，LXC 将容器视为对系统的封装，这局限了容器的发展。\n自动构建：Docker 提供了开发人员从在容器中构建产品的全部支持，开发人员无需关注目标机器的具体配置，即可使用任意的构建工具链，在容器中自动构建出最终产品。\n多版本支持：Docker 支持像 Git 一样管理容器的连续版本，进行检查版本间差异、提交或者回滚等操作。从历史记录中你可以查看到该容器是如何一步一步构建成的，并且只增量上传或下载新版本中变更的部分。\n组件重用：Docker 允许将任何现有容器作为基础镜像来使用，以此构建出更加专业的镜像。\n共享：Docker 拥有公共的镜像仓库，成千上万的 Docker 用户在上面上传了自己的镜像，同时也使用他人上传的镜像。\n工具生态：Docker 开放了一套可自动化和自行扩展的接口，在此之上，还有很多工具来扩展其功能，譬如容器编排、管理界面、持续集成等等。\n\n—— Solomon Hykes，Stackoverflow，2013\n以上这段回答也同时被收录到了Docker 官网的 FAQ上，从 Docker 开源至今从未改变。促使 Docker 的一问世就惊艳世间的，不是什么黑科技式的秘密武器，而是其符合历史潮流的创意与设计理念，还有充分开放的生态运营。可见，在正确的时候，正确的人手上有一个优秀的点子，确实有机会引爆一个时代。\n以上是 Docker 开源一年后（截至 2014 年 12 月）获得的成绩，图片来自Docker 官网\n从开源到现在也只过了短短数年时间，Docker 已成为软件开发、测试、分发、部署等各个环节都难以或缺的基础支撑，自身的架构也发生了相当大的改变，Docker 被分解为由 Docker Client、Docker Daemon、Docker Registry、Docker Container 等子系统，以及 Graph、Driver、libcontainer 等各司其职的模块组成，此时再说一百多行脚本能实现 Docker 核心功能，再说 Docker 没有太高的技术含量，就已经不再合适了。\n2014 年，Docker 开源了自己用 Golang 开发的libcontainer，这是一个越过 LXC 直接操作namespaces和cgroups的核心模块，有了 libcontainer 以后，Docker 就能直接与系统内核打交道，不必依赖 LXC 来提供容器化隔离能力了。\n2015 年，在 Docker 的主导和倡议下，多家公司联合制定了“开放容器交互标准”（Open Container Initiative，OCI），这是一个关于容器格式和运行时的规范文件，其中包含运行时标准（runtime-spec ）、容器镜像标准（image-spec）和镜像分发标准（distribution-spec，分发标准还未正式发布）。运行时标准定义了应该如何运行一个容器、如何管理容器的状态和生命周期、如何使用操作系统的底层特性（namespaces、cgroup、pivot_root等）；容器镜像标准规定了容器镜像的格式、配置、元数据的格式，可以理解为对镜像的静态描述；镜像分发标准则规定了镜像推送和拉取的网络交互过程。\n为了符合 OCI 标准，Docker 推动自身的架构继续向前演进，首先将 libcontainer 独立出来，封装重构成runC 项目，并捐献给了 Linux 基金会管理。runC 是 OCI Runtime 的首个参考实现，提出了“让标准容器无所不在”（Make Standard Containers Available Everywhere）的口号。为了能够兼容所有符合标准的 OCI Runtime 实现，Docker 进一步重构了 Docker Daemon 子系统，将其中与运行时交互的部分抽象为containerd 项目，这是一个负责管理容器执行、分发、监控、网络、构建、日志等功能的核心模块，内部会为每个容器运行时创建一个 containerd-shim 适配进程，默认与 runC 搭配工作，但也可以切换到其他 OCI Runtime 实现上（然而实际并没做到，最后 containerd 仍是紧密绑定于 runC）。2016 年，Docker 把 containerd 捐献给了 CNCF 管理，runC 与 containerd 两个项目的捐赠托管，即带有 Docker 对开源信念的追求，也带有 Docker 在众多云计算大厂夹击下自救的无奈，这两个项目将成为未来 Docker 消亡和存续的伏笔（看到本节末尾你就能理解这句矛盾的话了）。\n\n以上笔者列举的这些 Docker 推动的开源与标准化工作，既是对 Docker 为开源乃至整个软件业做出贡献的赞赏，也是为后面介绍容器编排时，讲述当前容器引擎的混乱关系做的前置铺垫。Docker 目前无疑在容器领域具有统治地位，但统治的稳固程度不仅没到高枕无忧，说是危机四伏都不为过。目前已经有了可见的、足以威胁动摇 Docker 地位的潜在可能性正在酝酿，风险源于虽然 Docker 赢得了容器战争，但 Docker Swarm 却输掉了容器编排战争。从结果回望当初，Docker 赢得容器战争有一些偶然，Docker Swarm 输掉的编排战争却是必然的。\n封装集群：Kubernetes如果说以 Docker 为代表的容器引擎将软件的发布流程从分发二进制安装包转变为直接分发虚拟化后的整个运行环境，令应用得以实现跨机器的绿色部署；那以 Kubernetes 为代表的容器编排框架，就是把大型软件系统运行所依赖的集群环境也进行了虚拟化，令集群得以实现跨数据中心的绿色部署，并能够根据实际情况自动扩缩。\n容器的崛起之路讲到 Docker 和 Kubernetes 这阶段，已经不再是介绍历史了，从这里开始发生的变化，都是近几年软件业界中的热点事件，也是这章要讨论的主要话题。现在笔者暂时不打算介绍 Kubernetes 的技术细节，它们将会被留到后面的文章中更详细的解析。这节里，我们首先从宏观层面去理解 Kubernetes 的诞生与演变的驱动力，这对正确理解未来云原生的发展方向至关重要。\nKubernetes 可谓出身名门，前身是 Google 内部已运行多年的集群管理系统 Borg，2014 年 6 月使用 Golang 完全重写后开源。自诞生之日起，只要与云计算能稍微扯上关系的业界巨头都对 Kubernetes 争相追捧，IBM、RedHat、Microsoft、VMware 和华为都是它最早期的代码贡献者。此时，云计算从实验室到工业化应用已经有十个年头，然而大量应用使用云计算的方式仍停滞在传统 IDC（Internet Data Center）时代，仅仅是用云端的虚拟机代替了传统的物理机。尽管早在 2013 年，Pivotal（持有着 Spring Framework 和 Cloud Foundry 的公司）就提出了“云原生”的概念，但是要实现服务化、具备韧性（Resilience）、弹性（Elasticity）、可观测性（Observability）的软件系统十分困难，在当时基本只能依靠架构师和程序员高超的个人能力，云计算本身帮不上什么忙。在云的时代不能充分利用云的强大能力，这让云计算厂商无比遗憾，也无比焦虑。直到 Kubernetes 横空出世，大家终于等到了破局的希望，认准了这就是云原生时代的操作系统，是让复杂软件在云计算下获得韧性、弹性、可观测性的最佳路径，也是为厂商们推动云计算时代加速到来的关键引擎之一。\n2015 年 7 月，Kubernetes 发布了第一个正式版本 1.0 版，更重要的事件是 Google 宣布与 Linux 基金会共同筹建云原生基金会（Cloud Native Computing Foundation，CNCF），并且将 Kubernetes 托管到 CNCF，成为其第一个项目。随后，Kubernetes 以摧枯拉朽之势覆灭了容器编排领域的其他竞争对手，哪怕 Docker Swarm 有着 Docker 在容器引擎方面的先天优势，DotCloud 后来甚至将 Swarm 直接内置入 Docker 之中都未能稍稍阻挡 Kubernetes 的前进的步伐。\nKubernetes 的成功与 Docker 的成功并不相同，Docker 靠的是优秀的理念，以一个“好点子”引爆了一个时代。笔者相信就算没有 Docker 也会有 Cocker 或者 Eocker 的出现，但由成立仅三年的 DotCloud 公司（三年后又倒闭）做成了这样的产品确实有一定的偶然性。而 Kubernetes 的成功不仅有 Google 深厚的技术功底作支撑，有领先时代的设计理念，更加关键的是 Kubernetes 的出现符合所有云计算大厂的切身利益，有着业界巨头不遗余力的广泛支持，它的成功便是一种必然。\n\nKubernetes 与 Docker 两者的关系十分微妙，把握住两者关系的变化过程，是理解 Kubernetes 架构演变与 CRI、OCI 规范的良好线索。在 Kubernetes 开源的早期，它是完全依赖且绑定 Docker 的，并没有过多考虑够日后有使用其他容器引擎的可能性。直至 Kubernetes 1.5 之前，Kubernetes 管理容器的方式都是通过内部的 DockerManager 向 Docker Engine 以 HTTP 方式发送指令，通过 Docker 来操作镜像的增删改查的，如上图最右边线路的箭头所示（图中的 kubelet 是集群节点中的代理程序，负责与管理集群的 Master 通信，其他节点的含义在后文介绍到时都会有解释）。将这个阶段 Kubernetes 与容器引擎的调用关系捋直，并结合上一节提到的 Docker 捐献 containerd 与 runC 后重构的调用，完整的调用链条如下所示：\n\nKubernetes Master → kubelet → DockerManager → Docker Engine → containerd → runC\n\n2016 年，Kubernetes 1.5 版本开始引入“容器运行时接口”（Container Runtime Interface，CRI），这是一个定义容器运行时应该如何接入到 kubelet 的规范标准，从此 Kubernetes 内部的 DockerManager 就被更为通用的 KubeGenericRuntimeManager 所替代（实际上在 1.6.6 之前都仍然可以看到 DockerManager），kubelet 与 KubeGenericRuntimeManager 之间通过 gRPC 协议通信。由于 CRI 是在 Docker 之后才发布的规范，Docker 是肯定不支持 CRI 的，所以 Kubernetes 又提供了 DockerShim 服务作为 Docker 与 CRI 的适配层，由它与 Docker Engine 以 HTTP 形式通信，实现了原来 DockerManager 的全部功能。此时，Docker 对 Kubernetes 来说只是一项默认依赖，而非之前的无可或缺了，它们的调用链为：\n\nKubernetes Master → kubelet → KubeGenericRuntimeManager → DockerShim → Docker Engine → containerd → runC\n\n2017 年，由 Google、RedHat、Intel、SUSE、IBM 联合发起的CRI-O（Container Runtime Interface Orchestrator）项目发布了首个正式版本。从名字就可以看出，它肯定是完全遵循 CRI 规范进行实现的，另一方面，它可以支持所有符合 OCI 运行时标准的容器引擎，默认仍然是与 runC 搭配工作的，若要换成Clear Containers、Kata Containers等其他 OCI 运行时也完全没有问题。虽然开源版 Kubernetes 是使用 CRI-O、cri-containerd 抑或是 DockerShim 作为 CRI 实现，完全可以由用户自由选择（根据用户宿主机的环境选择），但在 RedHat 自己扩展定制的 Kubernetes 企业版，即OpenShift 4中，调用链已经没有了 Docker Engine 的身影：\n\nKubernetes Master → kubelet → KubeGenericRuntimeManager → CRI-O→ runC\n\n由于此时 Docker 在容器引擎中的市场份额仍然占有绝对优势，对于普通用户来说，如果没有明确的收益，并没有什么动力要把 Docker 换成别的引擎，所以 CRI-O 即使摆出了直接挖掉 Docker 根基的凶悍姿势，其实也并没有给 Docker 带来太多即时可见的影响，不过能够想像此时 Docker 心中肯定充斥了难以言喻的危机感。\n2018 年，由 Docker 捐献给 CNCF 的 containerd，在 CNCF 的精心孵化下发布了 1.1 版，1.1 版与 1.0 版的最大区别是此时它已完美地支持了 CRI 标准，这意味着原本用作 CRI 适配器的 cri-containerd 从此不再需要。此时，再观察 Kubernetes 到容器运行时的调用链，你会发现调用步骤会比通过 DockerShim、Docker Engine 与 containerd 交互的步骤要减少两步，这又意味着用户只要愿意抛弃掉 Docker 情怀的话，在容器编排上便可至少省略一次 HTTP 调用，获得性能上的收益，且根据 Kubernetes 官方给出的测试数据，这些免费的收益还相当地可观。Kubernetes 从 1.10 版本宣布开始支持 containerd 1.1，在调用链中已经能够完全抹去 Docker Engine 的存在：\n\nKubernetes Master → kubelet → KubeGenericRuntimeManager → containerd → runC\n\n今天，要使用哪一种容器运行时取决于你安装 Kubernetes 时宿主机上的容器运行时环境，但对于云计算厂商来说，譬如国内的阿里云 ACK、腾讯云 TKE等直接提供的 Kubernetes 容器环境，采用的容器运行时普遍都已是 containerd，毕竟运行性能对它们来说就是核心生产力和竞争力。\n未来，随着 Kubernetes 的持续发展壮大，Docker Engine 经历从不可或缺、默认依赖、可选择、直到淘汰是大概率事件，这件事情表面上是 Google、RedHat 等云计算大厂联手所为，实际淘汰它的还是技术发展的潮流趋势，就如同 Docker 诞生时依赖 LXC，到最后用 libcontainer 取代掉 LXC 一般。同时，我们也该看到事情的另一面，现在连 LXC 都还没有挂掉，反倒还发展出了更加专注于与 OpenVZ 等系统级虚拟化竞争的LXD，相信 Docker 本身也很难彻底消亡的，已经养成习惯的 CLI 界面，已经形成成熟生态的镜像仓库等都应该会长期存在，只是在容器编排领域，未来的 Docker 很可能只会以 runC 和 containerd 的形式存续下去，毕竟它们最初都源于 Docker 的血脉。\n二、以容器构建系统自从 Docker 提出“以封装应用为中心”的容器发展理念，成功取代了“以封装系统为中心”的 LXC 以后，一个容器封装一个单进程应用已经成为被广泛认可的最佳实践。然而单体时代过去之后，分布式系统里应用的概念已不再等同于进程，此时的应用需要多个进程共同协作，通过集群的形式对外提供服务，以虚拟化方法实现这个目标的过程就被称为容器编排（Container Orchestration）。\n容器之间顺畅地交互通信是协作的核心需求，但容器协作并不仅仅是将容器以高速网络互相连接而已。如何调度容器，如何分配资源，如何扩缩规模，如何最大限度地接管系统中的非功能特性，让业务系统尽可能免受分布式复杂性的困扰都是容器编排框架必须考虑的问题，只有恰当解决了这一系列问题，云原生应用才有可能获得比传统应用更高的生产力。\n隔离与协作笔者并不打算过多介绍 Kubernetes 具体有哪些功能，向你说明 Kubernetes 由 Pod、Node、Deployment、ReplicaSet 等各种类型的资源组成可用的服务、集群管理平面与节点之间如何工作、每种资源该如何配置使用等等，并不是笔者的本意，如果你希望了解这方面信息，可以从 Kubernetes 官网的文档库或任何一本以 Kubernetes 为主题的使用手册中得到。\n笔者真正希望说清楚的问题是“为什么 Kubernetes 会设计成现在这个样子？”、“为什么以容器构建系统应该这样做？”，寻找这些问题的答案最好是从它们设计的实现意图出发，为此，笔者虚构了一系列从简单到复杂的场景供你代入其中，理解并解决这些场景中的问题，并不要求你对 Kubernetes 已有多深入的了解，但要求你至少使用过 Kubernetes 和 Docker，基本了解它的核心功能与命令；此外还会涉及到一点儿 Linux 系统内核资源隔离的基础知识，别担心，只要你仔细读懂了上一节“容器的崛起”，就已经完全够用了。\n现在就来设想一下，如果让你来设计一套容器编排系统，协调各种容器来共同来完成一项工作，会遇到什么问题？会如何着手解决？让我们从最简单的场景出发：\n\n场景一：假设你现在有两个应用，其中一个是 Nginx，另一个是为该 Nginx 收集日志的 Filebeat，你希望将它们封装为容器镜像，以方便日后分发。\n\n最直接的方案就将 Nginx 和 Filebeat 直接编译成同一个容器镜像，这是可以做到的，而且并不复杂，然而这样做会埋下很大隐患：它违背了 Docker 提倡的单个容器封装单进程应用的最佳实践。Docker 设计的 Dockerfile 只允许有一个 ENTRYPOINT，这并非无故添加的人为限制，而是因为 Docker 只能通过监视 PID 为 1 的进程（即由 ENTRYPOINT 启动的进程）的运行状态来判断容器的工作状态是否正常，容器退出执行清理，容器崩溃自动重启等操作都必须先判断状态。设想一下，即使我们使用了supervisord之类的进程控制器来解决同时启动 Nginx 和 Filebeat 进程的问题，如果因某种原因它们不停发生崩溃、重启，那 Docker 也无法察觉到，它只能观察到 supervisord 的运行状态，因此，以上需求会理所当然地演化成场景二。\n\n场景二：假设你现在有两个 Docker 镜像，其中一个封装了 HTTP 服务，为便于称呼，我们叫它 Nginx 容器，另一个封装了日志收集服务，我们叫它 Filebeat 容器。现在要求 Filebeat 容器能收集 Nginx 容器产生的日志信息。\n\n场景二依然不难解决，只要在 Nginx 容器和 Filebeat 容器启动时，分别将它们的日志目录和收集目录挂载为宿主机同一个磁盘位置的 Volume 即可，这种操作在 Docker 中是十分常用的容器间信息交换手段。不过，容器间信息交换不仅仅是文件系统，假如此时我又引入了一个新的工具confd——Linux 下的一种配置管理工具，作用是根据配置中心（Etcd、ZooKeeper、Consul）的变化自动更新 Nginx 的配置，这里便又会遇到新的问题。confd 需要向 Nginx 发送 HUP 信号以便通知 Nginx配置已经发生了变更，而发送 HUP 信号自然要求 confd 与 Nginx 能够进行 IPC 通信才行。尽管共享 IPC 名称空间不如共享 Volume 常见，但 Docker 同样支持了该功能，docker run 提供了--ipc参数，用于把多个容器挂载到同一个父容器的 IPC 名称空间之下，以实现容器间共享 IPC 名称空间的需求。类似地，如果要共享 UTS 名称空间，可以使用--uts参数，要共享网络名称空间的话，就使用--net参数。\n以上便是 Docker 针对场景二这种不跨机器的多容器协作所给出的解决方案，自动地为多个容器设置好共享名称空间其实就是Docker Compose提供的核心能力。这种针对具体应用需求来共享名称空间的方案，的确可以工作，却并不够优雅，也谈不上有什么扩展性。容器的本质是对 cgroups 和 namespaces 所提供的隔离能力的一种封装，在 Docker 提倡的单进程封装的理念影响下，容器蕴含的隔离性也多了仅针对于单个进程的额外局限，然而 Linux 的 cgroups 和 namespaces 原本都是针对进程组而不仅仅是单个进程来设计的，同一个进程组中的多个进程天然就可以共享着相同的访问权限与资源配额。如果现在我们把容器与进程在概念上对应起来，那容器编排的第一个扩展点，就是要找到容器领域中与“进程组”相对应的概念，这是实现容器从隔离到协作的第一步，在 Kubernetes 的设计里，这个对应物叫作 Pod。\n额外知识：Pod 名字的由来与含义\nPod 的概念在容器正式出现之前的 Borg 系统中就已经存在，从 Google 的发表的《Large-Scale Cluster Management at Google with Borg》可以看出，Kubernetes 时代的 Pod 整合了 Borg 时代的“Prod”（Production Task 的缩写）与“Non-Prod”的职能。由于 Pod 一直没有权威的中文翻译，笔者在后续文章中会尽量用英文指代，偶尔需要中文的场合就使用 Borg 中 Prod 的译法，即“生产任务”来指代。\n有了“容器组”的概念，场景二的问题便只需要将多个容器放到同一个 Pod 中即可解决。扮演容器组的角色，满足容器共享名称空间的需求，是 Pod 的两大最基本职责之一，同处于一个 Pod 内的多个容器，相互之间以超亲密的方式协作。请注意，“超亲密”在这里并非某种带强烈感情色彩的形容词，而是有一种有具体定义的协作程度。对于普通非亲密的容器，它们一般以网络交互方式（其他譬如共享分布式存储来交换信息也算跨网络）协作；对亲密协作的容器，是指它们被调度到同一个集群节点上，可以通过共享本地磁盘等方式协作；而超亲密的协作是特指多个容器位于同一个 Pod 这种特殊关系，它们将默认共享：\n\nUTS 名称空间：所有容器都有相同的主机名和域名。\n网络名称空间：所有容器都共享一样的网卡、网络栈、IP 地址，等等。因此，同一个 Pod 中不同容器占用的端口不能冲突。\nIPC 名称空间：所有容器都可以通过信号量或者 POSIX 共享内存等方式通信。\n时间名称空间：所有容器都共享相同的系统时间。\n\n同一个 Pod 的容器，只有 PID 名称空间和文件名称空间默认是隔离的。PID 的隔离令每个容器都有独立的进程 ID 编号，它们封装的应用进程就是 PID 为 1 的进程，可以通过 Pod 元数据定义中的spec.shareProcessNamespace来改变这点。一旦要求共享 PID 名称空间，容器封装的应用进程就不再具有 PID 为 1 的特征了，这有可能导致部分依赖该特征的应用出现异常。在文件名称空间方面，容器要求文件名称空间的隔离是很理所当然的需求，因为容器需要相互独立的文件系统以避免冲突。但容器间可以共享存储卷，这是通过 Kubernetes 的 Volume 来实现的。\n额外知识：Kubernetes 中 Pod 名称空间共享的实现细节\nPod 内部多个容器共享 UTS、IPC、网络等名称空间是通过一个名为 Infra Container 的容器来实现的，这个容器是整个 Pod 中第一个启动的容器，只有几百 KB 大小（代码只有很短的几十行，见这里），Pod 中的其他容器都会以 Infra Container 作为父容器，UTS、IPC、网络等名称空间实质上都是来自 Infra Container 容器。\n如果容器设置为共享 PID 名称空间的话，Infra Container 中的进程将作为 PID 1 进程，其他容器的进程将以它的子进程的方式存在，此时将由 Infra Container 来负责进程管理（譬如清理僵尸进程）、感知状态和传递状态。\n由于 Infra Container 的代码除了注册 SIGINT、SIGTERM、SIGCHLD 等信号的处理器外，就只是一个以 pause()方法为循环体的无限循环，永远处于 Pause 状态，所以也常被称为“Pause Container”。\nPod 的另外一个基本职责是实现原子性调度，如果容器编排不跨越集群节点，是否具有原子性都无关紧要。但是在集群环境中，容器可能跨机器调度时，这个特性就变得非常重要。如果以容器为单位来调度的话，不同容器就有可能被分配到不同机器上。两台机器之间本来就是物理隔离，依靠网络连接的，这时候谈什么名称空间共享、cgroups配额共享都失去了意义，我们由此从场景二又演化出以下场景三。\n\n场景三：假设你现在有 Filebeat、Nginx 两个 Docker 镜像，在一个具有多个节点的集群环境下，要求每次调度都必须让 Filebeat 和 Nginx 容器运行于同一个节点上。\n\n两个关联的协作任务必须一起调度的需求在容器出现之前就存在已久，譬如在传统的多线程（或多进程）并发调度中，如果两个线程（或进程）的工作是强依赖的，单独给谁分配处理时间、而另一个被挂起都会导致程序无法工作，如此就有了协同调度（Coscheduling）的概念，以保证一组紧密联系的任务能够被同时分配资源。如果我们在容器编排中仍然坚持将容器视为调度的最小粒度，那对容器运行所需资源的需求声明就只能设定在容器上，这样集群每个节点剩余资源越紧张，单个节点无法容纳全部协同容器的概率就越大，协同的容器被分配到不同节点的可能性就越高。\n协同调度是十分麻烦的，实现起来要么很低效，譬如 Apache Mesos 的 Resource Hoarding 调度策略，就要等所有需要调度的任务都完备后才会开始分配资源；要么就会实现得很复杂，譬如 Google 就曾针对 Borg 的下一代 Omega 系统发表过论文《Omega: Flexible, Scalable Schedulers for Large Compute Clusters》介绍它如何使用通过乐观并发（Optimistic Concurrency）、冲突回滚的方式做到高效率，也同样高度复杂的协同调度。但是如果将运行资源的需求声明定义在 Pod 上，直接以 Pod 为最小的原子单位来实现调度的话，由于多个 Pod 之间必定不存在超亲密的协同关系，只会通过网络非亲密地协作，那就根本没有协同的说法，自然也不需要考虑复杂的调度了，关于 Kubernetes 的具体调度实现，笔者会在“资源与调度”中展开讲解。\nPod 是隔离与调度的基本单位，也是我们接触的第一种 Kubernetes 资源。Kubernetes 将一切皆视为资源，不同资源之间依靠层级关系相互组合协作，这个思想是贯穿 Kubernetes 整个系统的两大核心设计理念之一，不仅在容器、Pod、主机、集群等计算资源上是这样，在工作负载、持久存储、网络策略、身份权限等其他领域中也都有着一致的体现。\n\n由于 Pod 是 Kubernetes 中最重要的资源，又是资源模型中一种仅在逻辑上存在、没有物理对应的概念（因为对应的“进程组”也只是个逻辑概念），是其他编排系统没有的概念，所以笔者专门花费了一些篇幅去介绍它的设计意图，而不是像帮助手册那样直接给出它的作用和特性。对于 Kubernetes 中的其他计算资源，像 Node、Cluster 等都有切实的物理对应物，很容易就能形成共同的认知，笔者也就不必逐一介绍了，仅将它们的设计意图列举如下：\n\n容器（Container）：延续了自 Docker 以来一个容器封装一个应用进程的理念，是镜像管理的最小单位。\n生产任务（Pod）：补充了容器化后缺失的与进程组对应的“容器组”的概念，Pod 中容器共享 UTS、IPC、网络等名称空间，是资源调度的最小单位。\n节点（Node）：对应于集群中的单台机器，这里的机器即可以是生产环境中的物理机，也可以是云计算环境中的虚拟节点，节点是处理器和内存等资源的资源池，是硬件单元的最小单位。\n集群（Cluster）：对应于整个集群，Kubernetes 提倡理念是面向集群来管理应用。当你要部署应用的时候，只需要通过声明式 API 将你的意图写成一份元数据（Manifests），将它提交给集群即可，而无需关心它具体分配到哪个节点（尽管通过标签选择器完全可以控制它分配到哪个节点，但一般不需要这样做）、如何实现 Pod 间通信、如何保证韧性与弹性，等等，所以集群是处理元数据的最小单位。\n集群联邦（Federation）：对应于多个集群，通过联邦可以统一管理多个 Kubernetes 集群，联邦的一种常见应用是支持跨可用区域多活、跨地域容灾的需求。\n\n韧性与弹性笔者曾看过一部叫作《Bubble Boy》的电影，讲述了一个体内没有任何免疫系统的小男孩，终日只能生活在无菌的圆形气球里，对常人来说不值一提的细菌，都能够直接威胁到他的性命。小男孩尽管能够降生于世间，但并不能真正与世界交流，这种生命是极度脆弱的。\n真实世界的软件系统与电影世界中的小男孩亦具有可比性。让容器得以相互连通，相互协作仅仅是以容器构建系统的第一步，我们不仅希望得到一个能够运行起来的系统，而且还希望得到一个能够健壮运行的系统、能够抵御意外与风险的系统。在 Kubernetes 的支持下，你确实可以直接创建 Pod 将应用运行起来，但这样的应用就如同电影中只能存活在气球中的小男孩一般脆弱，无论是软件缺陷、意外操作或者硬件故障，都可能导致在复杂协作的过程中某个容器出现异常，进而出现系统性的崩溃。为此，架构师专门设计了服务容错的策略和模式，Kubernetes 作为云原生时代的基础设施，也尽力帮助程序员以最小的代价来实现容错，为系统健壮运行提供底层支持。\n如何实现具有韧性与弹性的系统是展示 Kubernetes 控制器设计模式的最好示例，控制器模式是继资源模型之后，本节介绍的另一个 Kubernetes 核心设计理念。下面，我们就从如何解决以下场景四的问题开始。\n\n场景四：假设有一个由数十个 Node、数百个 Pod、近千个 Container 所组成的分布式系统，要避免系统因为外部流量压力、代码缺陷、软件更新、硬件升级、资源分配等各种原因而出现中断，作为管理员，你希望编排系统能为你提供何种支持？\n\n作为用户，当然最希望容器编排系统能自动把所有意外因素都消灭掉，让任何每一个服务都永远健康，永不出错。但永不出错的服务是不切实际的，只有凑齐七颗龙珠才有望办到。那就只能退而求其次，让编排系统在这些服务出现问题，运行状态不正确的时候，能自动将它们调整成正确的状态。这种需求听起来也是贪心的，却已经具备足够的可行性，应对的解决办法在工业控制系统里已经有非常成熟的应用，叫作控制回路（Control Loop）。\nKubernetes 官方文档是以房间中空调自动调节温度为例子介绍了控制回路的一般工作过程的：当你设置好了温度，就是告诉空调你对温度的“期望状态”（Desired State），而传感器测量出的房间实际温度是“当前状态”（Current State）。根据当前状态与期望状态的差距，控制器对空调制冷的开关进行调节控制，就能让其当前状态逐渐接近期望状态。\n\n将这种控制回路的思想迁移应用到容器编排上，自然会为 Kubernetes 中的资源附加上了期望状态与实际状态两项属性。不论是已经出现在上节的资源模型中，用于抽象容器运行环境的计算资源，还是没有登场的另一部分对应于安全、服务、令牌、网络等功能的资源，用户要想使用这些资源来实现某种需求，并不提倡像平常编程那样去调用某个或某一组方法来达成目的，而是通过描述清楚这些资源的期望状态，由 Kubernetes 中对应监视这些资源的控制器来驱动资源的实际状态逐渐向期望状态靠拢，以此来达成目的。这种交互风格被称为是 Kubernetes 的声明式 API，如果你已有过实际操作 Kubernetes 的经验，那你日常在元数据文件中的spec字段所描述的便是资源的期望状态。\n额外知识：Kubernates 的资源对象与控制器\n目前，Kubernetes 已内置支持相当多的资源对象，并且还可以使用CRD（Custom Resource Definition）来自定义扩充，你可以使用kubectl api-resources来查看它们。笔者根据用途分类列举了以下常见的资源：\n\n用于描述如何创建、销毁、更新、扩缩 Pod，包括：Autoscaling（HPA）、CronJob、DaemonSet、Deployment、Job、Pod、ReplicaSet、StatefulSet\n用于配置信息的设置与更新，包括：ConfigMap、Secret\n用于持久性地存储文件或者 Pod 之间的文件共享，包括：Volume、LocalVolume、PersistentVolume、PersistentVolumeClaim、StorageClass\n用于维护网络通信和服务访问的安全，包括：SecurityContext、ServiceAccount、Endpoint、NetworkPolicy\n用于定义服务与访问，包括：Ingress、Service、EndpointSlice\n用于划分虚拟集群、节点和资源配额，包括：Namespace、Node、ResourceQuota\n\n这些资源在控制器管理框架中一般都会有相应的控制器来管理，笔者列举常见的控制器，按照它们的启动情况分类如下：\n\n必须启用的控制器：EndpointController、ReplicationController、PodGCController、ResourceQuotaController、NamespaceController、ServiceAccountController、GarbageCollectorController、DaemonSetController、JobController、DeploymentController、ReplicaSetController、HPAController、DisruptionController、StatefulSetController、CronJobController、CSRSigningController、CSRApprovingController、TTLController\n默认启用的可选控制器，可通过选项禁止：TokenController、NodeController、ServiceController、RouteController、PVBinderController、AttachDetachController\n默认禁止的可选控制器，可通过选项启用：BootstrapSignerController、TokenCleanerController\n\n与资源相对应，只要是实际状态有可能发生变化的资源对象，通常都会由对应的控制器进行追踪，每个控制器至少会追踪一种类型的资源。为了管理众多资源控制器，Kubernetes 设计了统一的控制器管理框架（kube-controller-manager）来维护这些控制器的正常运作，以及统一的指标监视器（kube-apiserver）来为控制器工作时提供其追踪资源的度量数据。\n由于毕竟不是在写 Kubernetes 的操作手册，笔者只能针对两三种资源和控制器为代表来举例说明，而无法将每个控制器都详细展开讲解。这里只要将场景四进一步具体化，转换成下面的场景五，便可以得到一个很好的例子，以部署控制器（Deployment Controller）、副本集控制器（ReplicaSet Controller）和自动扩缩控制器（HPA Controller）为例来介绍 Kubernetes 控制器模式的工作原理。\n\n场景五：通过服务编排，对任何分布式系统自动实现以下三种通用的能力：\n\nPod 出现故障时，能够自动恢复，不中断服务；\nPod 更新程序时，能够滚动更新，不中断服务；\nPod 遇到压力时，能够水平扩展，不中断服务；\n\n\n前文曾提到虽然 Pod 本身也是资源，完全可以直接创建，但由 Pod 直接构成的系统是十分脆弱的，犹如气球中的小男孩，生产中并不提倡。正确的做法是通过副本集（ReplicaSet）来创建 Pod。ReplicaSet 也是一种资源，是属于工作负荷一类的资源，它代表一个或多个 Pod 副本的集合，你可以在 ReplicaSet 资源的元数据中描述你期望 Pod 副本的数量（即spec.replicas的值）。当 ReplicaSet 成功创建之后，副本集控制器就会持续跟踪该资源，如果一旦有 Pod 发生崩溃退出，或者状态异常（默认是靠进程返回值，你还可以在 Pod 中设置探针，以自定义的方式告诉 Kubernetes 出现何种情况 Pod 才算状态异常），ReplicaSet 都会自动创建新的 Pod 来替代异常的 Pod；如果异常多出现了额外数量的 Pod，也会被 ReplicaSet 自动回收掉，总之就是确保任何时候集群中这个 Pod 副本的数量都向期望状态靠拢。\nReplicaSet 本身就能满足场景五中的第一项能力，可以保证 Pod 出现故障时自动恢复，但是在升级程序版本时，ReplicaSet 不得不主动中断旧 Pod 的运行，重新创建新版的 Pod，这会造成服务中断。对于那些不允许中断的业务，以前的 Kubernetes 曾经提供过kubectl rolling-update命令来辅助实现滚动更新。\n所谓滚动更新（Rolling Updates）是指先停止少量旧副本，维持大量旧副本继续提供服务，当停止的旧副本更新成功，新副本可以提供服务以后，再重复以上操作，直至所有的副本都更新成功。将这个过程放到 ReplicaSet 上，就是先创建新版本的 ReplicaSet，然后一边让新 ReplicaSet 逐步创建新版 Pod 的副本，一边让旧的 ReplicaSet 逐渐减少旧版 Pod 的副本。\n之所以kubectl rolling-update命令会被淘汰，是因为这样的命令式交互完全不符合 Kubernetes 的设计理念（这是台面上的说法，笔者觉得淘汰的根本原因主要是因为它不够好用），如果你希望改变某个资源的某种状态，应该将期望状态告诉 Kubernetes，而不是去教 Kubernetes 具体该如何操作。因此，新的部署资源（Deployment）与部署控制器被设计出来，可以由 Deployment 来创建 ReplicaSet，再由 ReplicaSet 来创建 Pod，当你更新 Deployment 中的信息（譬如更新了镜像的版本）以后，部署控制器就会跟踪到你新的期望状态，自动地创建新 ReplicaSet，并逐渐缩减旧的 ReplicaSet 的副本数，直至升级完成后彻底删除掉旧 ReplicaSet，如图 11-6 所示。\n\n对于场景五的最后一种情况，遇到流量压力时，管理员完全可以手动修改 Deployment 中的副本数量，或者通过kubectl scale命令指定副本数量，促使 Kubernetes 部署更多的 Pod 副本来应对压力，然而这种扩容方式不仅需要人工参与，且只靠人类经验来判断需要扩容的副本数量，不容易做到精确与及时。为此 Kubernetes 又提供了 Autoscaling 资源和自动扩缩控制器，能够自动根据度量指标，如处理器、内存占用率、用户自定义的度量值等，来设置 Deployment（或者 ReplicaSet）的期望状态，实现当度量指标出现变化时，系统自动按照“Autoscaling→Deployment→ReplicaSet→Pod”这样的顺序层层变更，最终实现根据度量指标自动扩容缩容。\n故障恢复、滚动更新、自动扩缩这些特性，在云原生中时代里常被概括成服务的弹性（Elasticity）与韧性（Resilience），ReplicaSet、Deployment、Autoscaling 的用法，也属于是所有 Kubernetes 教材资料都会讲到的“基础必修课”。如果你准备学习 Kubernetes 或者其他云原生相关技术，笔者建议最好不要死记硬背地学习每个资源的元数据文件该如何编写、有哪些指令、有哪些功能，更好的方式是站在解决问题的角度去理解为什么 Kubernetes 要设计这些资源和控制器，理解为什么这些资源和控制器会被设计成现在这种样子。\n如果你觉得已经理解了前面的几种资源和控制器的例子，那不妨思考一下以下几个问题：假设我想限制某个 Pod 持有的最大存储卷数量，应该会如何设计？假设集群中某个 Node 发生硬件故障，Kubernetes 要让调度任务避开这个 Node，应该如何设计？假设一旦这个 Node 重新恢复，Kubernetes 要能尽快利用上面的资源，又该如何去设计？只要你真正接受了资源与控制器是贯穿整个 Kubernetes 的两大设计理念，即便不去查文档手册，也应该能推想出个大概轮廓，以此为基础当你再去看手册或者源码时，想必就能够事半功倍。\n三、以应用为中心的封装看完容器技术发展的历程，不知你会不会感到有种“套娃式”的迷惑感？容器的崛起缘于 chroot、namespaces、cgroups 等内核提供的隔离能力，系统级虚拟化技术使得同一台机器上互不干扰地运行多个服务成为可能；为了降低用户使用内核隔离能力的门槛，随后出现了 LXC，它是 namespaces、cgroups 特性的上层封装，使得“容器”一词真正走出实验室，走入工业界实际应用；为了实现跨机器的软件绿色部署，出现了 Docker，它（最初）是 LXC 的上层封装，彻底改变了软件打包分发的方式，迅速被大量企业广泛采用；为了满足大型系统对服务集群化的需要，出现了 Kubernetes，它（最初）是 Docker 的上层封装，让以多个容器共同协作构建出健壮的分布式系统，成为今天云原生时代的技术基础设施。\n那 Kubernetes 会是容器化崛起之路的终点线吗？它达到了人们对云原生时代技术基础设施的期望了吗？从能力角度讲，是可以说是的，Kubernetes 被誉为云原生时代的操作系统，自诞生之日起就因其出色的管理能力、扩展性与以声明代替命令的交互理念收获了无数喝彩声；但是，从易用角度讲，坦白说差距还非常大，云原生基础设施的其中一个重要目标是接管掉业务系统复杂的非功能特性，让业务研发与运维工作变得足够简单，不受分布式的牵绊，然而 Kubernetes 被诟病得最多的就是复杂，自诞生之日起就以陡峭的学习曲线而闻名。\n举个具体例子，用 Kubernetes 部署一套Spring Cloud 版的 Fenix’s Bookstore，你需要分别部署一个到多个的配置中心、注册中心、服务网关、安全认证、用户服务、商品服务、交易服务，对每个微服务都配置好相应的 Kubernetes 工作负载与服务访问，为每一个微服务的 Deployment、ConfigMap、StatefulSet、HPA、Service、ServiceAccount、Ingress 等资源都编写好元数据配置。这个过程最难的地方不仅在于繁琐，还在于要写出合适的元数据描述文件，既需要懂的开发（网关中服务调用关系、使用容器的镜像版本、运行依赖的环境变量这些参数等等，只有开发最清楚），又需要懂运维（要部署多少个服务，配置何种扩容缩容策略、数据库的密钥文件地址等等，只有运维最清楚），有时候还需要懂平台（需要什么的调度策略，如何管理集群资源，通常只有平台组、中间件组或者核心系统组的同学才会关心），一般企业根本找不到合适的角色来为它管理、部署和维护应用。\n这个事儿 Kubernetes 心里其实也挺委屈，因为以上复杂性不能说是 Kubernetes 带来的，而是分布式架构本身的原罪。对于大规模的分布式集群，无论是最终用户部署应用，还是软件公司管理应用都存在诸多痛点。这些困难的实质源于 Docker 容器镜像封装了单个服务，Kubernetes 通过资源封装了服务集群，却没有一个载体真正封装整个应用，将原本属于应用内部的技术细节圈禁起来，不要暴露给最终用户、系统管理员和平台维护者，让使用者去埋单；应用难以管理矛盾在于封装应用的方法没能将开发、运维、平台等各种角色的关注点恰当地分离。\n既然微服务时代，应用的形式已经不再限于单个进程，那也该到了重新定义“以应用为中心的封装”这句话的时候了。至于具体怎样的封装才算是正确，今天还未有特别权威的结论，不过经过人们的尝试探索，已经窥见未来容器应用的一些雏形，笔者将近几年来研究的几种主流思路列出供你参考。\nKustomize最初，由 Kubernetes 官方给出“如何封装应用”的解决方案是“用配置文件来配置配置文件”，这不是绕口令，你可以理解为一种针对 YAML 的模版引擎的变体。Kubernetes 官方认为应用就是一组具有相同目标的 Kubernetes 资源的集合，如果逐一管理、部署每项资源元数据过于繁琐的话，那就提供一种便捷的方式，把应用中不变的信息与易变的信息分离开来解决管理问题，把应用所有涉及的资源自动生成一个多合一（All-in-One）的整合包来解决部署问题。\n完成这项工作的工具叫作Kustomize，它原本只是一个独立的小程序，从 Kubernetes 1.14 起，被吸纳入kubectl命令之中，成为随着 Kubernetes 提供的内置功能。Kustomize 使用Kustomization 文件来组织与应用相关的所有资源，Kustomization 本身也是一个以 YAML 格式编写的配置文件，里面定义了构成应用的全部资源，以及资源中需根据情况被覆盖的变量值。\nKustomize 的主要价值是根据环境来生成不同的部署配置。只要建立多个 Kustomization 文件，开发人员就能以基于基准进行派生（Base and Overlay）的方式，对不同的模式（譬如生产模式、调试模式）、不同的项目（同一个产品对不同客户的客制化）定制出不同的资源整合包。在配置文件里，无论是开发关心的信息，还是运维关心的信息，只要是在元数据中有描述的内容，最初都是由开发人员来编写的，然后在编译期间由负责 CI/CD 的产品人员针对项目进行定制，最后在部署期间由运维人员通过 kubectl 的补丁（Patch）机制更改其中需要运维去关注的属性，譬如构造一个补丁来增加 Deployment 的副本个数，构造另外一个补丁来设置 Pod 的内存限制，等等。\nk8s ├── base │     ├── deployment.yaml │     ├── kustomization.yaml │     └── service.yaml └── overlays       └── prod       │     ├── load-loadbalancer-service.yaml       │     └── kustomization.yaml       └── debug             └── kustomization.yaml\n\nKustomize 使用 Base、Overlay 和 Patch 生成最终配置文件的思路与 Docker 中分层镜像的思路有些相似，既规避了以“字符替换”对资源元数据文件的入侵，也不需要用户学习额外的 DSL 语法（譬如 Lua）。从效果来看，使用由 Kustomize 编译生成的 All-in-One 整合包来部署应用是相当方便的，只要一行命令就能够把应用涉及的所有服务一次安装好，本文档附带的Kubernetes 版本和Istio 版本的 Fenix’s Booktstore 都使用了这种方式来发布应用的，你不妨实际体验一下。\n但是毕竟 Kustomize 只是一个“小工具”性质的辅助功能，对于开发人员，Kustomize 只能简化产品针对不同情况的重复配置，其实并没有真正解决应用管理复杂的问题，要做的事、要写的配置，最终都没有减少，只是不用反复去写罢了；对于运维人员，应用维护不仅仅只是部署那一下，应用的整个生命周期，除了安装外还有更新、回滚、卸载、多版本、多实例、依赖项维护等诸多问题都很麻烦。这些问题需要更加强大的管理工具去解决，譬如下一节的主角 Helm。不过 Kustomize 能够以极小的成本，在一定程度上分离了开发和运维的工作，无需像 Helm 那样需要一套独立的体系来管理应用，这种轻量便捷，本身也是一种可贵的价值。\nHelm 与 Chart另一种更具系统性的管理和封装应用的解决方案参考了各大 Linux 发行版管理应用的思路，代表为Deis 公司开发的Helm和它的应用格式 Chart。Helm 一开始的目标就很明确：如果说 Kubernetes 是云原生操作系统的话，那 Helm 就要成为这个操作系统上面的应用商店与包管理工具。\nLinux 下的包管理工具和封装格式，如 Debian 系的 apt-get 命令与 dpkg 格式、RHEL 系的 yum 命令与 rpm 格式相信大家肯定不陌生。有了包管理工具，你只要知道应用的名称，就可以很方便地从应用仓库中下载、安装、升级、部署、卸载、回滚程序，而且包管理工具自己掌握着应用的依赖信息和版本变更情况，具备完整的自管理能力，每个应用需要依赖哪些前置的第三方库，在安装的时候都会一并处理好。\nHelm 模拟的就是上面这种做法，它提出了与 Linux 包管理直接对应的 Chart 格式和 Repository 应用仓库，针对 Kubernetes 中特有的一个应用经常要部署多个版本的特点，也提出了 Release 的专有概念。\nChart 用于封装 Kubernetes 应用涉及到的所有资源，通常以目录内的文件集合的形式存在。目录名称就是 Chart 的名称（没有版本信息），譬如官方仓库中 WordPress Chart 的目录结构是这样的：\nWordPress ├── templates │     ├── NOTES.txt │     ├── deployment.yaml │     ├── externaldb-secrets.yaml │     └── 版面原因省略其他资源文件 │     └── ingress.yaml └── Chart.yaml └── requirements.yaml └── values.yaml\n\n其中有几个固定的配置文件：Chart.yaml给出了应用自身的详细信息（名称、版本、许可证、自述、说明、图标，等等），requirements.yaml给出了应用的依赖关系，依赖项指向的是另一个应用的坐标（名称、版本、Repository 地址），values.yaml给出了所有可配置项目的预定义值。可配置项是指需要部署期间由运维人员调整的那些参数，它们以花括号包裹在templates目录下的资源文件中，部署应用时，Helm 会先将管理员设置的值覆盖到values.yaml的默认值上，然后以字符串替换的形式传递给templates目录的资源模版，最后生成要部署到 Kubernetes 的资源文件。由于 Chart 封装了足够丰富的信息，所以 Helm 除了支持命令行操作外，也能很容易地根据这些信息自动生成图形化的应用安装、参数设置界面。\nRepository 仓库用于实现 Chart 的搜索与下载服务，Helm 社区维护了公开的 Stable 和 Incubator 的中央仓库（界面如下图所示），也支持其他人或组织搭建私有仓库和公共仓库，并能够通过 Hub 服务把不同个人或组织搭建的公共仓库聚合起来，形成更大型的分布式应用仓库，有利于 Chart 的查找与共享。\n\n（图片来自Helm 官网)\nHelm 提供了应用全生命周期、版本、依赖项的管理能力，同时，Helm 还支持额外的扩展插件，能够加入 CI/CD 或者其他方面的辅助功能，这样的定位已经从单纯的工具升级到应用管理平台了。强大的功能让 Helm 受到了不少支持，有很多应用主动入驻到官方的仓库中。从 2018 年起，Helm 项目被托管到 CNCF，成为其中的一个孵化项目。\nHelm 以模仿 Linux 包管理器的思路去管理 Kubernetes 应用，一定程度上是可行的，不过，Linux 与 Kubernetes 中部署应用还是存在一些差别，最重要的一点是在 Linux 中 99%的应用都只会安装一份，而 Kubernetes 里为了保证可用性，同一个应用部署多份副本才是常规操作。Helm 为了支持对同一个 Chart 包进行多次部署，每次安装应用都会产生一个版本（Release） ，版本相当于该 Chart 的安装实例。对于无状态的服务，Helm 靠着不同的版本就已经足够支持多个服务并行工作了，但对于有状态的服务来说，这些服务会与特定资源或者服务产生依赖关系，譬如要部署数据库，通常要依赖特定的存储来保存持久化数据，这样事情就变得复杂起来。Helm 无法很好地管理这种有状态的依赖关系，这一类问题就是 Operator 要解决的痛点了。\nOperator 与 CRDOperator不应当被称作是一种工具或者系统，它应该算是一种封装、部署和管理 Kubernetes 应用的方法，尤其是针对最复杂的有状态应用去封装运维能力的解决方案，最早由 CoreOS 公司（于 2018 年被 RedHat 收购）的华人程序员邓洪超所提出。\n如果上一节“以容器构建系统”介绍 Kubernetes 资源与控制器模式时你没有开小差的话，那么 Operator 中最核心的理念你其实就已经理解得差不多了。简单地说，Operator 是通过 Kubernetes 1.7 开始支持的自定义资源（Custom Resource Definitions，CRD，此前曾经以 TPR，即 Third Party Resource 的形式提供过类似的能力），把应用封装为另一种更高层次的资源，再把 Kubernetes 的控制器模式从面向于内置资源，扩展到了面向所有自定义资源，以此来完成对复杂应用的管理。下面是笔者引用了一段 RedHat 官方对 Operator 设计理念的阐述：\n\nOperator 设计理念Operator 是使用自定义资源（CR，笔者注：CR 即 Custom Resource，是 CRD 的实例）管理应用及其组件的自定义 Kubernetes 控制器。高级配置和设置由用户在 CR 中提供。Kubernetes Operator 基于嵌入在 Operator 逻辑中的最佳实践将高级指令转换为低级操作。Kubernetes Operator 监视 CR 类型并采取特定于应用的操作，确保当前状态与该资源的理想状态相符。\n​                                                                                                                                                   —— 什么是 Kubernetes Operator，RedHat\n\n以上这段文字不是笔者转述，而是直接由 RedHat 官方撰写和翻译成中文的，准确严谨但比较拗口，对于没接触过 Operator 的人并不友好，什么叫作“高级指令”？什么叫作“低级操作”？两者之间具体如何转换？为了能够理解这些问题，我们需要先弄清楚有状态和无状态应用的含义及影响，然后再来理解 Operator 所做的工作。\n有状态应用（Stateful Application）与无状态应用（Stateless Application）说的是应用程序是否要自己持有其运行所需的数据，如果程序每次运行都跟首次运行一样，不会依赖之前任何操作所遗留下来的痕迹，那它就是无状态的；反之，如果程序推倒重来之后，用户能察觉到该应用已经发生变化，那它就是有状态的。无状态应用在分布式系统中具有非常巨大的价值，我们都知道分布式中的 CAP 不兼容原理，如果无状态，那就不必考虑状态一致性，没有了 C，那 A 和 P 便可以兼得，换而言之，只要资源足够，无状态应用天生就是高可用的。但不幸的是现在的分布式系统中多数关键的基础服务都是有状态的，如缓存、数据库、对象存储、消息队列，等等，只有 Web 服务器这类服务属于无状态。\n站在 Kubernetes 的角度看，是否有状态的本质差异在于有状态应用会对某些外部资源有绑定性的直接依赖，譬如 Elasticsearch 建立实例时必须依赖特定的存储位置，重启后仍然指向同一个数据文件的实例才能被认为是相同的实例。另外，有状态应用的多个应用实例之间往往有着特定的拓扑关系与顺序关系，譬如 Etcd 的节点间选主和投票，节点们都需要得知彼此的存在。为了管理好那些与应用实例密切相关的状态信息，Kubernetes 从 1.9 版本开始正式发布了 StatefulSet 及对应的 StatefulSetController。与普通 ReplicaSet 中的 Pod 相比，由 StatefulSet 管理的 Pod 具备以下几项额外特性：\n\nPod 会按顺序创建和按顺序销毁：StatefulSet 中的各个 Pod 会按顺序地创建出来，创建后续的 Pod 前，必须要保证前面的 Pod 已经转入就绪状态。删除 StatefulSet 中的 Pod 时会按照与创建顺序的逆序来执行。\nPod 具有稳定的网络名称：Kubernetes 中的 Pod 都具有唯一的名称，在普通的副本集中这是靠随机字符产生的，而在 StatefulSet 中管理的 Pod，会以带有顺序的编号作为名称，且能够在重启后依然保持不变。。\nPod 具有稳定的持久存储：StatefulSet 中的每个 Pod 都可以拥有自己独立的 PersistentVolumeClaim 资源。即使 Pod 被重新调度到其它节点上，它所拥有的持久磁盘也依然会被挂载到该 Pod，这点会在“容器持久化”中进一步介绍。\n\n只是罗列出特性，应该很难快速理解 StatefulSet 的设计意图，笔者打个比方来帮助你理解：如果把 ReplicaSet 中的 Pod 比喻为养殖场中的“肉猪”，那 StatefulSet 就是被家庭当宠物圈养的“荷兰猪”，不同的肉猪在食用功能上并没有什么区别，但每只宠物猪都是独一无二的，有专属于自己的名字、习性与记忆，事实上，早期的 StatefulSet 就曾经有一段时间用过 PetSet 这个名字。\n当 StatefulSet 出现以后，Kubernetes 就能满足 Pod 重新创建后仍然保留上一次运行状态的需求，不过有状态应用的维护并不仅限于此，譬如对于一套 Elasticsearch 集群来说，通过 StatefulSet 最多只能做到创建集群、删除集群、扩容缩容等最基本的操作，其他的运维操作，譬如备份和恢复数据、创建和删除索引、调整平衡策略等操作也十分常用，但是 StatefulSet 并不能为此提供任何帮助。\n笔者再举个实际例子来说明 Operator 是如何解决那些 StatefulSet 覆盖不到的有状态服务管理需求的：假设要部署一套 Elasticsearch 集群，通常要在 StatefulSet 中定义相当多的细节，譬如服务的端口、Elasticsearch 的配置、更新策略、内存大小、虚拟机参数、环境变量、数据文件位置，等等，为了便于你对已经反复提及的 Kubernetes 的复杂有更加直观的体验，这里就奢侈一次，挥霍一次版面，将满足这个需求的 YAML 全文贴出如下：\napiVersion: v1kind: Servicemetadata:  name: elasticsearch-clusterspec:  clusterIP: None  selector:    app: es-cluster  ports:  - name: transport    port: 9300---apiVersion: v1kind: Servicemetadata:  name: elasticsearch-loadbalancerspec:  selector:    app: es-cluster  ports:  - name: http    port: 80    targetPort: 9200  type: LoadBalancer---apiVersion: v1kind: ConfigMapmetadata:  name: es-configdata:  elasticsearch.yml: |    cluster.name: my-elastic-cluster    network.host: &quot;0.0.0.0&quot;    bootstrap.memory_lock: false    discovery.zen.ping.unicast.hosts: elasticsearch-cluster    discovery.zen.minimum_master_nodes: 1    xpack.security.enabled: false    xpack.monitoring.enabled: false  ES_JAVA_OPTS: -Xms512m -Xmx512m---apiVersion: apps/v1beta1kind: StatefulSetmetadata:  name: esnodespec:  serviceName: elasticsearch  replicas: 3  updateStrategy:    type: RollingUpdate  template:    metadata:      labels:        app: es-cluster    spec:      securityContext:        fsGroup: 1000      initContainers:      - name: init-sysctl        image: busybox        imagePullPolicy: IfNotPresent        securityContext:          privileged: true        command: [&quot;sysctl&quot;, &quot;-w&quot;, &quot;vm.max_map_count=262144&quot;]      containers:      - name: elasticsearch        resources:            requests:                memory: 1Gi        securityContext:          privileged: true          runAsUser: 1000          capabilities:            add:            - IPC_LOCK            - SYS_RESOURCE        image: docker.elastic.co/elasticsearch/elasticsearch:7.9.1        env:        - name: ES_JAVA_OPTS          valueFrom:              configMapKeyRef:                  name: es-config                  key: ES_JAVA_OPTS        readinessProbe:          httpGet:            scheme: HTTP            path: /_cluster/health?local=true            port: 9200          initialDelaySeconds: 5        ports:        - containerPort: 9200          name: es-http        - containerPort: 9300          name: es-transport        volumeMounts:        - name: es-data          mountPath: /usr/share/elasticsearch/data        - name: elasticsearch-config          mountPath: /usr/share/elasticsearch/config/elasticsearch.yml          subPath: elasticsearch.yml      volumes:        - name: elasticsearch-config          configMap:            name: es-config            items:              - key: elasticsearch.yml                path: elasticsearch.yml  volumeClaimTemplates:  - metadata:      name: es-data    spec:      accessModes: [ &quot;ReadWriteOnce&quot; ]      resources:        requests:          storage: 5Gi\n\n出现如此大量的细节配置，其根本原因在于 Kubernetes 完全不知道 Elasticsearch 是个什么东西，所有 Kubernetes 不知道的信息、不能启发式推断出来的信息，都必须由用户在资源的元数据定义中明确列出，必须一步一步手把手地“教会”Kubernetes 如何部署 Elasticsearch，这种形式就属于 RedHat 在 Operator 设计理念介绍中所说的“低级操作”。\n如果我们使用Elastic.co 官方提供的 Operator，那情况就会简单得多了，Elasticsearch Operator 提供了一种kind: Elasticsearch的自定义资源，在它的帮助下，仅需十行代码，将用户的意图是“部署三个版本为 7.9.1 的 ES 集群节点”说清楚即可，便能实现与前面 StatefulSet 那一大堆配置相同乃至更强大的效果，如下面代码所示。\napiVersion: elasticsearch.k8s.elastic.co/v1kind: Elasticsearchmetadata:  name: elasticsearch-clusterspec:  version: 7.9.1  nodeSets:  - name: default    count: 3    config:      node.master: true      node.data: true      node.ingest: true      node.store.allow_mmap: false\n\n有了 Elasticsearch Operator 的自定义资源，相当于 Kubernetes 已经学会怎样操作了 Elasticsearch，知道所有它相关的参数含义与默认值，无需用户再手把手地教了，这种就是所谓的“高级指令”。\nOperator 将简洁的高级指令转化为 Kubernetes 中具体操作的方法，与前面 Helm 或者 Kustomize 的思路并不相同。Helm 和 Kustomize 最终仍然是依靠 Kubernetes 的内置资源来跟 Kubernetes 打交道的，Operator 则是要求开发者自己实现一个专门针对该自定义资源的控制器，在控制器中维护自定义资源的期望状态。通过程序编码来扩展 Kubernetes，比只通过内置资源来与 Kubernetes 打交道要灵活得多，譬如当需要更新集群中某个 Pod 对象的时候，由 Operator 开发者自己编码实现的控制器完全可以在原地对 Pod 进行重启，而无需像 Deployment 那样必须先删除旧 Pod，然后再创建新 Pod。\n使用 CRD 定义高层次资源、使用配套的控制器来维护期望状态，带来的好处不仅仅是“高级指令”的便捷，而是遵循 Kubernetes 一贯基于资源与控制器的设计原则的同时，又不必再受制于 Kubernetes 内置资源的表达能力。只要 Operator 的开发者愿意编写代码，前面曾经提到那些 StatfulSet 不能支持的能力，如备份恢复数据、创建删除索引、调整平衡策略等操作，都完全可以实现出来。\n把运维的操作封装在程序代码上，表面看最大的受益者是运维人员，开发人员要为此付出更多劳动。然而 Operator 并没有受到开发人员的抵制，让它变得小众，反而由于代码相对于资源配置的表达能力提升，让开发与运维之间的协作成本降低而备受开发者的好评。Operator 变成了近两、三年容器封装应用的一股新潮流，现在很多复杂分布式系统都有了官方或者第三方提供的 Operator（这里收集了一部分）。RedHat 公司也持续在 Operator 上面大量投入，推出了简化开发人员编写 Operator 的Operator Framework/SDK。\n目前看来，应对有状态应用的封装运维，Operator 也许是最有可行性的方案，但这依然不是一项轻松的工作。以Etcd 的 Operator为例，Etcd 本身不算什么特别复杂的应用，Operator 实现的功能看起来也相当基础，主要有创建集群、删除集群、扩容缩容、故障转移、滚动更新、备份恢复等功能，其代码就已经超过一万行了。现在开发 Operator 的确还是有着相对较高的门槛，通常由专业的平台开发者而非业务开发或者运维人员去完成，但是 Operator 符合技术潮流，顺应软件业界所提倡的 DevOps 一体化理念，等 Operator 的支持和生态进一步成熟之后，开发和运维都能从中受益，未来应该能成长为一种应用封装的主流形式。\n开放应用模型本节介绍的最后一种应用封装的方案，是阿里云和微软在 2019 年 10 月上海 QCon 大会上联合发布的开放应用模型（Open Application Model，OAM），它不仅是中国云计算企业参与制定乃至主导发起的国际技术规范，也是业界首个云原生应用标准定义与架构模型。\n开放应用模型思想的核心是如何将开发人员、运维人员与平台人员关注点分离，开发人员关注业务逻辑的实现，运维人员关注程序平稳运行，平台人员关注基础设施的能力与稳定性，长期让几个角色厮混在同一个 All-in-One 资源文件里，并不能擦出什么火花，反而将配置工作弄得越来越复杂，将“YAML Engineer”弄成了容器界的嘲讽梗。\n开放应用模型把云原生应用定义为“由一组相互关联但又离散独立的组件构成，这些组件实例化在合适的运行时上，由配置来控制行为并共同协作提供统一的功能”。没看明白定义并没有关系，为了便于跟稍后的概念对应，笔者首先把这句话拆解、翻译为你更加看不明白的另一种形式：\nOAM 定义的应用\n一个Application由一组Components构成，每个Component的运行状态由Workload描述，每个Component可以施加Traits来获取额外的运维能力，同时我们可以使用Application Scopes将Components划分到一或者多个应用边界中，便于统一做配置、限制、管理。把Components、Traits和Scopes组合在一起实例化部署，形成具体的Application Configuration，以便解决应用的多实例部署与升级。\n然后，笔者通过解析上述所列的核心概念来帮助你理解 OAM 对应用的定义。这段话里面每一个用英文标注出来的技术名词都是 OAM 在 Kubernetes 基础上扩展而来概念，每一个名词都有专门的自定义资源与之对应，换而言之，它们并非纯粹的抽象概念，而是可以被实际使用的自定义资源。这些概念的具体含义是：\n\n服务组件（Components）：由 Component 构成应用的思想自 SOA 以来就屡见不鲜，然而 OAM 的 Component 不仅仅是特指构成应用“整体”的一个“部分”，它还有一个重要职责是抽象那些应该由开发人员去关注的元素。譬如应用的名字、自述、容器镜像、运行所需的参数，等等。\n\n工作负荷（Workload）：Workload 决定了应用的运行模式，每个 Component 都要设定自己的 Workload 类型，OAM 按照“是否可访问、是否可复制、是否长期运行”预定义了六种 Workload 类型，如表 11-2 所示。如有必要还可以通过 CRD 与 Operator 去扩展。\n表 11-2 OAM 的六种工作负荷\n\n\n\n工作负荷\n可访问\n可复制\n长期运行\n\n\n\nServer\n√\n√\n√\n\n\nSingleton Server\n√\n×\n√\n\n\nWorker\n×\n√\n√\n\n\nSingleton Worker\n×\n×\n√\n\n\nTask\n×\n√\n×\n\n\nSingleton Task\n×\n×\n×\n\n\n\n运维特征（Traits）：开发活动有大量复用功能的技巧，但运维活动却很贫乏，平时能为运维写个 Shell 脚本或者简单工具已经算是个高级的运维人员了。OAM 的 Traits 就用于封装模块化后的运维能力，可以针对运维中的可重复操作预先设定好一些具体的 Traits，譬如日志收集 Trait、负载均衡 Trait、水平扩缩容 Trait，等等。 这些预定义的 Traits 定义里，会注明它们可以作用于哪种类型的工作负荷、包含能填哪些参数、哪些必填选填项、参数的作用描述是什么，等等。\n\n应用边界（Application Scopes）：多个 Component 共同组成一个 Scope，你可以根据 Component 的特性或者作用域来划分 Scope，譬如具有相同网络策略的 Component 放在同一个 Scope 中，具有相同健康度量策略的 Component 放到另一个 Scope 中。同时，一个 Component 也可能属于多个 Scope，譬如一个 Component 完全可能既需要配置网络策略，也需要配置健康度量策略。\n\n应用配置（Application Configuration）：将 Component（必须）、Trait（必须）、Scope（非必须）组合到一起进行实例化，就形成了一个完整的应用配置。\n\n\nOAM 使用上述介绍的这些自定义资源将原先 All-in-One 的复杂配置做了一定层次的解耦，开发人员负责管理 Component；运维人员将 Component 组合并绑定 Trait 变成 Application Configuration；平台人员或基础设施提供方负责提供 OAM 的解释能力，将这些自定义资源映射到实际的基础设施。不同角色分工协作，整体简化了单个角色关注的内容，使得不同角色可以更聚焦更专业的做好本角色的工作，整个过程如图 11-8 所示。\n\n（图片来自OAM 规范 GitHub)\nOAM 未来能否成功，很大程度上取决于云计算厂商的支持力度，因为 OAM 的自定义资源一般是由云计算基础设施负责解释和驱动的，譬如阿里云的EDAS就已内置了 OAM 的支持。如果你希望能够应用于私有 Kubernetes 环境，目前 OAM 的主要参考实现是Rudr（已声明废弃）和Crossplane，Crossplane 是一个仅发起一年多的 CNCF 沙箱项目，主要参与者包括阿里云、微软、Google、RedHat 等工程师。Crossplane 提供了 OAM 中全部的自定义资源以及控制器，安装后便可用 OAM 定义的资源来描述应用。\n\n后记：今天容器圈的发展是一日千里，各种新规范、新技术层出不穷，本节根据人气和代表性，列举了其中最出名的四种，其他笔者未提到的应用封装技术还有CNAB、Armada、Pulumi等等。这些封装技术会有一定的重叠之处，但并非都是重复的轮子，实际应用时往往会联合其中多个工具一起使用。应该如何封装应用才是最佳的实践，目前尚且没有定论，但是以应用为中心的理念却已经成为明确的共识。\n","categories":["凤凰架构"]},{"title":"分布式的基石【分布式共识算法】","url":"/posts/10488/","content":"在正式探讨分布式环境中面临的各种技术问题和解决方案前，我们先把目光从工业界转到学术界、学习几种具有代表性的分布式共识算法，为后续在分布式环境中操作共享数据准备好理论基础。下面笔者从一个最浅显的场景开始，引出本章的主题:\n\n如果你有一份很重要的数据，要确保它长期存储在电脑上不会丢失，你会怎么做?\n\n这不是什么脑筋急转弯的古怪问题，答案就是去买几块硬盘，在不同硬盘上多备份几个副本。假设一块硬盘每年损坏的概率是 5%，把文件复制到另一块备份盘上，两块硬盘同时损坏而丢失数据的概率就只有 0.25%，如果使用三块硬盘存储则丢失数据的概率是 0.00125%，四块是 0.0000625%，换言之，四块硬盘就可以保证数据在一年内有超过 99.9999% 的概率是安全可靠的。\n在软件系统里，要保障系统的可靠性，采用的办法与上面用几个备份硬盘来保障的方法并没有什么区别。单个节点的系统宕机导致数据无法访问的原因可能有很多，譬如程序出错、硬件损坏、网络分区、电源故障，等等，一年中出现系统宕机的概率也许还要高于 5%，这决定了软件系统也必须有多台机器，并且它们拥有一致的数据副本，才有可能对外提供可靠的服务。\n在软件系统里，要保障系统的可用性，面临的困难与硬盘备份面临的困难又有着本质的区别。硬盘之间是孤立的，不需要互相通信，备份数据是静态的，初始化后状态就不会发生改变，由人工进行的文件复制操作，很容易就保障了数据在各个备份盘中的一致性。然而在分布式系统中，我们必须考虑动态的数据如何在不可靠的网络通信条件下，依然能在各个节点之间正确复制的问题。将我们要讨论的场景做如下修改:\n\n如果你有一份会随时变动的数据，要确保它正确地存储于网络中的几台不同机器之上，你会怎么做?\n\n相信最容易想到的答案一定是 “数据同步”：每当数据发生变化，把变化情况在各个节点间的复制视作一种事务性的操作，只有系统里每一台机器都反馈成功、完成磁盘写人，数据的变化才宣告成功。笔者曾经在3.2节中介绍过，使用 2PC/3PC 就可以实现这种同步操作。一种真实的数据同步应用场景是数据库的主从全同步复制（Fully Synchronous Replication），譬如 MySQL 集群，它在进行全同步复制时，会等待所有 Slave 节点的 Binlog 都完成写人后，才会提交Master节点的事务。（这个场景中Binlog本身就是要同步的状态数据，不应将它看作指令日志的集合。然而这里有一个明显的缺陷，尽管可以确保 Master 节点和 Slave 节点中的数据是绝对一致的，但任何一个 Slave 节点因为任何原因未响应均会阻塞整个事务，每增加一个 Slave 节点，都会造成整个系统可用性风险增加一分。\n以同步为代表的数据复制方法，被称为状态转移（State Transfer），是较符合人类思准的可靠性保障手段，但通常要以牺牲可用性为代价。我们在建设分布式系统的时候，往往不能承受这样的代价，一些关键系统，在必须保障数据正确可靠的前提下，也对可用性有非常高的要求，譬如系统要保证数据达到 99.999999% 可靠，同时系统自身也要达到 99.999% 可用的程度。这就引出了我们的第三个问题：\n\n如果你有一份会随时变动的数据，要确保它正确地存储于网络中的几台不同机器之上，并且要尽可能保证数据是随时可用的，你会怎么做?\n\n可靠性与可用性的矛盾造成了增加机器数量反而带来可用性的降低。为缓解这个矛盾在分布式系统里主流的数据复制方法是以 操作转移（Operation Transfer）为基础的。我们想要改变数据的状态，除了直接将目标状态赋予它之外，还有另一种常用的方法是通过某种操作，令源状态转换为目标状态。能够使用确定的操作促使状态间产生确定的转移结果的计算模型，在计算机科学中被称为状态机（State Machine）。\n\n状态机的特性\n状态机有一个特性：任何初始状态一样的状态机，如果执行的命令序列一样，则最终达到的状态也一样。如果将此特性应用在多参与者的协商共识上，可以理解为系统中存在多个具有完全相同的状态机（参与者），这些状态机能最终保持一致的关键就是起始状态完全一致和执行命令序列完全一致。\n\n根据状态机的特性，要让多台机器的最终状态一致，只要确保它们的初始状态是一致的。并且接收到的操作指令序列也是一致的即可，无论这个操作指令是新增、修改、删除中或是其他任何可能的程序行为，都可以理解为要将一连串的操作日志正确地广播给各个分布式节点。在广播指令与指令执行期间，允许系统内部状态存在不一致的情况，即并不要求所有节点的每一条指令都是同时开始、同步完成的，只要求在此期间的内部状态不能被外部观察到，且当操作指令序列执行完毕时，所有节点的最终状态是一致的，则这种模型就被称为 状态机复制 （State Machine Replication）。\n考虑到分布式环境下网络分区现象是不可能消除的，甚至允许不再追求系统内所有节点在任何情况下的数据状态都一致，而是采用“少数服从多数”的原则，一旦系统中过半数的节点完成了状态的转换，就认为数据的变化已经被正确地存储在了系统中，这样就可以容忍少数（通常是不超过半数）的节点失联，减弱增加机器数量对系统整体可用性的影响，这种思想在分布式中被称为 “Quorum 机制”。\n根据上述讨论，我们需要设计出一种算法，能够让分布式系统内部暂时容忍不同的状态，但最终保证大多数节点的状态达成一致；同时，能够让分布式系统在外部看来始终表现出整体一致的结果。这个让系统各节点不受局部的网络分区、机器崩溃、执行性能或者其他因素影响，都能最终表现出整体一致的过程，就被称为各个节点的 协商共识（Consensus）。\n最后，笔者还要提醒你注意共识与一致性的区别：一致性是指数据不同副本之间的差异，而共识是指达成一致性的方法与过程。由于翻译的关系，很多中文资料把Consensus同样翻译为一致性，导致网络上大量的“二手中文资料”将这两个概念混淆起来，如果你在网上看到“分布式一致性算法”，应明白其指的其实是“Distributed Consensus Algorithm”。\n一、Paxos\n世界上只有一种共识协议，就是 Paxos，其他所有共识算法都是 Paxos 的退化版本。                                                                            ——Mike Burrows，Google Chubby 作者\n\nPaxos 是由 Leslie Lamport （就是大名鼎鼎的LaTeX中的 “La” ）提出的一种基于消息专递的协商共识算法，是当今分布式系统最重要的理论基础，几乎就是“共识”二字的代名词。这个极高的评价出自于提出Raft算法的论文，更显分量十足。虽然笔者认为 Mike3urrows 所言有些夸张，但是如果没有 Paxos，那后续的 Raft、ZAB 等算法，ZooKeeper、 etcd 等分布式协调框架、Hadoop、Consul 等在此基础上的各类分布式应用都很可能会延后好几年面世。\n1. Paxos 的诞生为了解释清楚 Paxos 算法，Lamport 虚构了一个名为 “Paxos” 的希腊城邦，这个城邦按照民主制度制定法律，却没有一个中心化的专职立法机构，而是靠着 “兼职议会”（Part- Time Parliament）来完成立法，无法保证所有城邦居民都能够及时了解新的法律提案，也无法保证居民会及时为提案投票。Paxos 算法的目标就是让城邦能够在每一位居民都不承诺一定会及时参与的情况下，依然可以按照少数服从多数的原则，最终达成一致意见。但是 Paxos 算法并不考虑拜占庭将军问题，即假设信息可能丢失也可能延迟，但不会被错误传递。\nLamport 在1990年首次发表了 Paxos 算法，选的论文题目就是 “The Part-Time Parliament” 由于算法本身极为复杂，用希腊城邦作为比喻反而使得描述更晦涩，论文的三个审稿人一致要求他把希腊城邦的故事删掉。这令Lamport感觉颇为不爽，干脆就撤稿不发了，所以 Paxos刚刚被提出的时候并没有引起什么反响。八年之后（1998年），Lamport 将此文章重整理后投到 ACM Transactions on Computer Systems。这次论文成功发表，Lamport的名也确实吸引了一些人去研究，但并没有多少人能弄懂他在说什么。时间又过去了三年（200年），Lamport认为前两次的论文没有引起反响，是因为同行们无法理解他以“希腊城邦”来讲故事的幽默感，所以这一次他以“Paxos Made Simple”为题，在SIGACT News 杂志上表文章，放弃了“希腊城邦”的比喻，尽可能用（他认为）简单直接、（他认为）可读性较强的方式去介绍Paxos算法。情况虽然比前两次要好上一些，但以Paxos本应获得的重视程来说，这次依然只能算是应者寥寥。这一段听起来如同网络段子一般的经历被Lamport 以自嘲的形式放到了他的个人网站上。尽管我们作为后辈应该尊重Lamport老爷子，但当笔翻开“Paxos Made Simple”的论文，见到只有“The Paxos algorithm, when presented in plain English, is very simple.”这一句话的“摘要”时，心里实在是不得不怀疑Lamport 这样写论文是不是在恶搞审稿人和读者，在嘲讽“你们这些愚蠢的人类”。\n虽然Lamport本人连发三篇文章都没能让大多数同行理解Paxos，但2006年，在 Google的Chubby、Megastore以及Spanner等分布式系统都使用Paxos 解决了分布式共识的问题，并将其整理成正式的论文发表之后，得益于Google的行业影响力，辅以Chubby作者Mike Burrows那略显夸张但足够吸引眼球的评价推波助澜，Paxos算法一夜间成为计算机科学分布式这条分支中最炙手可热的概念，开始被学术界众人争相研究。Lamport 本人因其对分布式系统的杰出理论贡献获得了2013年的图灵奖，随后才有了 Paxos 在区块链、分布式系统、云计算等多个领域大放异彩的故事。\n2. 算法流程下面，我们来正式学习 Paxos 算法（在本节中 Paxos 均特指最早的 Basic Paxos 算法）。\nPaxos 算法将分布式系统中的节点分为三类。\n\n提案节点（Proposer）：提出对某个值进行设置操作的节点，设置值这个行为就被称为提案（Proposal），值一旦设置成功，就是不会丢失也不可变的。注意，Paxos是典型的基于操作转移模型而非状态转移模型来设计的算法，不要把这里的“设置值”类比成程序中变量赋值操作。而应该类比成日志记录操作，在后面介绍的Raft算法中就直接把“提案”叫作“日志”了。\n决策节点（Acceptor）：是应答提案的节点，决定该提案是否可被投票、是否可被接受。提案一旦得到过半数决策节点的接受，即称该提案被批准（Accept）。提案被批准即意味着该值不能被更改，也不会丢失，且最终所有节点都会接受它。\n记录节点（Learner）：不参与提案，也不参与决策，只是单纯地从提案、决策节点中学习已经达成共识的提案，譬如少数派节点从网络分区中恢复时，将会进入这种状态。\n\n在使用Paxos算法的分布式系统里，所有的节点都是平等的，它们都可以承担以上某一种或者多种的角色。不过为了便于确保有明确的多数派，决策节点的数量应该被设定为奇数个，且在系统初始化时，网络中每个节点都应该知道整个网络所有决策节点的数量地址等信息。\n在分布式环境下，如果我们说各个节点 “就某个值（提案）达成一致”，指的是 “不存在某个时刻有一个值为A，另一个时刻又为B的情景”。解决这个问题的复杂度主要来源于以下两个方面因素的共同影响。\n\n系统内部各个节点通信是不可靠的，不论是对于系统中企图设置数据的提案节点抑或是决定是否批准设置操作的决策节点，其发出、收到的信息可能延迟送达、可能丢失，但不去考虑消息有传递错误的情况。\n系统外部各个用户访问是可并发的，如果系统只会有一个用户，或者每次只对系统进行串行访问，那单纯地应用 Quorum 机制，少数节点服从多数节点，就足以保证值被正确地读写。\n\n第一点是网络通信中客观存在的现象，也是所有共识算法都要重点解决的问题。对于第二点，详细解释如下。现在我们讨论的是 “分布式环境下并发操作的共享数据” 的问题即使先不考虑是否在分布式的环境下，只考虑并发操作，假设有一个变量 i 当前在系统中存储的数值为2，同时有外部请求A、B分别对系统发送操作指令，“把 i 的值加1” 和 “把 i 的值乘3”，如果不加任何并发控制，将可能得到 “(2+1)x3=9” 与 “2x3+1=7” 这两种可能的结果。因此，对同一个变量的并发修改必须先加锁后操作，不能让A、B的请求被交替处理，这也可以说是程序设计的基本常识。而在分布式的环境下，由于要同时考虑到分布式系统内可能在任何时刻出现的通信故障，如果一个节点在取得锁之后、在释放锁之前发生崩溃失联，这将导致整个操作被无限期的等待所阻塞，因此算法中的加锁就不完全等同于并发控制中以互斥量来实现的加锁，还必须提供一个其他节点能抢占锁的机制，以避免因通信问题而出现死锁。\n为了解决这个问题、分布式环境中的锁必须是可抢占的。Paxos算法包括两个阶段，一阶段“准备”（Prepare）就相当于上面抢占锁的过程。如果某个提案节点准备发起提案。必须先向所有的决策节点广播一个许可申请（称为Prepare 请求）。提案节点的 Prepare 请求中会附带一个全局唯一且单调递增的数字n作为提案ID，决策节点收到后，将会给予提案节点两个承诺与一个应答。\n两个承诺是指：\n\n承诺不会再接受提案ID小于或等于 n 的 Prepare 请求:\n承诺不会再接受提案ID小于 n 的 Accept 请求。\n\n一个应答是指：\n\n在不违背以前的承诺的前提下，回复已经批准过的提案中ID最大的那个提案所定的值和提案ID，如果该值从来没有被任何提案设定过，则返回空值。如果违反此前做出的承诺，即收到的提案ID并不是决策节点收到的最大的ID，那允许直接对此 Prepare 请求不予理会。\n\n当提案节点收到了多数派决策节点的应答（称为 Promise 应答）后，就可以开始第二阶段的“批准”（ Accept ）过程，这时有如下两种可能的结果:\n\n如果提案节点发现所有响应的决策节点此前都没有批准过该值（即为空），那说明它是第一个设置值的节点，可以随意地决定要设定的值，将自己选定的值与提案ID组成一个二元组“（id, value）”，再次广播给全部决策节点（称为 Accept 请求）;\n如果提案节点发现响应的决策节点中已经有至少一个节点的应答中包含值了，那它就不能够随意取值，而是必须无条件地从应答中找出提案ID最大的那个值并接收，组成一个二元组“（id, maxAcceptValue）”，再次广播给全部决策节点（称为 Accept 请求）。\n\n当每一个决策节点收到 Accept 请求时，都会在不违背以前的承诺的前提下。接收并持久化当前提案ID和提案附带的值。如果违反此前做出的承诺，即收到的提案ID并不是决策节点收到过的最大的ID，那允许直接对此Accept请求不予理会。\n当提案节点收到了多数派决策节点的应答（称为 Accepted 应答）后，协商结束，共识决议形成，然后将形成的决议发送给所有记录节点进行学习。整个过程的时序图如下图所示。\n![Paxos 算法整体时序图](../../../imgs/phoenix/Paxos 算法整体时序图.png)\n整个 Paxos 算法的工作流程至此结束，如果你此前并未专门学习过分布式的知识，可能还不能对Paxos算法究竟是如何解决协商共识的形成具体的概念。下面笔者以一个更具体例子来讲解 Paxos。\n3. 工作实例假设一个分布式系统有五个节点，分别命名为S1、S2、S3、S4、S5，五个节点都同时扮演着提案节点和决策节点的角色。这个例子中只讨论正常通信的场景，不涉及网络分区。\n此时，有两个并发的请求希望将同一个值分别设定为 X（由 S1 作为提案节点提出）和 Y （由 S5 作为提案节点提出），以 P 代表准备阶段，以 A 代表批准阶段，这时可能发生以下几种情况。\n\n情况一：譬如，S1 选定的提案ID是 3.1 （全局唯一ID加上节点编号），先取得了多数派决策节点的 Promise 和 Accepted 应答，此时 S5 选定提案ID 4.5，发起 Prepare 请求，收到的多数应答中至少会包含 1 个此前应答过 S1 的决策节点，假设是 S3，那么 S3 提供的 Promise 中必将包含 S1 已设定好的值 X，S5 就必须无条件地用 X 代替 Y 作为自己提案的值，由此整个系统对 “取值为 X” 这个事实达成一致，如图所示。\n\n\n情况二：事实上，对于情况一，X 被选定为最终值是必然结果，但从上图中可以看出，X 被选定为最终值并不是必须得到多数派的共同批准，而是只取决于 S5 提案时 Promise 应答中是否已包含了批准过 X 的决策节点，譬如下图所示，S5 发起提案的 Prepare 请求时，X 并未获得多数派批准，但由于 S3 已经批准，所以最终共识的结果仍是 X。\n\n\n\n\n情况三：另外一种可能的结果是 S5 提案时 Promise 应答中并未包含批准过 X 的决策节点，譬如应答 S5 提案时，节点 S1 已经批准了 X，节点 S2、S3 未批准但返回了 Promise 应答，此时 S5 以更大的提案 ID 获得了 S3、S4、S5 的 Promise 应答，由于这三个节点均未批准过任何值，所以 S3 将不再接收来自 S1 的 Accept 请求，因为它的提案 ID 已经不是最大的了，这三个节点将批准 Y 的取值，整个系统最终会对 “取值为Y” 达成一致，如下图所示。\n\n\n\n情况四：从情况三可以推导出另一种极端的情况，如果两个提案节点交替使用更大的提案 ID，使得准备阶段成功、批准阶段失败，那么这个过程理论上可以无限持续下去、形成活锁（Live Lock），如下图所示。在算法实现中会引入随机超时时间来避免活锁的产生。\n\n\n虽然 Paxos 是以复杂著称的算法，但以上介绍都是基于 Basic Paxos、以正常流程（未出现网络分区等异常）、通俗方式讲解的 Paxos 算法，并未涉及严谨的逻辑和数学原理，也未讨论 Paxos 的推导证明过程，理解起来应该不算太困难。\nBasic Paxos 的价值在于开拓了分布式共识算法的发展思路，但由于它有如下缺陷。一般不会直接用于实践：\nBasic Paxos 只能对单个值形成决议，并且决议的形成至少需要两次网络请求和应答（准备和批准阶段各一次），高并发情况下将产生较大的网络开销，极端情况下甚至可能形成活锁。总之，Basic Paxos 是一种很学术化但对工业化并不友好的算法现在几乎只用来做理论研究，实际的应用都是基于 Multi Paxos 和 Fast Paxos 算法，接下来我们将会了解 Multi Paxos 以及一些与它的理论等价的算法（如 Raft、ZAB 等算法）。\n二、 Multi Paxos在上一节的最后，举例介绍了 Basic Paxos 的活锁问题，即两个提案节点争相提出自己的提案，抢占同一个值的修改权限，导致整个系统在持续性地 “反复横跳”，外部看起来就像被锁住了一样。此外，笔者还讲述过一个观点，分布式共识的复杂性主要来源于网络的不可靠与请求的可并发两大因素，活锁问题与许多 Basic Paxos 异常场景中所遭遇的麻烦，都可以看作源于任何一个提案节点都能够完全平等地、与其他节点并发地提出提案而带来的复杂问题。为此，Lamport 提出了一种 Paxos 的改进版本—— Multi Paxos 算法，望能够找到一种两全其美的办法，既不破坏 Paxos 中 “众节点平等” 的原则，又能在提节点中实现主次之分，限制每个节点都有不受控的提案权利。这两个目标听起来似乎是矛盾的，但现实世界中的选举就很符合这种在平等节点中挑选意见领袖的情景。\nMulti Paxos 对 Basic Paxos 的核心改进是增加了“选主”的过程，提案节点会通过定时轮询（心跳），确定当前网络中的所有节点里是否存在一个主提案节点，一旦没有发现主节点，节点就会在心跳超时后使用 Basic Paxos 中定义的准备、批准的两轮网络交互过程，向所有其他节点广播自己希望竞选主节点的请求，希望整个分布式系统对 “由我作为主节点“ 这件事情协商达成一致共识，如果得到了决策节点中多数派的批准，便宣告竞选成功。选主完成之后，除非主节点失联之后发起重新竞选，否则从此往后，就只有主节点本身才能够提出提案。此时，无论哪个提案节点接收到客户端的操作请求，都会将请求转发给主节点来完成提案，而主节点提案时，就无须再次经过准备过程，因为可以认为在经过选举时的那一次准备之后，后续的提案都是对相同提案ID的一连串的批准过程。也可以通俗理解为选主过后，就不会再有其他节点与它竞争，相当于处于无并发的环境当中的有序操作，所以此时系统中要对某个值达成一致，只需要进行一次批准的交互即可，如图所示。\n\n可能有人注意到这时候的二元组（id, value）已经变成了三元组（id, i, value），这是因为需要给主节点增加一个 “任期编号”，这个编号必须是严格单调递增的，以应付主节点陷人网络分区后重新恢复，但另外一部分节点仍然有多数派，且已经完成了重新选主的情况，此时必须以任期编号大的主节点为准。节点有了选主机制的支持后，在整体来看，就可以进一步简化节点角色，不去区分提案、决策和记录节点，而是统统以 “节点” 来代替，节点只有主（Leader）和从（Follower）的区别，此时协商共识的时序图如下图所示。\n\n下面我们换一个角度来重新思考 “分布式系统中如何对某个值达成一致” 这个问题，可以把该问题划分为三个子问题来考虑，可以证明（具体证明就不列在这里了，感兴趣的读者可参考 Raft 的论文）当以下三个问题同时被解决时，即等价于达成共识:\n\n如何选主（Leader Election）\n如何把数据复制到各个节点上（EntityReplication）\n如何保证过程是安全的（Safety）\n\n尽管选主问题还涉及许多工程上的细节，譬如心跳、随机超时、并行竞选等，但只论原理的话，如果你已经理解了Paxos算法的操作步骤，相信对选主并不会有什么疑惑，因为这本质上仅仅是分布式系统对 “谁来当主节点” 这件事情达成的共识而已，我们在前一节已经讲述了分布式系统该如何对一件事情达成共识，这里就不再赘述了，下面直接来解决数据（Paxos 中的提案、Raft中的日志）在网络各节点间的复制问题。\n在正常情况下，客户端向主节点发起一个操作请求，譬如提出 “将某个值设置为X” 。此时主节点将 X 写入自己的变更日志，但先不提交，接着在下一次心跳包中把变更 X 的信息广播给所有的从节点，并要求从节点回复 “确认收到” 的消息，从节点收到信息后，将操作写入自己的变更日志，然后向主节点发送 “确认签收” 的消息，主节点收到过半数的签收消息后，提交自己的变更、应答客户端并且给从节点广播可以提交的消息，从节点收到提交消息后提交自己的变更，至此，数据在节点间的复制宣告完成。\n在异常情况下，网络出现了分区，部分节点失联，但只要仍能正常工作的节点的数量能够满足多数派（过半数）的要求，分布式系统就可以正常工作，这时的数据复制过程如下。\n\n假设有S1、S2、S3、S4、S5 五个节点，s1 是主节点，由于网络故障，导致S1、s2 和 S3、S4、s5 之间彼此无法通信，形成网络分区。\n\n一段时间后，S3、S4、s5 三个节点中的某一个（譬如是S3）最先达到心跳超时的阙值，获知当前分区中已经不存在主节点，则它向所有节点发出自己要竞选的广播，并收到了S4、S5 节点的批准响应，加上自己一共三票，即得到了多数派的批准，竞选成功，此时系统中会同时存在 S1 和 S3 两个主节点，但由于网络分区，它们不会知道对方的存在。\n\n这种情况下，客户端发起操作请求。\n\n如果客户端连接到了 S1、S2 其中之一，都将由 S1 处理。但由于操作只能获得最多两个节点的响应，不构成多数派的批准，所以任何变更都无法成功提交。\n\n如果客户端连接到了 S3、S4、s5 其中之一，都将由 S3 处理，此时操作可以获得最多三个节点的响应、构成多数派的批准，是有效的，变更可以被提交，即系统可以继续提供服务。\n\n事实上，以上两种情景很少能够并存。网络分区是由于软、硬件或者网络故障面导致的、内部网络出现了分区，但两个分区仍然能分别与外部网络的客户端正常通信的情况甚为少见。更多的场景是算法能容忍网络里下线了一部分节点，按照这个例子来说，如果下线了两个节点，系统仍能正常工作，如果下线了三个节点，那剩余的两个节点就不可能继续提供服务了。\n\n\n\n假设现在障复，分区解除，五个节点可以重新通信：\n\nS1 和 S3 都向所有节点发送心跳包，从各自的心跳中可以得知两个主节点里 S3 的任期编号更大，它是最新的，此时五个节点均只承认 S3 是唯一的主节点。 \nS1、S2 回滚它们所有未被提交的变更。\nS1、S2 从主节点发送的心跳包中获得它们失联期间发生的所有变更，将变更提交并写人本地磁盘。\n此时分布式系统各节点的状态达成最终一致。\n\n\n\n下面我们来看第三个问题：“如何保证过程是安全的”。不知你是否感觉到这个问题与前两个问题的差异呢？选主、数据复制都是很具体的行为，但是 “安全” 就很模糊，什么算安全或者算不安全？\n在分布式理论中，Safety 和 Liveness 两种属性是有预定义的术语，在专业的资料中一般翻译成 “协定性” 和 “终止性” 。这两个概念也是由 Lamport 最先提出，当时给出的定义如下。\n\n协定性（Safety）：所有的坏事都不会发生。\n终止性（Liveness）：所有的好事都终将发生，但不知道是什么时候。\n\n这里我们不去纠结严谨的定义，仍通过举例来说明它们的具体含义。譬如以选主问题为例，协定性保证了选主的结果一定有且只有唯一的一个主节点，不可能同时出现两个主节点；而终止性则要保证选主过程一定可以在某个时刻结束。由前面对活锁的介绍可知，在终止性这个属性上选主问题是存在理论上的瑕疵的，可能会由于活锁而导致一直无法选出明确的主节点。所以 Raft 论文中只写了对 Safety 的保证，但由于工程实现上的处理，现实中几乎不可能会出现终止性的问题。\n以上这种把共识问题分解为 “选主”、“复制” 和 “安全” 三个问题来思考、解决的思路，即 “Raft算法” （在 Raft 的《一种可以让人理解的共识算法》中提出），并获得了 USENIX ATC 2014 大会的 Best Paper，后来更是成为eted、LogCabin、Consul 等重要分布式程序的实现基础，ZooKeeper 的 ZAB 算法与 Raft 的思路也非常类似，这些算法都被认为是 Multi Paxos 的等价派生实现。\n三、Gossip协议Paxos、Raf、ZAB 等分布式算法经常会被称作 “强一致性” 的分布式共识协议，其实样的描述有语病嫌疑。但我们都明白它的意思其实是：“尽管系统内部节点可以存在不一的状态。但从系统外部看来，不一致的情况并不会被观察到，所以整体上看系统是强一致性的。”\n与它们相对的，还有另一类被冠以 “最终一致性” 的分布式共识协议，这表明系统中不一致的状态有可能会在一定时间内被外部直接观察到。一种典型且极为常见的最终一致的分布式系统就是DNS系统，在各节点缓存的 TTL 到期之前，都有可能与真实的域名翻译结果不一致。在本节中，笔者将介绍在比特币网络和许多重要分布式框架中都有应用的另一种具有代表性的 “最终一致性” 的分布式共识协议：Gossip 协议。\nGossip 最早由施乐公司Palo Alto 研究中心在论文 “Epidemic Algorithms for Replicated Database Maintenance” 中提出的一种用于分布式数据库在多节点间复制同步数据的算法。从论文题目中可以看出，最初它是被称作 “流行病算法” （Epidemic Algorithm）的，只是不太雅观，今天 Gossip 这个名字用得更为普遍，除此以外，它还有“流言算法” “八卦算法” “瘟疫算法” 等别名，这些名字都很形象地反映了 Gossip 的特点：要同步的信息如同流言一般传播，病毒一般扩散。\n笔者按照习惯也把 Gossip 称作 “共识协议”，但首先必须强调它并不是直接与 Paxos Raft 这些共识算法等价的，只是基于 Gossip 之上可以通过某些方法去实现与 Paxos、Raft 类似的目标而已。一个最典型的例子是比特币网络中使用了 Gossip 协议，用于在各个分布式节点中互相同步区块头和区块体的信息，这是整个网络能够正常交换信息的基础，但并不能称作共识；然后比特币使用工作量证明（ProofofWork，PoW）来对 “这个区块由谁来记账” 这一件事情在全网达成共识，这样这个目标才可以认为与 Paxos、Raft 的目标是一致的。\n下面，我们来了解 Gossip 的具体工作过程。相比 Paxos、Raft 等算法，Gossip 的过程十分简单，它可以看作以下两个步骤的简单循环。\n\n如果有某一项信息需要在整个网络的所有节点中传播，那从信息源开始，选择一个固定的传播周期（譬如1秒）。随机选择它相连接的 k 个节点（称为 Fan-Out）来传播消息。\n每一个节点收到消息后，如果这个消息是它之前没有收到过的，则在下一个周期内，该节点将向除了发送消息给它的那个节点外的其他相邻的 k 个节点发送相同的消息，直到最终网络中所有节点都收到了消息。尽管这个过程需要一定时间，但是理论上最终网络的所有节点都会拥有相同的消息。\n\n根据 Gossip 的过程描述，我们很容易发现 Gossip 对网络节点的连通性和稳定性几乎没有任何要求，它一开始就将网络某些节点只能与一部分节点部分连通（Partially Connected Network）而不是以全连通网络（Fully Connected Network）作为前提；能够容忍网络上节点随意地增加或者减少，随意地宕机或者重启；新增加或者重启的节点的状态最终会与其他节点同步达成一致。Gossip 把网络上所有节点都视为平等而普通的一员，没有任何中心化节点或者主节点的概念，这些特点使得 Gossip 具有极强的鲁棒性，而且非常适合在公众互联网中应用。\n同时我们也很容易找到 Gossip 的缺点。消息最终是通过多个轮次的散播到达全网的因此它必然会存在全网各节点状态不一致的情况，而且由于是随机选取发送消息的节点，所以尽管可以在整体上测算出统计学意义上的传播速率，但对于个体消息来说，无法准确地预计需要多长时间才能达成全网一致。另外一个缺点是消息的冗余，同样是由于随机选取发送消息的节点，所以就不可避免地存在消息重复发送给同一节点的情况，增加了网络的传输压力，也给消息节点带来了额外的处理负载。\n达到一致性耗费的时间与网络传播中消息冗余量这两个缺点存在一定对立，如果要改善其中一个，就会恶化另外一个，由此，Gossip 设计了两种可能的消息传播模式：反熵（Anti-Entropy）和传谣（Rumor-Mongering）。熵（Entropy）是生活中少见但科学中很常用的概念，它代表着事物的混乱程度。反熵的意思就是反混乱，以提升网络各个节点之间的相似度为目标。所以在反熵模式下，会同步节点的全部数据，以消除各节点之间的差异，目标是使整个网络各节点完全一致。但是，在节点本身就会发生变动的前提下，这个目标将使得整个网络中消息的数量非常庞大，给网络带来巨大的传输开销。而传谣模式是以传播消息为目标，只发送新到达节点的数据，即只对外发送变更消息，这样消息数据量将显著缩减，网络开销也相对减小。\n","categories":["凤凰架构"]},{"title":"架构师的视角【架构安全性】","url":"/posts/34708/","content":"即使只限定在 “软件架构设计” 这个语境下，系统安全仍然是一个很大的话题。我们讨论的计算机系统安全，不仅仅是指 “防御系统被黑客攻击” 这样狭隘的安全，还至少应包括（不限于）以下这些问题的具体解决方案。\n\n认证（Authentication）：系统如何正确分辨出操作用户的真实身份？\n授权（Authorization）：系统如何控制一个用户该看到哪些数据，操作哪些功能？\n凭证（Credential）：系统如何保证它与用户之间的承诺是双方当时真实意图的体现是准确、完整且不可抵赖的？\n保密（Confidentiality）：系统如何保证敏感数据无法被包括系统管理员在内的内外部人员所窃取、滥用？\n传输（Transport Security）：系统如何保证通过网络传输的信息无法被第三方窃听、篡改和冒充？\n验证（Verification）：系统如何确保提交到每项服务中的数据是合乎规则的，不会对系统稳定性、数据一致性、正确性产生风险？\n\n与安全相关的问题，一般不会直接创造价值，解决起来又烦琐复杂，费时费力，很容易被开发人员忽略，但庆幸的是这些问题基本上也都是与具体系统、具体业务无关的通用性问题，这意味着它们往往会存在业界通行的、已被验证过是行之有效的解决方案，甚至已经形成行业标准，不需要开发者自己从头去构思如何解决。\n此外，还有其他一些与安全相关的内容主要是由管理、运维、审计领域为主导，尽管也需要软件架构和开发人员的配合参与，但不列人本章的讨论范围之内，譬如安全审计、系统备份与恢复、信息系统安全法规与制度计算机防病毒制度、保护私有信息规则等。\n一、认证认证、授权和凭证可以说是一个系统中最基础的安全设计，哪怕再简陋的信息系统大概也不可能忽略 “用户登录” 功能。信息系统为用户提供服务之前，总是希望先弄清楚 “你是谁”（认证）、“你能干什么”（授权）以及 “你如何证明”（凭证）这三个基本问题。然而，这三个基本问题又不像部分开发者认为的那样，只是一个 “系统登录” 功能，仅仅是校验一下用户名、密码是否正确这么简单。账户和权限作为一种必须最大限度保障安全和隐私，同时又要兼顾各个系统模块甚至系统间共享访问的基础主数据，它的存储、管理与使用都面临一系列复杂的问题。对于某些大规模的信息系统，账户和权限的管理往往要由专门的基础设施来负责，譬如微软的活动目录（Active Directory，AD）或者轻量目录访问协议（Lightweight Directory Access Protocol，LDAP），跨系统的共享使用甚至会用到区块链技术。\n另外还有一个认知偏差：尽管 “认证” 是解决 “你是谁” 的问题，但这里的 “你” 并不一定是指人，也可能是指外部的代码，即第三方的类库或者服务。最初，对代码认证的重要程度甚至高于对最终用户的认证，譬如在早期的 Java 系统里，安全认证默认是特指 “代码级安全”，即你是否信任要在电脑中运行的代码。这是由 Java 当时的主要应用形式—— Java Applets 所决定的：类加载器从远端下载一段字节码，以 Applets 的形式在用户的浏览器中运行，由于Java操控计算机资源的能力要远远强于 JavaScript，因此必须先确保这些代码不会损害用户的计算机。这一阶段的安全观念催生了现在仍然存在于 Java 技术体系中的 “安全管理器”（java.lang.SecurityManager）、“代码权限许可”（java.lang.RuntimePermission）等概念。如今，对外部类库和服务的认证需求依然普遍，但相比起五花八门的最终用户认证来说，代码认证的研究方向已经很固定，基本上都统一到证书签名上。在本节中，认证的范围只限于对最终用户的认证，而代码认证会安排在后面9.2节中讲解。\n1. 认证的标准世纪之交，Java 迎来了 Web 的辉煌时代，互联网的迅速兴起促使 Java 进人快速发展时期。这时候，基于 HTML 和 JavaScript 的超文本Web 应用迅速超过了 “Java 2 时代” 之前的 Java Applets 应用，B/S 系统对最终用户认证的需求使得 “安全认证” 的重点逐渐从 “代码级安全” 转为 “用户级安全”，即你是否信任正在操作的用户。在1999年，随 J2EE 1.2 发布的 Servlet 2.2 中添加了一系列用于认证的 API，主要包括下列两部分内容：\n\n标准方面，添加了四种内置的、不可扩展的认证方案，即 Client-Cert、Basic、Digest 和 Form；\n实现方面，添加了一套与认证和授权相关的程序接口，譬如 HttpServletRequest::isUserInRole()、HttpServletRequest::getUserPrincipal()等方法。\n\n一项发布超过20年的老旧技术，原本并没有什么专门提起的必要性，笔者之所以引用这件事，是希望从它包含的两部分内容中引出一个架构安全性的经验原则：以标准规范为指导、以标准接口去实现。安全涉及的问题很麻烦，但解决方案已相当成熟，对于99%的系统来说、在安全上不去做轮子，不去想发明创造，严格遵循标准，就是最恰当的安全设计。\n引用 J2EE1.2 对安全的改进还有另一个原因，它内置的 Client-Cert、Basic、Digest 和 Form 这四种认证方案都很有代表性，刚好分别覆盖了通信信道、协议和内容层面的认证。而这三种层面的认证恰好涵盖了主流的三种认证方式，具体含义和应用场景列举如下。\n\n通信信道上的认证：你和我建立通信连接之前，要先证明你是谁。在网络传输（Network）场景中的典型应用是基于 SSL/TLS 传输安全层的认证。\n通信协议上的认证：你请求获取我的资源之前，要先证明你是谁。在互联网（Internet）场景中的典型应用是基于 HTTP 协议的认证。\n通信内容上的认证：你使用我提供的服务之前，要先证明你是谁。在万维网（World Wide Web）场景中的典型应用是基于Web内容的认证。\n\n关于通信信道上的认证，由于内容较多，又与后续介绍微服务安全方面的话题密切相关，所以将独立放到5.5节介绍、而且J2EE中的Client-Cert 其实并不是用于TLS的，以它引出 TLS 并不合适。下面重点了解基于通信协议和通信内容的两种认证方式。\n1）HTTP 认证前文已经提前用到了一个技术名词——认证方案（Authentication Scheme），它是指生成用户身份凭证的某种方法，这个概念最初源于  HTTP 协议的认证框架（Authentication Framework）。IETF 在 RFC 7235 中定义了 HTTP 协议的通用认证框架，要求所有支持 HTTP 协议的服务器，在未授权的用户意图访问服务端保护区域的资源时，应返回 401 Unauthorized 的状态码，同时应在响应报文头里附带以下两个分别代表网页认证和代理认证的 Header 之一，告知客户端应该采取何种方式产生能代表访问者身份的凭证信息：\n\nWWW-Authenticate：&lt;认证方案&gt; realm=&lt;保护区域的描述信息&gt;\nProxy-Authenticate：&lt;认证方案&gt; realm=&lt;保护区域的描述信息&gt;\n\n接收到该响应后，客户端必须遵循服务端指定的认证方案，在请求资源的报文头加入身份凭证信息，由服务端核实通过后才会允许该请求正常返回，否则将返回 403 Forbidden 错误。请求头报文应包含以下Header项之一：\n\nAuthorization： &lt;认证方案&gt; &lt;凭证内容&gt;Proxy-Authorization：&lt;认证方案&gt; &lt;凭证内容&gt;\n\nHTTP 认证框架提出认证方案是希望能把认证 “要产生身份凭证” 的目的与 “具体如何产生凭证” 的实现分离开来，无论客户端通过生物信息（指纹、人脸）、用户密码、数字证书抑或其他方式来生成凭证，都属于如何生成凭证的具体实现，都可以包含在HTTP协议预设的框架之内。HTTP认证框架的工作流程如下图所示。\n\n以上概念性的介绍可能会有些枯燥抽象，下面笔者将以最基础的认证方案 “HTTP Basic 认证” 为例来介绍认证是如何工作的。HTTP Basic 认证是一种主要以演示为目的的认证方案，也应用于一些不要求安全性的场合，譬如家里的路由器登录等。Basic 认证产生用户身份凭证的方法是让用户输入用户名和密码，经过 Base64 编码 “加密” 后作为身份凭证。譬如请求资源“GET/admin”后，浏览器会收到来自服务端的如下响应：\n\nHTTP/1.1 401 UnauthorizedDate:  Mon, 24 Feb 2020 16：50：53 GMTWWW-Authenticate: Basic realm=”example from icyfenix.cn”\n\n此时，浏览器必须询问最终用户，即弹出如图所示的 HTTP Basic 认证对话框，要求提供用户名和密码。\n![Http Basice认证对话框](../../../imgs/phoenix/Http Basice认证对话框.pic.png)\n用户在对话框中输入密码信息，譬如输入用户名 “icyfenix”，密码 “123456”，浏览器会将字符串 “icyfenix：123456” 编码为“aWN5ZmVuaXg6MTIzNDU2”，然后发送给服务端，HTTP 请求如下所示：\n\nGET /admin HTTP/1.1Authorization： Basic aWN5zmVuaXg6MTIzNDU2\n\n服务端接收到请求，解码后检查用户名和密码是否合法，如果合法就返回 “/admin“的资源，否则就返回 403 Forbidden 错误，禁止下一步操作。注意 Base64 只是一种编码方式，并非任何形式的加密，所以 Basic 认证的风险是显而易见的。除 Basic 认证外，IETF 还定义了很多种可用于实际生产环境的认证方案，列举如下。\n\nDigest：RFC 7616，HTTP摘要认证，可视为Basic认证的改良版本。针对Base64明文发送的风险，Digest认证把用户名和密码加盐（一个被称为 Nonce 的变化值作为盐值）后再通过MD5/SHA等哈希算法取摘要发送出去。但是这种认证方式依然是不安全的，无论客户端使用何种加密算法加密，无论是否采用了Nonce 这样的动态盐值去抵御重放和冒认，遇到中间人攻击时依然存在显著的安全风险。关于加解密的问题，将在后边详细讨论。\nBearer：RFC 6750，基于 OAuth2 规范来完成认证。OAuth 2 是一个同时涉及认证与授权的协议，具体将在后边节详细介绍。\nHOBA（HTTP Origin-Bound Authentication）：RFC 7486，一种基于自签名证书的认证方案。基于数字证书的信任关系主要有两类模型：一类是采用 CA（Certification Authority，认证机构）层次结构的模型，由CA中心签发证书；另一种是以 IETF的 Token Binding 协议为基础的 OBC（Origin Bound Certificate，原产地证书）自签名证书模型。后边将详细介绍数字证书。\n\nHTTP认证框架中的认证方案是允许自行扩展的，并不要求一定由RFC规范来定义，只要用户代理（UserAgent，通常是浏览器、泛指任何使用HTTP协议的程序）能够识别种私有的认证方案即可。因此，很多厂商也扩展了自己的认证方案。\n\nAWS4-HMAC-SHA256：亚马逊 AWS 基于 HMAC-SHA256 哈希算法的认证。\nNTLM / Negotiate：微软公司 NTLAN Manager（NTLM）用到的两种认证方式。\nWindows Live ID：微软公司开发并提供的 “统一登人” 认证。\nTwitter Basic：Twitter 改良的 HTTP 基础认证。\n\n2）Web 认证lETF 为 HTTP 认证框架设计了可插拔（Pluggable）的认证方案，原本是希望能涌现出各式各样的认证方案去支持不同的应用场景。尽管上节列举了一些还算常用的认证方案。但目前的信息系统，尤其是在系统对终端用户的认证场景中，直接采用 HTTP 认证框架的比例其实十分低。这不难理解，HTTP 是“超文本传输协议”，传输协议的根本职责是把资源从服务端传输到客户端，至于资源具体是什么内容，只能由客户端自行解析驱动。以 HTTP 协议为基础的认证框架也只能面向传输协议而不是具体传输内容来设计，如果用户想要从服务器中下载文件，弹出一个 HTTP 服务器的对话框，让用户登录是可接受的；但如果用户想访问信息系统中的具体服务，肯定希望身份认证是由系统本身的功能去完成，而不是由 HTTP 服务器来负责认证。这种依靠内容而不是传输协议来实现的认证方式，在万维网里被称为“Web认证”，由于实现形式上登录表单占了绝对的主流，因此通常也被称为“表单认证”（Form Authentication）。\n直至2019年以前，表单认证都没有什么行业标准可循，如表单是什么样，其中的用户字段、密码字段、验证码字段是否要在客户端加密，采用何种方式加密，接受表单的服务地址是什么，等等，都完全由服务端与客户端的开发者自行协商决定。“没有标准的约束”反倒成了表单认证的一大优点，它允许我们做出五花八门的页面，各种程序语言、框架或开发者本身都可以自行决定认证的全套交互细节。\n可能你还记得开篇说的 “遵循规范、别造轮子就是最恰当的安全”，这里又将表单认证的高自由度说成是一大优点，好话都让笔者给说全了。笔者提倡用标准规范去解决安全领域的共性问题，这条原则完全没有必要与界面是否美观合理、操作流程是否灵活便捷这些应用需求对立起来。譬如，想要支持密码或扫码等多种登录方式、想要支持图形验证码来驱逐爬虫与机器人、想要支持在登录表单提交之前进行必要的表单校验，等等，这些需求十分具体，不具备写入标准规范的通用性，却具备足够的合理性，应当在实现层面去满足。同时，如何控制权限保证不产生越权操作、如何传输信息保证内容不被窃听篡改、如何加密敏感内容保证即使泄漏也不被逆推出明文，等等，这些问题已有通行的解决方案，并明确定义在规范之中，也应当在架构层面去遵循。\n表单认证与 HTTP 认证不一定是完全对立的，两者有不同的关注点，可以结合使用。以 Fenix’s Bookstore 的登录功能为例，页面表单是一个自行设计的 Vue.js 页面。但认证的整个交互过程遵循 OAuth2 规范的密码模式。\n2019年3月，万维网联盟（World Wide Web Consortium.w3C）批准了由 FIDO（Fast IDentity Online，一个安全、开放、防钓鱼、无密码认证标准的联盟）领导起草的世界首份 Web 内容认证的标准 “WebAuthn”，这里也许有读者会感到矛盾与奇怪，不是刚说了 Web 表单是什么样、要不要验证码、登录表单是否在客户端校验等是十分具体的需求，不太可能定义在规范上吗？确实如此，所以 WebAuthn 彻底抛弃了传统的密码登录方式，改为直接采用生物识别（指纹、人脸、虹膜、声纹）或者实体密钥（以USB、蓝牙、NFC连接的物理密钥容器）来作为身份凭证，从根本上消灭了用户输入错误产生的校验需求和防止机器人模拟产生的验证码需求等问题，甚至可以省掉表单界面。\n由于 WebAuthn 相对复杂，在阅读下面内容之前，如果你的设备和环境允许，建议务在 GitHub 网站的 2FA 认证功能中实际体验一下如何通过 WebAuthn 完成两段式登录，再继续阅读后面的内容。硬件方面，要求用带有 TouchID 的 MacBook，或者其他支持指纹 FacelD 验证的手机(目前在售的移动设备基本都带有生物识别装置)。软件方面，直至 ios13.6.iPhone 和 iPad 仍未支持 WebAuthn，但 Android 和 Mac OS系统中的 Chrome，以及 Windows 的 Edge 浏览器都已经支持使用 WebAuthn了。\nWebAuthn 规范涵盖了“注册”与“认证”两大流程。先来介绍注册流程，它大致可以分为以下步骤。\n\n用户进入系统的注册页面，这个页面的格式、内容和用户注册时需要填写的信息均不包含在 WebAuthn 标准的定义范围内。\n当用户填写完信息、点击提交注册信息的按钮后，服务端先暂存用户提交的数据，生成一个随机字符串（在规范中称作 Challenge）和用户的 UserID （在规范中称作凭证ID），并返回客户端。\n客户端的 WebAuthnAPI 接收到 Challenge 和 UserID 后，把这些信息发送给验证器（Authenticator）。验证器可理解为用户设备上 TouchID、FaceID、实体密钥等认证设备的统一接口。\n验证器提示用户进行验证。如果支持多种认证设备，还会提示用户选择一个想要使用的设备。验证的结果是生成一个密钥对（公钥和私钥），由验证器存储私钥、用户信息以及当前的域名。然后使用私钥对 Challenge 进行签名，并将签名结果、UserID 和公钥一起返回客户端。\n浏览器将验证器返回的结果转发给服务器。\n服务器核验信息，检查 UserID 与之前发送的是否一致，并用公钥解密后得到的结果与之前发送的 Challenge 作对比，一致即表明注册通过，由服务端存储该UserlD 对应的公钥。\n\n\n登录流程与注册流程类似，大致可以分为以下步骤。\n\n用户访问登录页面，填入用户名后即可点击登录按钮。\n服务器返回随机字符串 Challenge、用户 UserID。\n浏览器将 Challenge 和 UserID 转发给验证器。\n验证器提示用户进行认证操作。由于在注册阶段验证器已经存储了该域名的私钥和户信息。所以如果域名和用户都相同的话，就不需要生成密钥对了，直接以存储的私钥加密 Challenge，然后返回浏览器。\n服务端接收到浏览器转发来的被私钥加密的 Challenge，并以此前注册时存储的公钥进行解密，如果解密成功则宣告登录成功。\n\nWebAuthn 采用非对称加密的公钥、私钥替代传统的密码，这是非常理想的认证方案。钥是保密的，只有验证器需要知道它，连用户本人都不需要知道，也就没有人为泄漏的可能。公钥是公开的，可以被任何人看到或存储。公钥可用于验证私钥生成的签名，但不能用来签名，除了得知私钥外，没有其他途径能够生成可被公钥验证为有效的签名，这样服务器就可以通过公钥是否能够解密来判断最终用户的身份是否合法。\nWebAuthn 还解决了传统密码在网络传输上的风险问题，后续5.4节我们会讲到无论密码是否在客户端进行加密以及如何加密，对防御中间人攻击来说都是没有意义的。更值得夸赞的是，WebAuthn为登录过程带来极大的便捷性，不仅注册和验证的用户体验十分优秀而且彻底避免了用户在一个网站上泄漏密码，所有使用相同密码的网站都受到攻击的问题这个优点使得用户无须再为每个网站想不同的密码。当前的 WebAuthn 还很年轻，普及率暂时还很有限，但笔者相信几年之内它必定会发展成Web认证的主流方式，被大多数网站和系统所支持。\n2. 认证的实现了解了业界标准的认证规范以后，本节将简要介绍一下在 Java 技术体系内通常是如何实现安全认证的。Java 其实也有自己的认证规范，第一个系统性的 Java 认证规范发布于 Java 1.3时代，是由 Sun 公司提出的同时面向代码级安全和用户级安全的认证授权服务—— JAAS（Java Authentication and Authorization Service，Java 认证和授权服务，Java 1.3 时处于扩展包中，Java 1.4 时被纳人标准包）。尽管JAAS 已经考虑到最终用户的认证，但代码级安全在规范中仍然占更主要的地位。可能今天用过甚至听过 JAAS 的 Java 程序员已经不多了，但是这个规范提出了很多在今天仍然活跃于主流 Java 安全框架中的概念，譬如一般把用户存放在 “Principal” 之中、密码存放在 “Credentials” 之中、登录后从安全上下文 “Context” 中获取状态等常见的安全概念，都可以追溯到这一时期所定下的 API：\n\nLoginModule（javax.security.auth.spi.LoginModule）；\nLoginContext（javax.security.auth.login.LoginContext）；\nSubject（javax.security.auth.Subject）；\nPrincipal（java.securityPrincipal）；\nCredentials（javax.security.auth.Destroyable、javax.security.auth.Refreshable）。\n\n虽然 JAAS 开创了这些沿用至今的安全概念，但规范本身实质上并没有得到广泛应用笔者认为有两大原因。一方面是由于 JAAS 同时面向代码级和用户级的安全机制，使得它过度复杂化，难以推广。在这个问题上 Java 社区一直有做持续的增强和补救，譬如 Java EE6 中的JASPIC、Java EE8 中的 EE Security。\n\nJSR 115: Java Authorization Contract for Containers (JACC)。\nJSR 196: Java Authentication Service Provider Interface for Containers (JASPIC)。\nJSR 375: Java EE Security API (EE Security)。\n\n而另一方面。可能是更重要的一个原因，在21世纪的第一个十年里，以 “With EJB“ 为口号，以 WebSphere、JBoss 等为代表的 J2EE 容器环境，与以 “Without EJB” 为口号以 Spring、Hibernate 等为代表的轻量化开发框架产生了激烈的竞争，结果是后者获得了全面胜利。这个结果使得依赖容器安全的 JAAS 无法得到大多数人的认可。\n在今时今日，实际活跃于 Java 安全领域的是两个私有的（私有的意思是不由 JSR 所规范的，即没有 java/javax.* 作为包名）的安全框架：Apache Shiro 和 Spring Security。\n相较而言，Apache Shiro 更便捷易用，而 Spring Security 的功能则更复杂强大一些。无论是单体架构还是微服务架构的 Fenix’s Bookstore，笔者都选择了 Spring Security 作为安全框架，这个选择与功能、性能之类的考量没什么关系，就只是因为 SpringBoot、Spring Cloud 全家桶的缘故。这里不打算罗列代码来介绍 Apache Shiro 与 Spring Security 的具体使用方法。只从目标上看，两个安全框架提供的功能都很类似，大致包括以下四类。\n\n认证功能：以HTTP协议中定义的各种认证、表单等认证方式确认用户身份，这是本节的主要话题。\n安全上下文：用户获得认证之后，要开放一些接口，让应用可以得知该用户的基本资料，拥有的权限、角色，等等。\n授权功能：判断并控制认证后的用户对什么资源拥有哪些操作许可，这部分内容会放到下一节介绍。\n密码的存储与验证：密码是 “烫手的山芋”，无论是存储、传输还是验证都应谨慎处理，这部分内容会放到第四节具体讨论。\n\n二、授权授权这个概念通常伴随着认证、审计、账号一同出现，并称为 AAAA（Authentication Authorization、Audit、Account，也有一些领域把 Account 解释为计费的意思）。授权行为在程序中的应用非常广泛，给某个类或某个方法设置范围控制符（public、protected、private、&lt;Package&gt;）在本质上也是一种授权（访问控制）行为。而在安全领域中所说的权就更具体一些。通常涉及以下两个相对独立的问题。\n\n确保授权的过程可靠：对于单一系统来说，授权的过程是比较容易控制的，以前多语境上提到授权。实质上讲的都是访问控制，但理论上两者是应该分开的。在涉及多方的系统中，授权过程则是一个比较困难却必须严肃对待的问题：如何既能让第三方系统访问到所需的资源，又能保证其不泄露用户的敏感数据呢？常用的多授权协议主要有 OAuth2 和 SAML2.0。\n确保授权的结果可控：授权的结果用于对程序功能或者资源的访问控制（Access Control），成理论体系的权限控制模型有很多，譬如自主访问控制（Discretionary Access Control，DAC）、强制访问控制（Mandatory Access Control，MAC）、基于属性的访问控制（Attribute-Based Access Control，ABAC），还有最为常用的基于角色的访问控制（Role-Based Access Control，RBAC）。\n\n由于篇幅原因，在这一节我们只介绍 Fenix’s Bookstore 的代码中直接使用到的，也是日常开发中最常用到的 RBAC 和 OAuth2 这两种访问控制和授权方案。\n1. RBAC所有的访问控制模型，实质上都是在解决同一个问题：“谁（User）拥有什么权限（Authority）去操作（Operation）哪些资源（Resource）。”\n这个问题初看起来并不难，一种直观的解决方案就是在用户对象上设定一些权限，当用户使用资源时，检查是否有对应的操作权限即可。很多著名的安全框架，譬如 Spring Security 的访问控制本质上就是这么做的。不过，这种把权限直接关联在用户身上的简单设计，在复杂系统上确实会导致一些比较烦琐的问题。试想一下，如果某个系统涉及成百上千的资源，又有成千上万的用户，若要为每个用户访问每个资源都分配合适的权限，必定导致巨大的操作量和极高的出错概率，这也正是 RBAC 所关注的问题之一。\nRBAC 模型在业界中有多种说法，其中以美国 George Mason 大学信息安全技术实验室提出的 RBAC96 模型最具系统性、得到了普遍的认可。为了避免对每一个用户设定权限，RBAC 将权限从用户身上剥离。改为绑定到 “角色”（Role）上，将权限控制变为对 “角拥有操作哪些资源的许可” 这个逻辑表达式的值是否为真的求解过程。RBAC 的主要元素的关系图如下。\n\n上图中出现了一个新的名词 “许可”。许可是抽象权限的具象化体现，权限在 RBAC 系统中的含义是 “允许何种操作作用于哪些资源之上” ，这句话的具体实例即为 “许可”。提出许可这个概念的目的其实与提出角色的目的是完全一致的，只是更为抽象。角色为的是解耦用户与权限之间的多对多关系，而许可为的是解耦操作与资源之间的多对多关系譬如不同的数据都能够有增、删、改等操作，如果将数据与操作搅和在一起也会面临配置膨胀问题。这里举个更具体的例子帮助你理清众多名词之间的关系。譬如某个论文管理系统的 UserStory 中，与访问控制相关的 Backlog 可能会是这样描述的：\n\nBacklog周同学（User）是某SCI杂志的审稿人（Role），职责之一是在系统中审核论文（Authority）。在审稿过程（Session）中，当他认为某篇论文（Resource）达到了可以公开发表的标准时，就会在后台点击通过按钮（Operation）来完成审核\n\n以上 Backlog 中 “给论文点击通过按钮” 就是一种许可，它是 “审核论文” 这项权限的具象化体现。\n采用 RBAC 不仅是为了简化配置操作，还天然地满足了计算机安全中的 “最小特权原则” （Least Privilege）。在 RBAC 模型中，角色拥有许可的数量是根据完成该角色工作职责所需的最小权限来赋予的，最典型的例子是操作系统权限管理中的用户组，根据对不同角色的职责分工，如管理员（Administrator）、系统用户（System）、验证用户（Authenticated User）、普通用户（User）、来宾用户（Guest）等分配各自的权限，既保证用户能够正常工作。也避免用户出现越权操作的风险。当用户的职责发生变化时，在系统中就体现为它所隶属的角色被改变，譬如将 “普通用户角色” 改变 “管理员角色”，从而迅速让该用户具备管理员的多个细分权限，降低权限分配错误的风险。\nRBAC 还允许对不同角色之间定义关联与约束关系，进一步强化它的抽象描述能力。如不同的角色之间可以有继承性，典型的是 RBAC-1 模型的角色权限继承关系。譬如描述开发经理应该和开发人员一样具有代码提交的权限，描述开发人员都应该和任何公司员工一样具有食堂就餐的权限，就可以直接将食堂就餐赋予到公司员工的角色上，把代码提交赋予到开发人员的角色上，再让开发人员的角色从公司员工派生，开发经理的角色从开发人员中派生。\n不同角色之间也可以具有互斥性，典型的是 RBAC-2 模型的角色职责分离关系。互斥性要求权限被赋予角色时，或角色被赋予用户时应遵循的强制性职责分离规定。举个例子。角色的互斥约束可限制同一用户只能分配到一组互斥角色集合中至多一个角色，譬如不能让同一名员工既当会计，也当出纳，否则资金安全无法保证。角色的基数约束可限制某个用户拥有的最大角色数目，譬如不能让同一名员工包揽产品、设计、开发、测试角色，否则产品质量无法保证。\n建立访问控制模型的基本目的是管理垂直权限和水平权限。垂直权限即功能权限，譬如前面提到的审稿编辑有通过审核的权限、开发经理有代码提交的权限、出纳有从账户报取资金的权限，这一类某个角色完成某项操作的许可，都可以直接翻译为功能权限。由于实际应用与权限模型具有高度对应关系，将权限从具体的应用中抽离出来，放到通用的模型中是相对容易的 Spring Security、Apache Shiro 等权限框架就是这样的抽象产物，大多数系统都能采用这些权限框架来管理功能权限。\n与此相对，水平权限即数据权限，但管理起来要困难许多。譬如用户 A、B 都属于同一个角色。但它们各自在系统中产生的数据完全有可能是私有的，A 访问或删除了 B 的数据也照样属于越权。一般来说、数据权限是很难抽象与通用的，仅在角色层面控制并不能满足全部业务的需要，很多时候只能具体到用户，甚至要具体管理到发生数据的某一行、一列之上，因此数据权限基本只能由信息系统自主完成，并不存在能放之四海皆准的通用数据权限框架。\n本书后面章节中的 “重要角色” —— Kubernetes 完全遵循了 RBAC 来进行服务访问控制，Fenix’s Bookstore 所使用的 Spring Security 也参考了（但并没有完全遵循）RBAC 来设计它的访问控制功能。在 Spring Security 的设计里，用户和角色都可以拥有权限，譬如在它的 HttpSecurity 接口就同时有 hasRole() 和 hasAuthority() 方法。Spring Security 的访问控制模型如图5-6所示，可与前面 RBAC 的关系图对比一下。\n\n从实现角度来看，Spring Security 中的角色和权限的差异很小，它们完全共享同一套存储结构。唯一的差别仅是角色会在存储时自动带上“ROLE” 前缀罢了。但从使用角度来看，角色和权限的差异可以很大，用户可以自行决定系统中许可是只能对应到角色身上，还是可以让用户也拥有某些角色中没有的权限。这一点不符合 RBAC 的思想，但笔者个人认同这是一种创新而非破坏，在 Spring Security 的文档上说得很清楚：这取决于你自己何使用。\n\n 角色和权限的核心差异取决于用户打算如何使用这些特性，在框架层面它们的差别是极小的，基本采用了完全相同的方式来进行处理\n\n通过 RBAC 很容易控制最终用户在广义和精细级别上能够做什么，可以指定用户是管理员、专家用户抑或普通用户，并使角色和访问权限与组织中员工的身份职位保持一致，仅根据需要为员工完成工作的最低限度来分配权限。这些都是大量软件系统、长时间积累下来的经验，将这些经验运用在软件产品上，绝大多数情况下要比自己发明、创造一个新的轮子更加安全。\n2. OAuth2了解过 RBAC 的内容后，下面我们再来看看相对更复杂烦琐的 OAuth2 认证授权协议（更烦琐的 OAuth1 已经完全被废弃了）。OAuth2 是在RFC 6749中定义的国际标准，在 RFC 6749 正文的第一句就阐明了 OAuth2 是面向于解决第三方应用（Third-Party Application）的认证授权协议。如果你的系统并不涉及第三方，譬如我们单体架构的 Fenix’s Bookstore 中就既不为第三方提供服务，也不使用第三方的服务，那引入 OAuth2 其实并无必要。为什么强调第三方？在多方系统授权过程具体会有什么问题需要专门制订一个标准协议来解决呢？笔者举个现实的例子来解释。\n譬如你现在正在阅读的这个网站（https://icyfenix.cn），它的建设和更新大致流程是：笔者以 Markdown 形式写好了某篇文章，上传到由GitHub 提供的代码仓库，接着由Travis-CI提供的持续集成服务会检测到该仓库发生了变化，触发一次 Vuepress 编译活动，生成目录和静态的 HTML 页面，然后推送回GitHub Pages，再触发国内的 CDN 缓存刷新。这个过程要能顺利进行，就存在一系列必须解决的授权问题，Travis-CI 只有得到了我的明确授权，GitHub 才能同意它读取我代码仓库中的内容，问题是它该如何获得我的授权呢？一种最简单粗暴的方案是把我的用户账号和密码都告诉 Travis-CI，但这显然导致了以下这些问题：\n\n密码泄漏：如果 Travis-CI 被黑客攻破，将导致我的 GitHub 的密码也同时被泄漏。\n访问范围：Travis-CI 将有能力读取、修改、删除、更新我放在 GitHub 上的所有代码仓库，而我并不希望它能够修改删除文件。\n授权回收：只有修改密码才能回收我授予给 Travis-CI 的权力，可是我在 GitHub 的密码只有一个，授权的应用除了 Travis-CI 之外却还有许多，修改了意味着所有别的第三方的应用程序会全部失效。\n\n以上列举的这些问题，也正是 OAuth2 所要解决的问题，尤其是要求第三方系统没有支持 HTTPS 传输安全的环境下依然能够解决这些问题，这并非易事。\nOAuth2 给出了多种解决办法，这些办法的共同特征是以令牌（Token）代替用户密码作为授权的凭证。有了令牌之后，哪怕令牌被泄漏，也不会导致密码的泄漏；令牌上可以设定访问资源的范围以及时效性；每个应用都持有独立的令牌，哪个失效都不会波及其他。这样上面提出的三个问题就都解决了。有了一层令牌之后，整个授权的流程如下图所示。\n\n这个时序图里面涉及到了 OAuth2 中几个关键术语，我们通过前面那个具体的上下文语境来解释其含义，这对理解后续几种认证流程十分重要：\n\n第三方应用（Third-Party Application）：需要得到授权访问我资源的那个应用，即此场景中的“Travis-CI”。\n授权服务器（Authorization Server）：能够根据我的意愿提供授权（授权之前肯定已经进行了必要的认证过程，但它与授权可以没有直接关系）的服务器，即此场景中的“GitHub”。\n资源服务器（Resource Server）：能够提供第三方应用所需资源的服务器，它与认证服务可以是相同的服务器，也可以是不同的服务器，此场景中的“我的代码仓库”。\n资源所有者（Resource Owner）： 拥有授权权限的人，即此场景中的“我”。\n操作代理（User Agent）：指用户用来访问服务器的工具，对于人类用户来说，这个通常是指浏览器，但在微服务中一个服务经常会作为另一个服务的用户，此时指的可能就是 HttpClient、RPCClient 或者其他访问途径。\n\n“用令牌代替密码”确实是解决问题的好方法，但这充其量只能算个思路，距离可实施的步骤还是不够具体的，时序图中的“要求/同意授权”、“要求/同意发放令牌”、“要求/同意开放资源”几个服务请求、响应该如何设计，这就是执行步骤的关键了。对此，OAuth2 一共提出了四种不同的授权方式（这也是 OAuth2 复杂烦琐的主要原因），分别为：\n\n授权码模式（Authorization Code）\n隐式授权模式（Implicit）\n密码模式（Resource Owner Password Credentials）\n客户端模式（Client Credentials）\n\n1）授权码模式授权码模式是四种模式中最严（luō）谨（suō）的，它考虑到了几乎所有敏感信息泄漏的预防和后果。具体步骤的时序如下图所示。\n\n开始进行授权过程以前，第三方应用先要到授权服务器上进行注册，所谓注册，是指向认证服务器提供一个域名地址，然后从授权服务器中获取 ClientID 和 ClientSecret，以便能够顺利完成如下授权过程：\n\n第三方应用将资源所有者（用户）导向授权服务器的授权页面，并向授权服务器提供 ClientID 及用户同意授权后的回调 URI，这是一次客户端页面转向。\n授权服务器根据 ClientID 确认第三方应用的身份，用户在授权服务器中决定是否同意向该身份的应用进行授权，用户认证的过程未定义在此步骤中，在此之前应该已经完成。\n如果用户同意授权，授权服务器将转向第三方应用在第 1 步调用中提供的回调 URI，并附带上一个授权码和获取令牌的地址作为参数，这是第二次客户端页面转向。\n第三方应用通过回调地址收到授权码，然后将授权码与自己的 ClientSecret 一起作为参数，通过服务器向授权服务器提供的获取令牌的服务地址发起请求，换取令牌。该服务器的地址应与注册时提供的域名处于同一个域中。\n授权服务器核对授权码和 ClientSecret，确认无误后，向第三方应用授予令牌。令牌可以是一个或者两个，其中必定要有的是访问令牌（Access Token），可选的是刷新令牌（Refresh Token）。访问令牌用于到资源服务器获取资源，有效期较短，刷新令牌用于在访问令牌失效后重新获取，有效期较长。\n资源服务器根据访问令牌所允许的权限，向第三方应用提供资源。\n\n这个过程设计，已经考虑到了几乎所有合理的意外情况，笔者再举几个最容易遇到的意外状况，以便你能够更好地理解为何要这样设计 OAuth2。\n\n会不会有其他应用冒充第三方应用骗取授权？ClientID 代表一个第三方应用的“用户名”，这项信息是可以完全公开的。但 ClientSecret 应当只有应用自己才知道，这个代表了第三方应用的“密码”。在第 5 步发放令牌时，调用者必须能够提供 ClientSecret 才能成功完成。只要第三方应用妥善保管好 ClientSecret，就没有人能够冒充它。\n为什么要先发放授权码，再用授权码换令牌？这是因为客户端转向（通常就是一次 HTTP 302 重定向）对于用户是可见的，换而言之，授权码可能会暴露给用户以及用户机器上的其他程序，但由于用户并没有 ClientSecret，光有授权码也是无法换取到令牌的，所以避免了令牌在传输转向过程中被泄漏的风险。\n为什么要设计一个时限较长的刷新令牌和时限较短的访问令牌？不能直接把访问令牌的时间调长吗？这是为了缓解 OAuth2 在实际应用中的一个主要缺陷，通常访问令牌一旦发放，除非超过了令牌中的有效期，否则很难（需要付出较大代价）有其他方式让它失效，所以访问令牌的时效性一般设计的比较短，譬如几个小时，如果还需要继续用，那就定期用刷新令牌去更新，授权服务器就可以在更新过程中决定是否还要继续给予授权。至于为什么说很难让它失效，我们将放到下一节“凭证”中去解释。\n\n尽管授权码模式是严谨的，但是它并不够好用，这不仅仅体现在它那繁复的调用过程上，还体现在它对第三方应用提出了一个“貌似不难”的要求：第三方应用必须有应用服务器，因为第 4 步要发起服务端转向，而且要求服务端的地址必须与注册时提供的地址在同一个域内。不要觉得要求一个系统要有应用服务器是天经地义理所当然的事情，你现在阅读文章的这个网站就没有任何应用服务器的支持，里面使用到了 Gitalk 作为每篇文章的留言板，它对 GitHub 来说照样是第三方应用，需要 OAuth2 授权来解决。除基于浏览器的应用外，现在越来越普遍的是移动或桌面端的客户端 Web 应用（Client-Side Web Applications），譬如现在大量的基于 Cordova、Electron、Node-Webkit.js 的PWA 应用，它们都没有应用服务器的支持。由于有这样的实际需求，因此引出了 OAuth2 的第二种授权模式：隐式授权。\n2）隐式授权模式隐式授权省略掉了通过授权码换取令牌的步骤，整个授权过程都不需要服务端支持，一步到位。代价是在隐式授权中，授权服务器不会再去验证第三方应用的身份，因为已经没有应用服务器了，ClientSecret 没有人保管，就没有存在的意义了。但其实还是会限制第三方应用的回调 URI 地址必须与注册时提供的域名一致，尽管有可能被 DNS 污染之类的攻击所攻破，但仍算是尽可能努力一下。同样的原因，也不能避免令牌暴露给资源所有者，不能避免用户机器上可能意图不轨的其他程序、HTTP 的中间人攻击等风险了。\n隐式授权的调用时序如下图（从此之后的授权模式，时序中笔者就不再画出资源访问部分的内容了，就是前面 opt 框中的那一部分，以便更聚焦重点）所示。\n\n在时序图所示的交互过程里，隐式模式与授权码模式的显著区别是授权服务器在得到用户授权后，直接返回了访问令牌，这显著地降低了安全性，但 OAuth2 仍然努力尽可能地做到相对安全，譬如在前面提到的隐式授权中，尽管不需要用到服务端，但仍然需要在注册时提供回调域名，此时会要求该域名与接受令牌的服务处于同一个域内。此外，同样基于安全考虑，在隐式模式中明确禁止发放刷新令牌。\n还有一点，在 RFC 6749 对隐式授权的描述中，特别强调了令牌必须是“通过 Fragment 带回”的。部分对超文本协议没有了解的读者，可能还根本不知道Fragment是个什么东西？\n\n 额外知识：Fragment\nIn computer hypertext, a fragment identifier is a string of characters that refers to a resource that is subordinate to another, primary resource. The primary resource is identified by a Uniform Resource Identifier (URI), and the fragment identifier points to the subordinate resource.\n​                                                                                                                                                                        ——URI Fragment，Wikipedia\n\n看了这段英文定义还是觉得概念不好的话，我简单告诉你，Fragment 就是地址中#号后面的部分，譬如这个地址：\nhttp://bookstore.icyfenix.cn/#/detail/1\n\n后面的/detail/1便是 Fragment，这个语法是在RFC 3986中定义的，RFC 3986 中解释了 Fragment 是用于客户端定位的 URI 从属资源，譬如 HTML 中就可以使用 Fragment 来做文档内的跳转而不会发起服务端请求，你现在可以点击一下这篇文章左边菜单中的几个子标题，看看浏览器地址栏的变化。此外，RFC 3986 还规定了如果浏览器对一个带有 Fragment 的地址发出 Ajax 请求，那 Fragment 是不会跟随请求被发送到服务端的，只能在客户端通过 Script 脚本来读取。所以隐式授权巧妙地利用这个特性，尽最大努力地避免了令牌从操作代理到第三方服务之间的链路存在被攻击而泄漏出去的可能性。至于认证服务器到操作代理之间的这一段链路的安全，则只能通过 TLS（即 HTTPS）来保证中间不会受到攻击了，我们可以要求认证服务器必须都是启用 HTTPS 的，但无法要求第三方应用同样都支持 HTTPS。\n3）密码模式前面所说的授权码模式和隐私模式属于纯粹的授权模式，它们与认证没有直接的联系，如何认证用户的真实身份是与进行授权互相独立的过程。但在密码模式里，认证和授权就被整合成了同一个过程了。\n密码模式原本的设计意图是仅限于用户对第三方应用是高度可信任的场景中使用，因为用户需要把密码明文提供给第三方应用，第三方以此向授权服务器获取令牌。这种高度可信的第三方是极为较罕见的，尽管介绍 OAuth2 的材料中，经常举的例子是“操作系统作为第三方应用向授权服务器申请资源”，但真实应用中极少遇到这样的情况，合理性依然十分有限。\n笔者认为，如果要采用密码模式，那“第三方”属性就必须弱化，把“第三方”视作是系统中与授权服务器相对独立的子模块，在物理上独立于授权服务器部署，但是在逻辑上与授权服务器仍同属一个系统，这样将认证和授权一并完成的密码模式才会有合理的应用场景。\n譬如 Fenix’s Bookstore 便直接采用了密码模式，将认证和授权统一到一个过程中完成，尽管 Fenix’s Bookstore 中的 Frontend 工程和 Account 工程都能直接接触到用户名和密码，但它们事实上都是整个系统的一部分，这个前提下密码模式才具有可用性。关于分布式系统各个服务之间的信任关系，后续会在“零信任网络”与“服务安全”中作进一步讨论。\n理解了密码模式的用途，它的调用过程就很简单了，就是第三方应用拿着用户名和密码向授权服务器换令牌而已。具体时序如下图所示。\n\n密码模式下“如何保障安全”的职责无法由 OAuth2 来承担，只能由用户和第三方应用来自行保障，尽管 OAuth2 在规范中强调到“此模式下，第三方应用不得保存用户的密码”，但这并没有任何技术上的约束力。\n4）客户端模式客户端模式是四种模式中最简单的，它只涉及到两个主体，第三方应用和授权服务器。如果严谨一点，现在称“第三方应用”其实已经不合适了，因为已经没有了“第二方”的存在，资源所有者、操作代理在客户端模式中都是不必出现的。甚至严格来说叫“授权”都已不太恰当，资源所有者都没有了，也就不会有谁授予谁权限的过程。\n客户端模式是指第三方应用（行文一致考虑，还是继续沿用这个称呼）以自己的名义，向授权服务器申请资源许可。此模式通常用于管理操作或者自动处理类型的场景中。举个具体例子，譬如笔者开了一家叫 Fenix’s Bookstore 的书店，因为小本经营，不像京东那样全国多个仓库可以调货，因此必须保证只要客户成功购买，书店就必须有货可发，不允许超卖。但经常有顾客下了订单又拖着不付款，导致部分货物处于冻结状态。所以 Fenix’s Bookstore 中有一个订单清理的定时服务，自动清理超过两分钟还未付款的订单。在这个场景里，订单肯定是属于下单用户自己的资源，如果把订单清理服务看作一个独立的第三方应用的话，它是不可能向下单用户去申请授权来删掉订单的，而应该直接以自己的名义向授权服务器申请一个能清理所有用户订单的授权。客户端模式的时序如下图所示。\n\n微服务架构并不提倡同一个系统的各服务间有默认的信任关系，所以服务之间调用也需要先进行认证授权，然后才能通信。此时，客户端模式便是一种常用的服务间认证授权的解决方案。Spring Cloud 版本的 Fenix’s Bookstore是采用这种方案来保证微服务之间的合法调用的，Istio 版本的 Fenix’s Bookstore则启用了双向 mTLS 通信，使用客户端证书来保障安全，它们可作为上一节介绍认证时提到的“通信信道认证”和“通信内容认证”例子，感兴趣的读者可以对比一下这两种方式的差异优劣。\nOAuth2 中还有一种与客户端模式类似的授权模式，在RFC 8628中定义为“设备码模式”（Device Code），这里顺带提一下。设备码模式用于在无输入的情况下区分设备是否被许可使用，典型的应用便是手机锁网解锁（锁网在国内较少，但在国外很常见）或者设备激活（譬如某游戏机注册到某个游戏平台）的过程。它的时序如下图所示。\n\n进行验证时，设备需要从授权服务器获取一个 URI 地址和一个用户码，然后需要用户手动或设备自动地到验证 URI 中输入用户码。在这个过程中，设备会一直循环，尝试去获取令牌，直到拿到令牌或者用户码过期为止。\n三、凭证在前面介绍 OAuth2 的内容中，每一种授权模式的最终目标都是拿到访间令牌，但未涉及过拿回来的令牌应该长什么样子。反而还挖了一些坑没有填（为何说 OAuth2 的一个主要缺陷是令牌难以主动失效）。这节讨论的主角是令牌，同时，还会讨论如果不使用 OAuth2，如何以最传统的方式完成认证、授权。\n对 “如何承载认证授权信息” 这个问题的不同看法，代表了软件架构对待共享状态信息的两种不同思路：状态应该维护在服务端，还是在客户端之中？在分布式系统崛起以前，这个问题原本已有了较为统一的结论，即以 HTTP 协议的 Cookie-Session 机制为代表的服务端状态存储在分布式崛起前的三十年中都是主流的解决方案。不过，到了最近十年，由于分布式系统中共享数据必然会受到 CAP 不兼容原理的打击限制，迫使人们重新去审视之前已基本放弃掉的客户端状态存储、这就让原本只在多方系统中采用的 JWT 令牌方案，在分布式系统中也有了另一块用武之地。本节将围绕 Cookie-Session 和 JWT 之间的相同与不同而展开。\n1. Cookie-Session大家知道HTTP协议是一种无状态的传输协议，无状态是指协议对事务处理没有上下文的记忆能力，每一个请求都是完全独立的，但是我们中肯定有许多人并没有意识到 HTTP 协议无状态的重要性。假如你做了一个简单的网页，其中包含1 个 HTML、2 个Script脚本、3个CSS，还有 10 张图片，若要这个网页成功展示在用户屏幕前，需要完成 16 次与服务端的交互来获取上述资源，由于网络传输等各种因素的影响，服务器发送的顺序与客户端请求的先后并没有必然的联系，按照可能出现的响应顺序，理论上最多会有 P(16，16)＝20922789888000 种可能性。试想一下，如果 HTTP 协议不是设计成无状态的，这 16 次请求每一次都有依赖关联，先调用哪一个、先返回哪一个，都会对结果产生影响的话，那协调工作会多么复杂。\n可是，HTTP 协议的无状态特性又有悖于我们最常见的网络应用场景，典型就是认证授权，系统总得要获知用户身份才能提供合适的服务，因此，我们也希望 HTTP 能有一种手段，让服务器至少能够区分出发送请求的用户是谁。为了实现这个目的，RFC6265规范定义了HTTP的状态管理机制，在 HTTP 协议中增加了 Set-Cookie 指令，该指令的含义是以键值对的方式向客户端发送一组信息，此信息将在此后一段时间内的每次 HTTP 请求中，以名为 Cookie 的 Header 附带着重新发给服务端，以便服务端区分来自不同客户端的请求。一个典型的 Set-Cookie 指令如下所示：\n\nSet-Cookie:id＝icyfenix；Expires＝Wed，21 Feb 2020 07:28:00 GMT；Secure；Httponly\n\n收到该指令以后，客户端再对同一个域的请求回传时就会自动附带键值对信息 “id＝icyfenix”，譬如以下代码所示：\n\nGET /index.html HTTP/2.0Host: icyfenix.cnCookie: id＝icyfenix\n\n根据每次请求传到服务端的 Cookie，服务端就能分辨出请求来自于哪一个用户。由于 Cookie 是放在请求头上的，属于额外的传输负担，不应该携带过多的内容，而且放在 Cookie 中传输并不安全，容易被中间人窃取或被篡改，所以通常不会设置例子中 “id＝icyfenix” 这样的明文信息。一般来说，系统会把状态信息保存在服务端，在 Cookie 里只传输一个无字面意义的、不重复的字符串，习惯上以 sessionid 或者 jsessionid 为名，然后服务端会把这个字符串作为Key，以 Key/Entity 的结构存储每一个在线用户的上下文状态，再辅以一些超时自动清理之类的管理措施。这种服务端的状态管理机制就是今天大家非常熟悉的 Session，Cookie-Session 也即最传统但今天依然广泛应用于大量系统中的，由服务端与客户端联动来完成的状态管理机制。\nCookie-Session 方案在本章的主题 “安全性” 上其实是有一定先天优势的：状态信息都存储于服务端，只要依靠客户端的同源策略和HTTPS 的传输层安全，保证 Cookie 中的键值不被窃取而出现被冒认身份的情况，就能完全规避掉信息在传输过程中被泄漏和篡改的风险。Cookie-Session 方案的另一大优点是服务端有主动的状态管理能力，可根据自己的意愿随时修改、清除任意上下文信息，臂如很轻易就能实现强制某用户下线的功能。\nSession-Cookie 在单节点的单体服务环境中是最合适的方案，但当需要水平扩展服务能力，要部署集群时就比较麻烦了，由于 Session 存储在服务器的内存中，当服务器水平拓展成多节点时，设计者必须在以下三种方案中选择其一。\n\n牺牲集群的一致性，让负载均衡器采用亲和式的负载均衡算法，臂如根据用户 IP 或者 Session 来分配节点，每一个特定用户发出的所有请求都一直被分配到其中某个节点来提供服务，每个节点都不重复地保存着一部分用户的状态，如果这个节点崩溃了，里面的用户状态便完全丢失。\n牺牲集群的可用性，让各个节点之间采用复制式的 Session，每一个节点中的 Session 变动都会发送到组播地址的其他服务器上，这样即使某个节点崩溃了，也不会中断某个用户的服务，但 Session 之间组播复制的同步代价高昂，节点越多时，同步成本越高。\n牺牲集群的分区容忍性，让普通的服务节点中不再保留状态，将上下文集中放在个所有服务节点都能访问到的数据节点中进行存储。此时的矛盾是数据节点成为单点，一旦数据节点损坏或出现网络分区，整个集群将都不能再提供服务。\n\n通过前面章节的内容，我们已经知道只要在分布式系统中共享信息，CAP 就不可兼得所以分布式环境中的状态管理一定会受到 CAP 的限制，无论怎样都不可能完美。但如果只是解决分布式下的认证授权问题，并顺带解决少量状态的问题，就不一定只能依靠共享信息去实现。这句话的言外之意是提醒读者，接下来的 JWT 令牌与 Cookie-Session 并不是完全对等的解决方案，JWT 令牌只用来处理认证授权问题，充其量只能携带少量非敏感的信息，是 Cookie-Session 在认证授权问题上的替代品，而不能说 JWT 要比 Cookie-Session 更先进，更不可能说 JWT 可以全面取代 Cookie-Session 机制。\n2. JWTCookie-Session 机制在分布式环境下会遇到 CAP 不可兼得的问题，而在多方系统中，就更不可能谈 Session 层面的数据共享了，哪怕服务端之间能共享数据，客户端的 Cookie 也没法跨域。所以我们不得不重新捡起最初被抛弃的思路，当服务器存在多个，客户端只有一个时，把状态信息存储在客户端，每次随着请求发回服务器去。前面说过，这样做的缺点是无法携带大量信息，而且有泄漏和篡改的安全风险。信息量受限的问题并没有太好的解决办法，不过要确保信息不被中间人篡改则还是可以实现的，JWT 便是这个问题的标准答案。\nJWT（JSON Web Token）定义于 RFC7519 标准之中，是目前广泛使用的一种令牌格式尤其经常与 OAuth2 配合应用于分布式的、涉及多方的应用系统中。介绍JWT的具体构成之前，我们先来直观地看一下它是什么样子的，如下图所示。\n\n以上截图来自 JWT 官网，数据则是笔者随意编的。右边的 JSON 结构是 JWT 令牌中携带的信息，左边的字符串呈现了 JWT 令牌的本体。它最常见的使用方式是附在名为 Authorization 的 Header 发送给服务端，前缀在 RFC6750 中被规定为 Bearere。如果你没有忘记 “认证方案” 与 “OAuth2” 的内容，那看到 Authorization 这个Header 与 Bearer 这个前缀时，便应意识到它是 HTTP 认证框架中的 OAuth2 认证方案。如下代码展示了一次采用 JWT 令牌的 HTTP 实际请求：\n\nGET /restful/products/1 HTTP/1.1Host: 1cyfenix.cnConnection: keep-aliveAuthorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c\n\n上图中右边的状态信息是对令牌使用 Bas64URL 转码后得到的明文，请特别注意是明文，JWT 只解决篡改的问题，并不解决泄漏的问题，因此令牌默认是不加密的。尽管你自己要加密也不难做到，接收时自行解密即可，但这样做其实没有太大意义，具体原因将在第四节中阐述。\n从明文中可以看到 JWT 令牌是以 JSON 结构（毕竟名字就叫 JSON Web Token）存储的，该结构总体上可划分为三个部分，每个部分间用点号 “.” 分隔开。第一部分是令牌头（Header），内容如下所示：\n&#123;  &quot;a1g&quot;: &quot;HS256&quot;,  &quot;typ&quot;: &quot;JWT&quot;&#125;\n\n它描述了令牌的类型（统一为typ:JWT）以及令牌签名的算法，示例中 HS256 为 HMAC SHA256算法的缩写，其他各种系统支持的签名算法则可以参考JWT官网。\n\n散列消息认证码在本节及后面其他关于安全的内容中，经常会在某种哈希算法前出现 “HMAC” 的前缀，这是指散列消息认证码（Hash-based Message Authentication Code，HMAC）。可以简单将它理解为一种带有密钥的哈希摘要算法，其实现形式上通常是把密钥以加盐方式混入，与内容一起做哈希摘要。\nHMAC 哈希与普通哈希算法的差别是普通的哈希算法通过 Hash 函数结果易变性保证了原有内容未被篡改，而 HMAC 不仅保证了内容未被篡改，还保证了该哈希确实是由密钥的持有人所生成的。如图所示。\n\n\n令牌的第二部分是负载（Payload），这是令牌真正需要向服务端传递的信息。针对认证问题，负载至少应该包含能够告知服务端 “这个用户是谁” 的信息；针对授权问题，令牌至少应该包含能够告知服务端 “这个用户拥有什么角色 / 权限” 的信息。JWT 的负载部分是可以完全自定义的，根据具体要解决的问题不同，设计自己所需要的信息，只是总容量不能太大，毕竟要受到 HTTP Header 大小的限制。一个 JWT 负载的例子如下所示：\n&#123;  &quot;username&quot;: &quot;icyfenix&quot;,  &quot;authorities&quot;: [  \t&quot;ROLE_USER&quot;,  \t&quot;ROLE_ADMIN&quot;  ],  &quot;scope&quot;: [  \t&quot;ALL&quot;  ],  &quot;exp&quot;: 1584948947,  &quot;jti&quot;: &quot;9d77586a-3f4f-4cbb-9924-fe2f77dfa33d&quot;,  &quot;client_id&quot;: &quot;bookstore frontend&quot;&#125;\n\nJWT 在 RFC 7519 中推荐（非强制约束）了七项声明名称（Claim Name），如需要用到这些内容，建议字段名与官方的保持一致。\n\niss（Issuer）:签发人。\nexp（Expiration Time）:令牌过期时间。\nsub（Subject）:主题。\naud（Audience）:令牌受众。\nnbf（Not Before）:令牌生效时间。\niat（Issued At）:令牌签发时间。\njti（JWT ID）:令牌编号。\n\n此外在 RFC 8225、RFC 8417、RFC 8485 等规范文档，以及 OpenID 等协议中，都定义了约定好公有含义的名称，内容比较多，这里不再赘述，感兴趣的读者可以参考 “IANA  JSON Web Token Registry。\n令牌的第三部分是签名（Signature），签名的意思是：使用在对象头中公开的特定签名算法，通过特定的密钥（由服务器进行保密，不能公开）对前面两部分内容进行加密计算，以例子里使用的 JWT 默认的 HMAC SHA256 算法为例，将通过以下公式产生签名值：HMACSHA256(base64Ur1Encode(header) + &quot;.&quot; + base64UrlEncode (payload), secret) 签名的意义在于确保负载中的信息是可信的、没有被篡改的，也没有在传输过程中丢失任何信息的。因为被签名的内容哪怕发生了一个字节的变动，也会导致整个签名发生显著变化。此外，由于签名这件事情只能由认证授权服务器完成（只有它知道密钥），任何人都无法在篡改后重新计算出合法的签名值，所以服务端才能够完全信任客户端传上来的 JWT 中的负载信息。\nJWT 默认的签名算法 HMAC SHA256 是一种带密钥的哈希摘要算法，加密与验证过程均只能由中心化的授权服务来提供，所以这种方式一般只适合于授权服务与应用服务处于同一个进程中的单体应用。在多方系统或者授权服务与资源服务分离的分布式应用中，通常会采用非对称加密算法来进行签名，这时候除了授权服务端持有的可以用于签名的私钥解决，而不在服务层次（譬如在令牌或接口其他参数上增加额外逻辑）上解决。\n\n只能携带相当有限的数据：HTTP 协议并没有强制约束 Header 的最大长度，但是，各种服务器、浏览器都会有自己的约束，譬如Tomcat 就要求 Header 最大不超过 8 KB，而在 Nginx 中则默认为 4 KB，因此在令牌中存储过多的数据不仅耗费传输带宽，还有额外的出错风险。\n必须考虑令牌在客户端如何存储：严谨地说，这个并不是 JWT 的问题而是系统设计的问题。如果授权之后，操作完关掉浏览器就结束了，那把令牌放到内存里面，压根不考虑持久化那是最理想的方案。但并不是谁都能忍受一个网站关闭之后下次就一定强制要重新登录的。这样的话，想想客户端该把令牌存放到哪里？Cookie？localStorage？Indexed DB？它们都有泄漏的可能，而令牌一旦泄漏，别人就可以冒充用户的身份做任何事情。\n无状态也不总是好的：这个其实也不是 JWT 的问题。如果不能想象无状态会有什么不好的话，笔者可以提个需求：请基于无状态 JWT的方案，做一个在线用户实时统计功能。\n\n四、保密保密是加密和解密的统称，是指以某种特殊的算法改变原有的信息数据，使得未授权的用户即使获得了已加密的信息，但因不知解密的方法，或者知晓解密的算法但缺少解密所需的必要信息，仍然无法了解数据的真实内容。\n按照需要保密的信息所处的环节不同，可以划分为 “信息在客户端时的保密”、“信息在传输时的保密” 和 “信息在服务端时的保密” 三类，或者进一步概括为 “端的保密” 和 “链路的保密” 两类。我们把最复杂、最有效，又早有标准解决方案的 “传输环节” 单独提取出来，放到下一节去讨论，本节将结合笔者的一些个人观点，重点讨论密码等敏感信息如何保障安全等级、是否应该从客户端开始加密、应该如何存储及如何验证等常见的安全保密问题。\n1. 保密的强度保密是有成本的，追求越高的安全等级，就要付出越多的工作量与算力消耗。笔者以用户登录为例，列举几种不同强度的保密手段，并讨论它们的防御关注点与弱点。\n\n以摘要代替明文：如果密码本身比较复杂，那一次简单的哈希摘要至少可以保证即使传输过程中有信息泄漏，也不会被逆推出原信息；即使密码在一个系统中泄漏了，也不至于威胁到其他系统的使用。但这种处理不能防止弱密码被彩虹表攻击所破解。\n先加盐值再做哈希是应对弱密码的常用方法：盐值可以为弱密码建立一道防御屏障，一定程度上防御已有的彩虹表攻击，但不能阻止加密结果被监听、窃取后，攻击者直接发送加密结果给服务端进行冒认。\n将盐值变为动态值能有效防止冒认：如果每次密码向服务端传输时都掺入了动态的盐值，让每次加密的结果都不同，那即使传输给服务端的加密结果被窃取了，也不能冒用来进行另一次调用。尽管在双方通信均可能泄漏的前提下协商出只有通信双方才知道的保密信息是完全可行的（后续第五节会提到），但这样协商出盐值的过程将变得极为复杂，而且每次协商只保护一次操作，也难以阻止对其他服务的重放攻击。\n给服务加入动态令牌，在网关或其他流量公共位置建立校验逻辑，这样服务端在愿意付出集群中分发令牌信息等代价的前提下，可以做到防止重放攻击，但是依然不能解决传输过程中被嗅探而泄漏信息的问题。\n启用 HTTPS 可以防御链路上的恶意嗅探，也以在通信层面解决了重放攻击的问题。但是依然有因客户端被攻破产生伪造根证书的风险、因服务端被攻破产生的证书泄漏而被中间人冒认的风险、因CRL更新不及时或者 OCSP Soft-fail 产生吊销证书被冒用的风险，以及因TLS 的版本过低或密码学套件选用不当产生加密强度不足的风险。\n为了抵御上述风险，保密强度还要进一步提升，譬如银行会使用独立于客户端的存储证书的物理设备（俗称的U盾）来避免根证书被客户端中的恶意程序窃取伪造；大型网站涉及账号、金钱等操作时，会使用双重验证开辟一条独立于网络的信息通道（如手机验证码、电子邮件）来显著提高冒认的难度；甚至一些关键企业（如国家电网）或机构（如军事机构）会专门建设遍布全国各地的与公网物理隔离的专用内部网络来保障通信安全。\n\n听了上述这些逐步升级的保密措施，你应该能对“更高安全强度同时也意味着更多代价”有更具体的理解，不是任何一个网站、系统、服务都需要无限拔高的安全性。也许这时候你会好奇另一个问题：安全的强度有尽头吗？存不存在某种绝对安全的保密方式？答案可能出乎多数人的意料，确实是有的。信息论之父香农严格证明了一次性密码（One TimePassword）的绝对安全性。但是使用一次性密码必须有个前提，就是已经提前安全地把密码或密码列表传达给对方。譬如，给你的朋友送去一本存储了完全随机密码的密码本，然后每次使用其中一条密码来进行加密通信，用完一条丢弃一条，理论上这样可以做到绝对的安全，但显然这种绝对安全对于互联网没有任何的可行性。\n2. 客户端加密关于客户端在用户登录、注册类场景里是否需要对密码进行加密的问题一直存有争议。笔者的观点很明确：为了保证信息不被黑客窃取而做客户端加密没有太大意义，对绝大多数的信息系统来说，启用 HTTPS 可以说是唯一的实际可行的方案。但是，为了保证密码不在服务端被滥用，在客户端就开始加密还是很有意义的。大网站被拖库的事情层出不穷，密码明文被写入数据库、被输出到日志中之类的事情也屡见不鲜，做系统设计时就应该把明文密码这种东西当成是最烫手的山芋来看待，越早消灭掉越好，将一个潜在的炸弹从客户端运到服务端，对绝大多数系统来说都没有必要。\n为什么客户端加密对防御泄密没有意义？原因是网络通信并非由发送方和接收方点对点进行的，客户端无法决定用户送出的信息能不能到达服务端，或者会经过怎样的路径到达服务端，在传输链路必定是不安全的假设前提下，无论客户端做什么防御措施，最终都会沦为 “马其诺防线”。之前笔者已经提到多次的中间人攻击，是通过劫持客户端到服务端之间的某个节点，包括但不限于代理（通过HTTP代理返回赝品）、路由器（通过路由导向赝品）、DNS 服务（直接将你机器的 DNS 查询结果替换为赝品地址）等，来给你访问的页面或服务注入恶意的代码，极端情况下，甚至可能会取代你要访问的整个服务或页面，此时不论你在页面上设计了多么精巧严密的加密措施，都不会起到任何保护作用，而攻击者只需劫持路由器，或在局域网内其他机器释放ARP病毒便有可能完成攻击。\n\n中间人攻击（Man-in-the-Middle Attack，MitM）在消息发出方和接收方之间拦截双方通信。以日常生活中的写信为例：你给朋友写了一封信，邮递员可以把每一份你寄出去的信都拆开看，甚至把信的内容改掉，然后重新封起来，再寄出去给你的朋友。朋友收到信之后给你回信，邮递员又可以拆开看，看完随便改，改完封好再送到你手上。你全程都不知道自己寄出去的信和收到的信都经过邮递员这个 “中间人” 转手和处理——换句话说，对于你和你朋友来讲，邮递员这个“中间人”角色是不可见的。\n\n对于 “不应把明文传递到服务端” 的观点，也是有一些不同意见的。譬如其中一种保存明文密码的理由是便于客户端做动态加盐，因为只有在服务端存储了明文，或者某种盐值/密钥是固定的加密结果的情况下，才能每次用新的盐值重新加密来与客户端传上来的加密结果进行比对。笔者的建议是每次从服务端请求动态盐值，在客户端加盐传输的做法通常都得不偿失，因为客户端无论是否动态加盐，都不可能代替 HTTPS。真正防御性的密码加密存储确实应该在服务端中进行，但这是为了降低服务端被攻破而批量泄漏密码的风险，并不是为了增加传输过程的安全。\n3. 密码存储和验证这节笔者以 Fenix＇s Bookstore 中的真实代码为例，介绍一个普通安全强度的信息系统是如何将密码从客户端传输到服务端，然后存储到数据库的全过程。“普通安全强度” 是指在具有一定保密安全性的基础上，尽量避免消耗过多的运算资源，这样后续验证起来也相对便捷。对多数信息系统来说，只要配合一定的密码规则约束，臂如密码要求长度、特殊字符等，再配合HTTPS传输，已足以抵御大多数风险了。即使用户采用了弱密码、客户端通信被监听、服务端被拖库、泄漏了存储的密文和盐值等问题同时发生，也能够最大限度避免用户明文密码被逆推出来。下面先介绍密码创建的过程。\n\n用户在客户端注册，输入明文密码：123456。\npassword ＝123456\n客户端对用户密码进行简单的哈希摘要运算，可选的算法有MD2/4/5、SHA1/256/512、BCrypt、PBKDFI1/2，等等。为了突出“简单”的哈希摘要，这里笔者故意没有排除掉 MD 这类已经有了高效碰撞手段的算法。\nclient hash MD5(password)   //e10adc3949ba59abbe56e057f20f883e\n为了防御彩虹表攻击，应加盐处理，客户端加盐只取固定的字符串即可，如实在不安心，也可用伪动态的盐值。\nclient hash MD5(MD5(password) ＋ salt)     //SALT ＝S2as10So5L.dWYEjZjaejOmN3x4Qu\n假设攻击者截获了客户端发出的信息，得到了摘要结果和采用的盐值，那攻击者就可以枚举遍历所有8位字符以内的弱密码，然后对每个密码再进行加盐计算，就得到一个针对固定盐值的对照彩虹表。为了应对这种暴力破解，并不提倡在盐值上做动态化，更理想的方式是引入慢哈希函数来解决。\n慢哈希函数是指执行时间可以调节的哈希函数，通常是以控制调用次数来实现的。BCrypt 算法就是一种典型的慢哈希函数，它做哈希计算时接受盐值 Salt 和执行成本 Cost 两个参数。如果我们将 BCrypt 的执行时间控制在 0.1 秒完成一次哈希计算的话，按照 1 秒生成 10 个哈希值的速度，算完所有的10位大小写字母和数字组成的弱密码大概需要P(62，10) / (3600＊24＊365) / 0.1＝1237204169 年时间。\nclient_hash BCrypt(MD5(password) + salt)  //MFfTW3uNI4eqhwDkG7HP9p2mzEUu/r2\n下一步将哈希值传输到服务端，在服务端只需防御被拖库后针对固定盐值的批量彩虹表攻击。具体做法是为每一个密码（指客户端传来的哈希值）产生一个随机的盐值。笔者建议采用 “密码学安全伪随机数生成器”（Cryptographically Secure Pseudo-Random Number Generator，CSPRNG）来生成一个长度与哈希值长度相等的随机字符串。对于Java语言，从 Java SE7 起提供了java.security.SecureRandom 类，用于支持 CSPRNG 字符串生成。\nSecureRandom random new SecureRandom();byte server_salt[] new byte[36];random.nextBytes(server_salt);  //tq2pdxrblkbgp8vt8kbdpmzdh1w8bex\n将动态盐值混入客户端传来的哈希值再做一次哈希，产生最终的密文，并和上一步随机生成的盐值一起写入同一条数据库记录中。由于慢哈希算法占用大量处理器资源，笔者并不推荐在服务端中采用。不过，如果你阅读了 Fenix＇s Bookstorel 的源码，会发现这步依然采用了 Spring Security5 中的 BcryptPasswordEncoder，但是请注意它默认构造函数中的 Cost 参数值为 -1，经转换后实际只进行了 1024(2^10) 次计算，并不会给服务端带来太大的压力。此外，代码中并未显式传入 CSPRNG 生成的盐值，这是因为  BCryptPasswordEncoder 本身就会自动调用 CSPRNG 产生盐值，并将该盐值输出在结果的前 32 位之中，因此也无须专门在数据库中设计存储盐值的字段。这个过程以伪代码表示如下：\nserver_hash = SHA256(client hash server salt);   //55b4b5815c216cf80599990e781cd8974a1e384d49fbde7776d096e1dd436f67DB.save(server hash，server_salt);\n\n以上加密存储的过程相对复杂，但是运算压力最大的过程（慢哈希）是在客户端完成的，对服务端压力很小，也不惧怕因网络通信被截获而导致明文密码泄漏。密码存储后，以后验证的过程与加密是类似的，具体步骤如下所示。1）客户端：用户在登录页面中输入密码明文，123456，经过与注册相同的加密过程，向服务端传输加密后的结果。\nauthentication_hash = MFfTW3uNI4eqhwDkG7HP9p2mzEUu/r2\n\n2）服务端：接收到客户端传输上来的哈希值，从数据库中取出登录用户对应的密文和盐值，采用相同的哈希算法，对客户端传来的哈希值、服务端存储的盐值计算摘要结果。\nresult = SHA256(authentication_hash + server_salt);    //55b4b5815c216cf80599990e781cd8974a1e384d49fbde7776d096e1dd436f67\n\n3）比较上一步的结果和数据库储存的哈希值是否相同，如果相同说明密码正确，反之说明密码错误。\nauthentication_compare(result, server_hash)     //yes\n\n五、传输前文中笔者已经为传输安全层挖了不少坑，臂如：基于信道的认证是怎样实现的？为什么HTTPS是绝大部分信息系统防御通信被窃听和篡改的唯一可行手段？传输安全层难道不也是一种自动化的加密吗？为何说无论客户端如何加密都不能代替HTTPS？本节将以 “假设链路上的安全得不到保障，攻击者如何摧毁之前认证、授权、凭证、保密中所提到的种种安全机制” 为场景，讲解传输安全层所要解决的问题，同时也是对前面这些疑问的回答。\n1. 摘要、加密与签名我们从 JWT 令牌的一小段 “题外话” 来引出现代密码学算法的三种主要用途：摘要、加密与签名。JWT 令牌携带信息的可信度源自于它是被签过名的信息，是令牌签发者真实意图的体现，因此是不可篡改的。然而，你是否了解签名具体做了什么？为什么有签名就能够让负载中的信息变得不可篡改和不可抵赖呢？要解释数字签名（Digital Signature），必须先从密码学算法的另外两种基础应用 “摘要” 和 “加密” 说起。\n摘要也称为数字摘要（Digital Digest）或数字指纹（Digital Fingerprint）。JWT令牌中默认的签名信息是对令牌头、负载和密钥三者通过令牌头中指定的哈希算法（HMACSHA256）计算出来的摘要值，如下所示：\nsignature =  Hash(base64UrlEncode(header) ＋＂.＂＋ base64UrlEncode(payload), secret)\n\n理想的哈希算法都具备两个特性。一是易变性，这是指算法的输入端发生了任何一点细微变动，都会引发雪崩效应（Avalanche Effect），使得输出端的结果产生极大的变化。这个特性常被用来做校验，以保证信息未被篡改，譬如互联网上下载大文件，常会附有一个哈希校验码，以确保下载下来的文件没有因网络或其他原因与原文件产生任何偏差。二是不可逆性，摘要的运算过程是单向的，不可能从摘要的结果中逆向还原出输入值来。世间的信息有无穷多种，而摘要的结果无论其位数是32、128、512位，甚至更多位，都是一个有限的数字，因此输入数据与输出的摘要结果必然不是一一对应的关系。例如，我对一部电影做摘要运算形成 256 位的哈希值，应该没有人会指望从这个哈希值中还原出一部电影。偶尔能听到 MD5、SHA1 或其他哈希算法被破解了的新闻，但这里的 “破解” 并不是 “解密” 的意思，而是指找到了该算法的高效率碰撞方法，能够在合理的时间内生成一个摘要结果为指定内容的输入比特流，但并不能代表这个碰撞产生的比特流就会是原来的输入源。\n由这两个特性可见，摘要的意义是在源信息不泄漏的前提下辨别其真伪。易变性保证了可以从公开的特征上甄别出信息是否来自于源信息，不可逆性保证了不会从公开的特征暴露出源信息，这与今天用作身份甄别的指纹、面容和虹膜的生物特征是具有高度可比性的。在一些场合中，摘要也会被借用来做加密（如保密中介绍的慢哈希Berypt算法）和签名（如JWT签名中的HMAC SHA256算法），但在严格意义上看，摘要与这两者有本质的区别。\n加密与摘要的本质区别在于加密是可逆的，逆过程就是解密。在经典密码学时代，加密的安全主要依靠机密性来保证，即依靠保护加密算法或算法的热行参数不被泄漏来保障信息的安全。而现代密码学不依靠机密性，加解密算法都是完全公开的，它的安全是建立在特定问题的计算复杂度之上，具体是指算法根据输入端计算输出结果耗费的算力资源很小，但根据输出端的结果反过来推算原本的输入时耗费的算力就极其庞大。以大数的质因数分解为例，我们可以轻而易举地（以O(nlogn) 的复杂度）计算出两个大素数的乘积，臂如：\n97667323933 * 128764321253 = 12576066674829627448049\n\n根据算术基本定理，质因数的分解形式是唯一的，且前面计算条件中给出的运算因子已经是质数、所以 12576066674829627448049 的分解形式就只有唯一的形式，即上面所示的唯一答案。然而如何对大数进行质因数分解，迄今还没有找到多项式时间的算法，甚至无法确切地知道这个问题属于哪个复杂度类（Complexity Class）。所以尽管这个过程在理论上一定是可逆的，但实际上算力差异决定了逆过程无法实现。\n根据加密与解密是否采用同一个密钥，可将现代密码学算法分为对称加密算法和非对称加密算法两大类型，这两类算法各有明确的优劣势与应用场景。对称加密算法的缺点显而易见，加密和解密使用相同的密钥，当通信的成员数量增加时，为保证两两通信都采用独立的密钥，密钥数量与成员数量的平方成正比，这必然面临密钥管理的难题。而更尴尬的难题是当通信双方原本不存在安全的信道时，如何将一个只能让通信双方才能知道的密钥传输给对方？如果有通道可以安全地传输密钥，那为何不使用现有的通道传输信息？这个“蛋鸡悖论”曾在很长的时间里严重阻碍了密码学在真实世界的推广应用。\n20世纪70年代中后期出现的非对称加密算法从根本上解决了密钥分发的难题，它将密钥分成公钥和私钥。公钥可以完全公开，无须安全传输的保证。私钥由用户自行保管，不参与任何通信传输。根据这两个密钥加解密方式的不同，使得算法可以提供两种不同的功能。\n\n公钥加密，私钥解密，这种就是加密，用于向私钥所有者发送信息，这个信息可能被他人篡改，但是无法被他人得知。如果甲想给乙发一个安全保密的数据，那么甲乙应该各有一个私钥，甲先用乙的公钥加密这段数据，再用自己的私钥加密这段加密后的数据，最后发给乙，这样确保了内容既不会被读取，也不能被篡改。\n私钥加密，公钥解密，这种就是签名，用于让所有公钥所有者验证私钥所有者的身份，并且防止私钥所有者发布的内容被篡改。但是它不用于保证内容不被他人获得。\n\n这两种用途在理论上肯定是成立的，在现实中却一般不成立。单靠非对称加密算法，既做不了加密也做不了签名。因为不论是加密还是解密，非对称加密算法的计算复杂度都相当高，其性能比对称加密要差上好几个数量级（不是好几倍）。加解密性能不仅影响速度，还导致现行的非对称加密算法都没有支持分组加密模式。这句话的含义是：由于明文长度与密钥长度在安全上具有相关性，通俗地说，多长的密钥决定了它能加密多长的明文，如果明文太短就需要进行填充，太长就需要进行分组。因非对称加密本身的效率所限，难以支持分组，所以主流的非对称加密算法都只能加密不超过密钥长度的数据，这也决定了非对称加密不能直接用于大量数据的加密。\n在加密方面，现在一般会结合对称与非对称加密的优点，以混合加密来保护信道安全，具体做法是用非对称加密来安全地传递少量数据给通信的另一方，再以这些数据为密钥，采用对称加密来安全高效地大量加密传输数据，这种由多种加密算法组合的应用形式称为 “密码学套件”。非对称加密在这个场景中发挥的作用称为 “密钥协商”。\n在签名方面，现在一般会结合摘要与非对称加密的优点，以对摘要结果做加密的形式来保证签名的适用性。由于对任何长度的输入源做摘要之后都能得到固定长度的结果，所以只要对摘要的结果进行签名，即相当于对整个输入源进行了背书，保证一旦内容遭到篡改，摘要结果就会变化，签名也就马上失效了。\n表5-1汇总了前面提到的三种算法，并列举了它们的主要特征、用途和局限性。\n\n\n\n类型\n特点\n常见实现\n主要用途\n主要局限\n\n\n\n哈希摘要\n1）不可逆，即不能解密，所以并不是加密算法，只是一些场景把它当作加密算法使用2）易变性，输入发生1位变动，就可能导致输出结果50％的内容发生改变3）无论输入长度多少，输出长度固定（2的N次幂）\nMD2/4/5/6、SHA0/1/256/512\n摘要\n无法解密\n\n\n对称加密\n1）加密和解密是一样的密钥2）设计难度相对较小，执行速度相对较快3）加密明文长度不受限制\nDES、AES、RC4、IDEA\n加密\n要解决如何把密钥安全地传递给解密者\n\n\n非对称加密\n1）加密和解密使用的是不同的密钥2）明文长度不能超过公钥长度\nRSA、BCDSA、ElGamal\n签名、传递密钥\n\n\n\n现在，让我们再回到开篇关于 JWT 令牌的几个问题中来。有了哈希摘要、对称和非对称加密算法，JWT 令牌的签名就能保证负载中的信息不可篡改、不可抵赖吗？其实还是不行的，在这个场景里，数字签名的安全性仍存在一个致命的漏洞：公钥虽然是公开的，但在网络世界里“公开”具体是一种什么操作？如何保证每一个获取公钥的服务，拿到的公钥就是授权服务器希望它拿到的？\n在网络传输是不可信任的前提下，公钥在网络传输过程中可能已经被篡改，如果获取公钥的网络请求被攻击者截获并篡改，返回了攻击者自己的公钥，那以后攻击者就可以用自己的私钥来签名，让资源服务器无条件信任它的所有行为了。现实世界中可以通过打电话、发邮件、短信息、登报纸、同时发布在多个网站上等很多网络通信之外的途径来公开公钥，但在程序与网络的世界中，就必须找到一种可信任的公开方法，而且这种方法不能依赖加密来实现，否则又将陷入 “蛋鸡” 问题之中。\n2. 数字证书当我们无法以“签名”的手段来达成信任时，就只能求助于其他途径。不妨先想一想真实的世界中，我们是如何达成信任的，其实不外乎以下两种。\n\n基于共同私密信息的信任。譬如某个陌生号码找你，说是你的老同学，生病了要找你借钱。你能够信任他的方式是向对方询问一些你们两个应该知道，且只有你们两个知道的私密信息，如果对方能够回答出来，他有可能真的是你的老同学，否则他十有八九就是个骗子。\n\n基于权威公证人的信任。如果有个陌生人找你，说他是警察，让你把存款转到他们的安全账号上。你能够信任他的方式是去一趟公安局，如果公安局担保他确实是个警察，那他有可能真的是警察，否则他十有八九就是个骗子。\n\n\n回到网络世界中，我们并不能假设授权服务器和资源服务器是互相认识的，所以通常不太会采用第一种方式，而第二种就是目前保证公钥可信分发的标准，即公开密钥基础设施（Public Key Infrastructure，PKI）。\n\n公开密钥基础设施\n又称公开密钥基础架构、公钥基础建设、公钥基础设施、公开密钥基础建设或公钥基础架构，是一组由硬件、软件、参与者、管理政策与流程组成的基础架构，其目的在于创造、管理、分配、使用、存储以及撤销数字证书。\n在密码学中，公开密钥基础建设借着数字证书认证中心（Certificate Authority，CA）将用户的个人身份跟公开密钥链接在一起，且每个证书中心用户的身份必须是唯一的。链接关系通过注册和发布过程创建，根据担保级别的差异，创建过程可由 CA 的各种软件或在人为监督下完成。PKI的确定链接关系的这一角色称为注册管理中心（Registration Authority，RA）。RA确保公开密钥和个人身份链接，可以防抵赖。\n\n咱们不必纠缠于 PKI 概念上的内容，只要知道里面定义的 “数字证书认证中心” 相当于前面例子中 “权威公证人” 的角色，是负责发放和管理数字证书的权威机构即可。任何人包括你我都可以签发证书，只是不权威罢了。CA 作为受信任的第三方，承担公钥体系中公钥的合法性检验的责任。可是，这里和现实世界仍然有一些区别，在现实世界你去找公安局，其办公大楼不大可能是剧场布景冒认的；而在网络世界，在假设所有网络传输都有可能被截获冒认的前提下，“去CA中心进行认证”本身也是一种网络操作，这与之前的 “去获取公钥” 本质上不是没什么差别吗？其实还是有差别的，世间公钥成千上万不可枚举，而权威的CA中心则应是可数的，“可数”意味着可以不通过网络，而是在浏览器与操作系统出厂时就预置好，或者提前安装好（如银行的证书）。\n到这里出现了本节的主角之一：证书（Certificate）。证书是权威 CA 中心对特定公钥信息的一种公证载体，也可以理解为权威 CA 对特定公钥未被篡改的签名背书。由于客户的机器上已经预置了这些权威CA中心本身的证书（称为CA证书或者根证书），所以我们能够在不依靠网络的前提下，使用根证书里面的公钥信息对其所签发的证书中的签名进行确认。到此，终于打破了鸡生蛋、蛋生鸡的循环，使得整套数字签名体系有了坚实的逻辑基础。\nPKI 中采用的证书格式是 X.509 标准格式，它定义了证书中应该包含哪些信息，并描述了这些信息是如何编码的，其中最关键的就是认证机构的数字签名和公钥信息两项内容。一个数字证书具体包含以下内容。\n\n版本号（Version）：指出该证书使用了哪种版本的 X.509 标准（版本1、版本2或是版本3），版本号会影响证书中的一些特定信息，目前的版本为 3。\n\nVersion:3 （0x2）\n\n\n序列号（Serial Number）：由证书颁发者分配的证书的唯一标识符。\n\nSeria1 Number: 04:00:00:00:00:01:15:4b:5a:c3:94\n\n\n签名算法标识符（Signature Algorithm ID）：用于签发证书的算法标识，由对象标识符加上相关的参数组成，用于说明本证书所用的数字签名算法。譬如，SHA1和RSA的对象标识符就用来说明该数字签名是利用RSA对SHA1的摘要结果进行加密。\n\nSignature Algorithm: shalWithRSAEncryption\n\n\n认证机构的数字签名（Certificate Signature）：这是使用证书发布者私钥生成的签名，以确保这个证书在发放之后没有被篡改过。\n\n认证机构（Issuer Name）：证书颁发者的可识别名。\n\nIssuer: C＝BE，O＝GlobalSign nv-sa，CN＝GlobalSign Organization Validation CA  -SHA256 - G2\n\n\n有效期限（Validity Period）：证书起始日期和时间以及终止日期和时间；指明证书在这两个时间内有效。\n\nValidity\n  Not Before :Nov 21 08:00:00 2020 GMT\n  Not After   : Nov 22 07:59:59 2021 GMT\n\n\n\n主题信息（Subject）：证书持有人唯一的标识符（Distinguished Name），这个名字在整个互联网上应该是唯一的，通常使用的是网站的域名。\n\nSubject:C＝CN，ST＝GuangDong，L＝Zhuhai，O＝Awosome-Fenix，CN＝＊.icyfenix.cn\n\n\n公钥信息（Public-Key）：包括证书持有人的公钥、算法（指明密钥属于哪种密码系统）的标识符和其他相关的密钥参数。\n\n\n3. 传输安全层至此，数字签名的安全性已经可以完全自洽了，但相信你大概也已经感受到了这条信任链的复杂与烦琐，如果从确定加密算法、生成密钥、公钥分发、CA认证、核验公钥、签名到验证，每一个步骤都要由最终用户来完成的话，这种意义的 “安全” 估计只能一直是存于实验室中的阳春白雪。如何把这套烦琐的技术体系自动化地应用于无处不在的网络通信之中，便是本节的主题。\n在计算机科学里，隔离复杂性的最有效手段（没有之一）就是分层，如果一层不够就再加一层，这点在网络中更是体现得淋漓尽致。OSI 模型、TCP/IP 模型将网络从物理特性（比特流）开始，逐层封装隔离，到了 HTTP 协议这种面向应用的协议里，使用者就已经不会去关心网卡/交换机如何处理数据帧、MAC 地址；不会去关心 ARP 如何做地址转换；不会去关心IP寻址、TCP 传输控制等细节。想要在网络世界中让用户无感知地实现安全通信，最合理的做法就是在传输层之上、应用层之下加入专门的安全层来实现，这样对上层原本基于 HTTP 的 Web 应用来说，影响甚至是无法察觉的。构建传输安全层的这个想法，几乎可以说是和万维网的历史一样长，早在1994年，就已经有公司开始着手去实践了。\n\n1994年，网景（Netscape）公司开发了SSL协议（Secure Sockets Layer）的1.0版，这是构建传输安全层的起源，但是SSL1.0从未正式对外发布过。\n1995年，Netscape 把 SSL 升级到 2.0 版，正式对外发布，但是刚刚发布不久就被发现有严重漏洞，所以并未大规模使用。\n1996年，修补好漏洞的 SSL3.0 对外发布，这个版本得到了广泛应用，很快成为Web网络安全层的事实标准。\n1999年，互联网标准化组织接替 Netscape，将 SSL 改名为 TLS（Transport Layer Security）后作为传输安全层的国际标准。第一个正式的版本是 RFC 2246 定义的 TLS1.0，该版 TLS 的生命周期极长，直至笔者写下这段文字的 2020 年 3 月，主流浏览器（Chrome、Firefox、IE、Safari）才刚刚宣布同时停止对 TLS1.0/.1 的支持。而讽刺的是，由于停止后许多政府网站被无法被浏览，此时又正值新冠肺炎疫情（COVID-19）爆发期，Firefox 紧急发布公告宣布撤回该改动，TLS1.0 的生命还在顽强延续。\n2006年，TLS 的第一个升级版 1.1 发布（RFC4346），但却沦为被遗忘的孩子，很少人使用，甚至到了 TLS1.1 从来没有已知的协议漏洞被提出的程度。\n2008年，在 TLS1.1 发布 2 年之后，TLS1.2 标准发布（RFC5246），迄今超过90％的互联网HTTPS流量是由 TLS1.2 所支持的，现在仍在使用的浏览器几乎都完美支持了该协议。\n2018年，最新的 TLS1.3（RFC8446）发布，比起前面版本相对温和的升级，TLS1.3 做出了一些激烈的改动，修改了从 1.0 起一直没有大变化的两轮四次（2-RTT）握手，首次连接仅需一轮（1-RTT）握手即可完成，在连接复用支持时，甚至将 TLS1.2 原本的 1-RTT 下降到 0-RTT，显著提升了访问速度。\n\n接下来，笔者以 TLS1.2 为例，介绍传输安全层是如何保障所有信息都是第三方无法窃听（加密传输）、无法篡改（一旦篡改通信算法会立刻发现）、无法冒充（证书验证身份）的。TLS1.2 在传输之前的握手过程一共需要进行上下两轮、共计四次通信，时序图如图\n\n1）客户端请求：Client Hello客户端向服务器请求进行加密通信，在这个请求里面，它会以 明文 的形式，向服务器端提供以下信息。\n\n支持的协议版本，如 TLS1.2。但是要注意，1.0 至 3.0 分别代表 SSL1.0 至 3.0，TLS1.0 则是 3.1，一直到 TLS1.3 的 3.4。\n一个客户端生成的 32 字节随机数，这个随机数将稍后用于产生加密的密钥。\n一个可选的 SessionlD，注意不要和前面的 Cookie-Session 机制混淆了，这个 SessionlD 是指传输安全层的 Session，是为了 TLS 的连接复用而设计的。\n一系列支持的密码学算法套件，例如 TLS_RSA_WITH AES128 GCM_SHA256，代表密钥交换算法是 RSA，加密算法是 AES128-GCM，消息认证码算法是 SHA256\n一系列支持的数据压缩算法。\n其他可扩展的信息，为了保证协议的稳定性，后续对协议的功能扩展大多都添加到这个变长结构中。譬如 TLS1.0 中由于发送的数据并不包含服务器的域名地址，导致一台服务器只能安装一张数字证书，这对虚拟主机来说很不方便，所以 TLS1.1 起就增加了名为 “Server Name” 的扩展信息，以便一台服务器给不同的站点安装不同的证书。\n\n2）服务器回应：Server Hello服务器接收到客户端的通信请求后，如果客户端声明支持的协议版本和加密算法组合与服务端相匹配的话，就向客户端发出回应。如果不匹配，将会返回一个握手失败的警告提示。这次回应同样以明文发送，包括以下信息。\n\n服务端确认使用的 TLS 协议版本。\n第二个 32 字节的随机数，稍后用于产生加密的密钥。\n一个 SessionID，以后可通过连接复用减少一轮握手。\n服务端在列表中选定的密码学算法套件。\n服务端在列表中选定的数据压缩算法。\n其他可扩展的信息。\n如果协商出的加密算法组合是依赖证书认证的，服务端还要发送出自己的 X.509 证书，而证书中的公钥是什么，也必须根据协商的加密算法组合来决定。\n密钥协商消息，这部分内容对于不同密码学套件有着不同的价值，譬如对于 ECDH + anon 这样的密钥协商算法组合（基于椭圆曲线的 ECDH 算法可以在双方通信都公开的情况下协商出一组只有通信双方知道的密钥）就不需要依赖证书中的公钥，而是通过 Server Key Exchange 消息协商出密钥。\n\n3）客户端确认：Client Handshake Finished由于密码学套件的组合复杂多样，这里仅以 RSA 算法为密钥交换算法为例介绍后续过程。\n客户端收到服务器应答后，先要验证服务器的证书合法性。如果证书不是可信机构颁布的，或者证书中信息存在问题，譬如域名与实际域名不一致、证书已经过期、通过在线证书状态协议得知证书已被吊销，等等，都会向访问者显示一个 “证书不可信任” 的警告，由用户自行选择是否还要继续通信。如果证书没有问题，客户端就会从证书中取出服务器的公钥，并向服务器发送以下信息。\n\n客户端证书（可选）。部分服务端并不是面向全公众，而是只对特定的客户端提供服务，此时客户端需要发送它自身的证书来证明身份。如果不发送，或者验证不通过，服务端可自行决定是否要继续握手，或者返回一个握手失败的信息。客户端需要证书的 TLS 通信也称为 “双向TLS”（Mutual TLS，常简写为 mTLS）这是云原生基础设施的主要认证方法，也是基于信道认证的最主流形式。\n第三个 32 字节的随机数，这个随机数不再是明文发送，而是以服务端传过来的公钥加密，被称为 PreMasterSecret，它将与前两次发送的随机数一起，根据特定算法计算出 48 字节的 MasterSecret，这个 MasterSecretI 即后续内容传输时的对称加密算法所采用的私钥。\n编码改变通知、表示随后的信息都将用双方商定的加密方法和密钥发送。\n客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时也是前面发送的所有内容的哈希值，以供服务器校验。\n\n4）服务端确认：Server Handshake Finished服务端向客户端回应最后的确认通知，包括以下信息。\n\n编码改变通知，表示随后的信息都将用双方商定的加密方法和密钥发送。\n\n服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时也是前面发送的所有内容的哈希值，以供客户端校验。\n\n\n至此，整个 TLS 握手阶段宣告完成，一个安全的连接就已成功建立。每一个连接建立时，客户端和服务端均通过上面的握手过程协商出了许多信息，譬如一个只有双方才知道的随机产生的密钥、传输过程中要采用的对称加密算法（例子中的AES128）、压缩算法等，此后该连接的通信将使用此密钥和加密算法进行加密、解密和压缩。这种处理方式对上层协议的功能是完全透明的，虽然在传输性能上会有下降，但在功能上完全不会感知到TS的存在。建立在这层传输安全层之上的 HTTP 协议，被称为 “HTTP over SSL/TLS”，也即大家所熟知的HTTPS。\n从上面握手协商的过程中我们还可以得知，HTTPS 并非只有 “启用了HTTPS” 和 “未启用HTTPS” 的差别，采用不同的协议版本、不同的密码学套件，证书是否有效，服务端/客户端面对无效证书时的处理策略等都导致了不同 HTTPS 站点的安全强度的不同，因此并不能说只要启用了 HTTPS 就必能安枕无忧。\n六、验证数据验证与程序如何编码是密切相关的，许多开发者都不会把它归入安全的范畴之中。 但请细想一下，关注 “你是谁”（认证）、“你能做什么”（授权）等问题是很合理的安全，关注 “你做得对不对”（验证）不也同样合理吗？从数量来讲，数据验证不严谨而导致的安全问题比其他安全攻击导致的安全问题要多得多；而从风险上讲，由数据质量导致的问题，风险有高有低，真遇到高风险的数据问题时，面临的损失不一定就比被黑客拖库来得小。相比其他富有挑战性的安全措施，如防御与攻击两者缠斗的精彩，数学、心理、社会工程和计算机等跨学科知识的结合运用，数据验证确实有些无聊、枯燥，这项常规的工作在日常的开发中贯穿于代码的各个层次，每个程序员都肯定写过。但这种常见的代码反而是迫切需要被架构约束的，缺失的校验影响数据质量，过度的校验不会使得系统更加健壮，某种意义上反而会制造垃圾代码，甚至带来副作用。请来看看下面这个实际的段子。\n\n前端：提交一份用户数据（姓名：某，性别：男，爱好：女，签名：xxx，手机：xxx，邮箱：null）控制器：发现邮箱是空的，抛 ValidationException（＂邮箱没填＂）前端：已修改，重新提交安全：发送验证码时发现手机号少一位，抛 RemoteInvokeException（＂无法发送验证码”）前端：已修改，重新提交服务层：邮箱怎么有重复啊，抛 BusinessRuntimeException（＂不允许开小号＂）前端：已修改，重新提交持久层：签名字段超长了插不进去，抛 SQLException（＂插入数据库失败，SQL:xxx＂）前端：你们这些管挖坑不管埋的后端，各种异常都往前抛！用户：这系统牙膏厂生产的？\n\n最基础的数据问题可以在前端做表单校验来处理，但服务端验证肯定也是要做的，看完了上面的段子后，那么服务端应该在哪一层做校验呢？可能会有这样的答案。\n\n在控制器层做，在服务层不做。理由是从服务开始会有同级重用，出现 ServiceA.foo(params) 调用 ServiceB.bar(params) 时，就会对 params 重复校验两次。\n在服务层做，在控制器层不做。理由是无业务含义的格式校验已在前端表单验证处理过，有业务含义的校验，放在控制器层无论如何都不合适。\n在控制器层、服务层各做各的。控制器层做格式校验，服务层做业务校验，听起来很合理，但这其实就是上面段子中被嘲笑的行为。\n还有其他一些意见，譬如在持久层做校验，理由是持久层是最终入口，把守好写入数据库的质量最重要。\n\n上述的讨论大概不会有统一、正确的结论，但是在 Java 里确实有验证的标准做法，笔者提倡的做法是把校验行为从分层中剥离出来，不是在哪一层做，而是在 Bean 上做，即 Java Bean Validation。从 2009 年 JSR 303 的 1.0，到 2013 年 JSR 349 更新的 1.1，到目前最新的 2017 年发布的 JSR 380，均定义了 Bean 验证的全套规范。单独将验证提取、封装，可以获得不少好处：\n\n对于无业务含义的格式验证，可以做到预置。\n\n对于有业务含义的业务验证，可以做到重用，一个Bean被多个方法用作参数或返回值是很常见的，针对Bean做校验比针对方法做校验更有价值。\n\n利于集中管理，臂如统一认证的异常体系，统一做国际化、统一给客户端的返回格式，等等。\n\n避免对输入数据的防御污染到业务代码，如果你的代码里有很多下面这样的条件判断，就应该考虑重构了：\n//一些已执行的逻辑if (someParam ＝null) &#123;\tthrow new RuntimeExcetpion(&quot;客官不可以！&quot;)&#125;\n利于多个校验器统一执行，统一返回校验结果，避免用户踩地雷、挤牙膏式的试错体验。\n\n\n据笔者所知，国内的项目使用Bean Validation的并不少见，但多数程序员都只使用到它的内置约束注解（Built-In Constraint）来做一些与业务逻辑无关的通用校验，即下面这堆注解：\n＠Null、＠NotNull、＠AssertTrue、＠AssertFalse、＠Min、＠Max、DecimalMin、＠DecimalMax、＠Negative、＠NegativeOrZero、＠Positive、＠PositiveorZeor、＠Szie、＠Digits、＠Pass、＠PastorPresent、＠Future、＠FutureOrPresent、＠Pattern、＠NotEmpty、NotBlank、＠Email\n\n但是与业务相关的校验往往才是最复杂的校验，将简单的校验交给Bean Validation，而把复杂的校验留给自己，这简直是买椟还珠的程序员版本。其实以 Bean Validation 的标准方式来做业务校验才是非常优雅的，以 Fenix＇s Bookstore 在用户资源上的两个方法为例：\n/** * 创建新的用户 */ @POST public Response createUser(@Valid @UniqueAccount Account user) &#123;  return CommonResponse.op(() -&gt; service.createAccount(user));/**  *更新用户信息  */  @PUT  @CacheEvict(key ＝＂＃user.username＂）  public Response updateUser(@Valid @AuthenticatedAccount @NotConflictAccount Account user) &#123;  \treturn CommonResponse.op(() -&gt; service.updateAccount(user));&#125;\n\n注意其中的三个自定义校验注解，它们的含义分别是：\n\n@UniqueAccount：传人的用户对象必须是唯一的，不与数据库中任何已有用户的名称、手机、邮箱重复。\n@AuthenticatedAccount：传入的用户对象必须与当前登录的用户一致。\n＠NotConflictAccount：传人的用户对象中的信息与其他用户是无冲突的，譬如将一 个注册用户的邮箱，修改成与另外一个已存在的注册用户一致的值，这便是冲突。\n\n这里的需求很容易理解，注册新用户时，应约束不与任何已有用户的关键信息重复； 而修改自己的信息时，只能与自己的信息重复，而且只能修改当前登录用户的信息。这些约束规则不仅仅为这两个方法服务，还可能在用户资源的其他入口被使用到，甚至在其他分层的代码中被使用到，在Bean上做校验就能一揽子地覆盖上述这些使用场景。下面代码是这三个自定义注解对应校验器的实现类：\npublic static class AuthenticatedAccountValidator extends Accountvalidationc&lt;AuthenticatedAccount&gt; &#123;  public void initialize（AuthenticatedAccount constraintAnnotation) &#123;    predicate = c -&gt; &#123;      AuthenticAccount loginUser ＝(AuthenticAccount)        SecurityContextHolder.getContext().getAuthentication()getPrincipal();      return c.getId().equals(loginUser.getId());    &#125;;  &#125;&#125;public static class UniqueAccountValidator extends AccountValidation＜UniqueAccount＞  public void initialize(UniqueAccount constraintAnnotation）&#123;    predicate = c -&gt; !repository.existsByUsernameOrEmailOrTelephone      (c.getUsername(), c.getEmail(), c.getTelephone());  &#125;&#125;public static class NotConflictAccountValidator extends Accountvalidation＜NotConflictAccount＞  public void initialize(NotConflictAccount constraintAnnotation）｛    predicate = c -&gt; &#123;      Collection&lt;Account&gt; collection repository.findByUsernameOrEmailOrT-elephone(c.getusername(), c.getEmail(), c.getTelephone())；    //将用户名、邮件、电话改成与现有信息完全不重复的，或者只与自己重复的，就不算冲突    return collection.isEmpty() ||（collection.size() == 1 &amp;&amp; collection.iterator().next().getId().equals(c.getId()));&#125;;\n\n这样业务校验便和业务逻辑完全分离开来，在需要校验时用 @Valid 注解自动触发，或者通过代码手动触发执行，具体可根据实际项目的要求，将这些注解应用于控制器、服务层、持久层等任何层次的代码之中。此外，对于校验结果不满足时的提示信息，也可以统一处理，如提供默认值、国际化支持（这里没做）、统一的客户端返回格式（创建一个用于 ConstraintViolationException 的异常处理器来实现，代码中有但这里没有贴出来），以及批量执行全部校验，避免给用户带来挤牙膏式的体验。\n对于 Bean 与 Bean 校验器，笔者另外有两条编码建议。第一条是对校验项预置好默认的提示信息，这样当校验不通过时用户能获得明确的修正提示，以下是代码示例：\n/*** 表示一个用户的信息是无冲突的* “无冲突”是指该用户的敏感信息与其他用户不重合，如将一个注册用户的邮箱，修改成与另外一个已存在的注册用户一致的值，这便是冲突*/@Documented@Retention(RUNTIME）@Target(&#123;FIELD, METHOD, PARAMETER, TYPE&#125;)@Constraint(validatedBy AccountValidation.NotConflictAccountValidator.class)public @interface NotConflictAccount &#123;\tString message() default &quot;用户名称、邮箱、手机号码与现存用户产生重复&quot;;\tClass&lt;?[] groups() default &#123;&#125;;\tClass&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125;\n\n另外一条建议是将不带业务含义的格式校验注解放到Bcan的类定义之上，将带业务辑的校验放到Bean的类定义的外面。这两者的区别是放在类定义中的注解能够自动运行，而放到类外面的注解需要明确标出才会运行。譬如用户账号实体中的部分代码为：\npublic class Account extenda BaseEntity &#123;  @NotEmpty（message = &quot;用户不允许为空&quot;）  private String username  @NotEmpty（message = &quot;用户姓名不允许为空&quot;）  private String name；  private String avatar；  @Pattern（regexp＝&quot;1\\\\d&#123;10&#125;&quot;，message = &quot;手机号格式不正确&quot;）  private String telephone；  @Emai1（message = &quot;邮箱格式不正确&quot;）  private String email；&#125;\n\n这些校验注解都直接放在类定义中，每次执行校验的时候它们都会被运行。由于BaValidation是Java的标准规范，它执行的频率可能比编写代码的程序所预想的更高，如使用Hibernate来做持久化时，便会自动执行Data Object上的校验注解。对于那些不带业务含义的注解，运行是不需要其他外部资源参与的，即不会调用远程服务、访问数据库，这种校验重复执行也不会产生什么成本。\n但带业务逻辑的校验，通常就需要外部资源参与执行，这不仅仅是多消耗一点时间和运算资源的问题，由于很难保证依赖的每个服务都是幂等的，重复执行校验很可能会带来额外的副作用。因此应该放到外面让使用者自行判断是否触发。\n还有一些 “需要触发一部分校验” 的非典型情况，警如 “新增” 操作 A 时需要执行全部校验规则，“修改”操作B时希望不校验某个字段，“删除” 操作 C 时希望改变某一条校验规则，这时就要启用分组校验来处理，设计一套“新增”“修改”“删除”这样的标识类，置入校验注解的 groups 参数中去实现\n","categories":["凤凰架构"],"tags":["HTTP"]},{"title":"I/O模型与单服务器高性能架构","url":"/posts/10118/","content":"一、前言高性能是每个程序员的追求，无论我们是做一个系统还是写一行代码，都希望能够达到高性能的效果，而高性能又是最复杂的一环，磁盘、操作系统、CPU、内存、缓存、网络、编程语言、架构等，每个都有可能影响系统达到高性能，一行不恰当的 debug 日志，就可能将服务器的性能从 TPS 30000 降低到 8000；一个 tcp_nodelay 参数，就可能将响应时间从 2 毫秒延长到 40 毫秒。因此，要做到高性能计算是一件很复杂很有挑战的事情，软件系统开发过程中的不同阶段都关系着高性能最终是否能够实现。\n站在架构师的角度，当然需要特别关注高性能架构的设计。\n高性能架构设计主要集中在两方面：\n\n尽量提升单服务器的性能，将单服务器的性能发挥到极致。\n如果单服务器无法支撑性能，设计服务器集群方案。\n\n除了以上两点，最终系统能否实现高性能，还和具体的实现及编码相关。但架构设计是高性能的基础，如果架构设计没有做到高性能，则后面的具体实现和编码能提升的空间是有限的。形象地说，架构设计决定了系统性能的上限，实现细节决定了系统性能的下限。\n单服务器高性能的关键之一就是 服务器采取的并发模型。\n并发模型有如下两个关键设计点：\n\n服务器如何管理连接。\n服务器如何处理请求。\n\n以上两个设计点最终都和操作系统的 I/O 模型及进程模型相关。\n\nI/O 模型：阻塞、非阻塞、同步、异步。\n进程模型：单进程、多进程、多线程。\n\n什么是SocketSocket是 应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。\n在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket 接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。\n\n二、I/O模型1.常见I/O模型\n同步阻塞IO（Blocking IO）：即传统IO模型，在内核准备好数据并传输完毕之前，用户线程都处于阻塞等待状态；\n同步非阻塞IO（Non-Blocking IO）：如果数据未准备好，立即返回错误不等待。用户线程采用轮询的方式不断尝试读。根据实现不同，也可以在数据就绪时不返回内容而返回就绪标志，随后调用方再发起获取数据的调用；\nIO多路复用（IO Multiplexing）：也被称为 异步阻塞IO。是对NIO的增强，可以同时监听多个NIO句柄的状态，每次调用时将轮询所有的NIO句柄并返回所有已就绪的集合。多路复用本身也是同步IO。Java中的 Selector 和 linux 中的 epoll 都是这种模型；\n异步IO（Asynchronous IO）：也被称为 异步非阻塞IO，无论数据是否准备好都立即返回空，系统内核准备好数据并传输完毕后，通过信号或回调函数通知用户线程。对于系统调用来说，AIO的数据传输操作由操作系统内核进程来执行。\n\n同步与异步同步与异步描述的是 用户线程与内核的交互方式，即用户线程从发起IO请求后到真正得到或写入数据完成的整个过程，这个过程包括下面的阻塞或非。\n\n同步：指用户线程发起IO请求后需要等待或者轮询内核IO操作，完成后才能继续执行。\n异步：指用户线程发起IO请求后仍继续执行，当内核IO操作完成后回通知用户线程，或者调用用户线程注册的回调函数。\n\n阻塞与非阻塞阻塞与非阻塞描述的是 用户线程调用内核IO操作方式，例如：read和write方法。\n\n\n阻塞：当 read buffer 为空时，用户线程阻塞等待可读，当 write buffer 满了或容量不足时，用户线程阻塞等待可写。\n非阻塞：指IO操作（read或write）被调用后立即返回给用户一个状态值，无需等待IO操作彻底完成。\n\n举个例子：\nNIO 之所以是非阻塞的，是因为用户线程在知道不可读时，并没有阻塞自己，而是不断轮询是否可读。\n在 Java IO多路复用 中，会用一个死循环调用 select 方法，而 select 方法是会阻塞线程的，所以它是阻塞IO。\n但是一个 Selector 可以同时监控多个 socket 的状态，造成了异步的假象。只有 AIO 是真正的异步。\n2.同步阻塞IO同步阻塞IO是最简单的IO模型，用户线程在内核进行IO操作时被阻塞。用户线程通过调用系统调用read发起IO读操作，由用户空间转到内核空间。内核等到数据包到达后，然后将接受的数据拷贝到用户空间，完成read操作。整个IO请求过程，用户线程都是被阻塞的，对CPU利用率不够友好。\n\n代码示例import java.io.BufferedWriter;import java.io.IOException;import java.io.OutputStreamWriter;import java.net.ServerSocket;import java.net.Socket;public class BIOTest &#123;    public static void main(String[] args) throws IOException &#123;        ServerSocket serverSocket = new ServerSocket(8888, 1024, null);        System.out.println(&quot;服务启动&quot;);        while (true) &#123;            Socket socket = serverSocket.accept();            System.out.println(socket.getRemoteSocketAddress());            BufferedWriter bufferedWriter = new BufferedWriter(new OutputStreamWriter(socket.getOutputStream()));            bufferedWriter.append(IOUtils.buildResp());            bufferedWriter.flush();        &#125;    &#125;&#125;\n\n3.同步非阻塞IO在同步基础上，将socket设置为NONBLOCK，这样用户线程可以在发起IO请求后立即返回。虽说可以立即返回，但并未读到任何数据，用户线程需要不断的发起IO请求，直到数据到达后才能真正读到数据，然后去处理。\n 整个IO请求中，虽然可以立即返回，但是因为是同步的，为了等到数据，需要不断的轮询、重复请求，消耗了大量的CPU资源。因此，这种模型很少使用，实际用处不大。\n\n4.IO多路复用不管是同步阻塞还是同步非阻塞，对系统性能的提升都是很小的。而通过复用可以使一个或一组线程（线程池）处理多个TCP连接。IO多路复用使用两个系统调用（select/poll/epoll和recvfrom），blocking IO只调用了recvfrom。select/poll/epoll核心是可以同时处理多个connection，而不是更快，所以连接数不高的话，性能不一定比多线程+阻塞IO好。\n select是内核提供的多路分离函数，使用它可以避免同步非阻塞IO中轮询等待问题。\n\n用户首先将需要进行IO操作的socket添加到select中，然后阻塞等待select系统调用返回。当数据到达时，socket被激活，select函数返回，用户线程正式发起read请求，读取数据并继续执行。\n 这么一看，这种方式和同步阻塞IO并没有太大区别，甚至还多了添加监视socket以及调用select函数的额外操作，效率更差。但是使用select以后，用户可以在一个线程内同时处理多个socket的IO请求，这就是它的最大优势。用户可以注册多个socket，然后不断调用select读取被激活的socket，即可达到同一个线程同时处理多个IO请求的目的。而在同步阻塞模型中，必须通过多线程方式才能达到这个目的。所以IO多路复用设计目的其实不是为了快，而是为了解决线程/进程数量过多对服务器开销造成的压力。\n虽然这种方式允许单线程内处理多个IO请求，但是每个IO请求的过程还是阻塞的（在select函数上阻塞），平均时间甚至比同步阻塞IO模型还要长。如果用户线程只注册自己感兴趣的socket，然后去做自己的事情，等到数据到来时在进行处理，则可以提高CPU利用率。\n5.异步非阻塞IO 在IO多路复用模型中，事件循环文件句柄的状态事件通知给用户线程，由用户线程自行读取数据、处理数据。而异步IO中，当用户线程收到通知时候，数据已经被内核读取完毕，并放在了用户线程指定的缓冲区内，内核在IO完成后通知用户线程直接使用就行了。因此这种模型需要操作系统更强的支持，把read操作从用户线程转移到了内核。\n 相比于IO多路复用模型，异步IO并不十分常用，不少高性能并发服务程序使用IO多路复用+多线程任务处理的架构基本可以满足需求。不过最主要原因还是操作系统对异步IO的支持并非特别完善，更多的采用IO多路复用模拟异步IO方式（IO事件触发时不直接通知用户线程，而是将数据读写完毕后放到用户指定的缓冲区）。\n三、单服务器高性能架构1.PPCPPC 是 Process Per Connection 的缩写，其含义是指每次有新的连接就新建一个进程去专门处理这个连接的请求，这是传统的 UNIX 网络服务器所采用的模型。基本的流程图是：\n\n\n父进程接受连接（图中 accept）\n父进程“fork”子进程（图中 fork）\n子进程处理连接的读写请求（图中子进程 read、业务处理、write）\n子进程关闭连接（图中子进程中的 close）\n\n注意，图中有一个小细节，父进程“fork”子进程后，直接调用了 close，看起来好像是关闭了连接，其实只是将连接的文件描述符引用计数减一，真正的关闭连接是等子进程也调用 close 后，连接对应的文件描述符引用计数变为 0 后，操作系统才会真正关闭连接，更多细节请参考《UNIX 网络编程：卷一》。\nPPC 模式实现简单，比较适合服务器的连接数没那么多的情况，例如数据库服务器。对于普通的业务服务器，在互联网兴起之前，由于服务器的访问量和并发量并没有那么大，这种模式其实运作得也挺好，世界上第一个 web 服务器 CERN httpd 就采用了这种模式（具体你可以参考https://en.wikipedia.org/wiki/CERN_httpd）。互联网兴起后，服务器的并发和访问量从几十剧增到成千上万，这种模式的弊端就凸显出来了，主要体现在这几个方面：\n\nfork 代价高：站在操作系统的角度，创建一个进程的代价是很高的，需要分配很多内核资源，需要将内存映像从父进程复制到子进程。即使现在的操作系统在复制内存映像时用到了 Copy on Write（写时复制）技术，总体来说创建进程的代价还是很大的。\n父子进程通信复杂：父进程“fork”子进程时，文件描述符可以通过内存映像复制从父进程传到子进程，但“fork”完成后，父子进程通信就比较麻烦了，需要采用 IPC（Interprocess Communication）之类的进程通信方案。例如，子进程需要在 close 之前告诉父进程自己处理了多少个请求以支撑父进程进行全局的统计，那么子进程和父进程必须采用 IPC 方案来传递信息。\n支持的并发连接数量有限：如果每个连接存活时间比较长，而且新的连接又源源不断的进来，则进程数量会越来越多，操作系统进程调度和切换的频率也越来越高，系统的压力也会越来越大。因此，一般情况下，PPC 方案能处理的并发连接数量最大也就几百。\n\npreforkPPC 模式中，当连接进来时才 fork 新进程来处理连接请求，由于 fork 进程代价高，用户访问时可能感觉比较慢，prefork 模式的出现就是为了解决这个问题。\n顾名思义，prefork 就是提前创建进程（pre-fork）。系统在启动的时候就预先创建好进程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去 fork 进程的操作，让用户访问更快、体验更好。prefork 的基本示意图是：\n\nprefork 的实现关键就是多个子进程都 accept 同一个 socket，当有新的连接进入时，操作系统保证只有一个进程能最后 accept 成功。\n但这里也存在一个小小的问题：“惊群”现象，就是指虽然只有一个子进程能 accept 成功，但所有阻塞在 accept 上的子进程都会被唤醒，这样就导致了不必要的进程调度和上下文切换了。\n幸运的是，操作系统可以解决这个问题，例如 Linux 2.6 版本后内核已经解决了 accept 惊群问题。\nprefork 模式和 PPC 一样，还是存在父子进程通信复杂、支持的并发连接数量有限的问题，因此目前实际应用也不多。Apache 服务器提供了 MPM prefork 模式，推荐在需要可靠性或者与旧软件兼容的站点时采用这种模式，默认情况下最大支持 256 个并发连接。\n2.TPCTPC 是 Thread Per Connection 的缩写，其含义是指每次有新的连接就新建一个线程去专门处理这个连接的请求。与进程相比，线程更轻量级，创建线程的消耗比进程要少得多；同时多线程是共享进程内存空间的，线程通信相比进程通信更简单。因此，TPC 实际上是解决或者弱化了 PPC fork 代价高的问题和父子进程通信复杂的问题。\nTPC 的基本流程是：\n\n\n父进程接受连接（图中 accept）\n父进程创建子线程（图中 pthread）\n子线程处理连接的读写请求（图中子线程 read、业务处理、write）\n子线程关闭连接（图中子线程中的 close）\n\n注意，和 PPC 相比，主进程不用“close”连接了。原因是在于子线程是共享主进程的进程空间的，连接的文件描述符并没有被复制，因此只需要一次 close 即可。\nTPC 虽然解决了 fork 代价高和进程通信复杂的问题，但是也引入了新的问题，具体表现在：\n\n创建线程虽然比创建进程代价低，但并不是没有代价，高并发时（例如每秒上万连接）还是有性能问题。\n无须进程间通信，但是线程间的互斥和共享又引入了复杂度，可能一不小心就导致了死锁问题。\n多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出（例如内存越界）。\n\n除了引入了新的问题，TPC 还是存在 CPU 线程调度和切换代价的问题。因此，TPC 方案本质上和 PPC 方案基本类似，在并发几百连接的场景下，反而更多地是采用 PPC 的方案，因为 PPC 方案不会有死锁的风险，也不会多进程互相影响，稳定性更高。\nprethreadTPC 模式中，当连接进来时才创建新的线程来处理连接请求，虽然创建线程比创建进程要更加轻量级，但还是有一定的代价，而 prethread 模式就是为了解决这个问题。\n和 prefork 类似，prethread 模式会预先创建线程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去创建线程的操作，让用户感觉更快、体验更好。\n由于多线程之间数据共享和通信比较方便，因此实际上 prethread 的实现方式相比 prefork 要灵活一些，常见的实现方式有下面几种：\n\n主进程 accept，然后将连接交给某个线程处理。\n子线程都尝试去 accept，最终只有一个线程 accept 成功，方案的基本示意图如下：\n\n\nApache 服务器的 MPM worker 模式本质上就是一种 prethread 方案，但稍微做了改进。Apache 服务器会首先创建多个进程，每个进程里面再创建多个线程，这样做主要是为了考虑稳定性，即：即使某个子进程里面的某个线程异常导致整个子进程退出，还会有其他子进程继续提供服务，不会导致整个服务器全部挂掉。\nprethread 理论上可以比 prefork 支持更多的并发连接，Apache 服务器 MPM worker 模式默认支持 16 × 25 = 400 个并发处理线程。\n小结对于 PPC 和 TPC，由于其连接数有限，所以比较适合以下场景：\n\n常量连接海量请求，如数据库、mq、中间件等。\n常量连接常量请求，如内部运营系统、管理系统、门户网站等。\n\n3.ReactorPPC 模式最主要的问题就是每个连接都要创建进程（为了描述简洁，这里只以 PPC 和进程为例，实际上换成 TPC 和线程，原理是一样的），连接结束后进程就销毁了，这样做其实是很大的浪费。为了解决这个问题，一个自然而然的想法就是资源复用，即不再单独为每个连接创建进程，而是创建一个进程池，将连接分配给进程，一个进程可以处理多个连接的业务。\n引入资源池的处理方式后，会引出一个新的问题：进程如何才能高效地处理多个连接的业务？当一个连接一个进程时，进程可以采用“read -&gt; 业务处理 -&gt; write”的处理流程，如果当前连接没有数据可以读，则进程就阻塞在 read 操作上。这种阻塞的方式在一个连接一个进程的场景下没有问题，但如果一个进程处理多个连接，进程阻塞在某个连接的 read 操作上，此时即使其他连接有数据可读，进程也无法去处理，很显然这样是无法做到高性能的。\n解决这个问题的最简单的方式是将 read 操作改为非阻塞，然后进程不断地轮询多个连接。这种方式能够解决阻塞的问题，但解决的方式并不优雅。首先，轮询是要消耗 CPU 的；其次，如果一个进程处理几千上万的连接，则轮询的效率是很低的。（例如NIO）\n为了能够更好地解决上述问题，很容易可以想到，只有当连接上有数据的时候进程才去处理，这就是 I/O 多路复用技术的来源。\nI/O 多路复用技术归纳起来有两个关键实现点：\n\n当多条连接共用一个阻塞对象后，进程只需要在一个阻塞对象上等待，而无须再轮询所有连接，常见的实现方式有 select、epoll、kqueue 等。\n当某条连接有新的数据可以处理时，操作系统会通知进程，进程从阻塞状态返回，开始进行业务处理。\n\nI/O 多路复用结合线程池，完美地解决了 PPC 和 TPC 的问题，而且“大神们”给它取了一个很牛的名字：Reactor，中文是“反应堆”。联想到“核反应堆”，听起来就很吓人，实际上这里的“反应”不是聚变、裂变反应的意思，而是“事件反应”的意思，可以通俗地理解为“来了一个事件我就有相应的反应”，这里的“我”就是 Reactor，具体的反应就是我们写的代码，Reactor 会根据事件类型来调用相应的代码进行处理。Reactor 模式也叫 Dispatcher 模式（在很多开源的系统里面会看到这个名称的类，其实就是实现 Reactor 模式的），更加贴近模式本身的含义，即 I/O 多路复用统一监听事件，收到事件后分配（Dispatch）给某个进程。\nReactor 模式的核心组成部分包括 Reactor 和处理资源池（进程池或线程池），其中 Reactor 负责监听和分配事件，处理资源池负责处理事件。初看 Reactor 的实现是比较简单的，但实际上结合不同的业务场景，Reactor 模式的具体实现方案灵活多变，主要体现在：\n\nReactor 的数量可以变化：可以是一个 Reactor，也可以是多个 Reactor。\n资源池的数量可以变化：以进程为例，可以是单个进程，也可以是多个进程（线程类似）。\n\n将上面两个因素排列组合一下，理论上可以有 4 种选择，但由于“多 Reactor 单进程”实现方案相比“单 Reactor 单进程”方案，既复杂又没有性能优势，因此“多 Reactor 单进程”方案仅仅是一个理论上的方案，实际没有应用。\n最终 Reactor 模式有这三种典型的实现方案：\n\n单 Reactor 单进程 / 线程\n单 Reactor 多线程\n多 Reactor 多进程 / 线程\n\n以上方案具体选择进程还是线程，更多地是和编程语言及平台相关。例如，Java 语言一般使用线程（例如，Netty），C 语言使用进程和线程都可以。例如，Nginx 使用进程，Memcache 使用线程。\n单 Reactor 单进程 / 线程单 Reactor 单进程 / 线程的方案示意图如下（以进程为例）：\n\n注意，select、accept、read、send 是标准的网络编程 API，dispatch 和“业务处理”是需要完成的操作，其他方案示意图类似。\n详细说明一下这个方案：\n\nReactor 对象通过 select 监控连接事件，收到事件后通过 dispatch 进行分发。\n如果是连接建立的事件，则由 Acceptor 处理，Acceptor 通过 accept 接受连接，并创建一个 Handler 来处理连接后续的各种事件。\n如果不是连接建立事件，则 Reactor 会调用连接对应的 Handler（第 2 步中创建的 Handler）来进行响应。\nHandler 会完成 read-&gt; 业务处理 -&gt;send 的完整业务流程。\n\n单 Reactor 单进程的模式优点就是很简单，没有进程间通信，没有进程竞争，全部都在同一个进程内完成。\n但其缺点也是非常明显，具体表现有：\n\n只有一个进程，无法发挥多核 CPU 的性能；只能采取部署多个系统来利用多核 CPU，但这样会带来运维复杂度，本来只要维护一个系统，用这种方式需要在一台机器上维护多套系统。\nHandler 在处理某个连接上的业务时，整个进程无法处理其他连接的事件，很容易导致性能瓶颈。\n\n因此，单 Reactor 单进程的方案在实践中应用场景不多，只适用于业务处理非常快速的场景，目前比较著名的开源软件中使用单 Reactor 单进程的是 Redis。\n需要注意的是，C 语言编写系统的一般使用单 Reactor 单进程，因为没有必要在进程中再创建线程；\n而 Java 语言编写的一般使用单 Reactor 单线程，因为 Java 虚拟机是一个进程，虚拟机中有很多线程，业务线程只是其中的一个线程而已。\n单 Reactor 多线程为了克服单 Reactor 单进程 / 线程方案的缺点，引入多进程 / 多线程是显而易见的，这就产生了第 2 个方案：单 Reactor 多线程。\n单 Reactor 多线程方案示意图是：\n\n介绍一下这个方案：\n\n主线程中，Reactor 对象通过 select 监控连接事件，收到事件后通过 dispatch 进行分发。\n如果是连接建立的事件，则由 Acceptor 处理，Acceptor 通过 accept 接受连接，并创建一个 Handler 来处理连接后续的各种事件。\n如果不是连接建立事件，则 Reactor 会调用连接对应的 Handler（第 2 步中创建的 Handler）来进行响应。\nHandler 只负责响应事件，不进行业务处理；Handler 通过 read 读取到数据后，会发给 Processor 进行业务处理。\nProcessor 会在独立的子线程中完成真正的业务处理，然后将响应结果发给主进程的 Handler 处理；Handler 收到响应后通过 send 将响应结果返回给 client。\n\n单 Reator 多线程方案能够充分利用多核多 CPU 的处理能力，但同时也存在下面的问题：\n\n多线程数据共享和访问比较复杂。例如，子线程完成业务处理后，要把结果传递给主线程的 Reactor 进行发送，这里涉及共享数据的互斥和保护机制。以 Java 的 NIO 为例，Selector 是线程安全的，但是通过 Selector.selectKeys() 返回的键的集合是非线程安全的，对 selected keys 的处理必须单线程处理或者采取同步措施进行保护。\nReactor 承担所有事件的监听和响应，只在主线程中运行，瞬间高并发时会成为性能瓶颈。\n\n你可能会发现，我只列出了“单 Reactor 多线程”方案，没有列出“单 Reactor 多进程”方案，这是什么原因呢？主要原因在于如果采用多进程，子进程完成业务处理后，将结果返回给父进程，并通知父进程发送给哪个 client，这是很麻烦的事情。因为父进程只是通过 Reactor 监听各个连接上的事件然后进行分配，子进程与父进程通信时并不是一个连接。如果要将父进程和子进程之间的通信模拟为一个连接，并加入 Reactor 进行监听，则是比较复杂的。而采用多线程时，因为多线程是共享数据的，因此线程间通信是非常方便的。虽然要额外考虑线程间共享数据时的同步问题，但这个复杂度比进程间通信的复杂度要低很多。\n多 Reactor 多进程 / 线程为了解决单 Reactor 多线程的问题，最直观的方法就是将单 Reactor 改为多 Reactor，这就产生了第 3 个方案：多 Reactor 多进程 / 线程。\n多 Reactor 多进程 / 线程方案示意图是（以进程为例）：\n\n方案详细说明如下：\n\n父进程中 mainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 接收，将新的连接分配给某个子进程。\n子进程的 subReactor 将 mainReactor 分配的连接加入连接队列进行监听，并创建一个 Handler 用于处理连接的各种事件。\n当有新的事件发生时，subReactor 会调用连接对应的 Handler（即第 2 步中创建的 Handler）来进行响应。\nHandler 完成 read→业务处理→send 的完整业务流程。\n\n多 Reactor 多进程 / 线程的方案看起来比单 Reactor 多线程要复杂，但实际实现时反而更加简单，主要原因是：\n\n父进程和子进程的职责非常明确，父进程只负责接收新连接，子进程负责完成后续的业务处理。\n父进程和子进程的交互很简单，父进程只需要把新连接传给子进程，子进程无须返回数据。\n子进程之间是互相独立的，无须同步共享之类的处理（这里仅限于网络模型相关的 select、read、send 等无须同步共享，“业务处理”还是有可能需要同步共享的）。\n\n目前著名的开源系统 Nginx 采用的是多 Reactor 多进程，采用多 Reactor 多线程的实现有 Memcache 和 Netty。\n我多说一句，Nginx 采用的是多 Reactor 多进程的模式，但方案与标准的多 Reactor 多进程有差异。具体差异表现为主进程中仅仅创建了监听端口，并没有创建 mainReactor 来“accept”连接，而是由子进程的 Reactor 来“accept”连接，通过锁来控制一次只有一个子进程进行“accept”，子进程“accept”新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程，更多细节请查阅相关资料或阅读 Nginx 源码。\n4.ProactorReactor 是非阻塞同步网络模型，因为真正的 read 和 send 操作都需要用户进程同步操作。这里的“同步”指用户进程在执行 read 和 send 这类 I/O 操作的时候是同步的，如果把 I/O 操作改为异步就能够进一步提升性能，这就是异步网络模型 Proactor。\nProactor 中文翻译为“前摄器”比较难理解，与其类似的单词是 proactive，含义为“主动的”，因此我们照猫画虎翻译为“主动器”反而更好理解。Reactor 可以理解为“来了事件我通知你，你来处理”，而 Proactor 可以理解为 “来了事件我来处理，处理完了我通知你”。\n这里的“我”就是操作系统内核，“事件”就是有新连接、有数据可读、有数据可写的这些 I/O 事件，“你”就是我们的程序代码。\nProactor 模型示意图是：\n\n详细介绍一下 Proactor 方案：\n\nProactor Initiator 负责创建 Proactor 和 Handler，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核。\nAsynchronous Operation Processor 负责处理注册请求，并完成 I/O 操作。\nAsynchronous Operation Processor 完成 I/O 操作后通知 Proactor。\nProactor 根据不同的事件类型回调不同的 Handler 进行业务处理。\nHandler 完成业务处理，Handler 也可以注册新的 Handler 到内核进程。\n\n理论上 Proactor 比 Reactor 效率要高一些，异步 I/O 能够充分利用 DMA 特性，让 I/O 操作与计算重叠，但要实现真正的异步 I/O，操作系统需要做大量的工作。\n目前 Windows 下通过 IOCP 实现了真正的异步 I/O，而在 Linux 系统下的 AIO 并不完善，因此在 Linux 下实现高并发网络编程时都是以 Reactor 模式为主。\n所以即使 Boost.Asio 号称实现了 Proactor 模型，其实它在 Windows 下采用 IOCP，而在 Linux 下是用 Reactor 模式（采用 epoll）模拟出来的异步模型。\n","categories":["Java","IO"]},{"title":"Java NIO：Buffer、Channel 和 Selector","url":"/posts/46709/","content":"一、Buffer一个 Buffer 本质上是内存中的一块，我们可以将数据写入这块内存，之后从这块内存获取数据。\njava.nio 定义了以下几个 Buffer 的实现。\n\n其实核心是最后的 ByteBuffer，前面的一大串类只是包装了一下它而已，我们使用最多的通常也是 ByteBuffer。\n我们应该将 Buffer 理解为一个数组，IntBuffer、CharBuffer、DoubleBuffer 等分别对应 int[]、char[]、double[] 等。\nMappedByteBuffer 用于实现内存映射文件，也不是本文关注的重点。\n我觉得操作 Buffer 和操作数组、类集差不多，只不过大部分时候我们都把它放到了 NIO 的场景里面来使用而已。下面介绍 Buffer 中的几个重要属性和几个重要方法。\nposition、limit、capacity就像数组有数组容量，每次访问元素要指定下标，Buffer 中也有几个重要属性：position、limit、capacity。\n\n最好理解的当然是 capacity，它代表这个缓冲区的容量，一旦设定就不可以更改。比如 capacity 为 1024 的 IntBuffer，代表其一次可以存放 1024 个 int 类型的值。一旦 Buffer 的容量达到 capacity，需要清空 Buffer，才能重新写入值。\nposition 和 limit 是变化的，我们分别看下读和写操作下，它们是如何变化的。\nposition 的初始值是 0，每往 Buffer 中写入一个值，position 就自动加 1，代表下一次的写入位置。读操作的时候也是类似的，每读一个值，position 就自动加 1。\n从写操作模式到读操作模式切换的时候（flip），position 都会归零，这样就可以从头开始读写了。\nLimit：写操作模式下，limit 代表的是最大能写入的数据，这个时候 limit 等于 capacity。写结束后，切换到读模式，此时的 limit 等于 Buffer 中实际的数据大小，因为 Buffer 不一定被写满了。\n\n初始化 Buffer每个 Buffer 实现类都提供了一个静态方法 allocate(int capacity) 帮助我们快速实例化一个 Buffer。如：\nByteBuffer byteBuf = ByteBuffer.allocate(1024);IntBuffer intBuf = IntBuffer.allocate(1024);LongBuffer longBuf = LongBuffer.allocate(1024);// ...\n\n另外，我们经常使用 wrap 方法来初始化一个 Buffer。\npublic static ByteBuffer wrap(byte[] array) &#123;    ...&#125;\n\n填充 Buffer各个 Buffer 类都提供了一些 put 方法用于将数据填充到 Buffer 中，如 ByteBuffer 中的几个 put 方法：\n// 填充一个 byte 值public abstract ByteBuffer put(byte b);// 在指定位置填充一个 int 值public abstract ByteBuffer put(int index, byte b);// 将一个数组中的值填充进去public final ByteBuffer put(byte[] src) &#123;...&#125;public ByteBuffer put(byte[] src, int offset, int length) &#123;...&#125;\n\n上述这些方法需要自己控制 Buffer 大小，不能超过 capacity，超过会抛 java.nio.BufferOverflowException 异常。\n对于 Buffer 来说，另一个常见的操作中就是，我们要将来自 Channel 的数据填充到 Buffer 中，在系统层面上，这个操作我们称为读操作，因为数据是从外部（文件或网络等）读到内存中。\nint num = channel.read(buf);\n\n上述方法会返回从 Channel 中读入到 Buffer 的数据大小。\n提取 Buffer 中的值前面介绍了写操作，每写入一个值，position 的值都需要加 1，所以 position 最后会指向最后一次写入的位置的后面一个，如果 Buffer 写满了，那么 position 等于 capacity（position 从 0 开始）。\n如果要读 Buffer 中的值，需要切换模式，从写入模式切换到读出模式。注意，通常在说 NIO 的读操作的时候，我们说的是从 Channel 中读数据到 Buffer 中，对应的是对 Buffer 的写入操作，初学者需要理清楚这个。\n调用 Buffer 的 flip() 方法，可以从写入模式切换到读取模式。其实这个方法也就是设置了一下 position 和 limit 值罢了。\npublic final Buffer flip() &#123;    limit = position; // 将 limit 设置为实际写入的数据数量    position = 0; // 重置 position 为 0    mark = -1; // mark 之后再说    return this;&#125;\n\n对应写入操作的一系列 put 方法，读操作提供了一系列的 get 方法：\n// 根据 position 来获取数据public abstract byte get();// 获取指定位置的数据public abstract byte get(int index);// 将 Buffer 中的数据写入到数组中public ByteBuffer get(byte[] dst)\n\n附一个经常使用的方法：\nnew String(buffer.array()).trim();\n\n当然了，除了将数据从 Buffer 取出来使用，更常见的操作是将我们写入的数据传输到 Channel 中，如通过 FileChannel 将数据写入到文件中，通过 SocketChannel 将数据写入网络发送到远程机器等。对应的，这种操作，我们称之为写操作。\nint num = channel.write(buf);\n\nmark() &amp; reset()除了 position、limit、capacity 这三个基本的属性外，还有一个常用的属性就是 mark。\nmark 用于临时保存 position 的值，每次调用 mark() 方法都会将 mark 设值为当前的 position，便于后续需要的时候使用。\npublic final Buffer mark() &#123;    mark = position;    return this;&#125;\n\n那到底什么时候用呢？考虑以下场景，我们在 position 为 5 的时候，先 mark() 一下，然后继续往下读，读到第 10 的时候，我想重新回到 position 为 5 的地方重新来一遍，那只要调一下 reset() 方法，position 就回到 5 了。\npublic final Buffer reset() &#123;    int m = mark;    if (m &lt; 0)        throw new InvalidMarkException();    position = m;    return this;&#125;\n\nrewind() &amp; clear() &amp; compact()**rewind()**：会重置 position 为 0，通常用于重新从头读写 Buffer。\npublic final Buffer rewind() &#123;    position = 0;    mark = -1;    return this;&#125;\n\n**clear()**：有点重置 Buffer 的意思，相当于重新实例化了一样。\n通常，我们会先填充 Buffer，然后从 Buffer 读取数据，之后我们再重新往里填充新的数据，我们一般在重新填充之前先调用 clear()。\npublic final Buffer clear() &#123;    position = 0;    limit = capacity;    mark = -1;    return this;&#125;\n\n**compact()**：和 clear() 一样的是，它们都是在准备往 Buffer 填充新的数据之前调用。\n前面说的 clear() 方法会重置几个属性，但是我们要看到，clear() 方法并不会将 Buffer 中的数据清空，只不过后续的写入会覆盖掉原来的数据，也就相当于清空了数据了。\n而 compact() 方法有点不一样，调用这个方法以后，会先处理还没有读取的数据，也就是 position 到 limit 之间的数据（还没有读过的数据），先将这些数据移到左边，然后在这个基础上再开始写入。很明显，此时 limit 还是等于 capacity，position 指向原来数据的右边。\n二、Channel所有的 NIO 操作始于通道，通道是数据来源或数据写入的目的地，主要地，我们将关心 java.nio 包中实现的以下几个 Channel：\n\n\nFileChannel：文件通道，用于文件的读和写\nDatagramChannel：用于 UDP 连接的接收和发送\nSocketChannel：把它理解为 TCP 连接通道，简单理解就是 TCP 客户端\nServerSocketChannel：TCP 对应的服务端，用于监听某个端口进来的请求\n\n这里不是很理解这些也没关系，后面介绍了代码之后就清晰了。还有，我们最应该关注，也是后面将会重点介绍的是 SocketChannel 和 ServerSocketChannel。\nChannel 经常翻译为通道，类似 IO 中的流，用于读取和写入。它与前面介绍的 Buffer 打交道，读操作的时候将 Channel 中的数据填充到 Buffer 中，而写操作时将 Buffer 中的数据写入到 Channel 中。\n\n\n至少读者应该记住一点，这两个方法都是 channel 实例的方法。\nFileChannel我想文件操作对于大家来说应该是最熟悉的，不过我们在说 NIO 的时候，其实 FileChannel 并不是关注的重点。而且后面我们说非阻塞的时候会看到，FileChannel 是不支持非阻塞的。\n这里算是简单介绍下常用的操作吧，感兴趣的读者瞄一眼就是了。\n初始化：\nFileInputStream inputStream = new FileInputStream(new File(&quot;/data.txt&quot;));FileChannel fileChannel = inputStream.getChannel();\n\n当然了，我们也可以从 RandomAccessFile#getChannel 来得到 FileChannel。\n读取文件内容：\nByteBuffer buffer = ByteBuffer.allocate(1024);int num = fileChannel.read(buffer);\n\n前面我们也说了，所有的 Channel 都是和 Buffer 打交道的。\n写入文件内容：\nByteBuffer buffer = ByteBuffer.allocate(1024);buffer.put(&quot;随机写入一些内容到 Buffer 中&quot;.getBytes());// Buffer 切换为读模式buffer.flip();while(buffer.hasRemaining()) &#123;    // 将 Buffer 中的内容写入文件    fileChannel.write(buffer);&#125;\n\nSocketChannel我们前面说了，我们可以将 SocketChannel 理解成一个 TCP 客户端。虽然这么理解有点狭隘，因为我们在介绍 ServerSocketChannel 的时候会看到另一种使用方式。\n打开一个 TCP 连接：\nSocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(&quot;https://www.javadoop.com&quot;, 80));\n\n当然了，上面的这行代码等价于下面的两行：\n// 打开一个通道SocketChannel socketChannel = SocketChannel.open();// 发起连接socketChannel.connect(new InetSocketAddress(&quot;https://www.javadoop.com&quot;, 80));\n\nSocketChannel 的读写和 FileChannel 没什么区别，就是操作缓冲区。\n// 读取数据socketChannel.read(buffer);// 写入数据到网络连接中while(buffer.hasRemaining()) &#123;    socketChannel.write(buffer);   &#125;\n\n不要在这里停留太久，先继续往下走。\nServerSocketChannel之前说 SocketChannel 是 TCP 客户端，这里说的 ServerSocketChannel 就是对应的服务端。\nServerSocketChannel 用于监听机器端口，管理从这个端口进来的 TCP 连接。\n// 实例化ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();// 监听 8080 端口serverSocketChannel.socket().bind(new InetSocketAddress(8080));while (true) &#123;    // 一旦有一个 TCP 连接进来，就对应创建一个 SocketChannel 进行处理    SocketChannel socketChannel = serverSocketChannel.accept();&#125;\n\n\n这里我们可以看到 SocketChannel 的第二个实例化方式\n\n到这里，我们应该能理解 SocketChannel 了，它不仅仅是 TCP 客户端，它代表的是一个网络通道，可读可写。\nServerSocketChannel 不和 Buffer 打交道了，因为它并不实际处理数据，它一旦接收到请求后，实例化 SocketChannel，之后在这个连接通道上的数据传递它就不管了，因为它需要继续监听端口，等待下一个连接。\nDatagramChannelUDP 和 TCP 不一样，DatagramChannel 一个类处理了服务端和客户端。\n\n科普一下，UDP 是面向无连接的，不需要和对方握手，不需要通知对方，就可以直接将数据包投出去，至于能不能送达，它是不知道的\n\n监听端口：\nDatagramChannel channel = DatagramChannel.open();channel.socket().bind(new InetSocketAddress(9090));复制代码ByteBuffer buf = ByteBuffer.allocate(48);channel.receive(buf);\n\n发送数据：\nString newData = &quot;New String to write to file...&quot;                    + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.put(newData.getBytes());buf.flip();int bytesSent = channel.send(buf, new InetSocketAddress(&quot;jenkov.com&quot;, 80));\n\n三、SelectorNIO 三大组件就剩 Selector 了，Selector 建立在非阻塞的基础之上，大家经常听到的 多路复用 在 Java 世界中指的就是它，用于实现一个线程管理多个 Channel。\n读者在这一节不能消化 Selector 也没关系，因为后续在介绍非阻塞 IO 的时候还得说到这个，这里先介绍一些基本的接口操作。\n\n首先，我们开启一个 Selector。你们爱翻译成选择器也好，多路复用器也好。\nSelector selector = Selector.open();\n将 Channel 注册到 Selector 上。前面我们说了，Selector 建立在非阻塞模式之上，所以注册到 Selector 的 Channel 必须要支持非阻塞模式，FileChannel 不支持非阻塞，我们这里讨论最常见的 SocketChannel 和 ServerSocketChannel。\n// 将通道设置为非阻塞模式，因为默认都是阻塞模式的channel.configureBlocking(false);// 注册SelectionKey key = channel.register(selector, SelectionKey.OP_READ);\n\nregister 方法的第二个 int 型参数（使用二进制的标记位）用于表明需要监听哪些感兴趣的事件，共以下四种事件：\n\nSelectionKey.OP_READ\n\n对应 00000001，通道中有数据可以进行读取\n\n\nSelectionKey.OP_WRITE\n\n对应 00000100，可以往通道中写入数据\n\n\nSelectionKey.OP_CONNECT\n\n对应 00001000，成功建立 TCP 连接\n\n\nSelectionKey.OP_ACCEPT\n\n对应 00010000，接受 TCP 连接\n\n\n\n我们可以同时监听一个 Channel 中的发生的多个事件，比如我们要监听 ACCEPT 和 READ 事件，那么指定参数为二进制的 00010001 即十进制数值 17 即可。\n注册方法返回值是 SelectionKey 实例，它包含了 Channel 和 Selector 信息，也包括了一个叫做 Interest Set 的信息，即我们设置的我们感兴趣的正在监听的事件集合。\n\n调用 select() 方法获取通道信息。用于判断是否有我们感兴趣的事件已经发生了。\n\n\nSelector 的操作就是以上 3 步，这里来一个简单的示例，大家看一下就好了。之后在介绍非阻塞 IO 的时候，会演示一份可执行的示例代码。\nSelector selector = Selector.open();channel.configureBlocking(false);SelectionKey key = channel.register(selector, SelectionKey.OP_READ);while(true) &#123;  // 判断是否有事件准备好  int readyChannels = selector.select();  if(readyChannels == 0) continue;  // 遍历  Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys();  Iterator&lt;SelectionKey&gt; keyIterator = selectedKeys.iterator();  while(keyIterator.hasNext()) &#123;    SelectionKey key = keyIterator.next();    if(key.isAcceptable()) &#123;        // a connection was accepted by a ServerSocketChannel.    &#125; else if (key.isConnectable()) &#123;        // a connection was established with a remote server.    &#125; else if (key.isReadable()) &#123;        // a channel is ready for reading    &#125; else if (key.isWritable()) &#123;        // a channel is ready for writing    &#125;    keyIterator.remove();  &#125;&#125;\n\n对于 Selector，我们还需要非常熟悉以下几个方法：\n\nselect()\n调用此方法，会将上次 select 之后的准备好的 channel 对应的 SelectionKey 复制到 selected set 中。如果没有任何通道准备好，这个方法会阻塞，直到至少有一个通道准备好。\n\nselectNow()\n功能和 select 一样，区别在于如果没有准备好的通道，那么此方法会立即返回 0。\n\nselect(long timeout)\n看了前面两个，这个应该很好理解了，如果没有通道准备好，此方法会等待一会\n\nwakeup()\n这个方法是用来唤醒等待在 select() 和 select(timeout) 上的线程的。如果 wakeup() 先被调用，此时没有线程在 select 上阻塞，那么之后的一个 select() 或 select(timeout) 会立即返回，而不会阻塞，当然，它只会作用一次。\n\n\n","categories":["Java","IO"]},{"title":"G1 垃圾收集器","url":"/posts/63375/","content":"一、介绍Gartage First (简称G1)收集器是垃圾收集器技术发展历史上的里程碑式的成果，它开创了收集器面向局部收集的设计思路和基于 Region 的内存布局形式。\n早在 JDK7 刚刚确立项目目标、Oracle公司制定的 JDK7 RoadMap 里面，G1 收集器就被视作 JDK7 中 HotSpot 虚拟机的一项重要进化特征。从JDK 6 Update 14开始就有Early Access版本的 G1 收集器供开发人员实验和试用，但由此开始 G1 收集器的“实验状态”（Experimental）持续了数年时间，直至JDK 7 Update 4，Oracle 才认为它达到足够成熟的商用程度，移除了 “Experimental” 的标识；\n到了JDK 8 Update 40的时候，G1 提供并发的类卸载的支持，补全了其计划功能的最后一块拼图。这个版本以后的 G1 收集器才被 Oracle 官方称为 “全功能的垃圾收集器” （Fully-Featured Garbage Collector）。\nG1是一款主要面向服务端应用的垃圾收集器。\nHotSpot 开发团队最初赋予它的期望是（在比较长期的）未来可以替换掉 JDK 5 中发布的 CMS 收集器。现在这个期望目标已经实现过半了，JDK9 发布之日，G1 宣告取代 Parallel Scavenge 加 Parallel Old 组合，成为服务端模式下的默认垃圾收集器，而 CMS 则沦落至被声明为不推荐使用（Deprecate）的收集器日。\n如果对 JDK9 及以上版本的 HotSpot 虚拟机使用参数  -XX:+UseConcMarkSweepGC 来开启 CMS 收集器的话，用户会收到一个警告信息，提示 CMS 未来将会被废弃：\n\nJavaHot Spot (TM)64-BitServer VMwarning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\n\n但作为一款曾被广泛运用过的收集器，经过多个版本的开发迭代后，CMS（以及之前几款收集器）的代码与 HotSpot 的内存管理、执行、编译、监控等子系统都有千丝万缕的联系，这是历史原因导致的，并不符合职责分离的设计原则。\n为此，规划 JDK 10 功能目标时，HotSpot虚拟机提出了 “统一垃圾收集器接口”，将内存回收的 “行为” 与 “实现” 进行分离，CMS 以及其他收集器都重构成基于这套接口的一种实现。以此为基础，日后要移除或者加入某一款收集器，都会变得容易许多，风险也可以控制，这算是在为 CMS 退出历史舞台铺下最后的道路了。\n作为 CMS 收集器的替代者和继承人，设计者们希望做出一款能够建立起 “停顿时间模型” （Pause Prediction Model）的收集器，停顿时间模型的意思是能够支持指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间大概率不超过N毫秒这样的目标，这几乎已经是实时 Java（RTSJ）的中软实时垃圾收集器特征了。\n二、G1设计思想在 G1 收集器出现之前的所有其他收集器，包括 CMS 在内，垃圾收集的目标范围要么是整个新生代（Minor GC），要么就是整个老年代（Major GC），再要么就是整个Java堆（Full GC）。\n而 G1 跳出了这个樊笼，它可以面向堆内存任何部分来组成 回收集（Collection Set，一般简称 CSet）进行回收，衡量标准不再是它属于哪个分代，而是哪块内存中存放的垃圾数量最多，回收收益最大，这就是 G1 收集器的 Mixed GC 模式。\nG1 开创的基于 Region 的堆内存布局是它能够实现这个目标的关键。\n三、重要概念1. RegionG1 不再坚持固定大小以及固定数量的分代区域划分，而是把连续的 Java 堆划分为多个大小相等的独立区域（Region），每一个 Region 都可以根据需要，扮演新生代的 Eden 空间、Survivor 空间，或者老年代空间。\n收集器能够对扮演不同角色的 Region 采用不同的策略去处理，这样无论是新创建的对象还是已经存活了一段时间、熬过多次收集的旧对象都能获取很好的收集效果。\nRegion 中还有一类特殊的 Humongous 区域，专门用来存储大对象。\nG1 认为只要大小超过了一个 Region 容量一半的对象即可判定为大对象。\n每个 Region 的大小可以通过参数 -XX:G1HeapRegionSize 设定，取值范围为1MB～32MB，且应为 2 的N次幂。如果不设定，那么G1会根据Heap大小自动决定。\n而对于那些超过了整个 Region 容量的超级大对象，将会被存放 在N个连续的Humongous Region 之中，G1 的大多数行为都把 Humongous Region 作为老年代的一部分来进行看待。\n\n虽然 G1 仍然保留新生代和老年代的概念，但新生代和老年代不再是固定的了，它们都是一系列区域（不需要连续）的动态集合。\nG1 收集器之所以能建立可预测的停顿时间模型，是因为它将 Region 作为单次回收的最小单元，即每次收集到的内存空间都是 Region 大小的整数倍，这样可以有计划地避免在整个Java堆中进行全区域的垃圾收集。\n更具体的处理思路是让 G1 收集器去跟踪各个 Region 里面的垃圾堆积的 “价值” 大小，价值即回收所获得的空间大小以及回收所需时间的经验值，然后在后台维护一个优先级列表，每次根据用户设定允许的 收集停顿时间（使用参数 -XX:MasGCPatseMs 指定，默认值是200毫秒）优先处理回收价值收益最大的那些 Region。\n这也就是 “CartbolseFirst” 名字的由来。这种使用 Region 划分内存空间，以及具有优先级的区域回收方式，保证了 G1 收集器在有限的时间内获取尽可能高的收集效率。\nG1 将堆内存 “化整为零” 的 “解题思路” ，看起来似乎没有太多令人惊讶之处，也完全不难理解，但其中的实现细节可是远远没有想象中那么简单，否则就不会从2004年Sun实验室发表第一篇关于 G1的论文后一直拖到2012年4月JDK 7 Update 4发布，用将近10年时间才倒腾出能够商用的 G1 收集器来。\n2. RSet全称是 Remembered Set，是辅助 GC 过程的一种结构，典型的空间换时间工具，和 Card Table有些类似。还有一种数据结构也是辅助GC的：Collection Set（CSet），它记录了GC要收集的Region集合，集合里的Region可以是任意年代的。在 GC 的时候，对于old-&gt;young和 young-&gt;old的跨代对象引用，只要扫描对应的 CSet 中的 RSet 即可。\n使用 RSet 可以避免全堆作为 GC Roots 扫描，但在 G1 收集器上 RSet 的应用其实要复杂很多，它的每个 Region 都维护有自己的 RSet，这些记忆集会记录下别的 Region 指向自己的指针。并标记这些指针分别在哪些卡页的范围之内。\nG1 的 RSet 在存储结构的本质上是一种哈希表，Key 是别的 Region 的起始地址（表示这个 Region 中有指针指向 ”我“），Value 是一个集合，里面存储的元素是卡表的索引号（即这个 Region 中哪个 card 中有对象指向了我）。\n这种 “双向” 的卡表结构（卡表是 “我指向谁”，这种结构还记录了 “谁指向我” ）比原来的卡表实现起来更复杂，同时由于 Region 数量比传统收集器的分代数量明显要多得多，因此 G1 收集器要比其他的传统垃圾收集器有着更高的内存占用负担。\n根据经验，G1 至少要耗费大约相当于 Java 堆容量 10% 至 20% 的额外内存来维持收集器工作。\n\n上图中有三个 Region，每个 Region 被分成了多个 Card，在不同 Region 中的 Card 会相互引用，Region1 中的 Card 中的对象引用了 Region2 中的 Card 中的对象，蓝色实线表示的就是points-out的关系，而在 Region2 的 RSet 中，记录了 Region1 的 Card，即红色虚线表示的关系，这就是 points-into。 而维系 RSet 中的引用关系靠 post-write barrier 和 Concurrent refinement threads 来维护\n3. TAMS并发标记阶段如何保证收集线程与用户线程互不干扰地运行？\n这里首先要解决的是用户线程改变对象引用关系时，必须保证其不能打破原本的对象图结构，导致标记结果出现错误。\nCMS 收集器采用 增量更新算法 实现，而 G1 收集器则是通过 原始快照（SATB）算法来实现的。\n此外，垃圾收集对用户线程的影响还体现在回收过程中新创建对象的内存分配上，程序要继续运行就肯定会持续有新对象被创建。\nG1 为每一个 Region 设计了两个名为 TAMS（Top at Mark Start） 的指针，把 Region 中的一部分空间划分出来用于并发回收过程中的新对象分配，并发回收时新分配的对象地址都必须要在这两个指针位置以上。G1 收集器默认在这个地址以上的对象是被隐式标记过的，即默认它们是存活的，不纳入回收范围。\n与 CMS 中的 “ConcurrentMode Failure” 失败会导致 Full GC 类似，如果内存回收的速度赶不上内存分配的速度，G1 收集器也要被迫冻结用户线程执行，导致 Full GC 而产生长时间 “Stop TheWorld“。\n4. 停顿预测模型（Pause Prediction Model）用户通过  -XX:MaxGCPauseMillis 参数指定的停顿时间只意味着垃圾收集发生之前的期望值，但G1收集器要怎么做才能满足用户的期望呢？\nG1 收集器的停顿预测模型是以 衰减均值（Decaying Average）为理论基础来实现的。\n在垃圾收集过程中，G1 收集器会记录每个 Region 的回收耗时、每个 Region 记忆集里的脏卡数量等各个可测量的步骤花费的成本，并分析得出平均值、标准偏差、置信度等统计信息。\n这里强调的 “衰减平均值” 是指它会比普通的平均值更容易受到新数据的影响，平均值代表整体平均状态，但衰减平均值更准确地代表 “最近的” 平均状态。\n换句话说，Region的统计状态越新越能决定其回收的价值。然后通过这些信息预测现在开始回收的话，由哪些 Region 组成回收集才可以在不超过期望停顿时间的约束下获得最高的收益。\n5. Mixed GC如果我们不去计算用户线程运行过程中的动作（如使用写屏障维护记忆集的操作），G1 收集器的运作过程大致可划分为以下四个步骤：初始标记、并发标记、最终标记、筛选回收。\n\n初始标记（Initial Marking）：仅仅只是标记一下 GC Roots 能直接关联到的对象，并且修改 TAMS 指针的值，让下一阶段用户线程并发运行时，能正确地在可用的 Region 中分配新对象。这个阶段需要停顿线程，但耗时很短，而且是借用进行 Minor GC 的时候同步完成的，所以 G1 收集器在这个阶段实际并没有额外的停顿。\n并发标记（Concurrent Marking）：从 GC Root 开始对堆中对象进行可达性分析，递归扫描整个堆里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。当对象图扫描完成以后，还要重新处理 SATB 记录下的在并发时有引用变动的对象。\n最终标记（Final Marking）：对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留下来的最后那少量的 SATB 记录。\n筛选回收（Live Data Counting and Evacuation）：负责更新 Region 的统计数据，对各个 Region 的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个 Reion 构成回收集，然后把决定回收的那一部分 Region 的存活对象复制到空的 Rerion 中，再清理掉整个旧 Region 的全部空间。这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行完成的。\n\n从上述阶段的描述可以看出，G1收集器除了并发标记外，其余阶段也是要完全暂停用户线程的。\n换言之，它并非纯粹地追求低延迟，官方给它设定的目标是在延迟可控的情况下获得尽可能高的吞吐量，所以才能担当起 “全功能收集器”  的重任与期望。\n从Oracle官方透露出来的信息可获知，回收阶段（Evacuation）其实本也有想过设计成与用户程序一起并发执行，但这件事情做起来比较复杂，考虑到 G1 只是回收一部分 Region，停顿时间是用户可控制的，所以并不迫切去实现，而选择把这个特性放到了 G1 之后出现的低延迟垃圾收集器（即ZGC）中。\n另外，还考虑到 G1 不是仅仅面向低延迟，停顿用户线程能够最大幅度提高垃圾收集效率，为了保证吞吐量所以才选择了完全暂停用户线程的实现方案。\n通过下图可以比较清楚地看到 G1 收集器的运作步骤中并发和需要停顿的阶段。\n\n毫无疑问，可以由用户指定期望的停顿时间是 G1 收集器很强大的一个功能，设置不同的期望停顿时间，可使得 G1 在不同应用场景中取得关注吞吐量和关注延迟之间的最佳平衡。\n不过，这里设置的 “期望值” 必须是符合实际的，不能异想天开，毕竟 G1 是要冻结用户线程来复制对象的，这个停顿时间再怎么低也得有个限度。\n它默认的停顿目标为两百毫秒，一般来说，回收阶段占到几十到一百甚至接近两百毫秒都很正常，但如果我们把停顿时间调得非常低，譬如设置为二十毫秒，很可能出现的结果就是由于停顿目标时间太短，导致每次选出来的回收集只占堆内存很小的一部分，收集器收集的速度逐渐跟不上分配器分配的速度，导致垃圾慢慢堆积。\n很可能一开始收集器还能从空闲的堆内存中获得一些喘息的时间，但应用运行时间一长就不行了，最终占满堆引发 Full GC 反而降低性能，所以通常把期望停顿时间设置为一两百毫秒或者两三百毫秒会是比较合理的。\n从 G1 开始，最先进的垃圾收集器的设计导向都不约而同地变为追求能够应付应用的内存分配速率（Allocation Rate），而不追求一次把整个 Java 堆全部清理干净。\n这样，应用在分配，同时收集器在收集，只要收集的速度能跟得上对象分配的速度，那一切就能运作得很完美。这种新的收集器设计思路从工程实现上看是从 G1 开始兴起的，所以说 G1 是收集器技术发展的一个里程碑。\n四、G1垃圾收集器运行细节1. 何时触发 Minor GC在 G1 中，Eden、Survivor、老年代的大小是在动态变化的。在初始时，新生代占整个堆内存的 5%，可以通过参数 G1NewSizePercent 设置，默认值为 5。\n在 G1 中，虽然进行了 Region 分区，但是新生代依旧可以被分为 Eden 区和 Survivor 区，参数 SurvivorRatio 依旧表示 Eden/Survivor 的比值。\n随着系统的运行，Eden 区的对象越来越多，当达到 Eden 区的最大大小时，就会触发 Minor GC。新生代的最大大小默认为整个堆内存的 60%，可以通过参数 G1MaxNewSizePercent 控制，默认值为 60。\nG1 垃圾回收器在进行新生代的垃圾回收时，会采用复制算法来回收垃圾，不用考虑并发的场景，全程都是 STW，它会根据设置的停顿时间，尽可能的最大效率的回收新生代区域。\n2. 对象何时进入到老年代新生代的对象要进入老年代，需要达到以下两个条件中的其中之一即可。\n\n多次躲过新生代的回收，对象年龄达到 MaxTenuringThreshold，该参数默认值为 15。 在每次 Minor GC 时，新生代的对象如果存活，会被移动到 Survivor 区中，同时会将对象的年龄加 1，当对象的年龄达到 MaxTenuringThreshold 后，就被被移到老年代中。\n符合动态年龄判断规则。如果某次 Minor GC 过后，发现 Survivor 区中相同年龄的对象达到了 Survivor 的 50%，那么该年龄及以上的对象，会被直接移动到老年代中。 例如某次 Minor GC 过后，Survivor 区中存在年龄分别为 1、2、3、4 的对象，而年龄为 3 的对象超过了 Survivor 区的 50%，那么年龄大于等于 3 的对象，就会被全部移动到老年代中。\n\n3. 何时触发 Mixed GC在 G1 中，不存在单独回收老年代的行为，而是当要发生老年代的回收时，同时也会对新生代以及大对象进行回收，因此这个阶段称之为混合回收（Mixed GC）。\n当老年代对堆内存的占比达到 45%时，就会触发 Mixed GC。可以通过参数 InitiatingHeapOccupancyPercent 来设置当堆内存达到多少时，触发 Mixed GC，该参数的默认值为 45。\n当触发 Mixed GC 时，会依次执行初始标记（在 Minor GC 时完成）、并发标记、最终标记、筛选回收这四个过程。最终会根据设置的最大停顿时间，来计算对哪些 Region 区域进行回收带来的收益最大。\n实际上，在筛选回收阶段，可以分多次回收 Region，具体多少次可以通过参数 G1MixedGCCountTarget 控制，默认值为 8 次。具体什么意思呢？\n假如现在有 80 个 Region 需要被回收，因为筛选回收阶段会造成 STW，如果一下子全部回收这 80 个 Region，可能造成的停顿时间较长，因此 JVM 会分 8 次来回收这些 Region，每次先回收 10 个 Region，然后让用户线程执行一会，接着再让 GC 线程回收 10 个 Region，直至回收完这 80 个 Region，这样尽可能的降低了系统的暂停时间。\nG1 垃圾回收器的回收思路是：不需要对整个堆进行回收，只需要保证垃圾回收的速度大于内存分配的速度即可。因此在每次进行 Mixed GC 时，虽然我们设置了停顿时间，但是当回收得到的空闲 Region 数量达到了整个堆内存的 5%，那么就会停止回收。可以由参数 G1HeapWaterPercent 控制，默认值为 5%。\n另外，在混合回收的过程中，由于使用的是复制算法，因此当一个 Region 中存活的对象过多的话，复制这个 Region 所耗费的时间就会较多，因此 G1 提供了一个参数，用来控制当存活对象占当前 Region 的比例超过多少后，就不会对该 Region 进行回收。该参数为 G1MixedGCLiveThresholdPercent ，默认值为 85%。\n4. 何时触发 Full GC在进行混合回收时，使用的是复制算法，如果当发现空闲的 Region 大小无法放得下存活对象的内存大小，那么这个时候使用复制算法就会失败，因此此时系统就不得不暂停应用程序，进行一次 Full GC。进行 Full GC 时采用的是单线程进行标记、清理和整理内存，这个过程是非常漫长的，因此应该尽量避免 Full GC 的触发。\n五、G1与CMS的比较G1收集器常会被拿来与CMS收集器互相比较，毕竟它们都非常关注停顿时间的控制，官方资料中将它们两个并称为 “The Mostly Concurrent Collectors” 。\n在未来，G1 收集器最终还是要取代 CMS 的，而当下它们两者并存的时间里，分个高低优劣就无可避免。\n相比 CMS，G1 的优点有很多，暂且不论可以指定最大停顿时间、分 Region 的内存布局、按收益动态确定回收集这些创新性设计带来的红利，单从最传统的算法理论上看，G1 也更有发展潜力。\n与 CMS 的 “标记-清除” 算法不同，G1 从整体来看是基于 “标记-整理” 算法实现的收集器，但从局部（两个Region之间）上看又是基于 “标记-复制” 算法实现。\n无论如何，这两种算法都意味着 G1 运作期间不会产生内存空间碎片，垃圾收集完成之后能提供规整的可用内存。这种特性有利于程序长时间运行，在程序为大对象分配内存时不容易因无法找到连续内存空间而提前触发下一次收集。\n不过，G1 相对于 CMS 仍然不是占全方位、压倒性优势的，从它出现几年仍不能在所有应用场景中代替 CMS 就可以得知这个结论。\n比起 CMS，G1 的弱项也可以列举出不少，如在用户程序运行过程中，G1 无论是为了垃圾收集产生的内存占用（Footprint）还是程序运行时的额外执行负载（Overload）都要比 CMS 要高。\n就内存占用来说，虽然 G1 和 CMS 都使用卡表来处理跨代指针，但 G1 的卡表实现更为复杂，而且堆中每个 Region，无论扮演的是新生代还是老年代角色，都必须有一份卡表，这导致 G1 的记忆集（和其他内存消耗）可能会占整个堆容量的 20% 乃至更多的内存空间。\n相比起来 CMS 的卡表就相当简单，只有唯一一份，而且只需要处理老年代到新生代的引用，反过来则不需要，由于新生代的对象具有朝生夕灭的不稳定性，引用变化频繁，能省下这个区域的维护开销是很划算的。\n在执行负载的角度上，同样由于两个收集器各自的细节实现特点导致了用户程序运行时的负载会有不同，譬如它们都使用到写屏障，CMS 用写后屏障来更新维护卡表。\n而 G1 除了使用写后屏障来进行同样的（由于 G1 的卡表结构复杂，其实是更烦琐的）卡表维护操作外，为了实现原始快照搜索（SATB）算法，还需要使用写前屏障来跟踪并发时的指针变化情况。\n相比起增量更新算法，原始快照搜索能够减少并发标记和重新标记阶段的消耗，避免 CMS 那样在最终标记阶段停顿时间过长的缺点，但是在用户程序运行过程中确实会产生由跟踪引用变化带来的额外负担。\n由于 G1 对写屏障的复杂操作要比 CMS 消耗更多的运算资源，所以 CMS 的写屏障实现是直接的同步操作，而 G1 就不得不将其实现为类似于消息队列的结构，把写前屏障和写后屏障中要做的事情都放到队列里，然后再异步处理。\n以上的优缺点对比仅仅是针对 G1 和 CMS 两款垃圾收集器单独某方面的实现细节的定性分析，通常我们说哪款收集器要更好、要好上多少，往往是针对具体场景才能做的定量比较。\n按照笔者的实践经验，目前在小内存应用上 CMS 的表现大概率仍然要会优于 G1，而在大内存应用上 G1 则大多能发挥其优势，这个优劣势的 Java 堆容量平衡点通常在 6GB 至 8GB 之间，当然，以上这些也仅是经验之谈，不同应用需要量体裁衣地实际测试才能得出最合适的结论，随着HotSpot的开发者对 G1 的不断优化，也会让对比结果继续向 G1 倾斜。\n六、G1 GC 相关参数\n\n\n参数\n含义\n\n\n\n-XX:G1HeapRegionSize=n\n设置Region大小，并非最终值\n\n\n-XX:MaxGCPauseMillis\n设置G1收集过程目标时间，默认值200ms，不是硬性条件\n\n\n-XX:G1NewSizePercent\n新生代最小值，默认值5%\n\n\n-XX:G1MaxNewSizePercent\n新生代最大值，默认值60%\n\n\n-XX:ParallelGCThreads\nSTW期间，并行GC线程数\n\n\n-XX:ConcGCThreads=n\n并发标记阶段，并行执行的线程数\n\n\n-XX:InitiatingHeapOccupancyPercent\n设置触发标记周期的 Java 堆占用率阈值。默认值是45%。这里的java堆占比指的是non_young_capacity_bytes，包括old+humongous\n\n\n","categories":["Java","JVM"],"tags":["深入理解JVM虚拟机（第三版）"]},{"title":"Java 与线程","url":"/posts/56705/","content":"一、线程的实现我们知道，线程是比进程更轻量级的调度执行单位，线程的引入，可以把一个进程的资源分配和执行调度分开，各个线程既可以共享进程资源(内存地址、文件I/O等)，又可以独立调度。目前线程是 Java 里面进行处理器资源调度的最基本单位，不过如果日后 Loom 项目能成功为 Java 引人纤程(Fiber)的话，可能就会改变这一点。\n主流的操作系统都提供了线程实现，Java 语言则提供了在不同硬件和操作系统平台下对线程操作的统一处理，每个已经调用过 start() 方法且还未结束的 java.lang.Thread 类的实例就代表着一个线程。我们注意到 Thread 类与大部分的 Java 类库 API 有着显著差别，它的所有关键方法都被声明为 Native。在 Java 类库 API 中，一个 Native 方法往往就意味着这个方法没有使用或无法使用平台无关的手段来实现（当然也可能是为了执行效率而使用 Native方法，不过通常最高效率的手段也就是平台相关的手段）。正因为这个原因，本节的标题被定为 “线程的实现” 而不是 “Java线程的实现”，在稍后介绍的实现方式中，我们也先把 Java 的技术背景放下，以一个通用的应用程序的角度来看看线程是如何实现的。\n实现线程主要有三种方式：\n\n使用内核线程实现（1:1实现）\n使用用户线程实现（1:N实现）\n使用用户线程加轻量级进程混合实现（N:M实现）\n\n1.内核线程实现使用内核线程实现的方式也被称为 1:1 实现。内核线程（Kernel-Level Thread，KLT）就是直接由操作系统内核（Kernel，下称内核）支持的线程，这种线程由内核来完成线程切换，内核通过操纵调度器（Scheduler）对线程进行调度，并负责将线程的任务映射到各个处理器上。每个内核线程可以视为内核的一个分身，这样操作系统就有能力同时处理多件事背，支持多线程的内核就称为多线程内核（Multi-Threads Kernel）。\n程序一般不会直接使用内核线程，而是使用内核线程的一种高级接口——轻量级进程（Light Weight Process，LWP），轻量级进程就是我们通常意义上所讲的线程，由于每个轻量级进程都由一个内核线程支持，因此只有先支持内核线程，才能有轻量级进程。这种轻量级进程与内核线程之间 1:1 的关系称为一对一的线程模型，如图所示。\n\n由于内核线程的支持，每个轻量级进程都成为一个独立的调度单元，即使其中某一个轻量级进程在系统调用中被阻塞了，也不会影响整个进程继续工作。轻量级进程也具有它的局限性：首先，由于是基于内核线程实现的，所以各种线程操作，如创建、析构及同步，都需要进行系统调用。而系统调用的代价相对较高，需要在用户态（User Mode）和内核态（Kernel Mode）中来回切换。其次，每个轻量级进程都需要有一个内核线程的支持，因此轻量级进程要消耗一定的内核资源（如内核线程的栈空间），因此一个系统支持轻量级进程的数量是有限的。\n2.用户线程实现使用用户线程实现的方式被称为 1:N 实现。广义上来讲，一个线程只要不是内核线程，都可以认为是用户线程（User Thread，UT）的一种，因此从这个定义上看，轻量级进程也属于用户线程，但轻量级进程的实现始终是建立在内核之上的，许多操作都要进行系统调用，因此效率会受到限制，并不具备通常意义上的用户线程的优点。\n而狭义上的用户线程指的是完全建立在用户空间的线程库上，系统内核不能感知到用户线程的存在及如何实现的。用户线程的建立、同步、销毁和调度完全在用户态中完成，不需要内核的帮助。如果程序实现得当，这种线程不需要切换到内核态，因此操作可以是非常快速且低消耗的，也能够支持规模更大的线程数量，部分高性能数据库中的多线程就是由用户线程实现的。这种进程与用户线程之间 1:N 的关系称为一对多的线程模型，如图所示。\n\n用户线程的优势在于不需要系统内核支援，劣势也在于没有系统内核的支援，所有的线程操作都需要由用户程序自己去处理。线程的创建、销毁、切换和调度都是用户必须考虑的问题，而且由于操作系统只把处理器资源分配到进程，那诸如“阻塞如何处理” “多处理器系统中如何将线程映射到其他处理器上” 这类问题解决起来将会异常困难，甚至有些是不可能实现的。因为使用用户线程实现的程序通常都比较复杂（此处所讲的“复杂”与“程序自己完成线程操作”，并不限于程序直接编写了复杂的实现用户线程的代码，使用用户线程的程序时，很多都依赖特定的线程库来完成基本的线程操作，这些复杂性都封装在线程库之中），除了有明确的需求外（譬如以前在不支持多线程的操作系统中的多线程程序、需要支持大规模线程数量的应用），一般的应用程序都不倾向使用用户线程。Java、Ruby 等语言都曾经使用过用户线程，最终又都放弃了使用它。但是近年来许多新的、以高并发为卖点的编程语言又普遍支持了用户线程，譬如 Golang、Erlang 等，使得用户线程的使用率有所回升。\n3.混合实现线程除了依赖内核线程实现和完全由用户程序自己实现之外，还有一种将内核线程与用户线程一起使用的实现方式，被称为 N:M 实现。\n在这种混合实现下，既存在用户线程，也存在轻量级进程。用户线程还是完全建立在用户空间中，因此用户线程的创建、切换、析构等操作依然廉价，并且可以支持大规模的用户线程并发。而操作系统支持的轻量级进程则作为用户线程和内核线程之间的桥梁，这样可以使用内核提供的线程调度功能及处理器映射，并且用户线程的系统调用要通过轻量级进程来完成，这大大降低了整个进程被完全阻塞的风险。在这种混合模式中，用户线程与轻量级进程的数量比是不定的，是 N:M 的关系，如图所示，这种就是多对多的线程模型。\n许多 UNIX 系列的操作系统，如 Solaris、HP-UX 等都提供了 M:N 的线程模型实现。在这些操作系统上的应用也相对更容易应用 M:N 的线程模型。\n4.Java 线程的实现Java 线程如何实现并不受 Java 虚拟机规范的约束，这是一个与具体虚拟机相关的话题。Java 线程在早期的 Classic 虚拟机上（JDK 1.2 以前），是基于一种被称为 “绿色线程“ （Green Threads）的用户线程实现的，但从 JDK 1.3 起，“主流” 平台上的 “主流” 商用 Java 虚拟机的线程模型普遍都被替换为基于操作系统原生线程模型来实现，即采用 1:1 的线程模型。\n以 HotSpot 为例，它的每一个 Java 线程都是直接映射到一个操作系统原生线程来实现的，而且中间没有额外的间接结构，所以 HotSpot 自己是不会去干涉线程调度的（可以设置线程优先级给操作系统提供调度建议），全权交给底下的操作系统去处理，所以何时冻结唤醒线程、该给线程分配多少处理器执行时间、该把线程安排给哪个处理器核心去执行都是由操作系统完成的，也都是由操作系统全权决定的。\n前面强调是两个 “主流”，那就说明肯定还有例外的情况，这里举两个比较著名的例子，一个是用于 Java ME 的 CLDC HotSpot Implementation（CLDC-HI，介绍可见第1章）。它同时支持两种线程模型，默认使用 1:N 由用户线程实现的线程模型，所有 Java 线程都映射到一个内核线程上；不过它也可以使用另一种特殊的混合模型，Java 线程仍然全部映射到一个内核线程上，但当 Java 线程要执行一个阻塞调用时，CLDC-HI 会为该调用单独开一个内核线程，并且调度执行其他 Java 线程，等到那个阻塞调用完成之后再重新调度之前的 Java 线程继续执行。\n另外一个例子是在 Solaris 平台的 HotSpot 虚拟机，由于操作系统的线程特性本来就可以同时支持 1:1 （通过Bound Threads 或Alternate Libthread 实现）及 N:M （通过LWP/Thread Based Synchronization 实现）的线程模型，因此 Solaris 版的 HotSpot 也对应提供了两个平台专有的虚拟机参数，即 -XX:+UseLWPSynchronization （默认值）和 -XX:+UseBoundThreads 来明确指定虚拟机使用哪种线程模型。\n操作系统支持怎样的线程模型，在很大程度上会影响上面的 Java 虚拟机的线程是怎样映射的，这一点在不同的平台上很难达成一致，因此《 Java 虚拟机规范》中才不去限定 Java 线程需要使用哪种线程模型来实现。线程模型只对线程的并发规模和操作成本产生影响，对Java 程序的编码和运行过程来说，这些差异都是完全透明的。\n二、 Java 线程调度线程调度是指系统为线程分配处理器使用权的过程，调度主要方式有两种，分别是协同式（CooperativeThreads-Scheduling）线程调度和抢占式（Preemptive Threads-Schedulin）线程调度。\n如果使用协同式调度的多线程系统，线程的执行时间由线程本身来控制，线程把自己的工作执行完了之后，要主动通知系统切换到另外一个线程上去。协同式多线程的最大好处是实现简单，而且由于线程要把自己的事情干完后才会进行线程切换，切换操作对线程自己是可知的，所以一般没有什么线程同步的问题。Lua 语言中的 “协同例程” 就是这实现。它的坏处也很明显：线程执行时间不可控制，甚至如果一个线程的代码编写有问题，一直不告知系统进行线程切换，那么程序就会一直阻塞在那里。很久以前的 Windows 3.x 系统就是使用协同式来实现多进程多任务的，那是相当不稳定的，只要有一个进程坚持不让出处理器执行时间，就可能会导致整个系统崩溃。\n如果使用抢占式调度的多线程系统，那么每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定。譬如在 Java 中，有 Thread::yield() 方法可以主动让出执行时间。但是如果想要主动获取执行时间，线程本身是没有什么办法的。在这种实现线程调度的方式下，线程的执行时间是系统可控的，也不会有一个线程导致整个进程甚至整个系统阻塞的问题。Java 使用的线程调度方式就是抢占式调度。与前面所说的 Windows 3.x 的例子相对，在 Windows 9x/NT 内核中就是使用抢占式来实现多进程的，当一个进程出了问题，我们还可以使用任务管理器把这个进程杀掉，而不至于导致系统崩溃。\n虽然说 Java 线程调度是系统自动完成的，但是我们仍然可以 “建议” 操作系统给某些线程多分配一点执行时间，另外的一些线程则可以少分配一点——这项操作是通过设置线程优先级来完成的。Java 语言一共设置了 10 个级别的线程优先级（Thread.MIN PRIORITY 至 Thread.MAX PRIORITY0。在两个线程同时处于 Ready 状态时，优先级越高的线程越容易被系统选择执行。\n不过，线程优先级并不是一项稳定的调节手段，很显然因为主流虚拟机上的 Java 线程是被映射到系统的原生线程上来实现的，所以线程调度最终还是由操作系统说了算。尽管现代的操作系统基本都提供线程优先级的概念，但是并不见得能与 Java 线程的优先级一一对应，如 Solaris 中线程有 2147483648（2 的 31 次幂）种优先级，但 Windows 中就只有七种优先级。如果操作系统的优先级比 Java 线程优先级更多，那问题还比较好处理，中间留出一点空位就是了，但对于比 Java 线程优先级少的系统，就不得不出现几个线程优先级对应到同一个操作系统优先级的情况了。表 12-1 显示了 Java 线程优先级与 Windows 线程优先级之间的对应关系，Windows 平台的虚拟机中使用了除 THREAD_PRIORITY_IDLE 之外的其余6种线程优先级，因此在Windows下设置线程优先级为1和2、3和4、6和7、8和9的效果是完全相同的。\n\n\n\nJava 线程优先级\nWindows 线程优先级\n\n\n\n1（Thread.MIN_PRIORITY）\nTHREAD_PRIORITY_LOWEST\n\n\n2\nTHREAD_PRIORITY_LOWEST\n\n\n3\nTHREAD_PRIORITY_BELOW_NORMAL\n\n\n4\nTHREAD_PRIORITY_BELOW_NORMAL\n\n\n5（Thread.NORM_PRIORITY）\nTHREAD_PRIORITY _NORMAL\n\n\n6\nTHREAD_PRIORITY _ABOVE_NORMAL\n\n\n7\nTHREAD_PRIORITY_ABOVE_NORMAL\n\n\n8\nTHREAD_PRIORITY_HIGHEST\n\n\n9\nTHREAD_PRIORITY_HIGHEST\n\n\n10（Thread.MAX PRIORITY）\nTHREAD_PRIORITY_CRITICAL\n\n\n线程优先级并不是一项稳定的调节手段，这不仅仅体现在某些操作系统上不同的优先级实际会变得相同这一点上，还有其他情况让我们不能过于依赖线程优先级：优先级可能会被系统自行改变，例如在 Windows 系统中存在一个叫 “优先级推进器” 的功能（Priority Boosting，当然它可以被关掉），大致作用是当系统发现一个线程被执行得特别频繁时，可能会越过线程优先级去为它分配执行时间，从而减少因为线程频繁切换而带来的性能损耗。因此，我们并不能在程序中通过优先级来完全准确判断一组状态都为 Ready 的线程将会先执行哪一个。\n三、状态转换Java 语言定义了6种线程状态，在任意一个时间点中，一个线程只能有且只有其中的一种状态，并且可以通过特定的方法在不同状态之间转换。这6种状态分别是：\n\n新建（New）：创建后尚未启动的线程处于这种状态。\n\n运行（Runnable）：包括操作系统线程状态中的 Running 和 Ready，也就是处于此状态的线程有可能正在执行，也有可能正在等待着操作系统为它分配执行时间。\n\n无限期等待（Waiting）：处于这种状态的线程不会被分配处理器执行时间，它们要等待被其他线程显式唤醒。以下方法会让线程陷入无限期的等待状态：\n\n没有设置 Timeout 参数的 Object::wait() 方法;\n\n没有设置 Timeout 参数的 Thread::join() 方法; \n\nLockSupport::park() 方法。\n\n\n\n限期等待（Timed Waiting）：处于这种状态的线程也不会被分配处理器执行时间，不过无须等待被其他线程显式唤醒，在一定时间之后它们会由系统自动唤醒。以下方法会让线程进入限期等待状态：\n\nThread::sleep() 方法；\n设置了 Timeout 参数的 Object::wait() 方法；\n设置了 Timeout 参数的 Thread::join() 方法；\nLockSupport::parkNanos() 方法；\nLockSupport::parkUntilO) 方法。\n\n\n阻塞（Blocked）：线程被阻塞了，“阻塞状态” 与 “等待状态” 的区别是 “阻塞状态” 在等待着获取到一个排它锁，这个事件将在另外一个线程放弃这个锁的时候发生；而 “等待状态” 则是在等待一段时间，或者唤醒动作的发生。在程序等待进入同步区域的时候，线程将进入这种状态。\n\n结束（Terminated）：已终止线程的线程状态，线程已经结束执行。\n\n\n上述6种状态在遇到特定事件发生的时候将会互相转换，它们的转换关系如图所示。\n\n补充图：\n\n","categories":["Java","JVM"],"tags":["深入理解JVM虚拟机（第三版）"]},{"title":"Java 内存模型","url":"/posts/1904/","content":"《Java虚拟机规范》中曾试图定义一种 “Java内存模型”（Java Memory Model，JMM）来 屏蔽各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存访问效果。\n定义Java内存模型并非一件容易的事情，这个模型必须定义得足够严谨，才能让Java的并发访问操作不会产生歧义。\n但是也必须定义得足够宽松，使得虚拟机的实现能有足够的自由空间去利用硬件的各种特性（寄存器、高速缓存和指令集中某些特有的指令）来获取更好执行速度。\nJava内存模型在 JDK1.2 之后建立起来，直到 JDK5 （JSR-133）发布后，Java内存模型才终于成熟、完善起来。\n\n在《Java虚拟机规范》的第2版及之前，专门有一章 “Threads and Locks” 来描述内存模型，后来由于这部分内容难以把握宽紧限度，被反复修正更新，从第3版（Java SE 7版）开始索性就被移除出规范，独立以 JSR 形式维护。\n\n1. Java内存模型作用Java 内存模型的主要目的是定义程序中各种 变量 的访问规则，即关注 在虚拟机中把变量值存储到内存和从内存中取出变量值这样的底层细节。\n此处的变量与 Java 编程中所说的变量有所区别，它包括了：\n\n实例字段\n静态字段\n构成数组对象的元素\n\n但是不包括 局部变量 与 方法参数，因为后者 是线程私有的，不会被共享，自然不存在竞争问题。\n\n如果局部变量是一个 reference 类型，它引用的对象在Java堆中可被各个线程共享，但是 reference 本身在Java栈的局部变量表中是线程私有的。\n\n为了获得更好的执行效能，Java内存模型 并没有限制执行引擎使用处理器的特定寄存器或缓存来和主内存交互， 也没有限制即时编译器是否要进行调整代码执行顺序这类优化措施。\n2. 主内存与工作内存主内存、工作内存 与Java内存区域的 Java堆、栈、方法区 等并不是同一个层次的对内存的划分，这两者基本上是没有任何关系的。\n如果两者一定要勉强对应起来，那么从变量、主内存、工作内存的定义来看，\n主内存主要对应于Java堆中的对象实例数据部分\n\n之所以说对应的是Java堆中的对象实例数据，是因为：\n除了实例数据，Java堆还保存了对象的其他信息，对于 HotSpot 虚拟机来讲，有 Mark word（存储对象哈希码、GC标志、GC年龄、同步锁等信息）、Klass Point（指向存储类型元数据的指针）及一些用于字节码对齐补白的填充数据（如果实例数据刚好满足 8 字节对齐，则可以不补白）。\n\n工作内存对应于虚拟机栈的部分区域\n从更基础的层次上说，主内存直接对应于物理硬件的内存，而为了更好的运行速度，虚拟机（或者是硬件、操作系统本身的优化措施）可能会让工作内存优先存储于寄存器和高速缓存中，因为程序运行时主要访问的是工作内存。\n2.1 主内存Java 内存模型规定了所有的变量都存储在主内存。\n\n此处的主内存与物理硬件的主内存名字一样，两者也可以类比，但 物理上它仅是虚拟机内存的一部分。\n\n2.2 工作内存每条线程都有自己的工作内存。\n（1）线程的工作内存中保存了被该线程使用的变量的主内存副本。\n\n假如线程中访问一个 10MB 大小的对象，也会把这 10MB 的内存复制一份出来吗？\n事实上并不会如此，这个对象的引用、对象中某个在线程访问到的字段是有可能被复制的，但不会有虚拟机把整个对象复制一次。\n\n（2）线程对变量的所有操作（读取、赋值等）都必须在工作内存中进行，而不能直接读取主内存中的数据。\n\n根据《Java虚拟机规范》的规定，volatile 变量依然有工作内存的拷贝，但是由于它特殊的操作顺序性规定，所以看起来如同直接在主内存中读写访问一般，因此这里的描述对于 volatile 也并不存在例外。\n\n（3）不同的线程之间无法直接访问对方工作内存中的变量。\n（4）线程间变量值的传递均需要通过主内存来完成。\n\n3. 内存间交互操作3.1 八种基本操作关于主内存与工作内存之间具体的交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步回主内存这一类的实现细节，Java 内存模型 中定义了以下 8 种操作来完成。\n\nJava 虚拟机实现时必须保证下面提到的每一种操作都是原子的、不可再分的。\n对于 double 和 long 类型的变量来说，load、store、read 和 write 操作在某些平台上允许有例外。\n\n\nlock（锁定）： 作用于 主内存 的变量，它把一个变量标识为一条线程独占的状态。\nunlock（解锁）： 作用于 主内存 的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。\nread（读取）： 作用于 主内存 的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的 load 动作使用。\nload（载入）：作用于 工作内存 的变量，它把 read 操作从主内存中得到的变量值放入工作内存的变量副本中。\nuse（使用）：作用于 工作内存 的变量，它把工作内存中的一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时，将会执行这个操作。\nassign（赋值）：作用于 工作内存 的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。\nstore（存储）：作用于 工作内存 的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的 write 操作使用。\nwrite（写入）：作用于 主内存 的变量，它把 store 操作从工作内存中得到的变量的值放入主内存的变量中。\n\n如果要把一个变量从主内存拷贝到工作内存，那就要按顺序执行 read 和 load 操作，\n如果要把变量从工作内存同步回主内存，就要按顺序中 store 和 write 操作。\n注意：\n\nJava 内存模型只要求上述两个操作必须按顺序执行，但不要求连续执行。\n也就是说 read 和 load 之间、store 与 write 之间是可以插入其他指令的。\n如果对主内存中的变量a、b进行访问时，一种可能出现的顺序是 read a、read b、load b、load a。\n\n3.2 八种操作必须满足的规则\n不允许 read 和 load 、store 和 write 操作之一单独出现，即不允许一个变量从主内存读取了但工作内存不接受，或者工作内存发起了回写但主内存不接受的情况。\n不允许一个线程丢弃它最近的 assign 操作，即变量在工作内存中改变了之后必须把该变化同步回主内存。\n不允许一个线程无原因地（没有发生过任何 assign 操作）把数据从线程的工作内存同步回主内存中。\n一个新的变量只能在主内存中 “诞生”，不允许在工作内存中直接使用一个未被初始化（load 或 assign）的变量，换句话说就是对一个变量实施 use、store 操作之前，必须先执行 assign 和 load 操作。\n一个变量在同一时刻只允许一条线程对其进行 lock 操作，但 lock 操作可以被同一条线程重复执行多次，多次执行 lock 后，只有执行相同次数的 unlock 操作，变量才会被解锁。\n如果对一个变量执行 lock 操作，那将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行 load 或 assign 操作以初始化变量的值。\n如果一个变量事先没有被 lock 操作锁定，那就不允许对它执行 unlock 操作，也不允许去 unlock 一个被其他线程锁定的变量。\n对一个变量执行 unlock 操作之前，必须先把此变量同步回主内存（执行 store 、write 操作）。\n\n3.3 小结这 8 种内存访问操作以及上述规则限定，再加上 volatile 的一些特殊规定，就已经能够准确地描述出 Java 程序中哪些内存访问操作在并发下才是安全的。\n这种定义相当严谨，但也是极为烦琐，后来 Java 设计团队将 Java 内存模型的操作简化为 read、write、lock 和 unlock 四种，但 这只是语言描述上的等价化简，Java 内存模型的基础设计并未改变。\n4. volatile 型变量的特殊规则关键字 volatile 可以说是 Java 虚拟机提供的 最轻量级的同步机制。\n4.1 volatile 修饰的变量两个特性（1）保证此变量对所有线程的可见性这里的 “可见性” 是指 当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。\n而 普通变量并不能做到这一点，普通变量的值在线程间传递时均需要通过主内存来完成。\nvolatile 变量在各个线程的工作内存中是不存在一致性问题的。\n\n从物理存储的角度看，各个线程的工作内存中 volatile 变量也可以存在不一致的情况，但由于每次使用之前都要先刷新，执行引擎看不到不一致的情况，因此可以认为不存在一致性问题）\n\n但是 Java 里面的运算操作符并非原子操作，这导致 volatile 变量的运算在并发下一样是不安全的。\n应用场景由于 volatile 变量只能保证可见性，在不符合以下两条规则的运算场景中，我们仍要通过加锁来保证原子性：\n\n运算结果并不依赖变量的当前值，或者能够保证只有单一的线程修改变量的值。\n变量不需要与其他的状态变量共同参与不变约束。\n\n（2）禁止指令重排序优化普通的变量仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而 不能保证变量赋值操作的顺序与程序中的执行顺序一致。\n这就是 Java 内存模型中描述的所谓 “线程内表现为串行的语义“。\n如何实现在汇编代码中，被 volatile 修饰的变量，在赋值后，多执行了一个 “lock add1 $0x0,(%sep)” 操作。\n这个操作的作用相当于一个内存屏障，只有一个处理器访问内存时，并不需要内存屏障。\n但如果有两个或更多处理器访问同一块内存，且其中有一个在观测另一个，就需要内存屏障来保证一致性了。\n这句指令中的 “add1 $0x0,(%sep)” （把 ESP 寄存器的值加0）显然是一个空操作，之所以用这个空操作而不是空操作专用指令 nop ，是因为 IA32 手册规定 lock 前缀不允许配合 nop 指令使用。\n这里的 lock 前缀，作用是 将本处理器的缓存写入了内存，该写入动作也会引起别的处理器或者别的内核无效化其缓存。\n这种操作相当于对缓存中的变量做了一次前面介绍Java内存模型中的 “store 和 write 操作。\n所以通过这样一个空操作，可让前面 volatile 变量的修改对其他处理器立即可见。\n\n从硬件 架构上讲，指令重排序是指处理器采用了允许将多条指令不按程序规定的顺序分开发送给各个相应的电路单元进行处理。\n\n4.2 小结规则总结\n在工作内存中，每次使用 volatile 变量前都必须从主内存刷新最新的值，用于保证能看见其他线程对变量所做的修改。\n在工作内存中，每次修改使用 volatile 的变量后都必须立刻同步回主内存中，用于保证其他线程可以看见自己对变量做的修改。\nvolatile 修饰的变量不会被指令重排序优化，从而保证代码的执行顺序与程序的顺序相同。\n\n性能在某些情况下，volatile 的同步机制的性能确实要优于锁（使用 synchronized 关键字或 java.util.concurrent 包里的锁），但是由于虚拟机对锁实行的许多消除和优化，使得我们很难确切地说 volatile 就会比 synchronized 快上多少。\n&lt;font color=red&gt;volatile  变量读操作的性能消耗与普通变量几乎没有什么差别，但是写操作则可能会慢上一些，因为它需要在本地代码中插入许多内存屏障指令来保证处理器不发生乱序执行，不过即便如此，大多数场景下 volatile 的总开销仍然要比锁来的更低。&lt;/font&gt;\n5. 针对 long 和 double 类型变量的特殊规则5.1 long 和 double 的非原子性协定Java 内存模型要求 lock、unlock、read、load、assign、use、store、write 这八种操作都具有原子性，但是对于 64 位的数据类型（long 和 double），在模型中特别定义了一条宽松的规定：\n允许虚拟机将没有被 volatile 修饰的 64 位数据的读写操作划分为两次 32 位的操作来进行。\n即 允许虚拟机实现自行选择是否要保证 64 位数据类型的 load、store、read 和 write 这四个操作的原子性。\n5.2 小结如果有多个线程共享一个并未声明为 volatile 的 long 或 double 类型的变量，并且同时对它们进行读取和修改操作，那么某些线程可能会读取到一个既不是原值，也不是其他线程修改值的代表了 ”半个变量“ 的数值。\n进过实际测试，在目前主流平台下商用的 64 位 Java 虚拟机中并不会出现非原子性访问行为，但是对于 32 位的Java虚拟机，譬如比较常用的 32 位 x86 平台下的 HotSpot 虚拟机，对 long 类型的数据确实存在非原子性访问的风险。\n从 JDK9 起，HotSpot 虚拟机增加了一个实验性的参数 -XX:+AlwaysAtomicAccesses （这是 JEP 188 对 Java 内存模型更新的一部分内容）来约束虚拟机对所有数据类型进行原子性的访问。\n而针对 double 类型，由于现代中央处理器中一般都包含专门用于处理浮点数据的浮点运算器（Floating Point unit，FPU），用来专门处理单、双精度的浮点数据，所以哪怕是 32 位虚拟机中通常也不会出现非原子性访问的问题。\n在实际开发中，除非该数据有明确可知的线程竞争，否则我们在编写代码时一般不需要因为这个原因刻意把用到的    long 和 double 变量专门声明为 volatile。\n6. 原子性、可见性与有序性Java 内存模型是围绕着在并发过程中如何处理原子性、可见性和有序性这三个特征来建立的。\n6.1 原子性由 Java 内存模型来直接保证的原子性变量操作包括 read、load、assign、use、store 和 write 这六个。\n我们大致可以认为，基本数据类型的访问、读写都是具备原子性的 （例外就是 long 和 double 的非原子性协定，只要知道这件事情就可以了，无须太过在意这些几乎不会发生的例外情况）。\n如果需要一个更大范围的原子性保证，Java 内存模型还提供了 lock 和 unlock 操作来满足需求。\n尽管虚拟机未把 lock 和 unlock 操作直接开放给用户使用，但却提供了更高层次的字节码指令 monitorenter 和 monitorexit 来隐式地使用这两个操作。\n这两个字节码指令反映到 Java 代码中就是同步块 synchronized 关键字，因此在 synchronized 块之间的操作也具备了原子性。\n6.2 可见性可见性就是指，当一个线程修改了共享变量的值时，其他线程能够立即得知这个修改。\nJava 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性的，无论是普通变量还是 volatile 变量都是如此。\n普通变量与 volatile 变量的区别是，volatile 的特殊规则保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。因此 volatile 保证了多线程操作时变量的可见性，而普通变量则不能保证这一点。\n（1）synchronized 实现可见性同步块的可见性是由 ”对一个变量执行 unlock 操作之前，必须先把此变量同步回主内存中（执行 store、write操作）“ 这条规则获得的。\n（2）final 关键字实现可见性final 关键字的可见性是指：\n被 final 修饰的字段在构造器中一旦被初始化完成，并且构造器没有把 “this” 的引用传递出去（this 引用逃逸是一件很危险的事情，其他线程有可能通过这个引用访问的 “初始化了一半” 的对象），那么在其他线程中就能看见 final 字段的值。\n6.3 有序性Java 程序中天然的有序性可以总结为一句话：\n如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。\n前半句是指 “线程内似表现为串行的语义。\n后半句是指 ”指令重排序“ 现象和 ”工作内存与主内存同步延迟“ 现象。\n（1）volatile 实现有序性volatile 关键字本身就包含了禁止指令重排序的语义。\n（2）synchronized 实现有序性synchronized 由 “一个变量在同一时刻只允许一条线程对其进行 lock 操作” 这条规则获得的，\n这个规则决定了持有同一个锁的两个同步块只能串行地进入。\n7. 先行发生原则7.1 什么是先行发生原则先行发生原则是 Java 内存模型中定义的两项操作之间的偏序关系。\n比如果操作 A 先行发生于操作 B，其实就是说在发生操作 B 之前，操作 A 产生的影响能被操作 B 观察到。\n“影响” 包括修改了内存中共享变量的值、发送了消息、调用了方法等。\n例如：\n// 以下操作在线程A中执行i = 1;// 以下操作在线程B中执行j = i;// 以下操作在线程C中执行i = 2;\n\n7.2 先行发生规则以下是 Java 内存模型下一些 “天然的” 先行发生关系，这些先行发生关系无需任何同步器协助就已经存在，可以在编码中直接使用。\n如果两个操作之间的关系不在此列，并且无法从下列规则推导出来，则它们就没有顺序性保障，虚拟机可以对它们随意进行重排序。\n\n程序次序规则： 在一个线程内，按照控制流顺序，书写在前面的操作先行发生于写在后面的操作。注意，这里说的是 控制流顺序 而不是 程序代码顺序 ，因为要考虑分支、循环等结构。\n管程锁定规则：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。这里必须强调的是 “同一个锁”，而 “后面” 是指时间上的先后。\nvolatile 变量规则：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作，这里的 “后面” 同样是指时间上的先后。\n线程启动规则：Thread 对象的 start() 方法先行发生于此线程的每一个动作。\n线程终止规则：线程中的所有操作都先行发生于对此线程的终止检测。我们可以通过 Thread::join 方法是否结束、Thread::isAlive() 方法的返回值等手段检测线程是否已经终止执行。\n线程中断规则：对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断时间的发生。可以通过 Thread::interrupted() 方法检测到是否有中断发生。\n对象终结规则：一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始。\n传递性：如果操作 A 先行发生于操作 B ，操作 B 先行发生于操作 C，那就可以得出操作 A 先行发生于操作 C 的结论。\n\n","categories":["Java","JVM"],"tags":["深入理解JVM虚拟机（第三版）"]},{"title":"RocketMQ 源码之safepoint","url":"/posts/12530/","content":"这篇文章要从一个奇怪的注释说起，就是下面这张图：\n\n我们可以不用管具体的代码逻辑，只是单单看这个 for 循环。\n在循环里面，专门有个变量 j，来记录当前循环次数。\n第一次循环以及往后每 1000 次循环之后，进入一个 if 逻辑。\n在这个 if 逻辑之上，标注了一个注释：prevent gc.\nprevent，这个单词如果不认识的同学记一下，考试肯定要考的：\n\n这个注释翻译一下就是：防止 GC 线程进行垃圾回收。\n具体的实现逻辑是这样的：\n\n核心逻辑其实就是这样一行代码：\n\nThread.sleep(0);\n\n这样就能实现 prevent gc 了？\n懵逼吗？\n懵逼就对了，懵逼就说明值得把玩把玩。\n这个代码片段，其实是出自 RocketMQ 的源码：\n\norg.apache.rocketmq.store.logfile.DefaultMappedFile#warmMappedFile\n\n\n事先需要说明的是，我并没有找到写这个代码的人问他的意图是什么，所以我只有基于自己的理解去推测他的意图。如果推测的不对，还请多多指教。\n虽然这是 RocketMQ 的源码，但是基于我的理解，这个小技巧和 RocketMQ 框架没有任何关系，完全可以脱离于框架存在。\n我给出的修改意见是这样的：\n\n把 int 修改为 long，然后就可以直接把 for 循环里面的 if 逻辑删除掉了。\n这样一看是不是更加懵逼了？\n不要慌，接下来，我给你抽丝剥个茧。\n另外，在“剥茧”之前，我先说一下结论：\n\n提出这个修改方案的理论立足点是 Java 的安全点相关的知识，也就是 safepoint。\n官方最后没有采纳这个修改方案。\n官方采没采纳不重要，重要的是我高低得给你“剥个茧”。\n\n探索当我知道这个代码片段是属于 RocketMQ 的时候，我想到的第一个点就是从代码提交记录中寻找答案。\n看提交者是否在提交代码的时候说明了自己的意图。\n于是我把代码拉了下来，一看提交记录是这样的：\n\n我就知道这里不会有答案了。\n因为这个类第一次提交的时候就已经包含了这个逻辑，而且对应这次提交的代码也非常多，并没有特别说明对应的功能。\n从提交记录上没有获得什么有用的信息。\n于是我把目光转向了 github 的 issue，拿着关键词 prevent gc 搜索了一番。\n除了第一个链接之外，没有找到什么有用的信息：\n\n而第一个链接对应的 issues 是这个：\n\nhttps://github.com/apache/rocketmq/issues/4902\n\n这个 issues 其实就是我们在讨论这个问题的过程中提出来的，也就是前面出现的修改方案：\n也就是说，我想通过源码或者 github 找到这个问题权威的回答，是找不到了。\n于是我又去了这个神奇的网站，在里面找到了这个 2018 年提出的问题：\n\nhttps://stackoverflow.com/questions/53284031/why-thread-sleep0-can-prevent-gc-in-rocketmq\n\n\n问题和我们的问题一模一样，但是这个问题下面就这一个回答：\n\n这个回答并不好，因为我觉得没答到点上，但是没关系，我刚好可以把这个回答作为抓手，把差的这一点拉通对齐一下，给它赋能。\n先看这个回答的第一句话：It does not（它没有）。\n问题就来了：“它”是谁？“没有”什么？\n“它”，指的就是我们前面出现的代码。\n“没有”，是说没有防止 GC 线程进行垃圾回收。\n这个的回答说：通过调用 Thread.sleep(0) 的目的是为了让 GC 线程有机会被操作系统选中，从而进行垃圾清理的工作。它的副作用是，可能会更频繁地运行 GC，毕竟你每 1000 次迭代就有一次运行 GC 的机会，但是好处是可以防止长时间的垃圾收集。\n换句话说，这个代码是想要“触发”GC，而不是“避免”GC，或者说是“避免”时间很长的 GC。从这个角度来说，程序里面的注释其实是在撒谎或者没写完整。\n不是 prevent gc，而是对 gc 采取了“打散运行，削峰填谷”的思想，从而 prevent long time gc。\n但是你想想，我们自己编程的时候，正常情况下从来也没冒出过“这个地方应该触发一下 GC”这样想法吧？\n因为我们知道，Java 程序员来说，虚拟机有自己的 GC 机制，我们不需要像写 C 或者 C++ 那样得自己管理内存，只要关注于业务代码即可，并没有特别注意 GC 机制。\n那么本文中最关键的一个问题就来了：为什么这里要在代码里面特别注意 GC，想要尝试“触发”GC 呢？\n先说答案：safepoint，安全点。\n关于安全点的描述，我们可以看看《深入理解 JVM 虚拟机(第三版)》的 3.4.2 小节：\n\n注意书里面的描述：\n\n有了安全点的设定，也就决定了用户程序执行时并非在代码指令流的任意位置都能够停顿下来开始垃圾收集，而是强制要求必须执行到达安全点后才能够暂停。\n\n换言之：没有到安全点，是不能 STW，从而进行 GC 的。\n如果在你的认知里面 GC 线程是随时都可以运行的。那么就需要刷新一下认知了。\n接着，让我们把目光放到书的 5.2.8 小节：由安全点导致长时间停顿。\n里面有这样一段话：\n\n我把划线的部分单独拿出来，你仔细读一遍：\n\n是 HotSpot 虚拟机为了避免安全点过多带来过重的负担，对循环还有一项优化措施，认为循环次数较少的话，执行时间应该也不会太长，所以使用 int 类型或范围更小的数据类型作为索引值的循环默认是不会被放置安全点的。这种循环被称为可数循环（Counted Loop），相对应地，使用 long 或者范围更大的数据类型作为索引值的循环就被称为不可数循环（Uncounted Loop），将会被放置安全点。\n\n意思就是在可数循环（Counted Loop）的情况下，HotSpot 虚拟机搞了一个优化，就是等循环结束之后，线程才会进入安全点。\n反过来说就是：循环如果没有结束，线程不会进入安全点，GC 线程就得等着当前的线程循环结束，进入安全点，才能开始工作。\n什么是可数循环（Counted Loop）？\n书里面的这个案例来自于这个链接：\n\nhttps://juejin.cn/post/6844903878765314061\nHBase 实战：记一次 Safepoint 导致长时间 STW 的踩坑之旅\n\n如果你有时间，我建议你把这个案例完整的看一下，我只截取问题解决的部分：\n\n截图中的 while(i &lt; end) 就是一个可数循环，由于执行这个循环的线程需要在循环结束后才进入 Safepoint，所以先进入 Safepoint 的线程需要等待它。从而影响到 GC 线程的运行。\n所以，修改方案就是把 int 修改为 long。\n原理就是让其变为不可数循环（Uncounted Loop），从而不用等循环结束，在循环期间就能进入 Safepoint。\n接着我们再把目光拉回到这里：\n\n这个循环也是一个可数循环。\nThread.sleep(0) 这个代码看起来莫名其妙，但是我是不是可以大胆的猜测一下：故意写这个代码的人，是不是为了在这里放置一个 Safepoint 呢，以达到避免 GC 线程长时间等待，从而加长 stop the world 的时间的目的？\n所以，我接下来只需要找到 sleep 会进入 Safepoint 的证据，就能证明我的猜想。\n你猜怎么着？\n本来是想去看一下源码，结果啪的一下，在源码的注释里面，直接找到了：\n\nhttps://hg.openjdk.java.net/jdk8u/jdk8u/hotspot/file/tip/src/share/vm/runtime/safepoint.cpp\n\n\n注释里面说，在程序进入 Safepoint 的时候， Java 线程可能正处于框起来的五种不同的状态，针对不同的状态有不同的处理方案。\n本来我想一个个的翻译的，但是信息量太大，我消化起来有点费劲儿，所以就不乱说了。\n主要聚焦于和本文相关的第二点：Running in native code。\n\nWhen returning from the native code, a Java thread must check the safepoint _state to see if we must block.\n\n第一句话，就是答案，意思就是一个线程在运行 native 方法后，返回到 Java 线程后，必须进行一次 safepoint 的检测。\n同时我在知乎看到了 R 大的这个回答，里面有这样一句，也印证了这个点：\n\nhttps://www.zhihu.com/question/29268019/answer/43762165\n\n\n那么接下来，就是见证奇迹的时刻了：\n\n根据 R 大的说法：正在执行 native 函数的线程看作“已经进入了 safepoint”，或者把这种情况叫做“在 safe-region 里”。\nsleep 方法就是一个 native 方法，你说巧不巧？\n所以，到这里我们可以确定的是：调用 sleep 方法的线程会进入 Safepoint。\n另外，我还找到了一个 2013 年的 R 大关于类似问题讨论的帖子：\n\nhttps://hllvm-group.iteye.com/group/topic/38232?page=2\n\n\n这里就直接点名道姓的指出了：Thread.sleep(0).\n这让我想起以前有个面试题问：Thread.sleep(0) 有什么用。\n当时我就想：这题真难（S）啊（B）。现在发现原来是我道行不够，小丑竟是我自己。\n还真的是有用。\n实践前面其实说的都是理论。\n这一部分我们来拿代码实践跑上一把，就拿我之前分享过的《真是绝了！这段被 JVM 动了手脚的代码！》文章里面的案例。\npublic class MainTest &#123;    public static AtomicInteger num = new AtomicInteger(0);    public static void main(String[] args) throws InterruptedException &#123;        Runnable runnable=()-&gt;&#123;            for (int i = 0; i &lt; 1000000000; i++) &#123;                num.getAndAdd(1);            &#125;            System.out.println(Thread.currentThread().getName()+&quot;执行结束!&quot;);        &#125;;        Thread t1 = new Thread(runnable);        Thread t2 = new Thread(runnable);        t1.start();        t2.start();        Thread.sleep(1000);        System.out.println(&quot;num = &quot; + num);    &#125;&#125;\n\n这个代码，你直接粘到你的 IDEA 里面去就能跑。\n按照代码来看，主线程休眠 1000ms 后就会输出结果，但是实际情况却是主线程一直在等待 t1,t2 执行结束才继续执行。\n\n这个循环就属于前面说的可数循环（Counted Loop）。\n这个程序发生了什么事情呢？\n\n1.启动了两个长的、不间断的循环（内部没有安全点检查）。\n2.主线程进入睡眠状态 1 秒钟。\n3.在 1000 ms 之后，JVM 尝试在 Safepoint 停止，以便 Java 线程进行定期清理，但是直到可数循环完成后才能执行此操作。\n4.主线程的 Thread.sleep 方法从 native 返回，发现安全点操作正在进行中，于是把自己挂起，直到操作结束。\n\n所以，当我们把 int 修改为 long 后，程序就表现正常了：\n\n受到 RocketMQ 源码的启示，我们还可以直接把它的代码拿过来：\n\n这样，即使 for 循环的对象是 int 类型，也可以按照预期执行。因为我们相当于在循环体中插入了 Safepoint。\n另外，我通过不严谨的方式测试了一下两个方案的耗时：\n\n在我的机器上运行了几次，时间上都差距不大。\n但是要论逼格的话，还得是右边的 prevent gc 的写法。没有二十年功力，写不出这一行“看似无用”的代码！\n额外提一句再说一个也是由前面的 RocketMQ 的源码引起的一个思考：\n\n这个方法是在干啥？\n预热文件，按照 4K 的大小往 byteBuffer 放 0，对文件进行预热。\n\nbyteBuffer.put(i, (byte) 0);\n\n为什么我会对这个 4k 的预热比较敏感呢？\n去年的天池大赛有这样的一个赛道：\n\nhttps://tianchi.aliyun.com/competition/entrance/531922/information\n\n\n其中有两个参赛选大佬都提到了“文件预热”的思路。\n我把链接放在下面了，有兴趣的可以去细读一下：\n\nhttps://tianchi.aliyun.com/forum/postDetail?spm=5176.12586969.0.0.13714154spKjib&amp;postId=300892\n\n\n\n\nhttps://tianchi.aliyun.com/forum/postDetail?spm=5176.21852664.0.0.4c353a5a06PzVZ&amp;postId=313716\n\n\n","categories":["Java","JVM"]},{"title":"ZGC 垃圾收集器","url":"/posts/39233/","content":"一、介绍ZGC (“Z”并非什么专业名词的缩写，这款收集器的名字就叫作ZGarbage Collector)是一款在JDK 11中新加入的具有实验性质的低延迟垃圾收集器，是由Oracle 公司研发的。\n2018年Oracle创建了JEP333将ZGC提交给OpenJDK，推动其进入OpenJDK11的发布清单之中。\nZGC 和 Shenandoah 的目标是高度相似的，都希望在尽可能对吞吐量影响不太大的前提下目，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。\n但是 ZGC 和 Shenandoah 的实现思路又是差异显著的。\n如果说 RedHat 公司开发的 Shenandoah 像是Oracle的 G1 收集器的实际继承者的话，那Oracle 公司开发的 ZGC 就更像是Azul System公司独步天下的 PGC (Pauseless GC)和 C4 (Concurrent Continuously Compacting Collector)收集器的同胞兄弟。\n早在2005年，运行在AzulVM上的 PGC 就已经实现了标记和整理阶段都全程与用户线程并发运行的垃圾收集，而运行在ZingVM上的C4收集器是PGC继续演进的产物，主要增加了分代收集支持，大幅提升了收集器能够承受的对象分配速度。\n无论从算法还是实现原理上来讲，PGC 和 C4 肯定算是一脉相承的，而ZGC虽然并非Azul公司的产品，但也应视为这条脉络上的另一个节点，因为 ZGC 几乎所有的关键技术上，与PGC和C4都只存在术语称谓上的差别，实质内容几乎是一模一样的。\n相信到这里读者应该已经对 Java 虚拟机收集器常见的专业术语都有所了解了，如果不避讳专业术语的话，我们可以给ZGC下一个这样的定义来概括它的主要特征：\nZGC 收集器是一款基于 Region 内存布局的，(暂时)不设分代的，使用了读屏障、染色指针和内存多重映射等技术来实现可并发的 标记-整理 算法的，以低延迟为首要目标的一款垃圾收集器。\n二、ZGC 内存布局与 Shenandoah 和 G1 一样，ZGC 也采用基于 Region 的堆内存布局，但与它们不同的是，\nZGC 的 Region （在一些官方资料中将它称为 Page 或者 ZPage，本章为行文一致继续称为 Region）具有动态性——动态创建和销毁，以及动态的区域容量大小。\n在x64硬件平台下，ZGC 的 Region 可以具有如图3-19所示的大、中、小三类容量：\n\n小型 Region（Small Region）：容量固定为2MB，用于放置小于256KB的小对象。\n中型 Region（Medium Region）：容量固定为32MB，用于放置大于等于256KB 但小于4MB的对象。\n大型Region（Large Region）：容量不固定，可以动态变化，但必须为2MB的整数倍，用于放置4MB或以上的大对象。每个大型 Region 中只会存放一个大对象，这也预示着虽然名字叫作”大型Region”，但它的实际容量完全有可能小于中型Region，最小容量可低至4MB。大型Region在ZGC的实现中是不会被重分配（重分配是ZGC的一种处理动作，用于复制对象的收集器阶段，稍后会介绍到）的，因为复制一个大对象的代价非常高昂。\n\n\n三、染色指针Shenandoah 使用 转发指针 和 读屏障 来实现并发整理，ZGC 虽然同样用到了 读屏障，但用的却是一条与 Shenandoah 完全不同，更加复杂精巧的解题思路。\nZGC 收集器有一个标志性的设计是它采用的 染色指针 技术（Colored Pointer，其他类似的技术中可能将它称为Tag Pointer或者Version Pointer）。\n（1）为什么需要染色指针？从前，如果我们要在对象上存储一些额外的、只供收集器或者虚拟机本身使用的数据，通常会在对象头中增加额外的存储字段，如对象的哈希码、分代年龄、锁记录等就是这样存储的。\n这种记录方式在有对象访问的场景下是很自然流畅的，不会有什么额外负担。但如果对象存在被移动过的可能性，即不能保证对象访问能够成功呢？又或者有一些根本就不会去访问对象，但又希望得知该对象的某些信息的应用场景呢？\n能不能从指针或者与对象内存无关的地方得到这些信息，譬如是否能够看出来对象被移动过？\n这样的要求并非不合理的刁难，先不去说并发移动对象可能带来的可访问性问题，此前我们就遇到过这样的要求——追踪式收集算法 的标记阶段就可能存在只跟指针打交道而不必涉及指针所引用的对象本身的场景。\n例如对象标记的过程中需要给对象打上三色标记，这些标记本质上就只和对象的引用有关，而与对象本身无关——某个对象只有它的引用关系能决定它存活与否，对象上其他所有的属性都不能够影响它的存活判定结果。\nHotSpot 虚拟机的几种收集器有不同的标记实现方案。\n有的把标记直接记录在对象头上(如Serial收集器)，有的把标记记录在与对象相互独立的数据结构上（如G1、Shenandoah 使用了一种相当于堆内存的1/64大小的、称为BitMap的结构来记录标记信息）。\n而 ZGC 的染色指针是最直接的、最纯粹的，它直接把标记信息记在引用对象的指针上，这时，与其说可达性分析是遍历对象图来标记对象、还不如说是遍历“引用图”来标记“引用”了。\n（2）为什么指针本身也可以存储额外信息？染色指针是一种直接将少量额外的信息存储在指针上的技术，可是为什么指针本身也可以存储额外信息呢？\n在64位系统中，理论可以访问的内存高达16EB（2的64次幂）字节。\n实际上，基于需求（用不到那么多内存）、性能（地址越宽在做地址转换时需要的页表级数越多）和成本（消耗更多晶体管）的考虑，在AMD64架构中只支持到52位（4PB）的地址总线和48位（256TB）的虚拟地址空间，所以目前64位的硬件实际能够支持的最大内存只有256TB。\n此外，操作系统一侧也还会施加自己的约束，64位的Linux则分别支持47位（128TB）的进程虚拟地址空间和46位（64TB）的物理地址空间，64位的Windows系统甚至只支持44位（16TB）的物理地址空间。\n尽管 Linux下64位指针的高18位不能用来寻址，但剩余的46位指针所能支持的 64TB内存在今天仍然能够充分满足大型服务器的需要。\n鉴于此，ZGC 的染色指针技术继续盯上了这剩下的46位指针宽度，将其高4位提取出来存储四个标志信息。\n通过这些标志位，虚拟机可以直接从指针中看到其引用对象的三色标记状态、是否进入了重分配集（即被移过）、是否只能通过finalize()方法才能被访问到。\n\n由于这些标志进一步压缩了原本就只有46位的地址空间，也直接导致 ZGC 能够管理的内存不可以超过4TB（2的42次幂）。\n虽然 染色指针有4TB的内存限制，不能支持32位平台，不能支持压缩指针（-XX:+UseCompressedOops）等诸多约束，但它带来的收益也是非常可观的。\n（3）染色指针三大优势\n染色指针可以使得一旦某个 Region 的存活对象被移走之后，这个 Region 立即就能够被释放和重用掉，而不必等待整个堆中所有指向该 Region 的引用都被修正后才能清理。这点相比起 Shenandoah 是一个颇大的优势，使得理论上只要还有一个空闲 Region，ZGC 就能完成收集，而 Shenandoah 需要等到引用更新阶段结束以后才能释放回收集中的 Region，这意味着堆中几乎所有对象都存活的极端情况，需要1:1复制对象到新 Region 的话，就必须要有一半的空闲 Region 来完成收集。至于为什么染色指针能够导致这样的结果，笔者将在后续解释其“自愈”特性的时候进行解释。\n染色指针可以大幅减少在垃圾收集过程中内存屏障的使用数量，设置内存屏障，尤其是写屏障的目的通常是为了记录对象引用的变动情况，如果将这些信息直接维护在指针中，显然就可以省去一些专门的记录操作。实际上，到目前为止 ZGC 都并未使用任何写屏障，只使用了读屏障（一部分是染色指针的功劳，一部分是ZGC现在还不支持分代收集，天然就没有跨代引用的问题）。内存屏障对程序运行时性能的损耗在前面章节中已经讲解过，能够省去一部分的内存屏障，显然对程序运行效率是大有裨益的，所以 ZGC 对吞吐量的影响也相对较低。\n染色指针可以作为一种可扩展的存储结构用来记录更多与对象标记、重定位过程相关的数据，以便日后进一步提高性能。现在Linux下的64位指针还有前18位并未使用，它们虽然不能用来寻址，却可以通过其他手段用于信息记录。如果开发了这18位，既可以腾出已用的4个标志位，将 ZGC 可支持的最大堆内存从4TB 拓展到64TB，也可以利用其余位置再存储更多的标志，譬如存储一些追踪信息来让垃圾收集器在移动对象时能将低频次使用的对象移动到不常访问的内存区域。\n\n不过要顺利应用染色指针有一个必须解决的前置问题：\nJava 虚拟机作为一个普普通通的进程，这样随意重新定义内存中某些指针的其中几位，操作系统是否支持？处理器是否支持？\n这是很现实的问题，无论中间过程如何，程序代码最终都要转换为机器指令流交付给处理器去执行，处理器可不会管指令流中的指针哪部分存的是标志位，哪部分才是真正的寻址地址，只会把整个指针都视作一个内存地址来对待。\n这个问题在 Solaris/SPARC 平台上比较容易解决，因为 SPARC 硬件层面本身就支持虚拟地址掩码，设置之后其机器指令直接就可以忽略掉染色指针中的标志位。\n但在x86-64平台上并没有提供类似的黑科技 ZGC 设计者就只能采取其他的补救措施了，这里面的解决方案要涉及虚拟内存映射技术，让我们先来复习一下这个x86计算机体系中的经典设计。\n（4）虚拟内存映射技术在远古时代的 x86 计算机系统中，所有进程都是共用同一块物理内存空间的，这样会导致不同进程之间的内存无法相互隔离，当一个进程污染了别的进程内存后，就只能对整个系统进行复位后才能得以恢复。\n为了解决这个问题，从 Intel 80386 处理器开始，提供了 “保护模式” 用于隔离进程， 386 处理器的全部 32 条地址寻址线都有效，进程可访问最高内存也可达 4 GB 的内存空间，但此时已不同于之前实模式下的物理内存寻址了。\n处理器会使用分页管理机制把线性地址空间和物理地址空间分别划分为大小相同的块，这样的内存块被称为“页”（Page）。\n通过在线性虚拟空间的页与物理地址空间的页之间建立的映射表，分页管理机制会进行线性地址到物理地址空间的映射，完成线性地址到物理地址的转换。\n如果读者对计算机结构体系了解不多的话，不妨设想这样一个场景来类比：假如你要去“中山一路3号”这个地址拜访一位朋友，根据你所处城市的不同，譬如在广州或者在上海，是能够通过这个“相同的地址”定位到两个完全独立的物理位置的，这时地址与物理位置是一对多关系映射。\n不同层次的虚拟内存到物理内存的转换关系可以在硬件层面、操作系统层面或者软件进程层面实现，如何完成地址转换，是一对一、多对一还是一对多的映射，也可以根据实际需要来设计。\nLinux/x86-64平台上的 ZGC 使用了 多重映射（Multi-Mapping）将多个不同的虚拟内存地址映射到同一个物理内存地址上，这是一种多对一映射，意味着 ZGC 在虚拟内存中看到的地址空间要比实际的堆内存容量来得更大。\n把染色指针中的标志位看作是地址的分段符，那只要将这些不同的地址段都映射到同一个物理内存空间，经过多重映射转换后，就可以使用染色指针正常进行寻址了。\n\n在某些场景下，多重映射技术确实可能会带来一些诸如复制大对象时会更容易这样的额外好处，可从根源上讲，ZGC 的多重映射只是它采用染色指针技术的伴生产物，并不是专门为了实现其他某种特性需求而去做的。\n四、ZGC 收集过程ZGC 的运作过程大致可划分为以下四个大的阶段。\n全部四个阶段都是可以并发执行的，仅是两个阶段中间会存在短暂的停顿小阶段，这些小阶段，譬如初始化 GCRoot 直接关联对象的 MarkStart，与之前 G1 和 Shenandoah 的 Initial Mark 阶段并没有什么差异。\n\n\n并发标记（Concurrent Mark）：与 G1、Shenandoah 一样，并发标记是遍历对象图做可达性分析的阶段，前后也要经过类似于G1、Shenandoah 的初始标记、最终标记（尽管 ZGC 中的名字不叫这些）的短暂停顿，而且这些停顿阶段所做的事情在目标上也是相类似的。与 G1、Shenandoah 不同的是，ZGC 的标记是在指针上而不是在对象上进行的，标记阶段会更新染色指针中的 Marked0、Marked1 标志位。\n并发预备重分配（Concurrent Prepare for Relocate）：这个阶段需要根据特定的查询条件统计得出本次收集过程要清理哪些 Region，将这些 Region 组成重分配集（Relocation Set）。重分配集与 G1 收集器的回收集（Collection Set）还是有区别的 ZGC 划分 Region 的目的并非为了像 G1 那样做收益优先的增量回收。相反，ZGC 每次回收都会扫描所有的 Region，用范围更大的扫描成本换取省去 G1 中记忆集的维护成本。因此，ZGC 的重分配集只是决定了里面的存活对象会被重新复制到其他的 Region 中，里面的 Region 会被释放，而并不能说回收行为就只是针对这个集合里面的 Region 进行，因为标记过程是针对全堆的。此外，在JDK 12 的 ZGC 中开始支持的类卸载以及弱引用的处理，也是在这个阶段中完成的。\n并发重分配（Concurrent Relocate）：重分配是 ZGC 执行过程中的核心阶段，这个过程要把重分配集中的存活对象复制到新的 Region 上，并为重分配集中的每个 Region 维护一个转发表（Forward Table），记录从旧对象到新对象的转向关系。得益于染色指针的支持，ZGC 收集器能仅从引用上就明确得知一个对象是否处于重分配集之中，如果用户线程此时并发访问了位于重分配集中的对象，这次访问将会被预置的内存屏障所截获，然后立即根据 Region 上的转发表记录将访问转发到新复制的对象上，并同时修正更新该引用的值，使其直接指向新对象，ZGC 将这种行为称为指针的 “自愈” （Self-Healing）能力。这样做的好处是只有第一次访问旧对象会陷入转发，也就是只慢一次，对比 Shenandoah 的 Brooks 转发指针，那是每次对象访问都必须付出的固定开销，简单地说就是每次都慢，因此 ZGC 对用户程序的运行时负载要比 Shenandoah 来得更低一些。还有另外一个直接的好处是由于染色指针的存在，一旦重分配集中某个 Region 的存活对象都复制完毕后，这个 Region 就可以立即释放用于新对象的分配（但是转发表还得留着不能释放掉），哪怕堆中还有很多指向这个对象的未更新指针也没有关系，这些旧指针一旦被使用，它们都是可以自愈的。 \n并发重映射（Concurrent Remap）：重映射所做的就是修正整个堆中指向重分配集中旧对象的所有引用，这一点从目标角度看是与 Shenandoah 并发引用更新阶段一样的，但是 ZGC 的并发重映射并不是一个必须要 “迫切” 去完成的任务，因为前面说过，即使是旧引用，它也是可以自愈的，最多只是第一次使用时多一次转发和修正操作。重映射清理这些旧引用的主要目的是为了不变慢（还有清理结束后可以释放转发表这样的附带收益），所以说这并不是很 “迫切”。因此，ZGC 很巧妙地把并发重映射阶段要做的工作，合并到了下一次垃圾收集循环中的并发标记阶段里去完成反正它们都是要遍历所有对象的，这样合并就节省了一次遍历对象图的开销。一旦所有指针都被修正之后，原来记录新旧对象关系的转发表就可以释放掉了。\n\nZGC 的设计理念与AzulSystem公司的 PGC 和 C4 收集器一脉相承，是迄今垃圾收集器研究的最前沿成果，它与 Shenandoah 一样做到了几乎整个收集过程都全程可并发，短暂停顿也只与 GC Roots 大小相关而与堆内存大小无关，因而同样实现了任何堆上停顿都小于十毫秒的目标。\n相比 G1、Shenandoah 等先进的垃圾收集器，ZGC 在实现细节上做了一些不同的权衡选择。\n譬如 G1 需要通过写屏障来维护记忆集，才能处理跨代指针，得以实现 Region 的增量回收。记忆集要占用大量的内存空间，写屏障也对正常程序运行造成额外负担，这些都是权衡选择的代价。\nZGC就完全没有使用记忆集，它甚至连分代都没有，连像 CMS 中那样只记录新生代和老年代间引用的卡表也不需要，因而完全没有用到写屏障，所以给用户线程带来的运行负担也要小得多。\n可是，必定要有优有劣才会称作权衡。\nZGC 劣势ZGC 的这种选择也限制了它能承受的对象分配速率不会太高，可以想象以下场景来理解 ZGC 的这个劣势：\nZGC 准备要对一个很大的堆做一次完整的并发收集，假设其全过程要持续十分钟以上（请读者切勿混淆并发时间与停顿时间，ZGC 立的Flag是停顿时间不超过十毫秒），在这段时间里面，由于应用的对象分配速率很高，将创造大量的新对象，这些新对象很难进入当次收集的标记范围，通常就只能全部当作存活对象来看待——尽管其中绝大部分对象都是朝生夕灭的，这就产生了大量的浮动垃圾。\n如果这种高速分配持续维持的话，每一次完整的并发收集周期都会很长，回收到的内存空间持续小于期间并发产生的浮动垃圾所占的空间堆中剩余可腾挪的空间就越来越小了。\n目前唯一的办法就是尽可能地增加堆容量大小，获得更多喘息的时间。但是若要从根本上提升 ZGC 能够应对的对象分配速率，还是需要引入分代收集，让新生对象都在一个专门的区域中创建，然后专门针对这个区域进行更频繁、更快的收集。\nAzul的 C4 收集器实现了分代收集后，能够应对的对象分配速率就比不分代的 PGC 收集器提升了十倍之多。\nZGC 优点ZGC 还有一个常在技术资料上被提及的优点是支持 “NUMA-Aware” 的内存分配。\nNUMA（Non-Uniform Memory Access，非统一内存访问架构）是一种为多处理器或者多核处理器的计算机所设计的内存架构。\n由于摩尔定律逐渐失效，现代处理器因频率发展受限转而向多核方向发展，以前原本在北桥芯片中的内存控制器也被集成到了处理器内核中，这样每个处理器核心所在的裸晶（DIE）都有属于自己内存管理器所管理的内存，如果要访问被其他外理器核心管理的内存，就必须通过 Inter-Connect 通道来完成，这要比访问处理器的本地内存慢得多。\n在 NUMA 架构下，ZGC 收集器会优先尝试在请求线程当前所处的处理器的本地内存上分配对象，以保证高效内存访问。\n在 ZGC 之前的收集器就只有针对吞吐量设计的 Parallel Scavenge 支持 NUMA 内存分配，如今 ZGC 也成为另外一个选择。\n在性能方面，尽管目前还处于实验状态，还没有完成所有特性，稳定性打磨和性能调优也仍在进行，但即使是这种状态下的 ZGC，其性能表现已经相当亮眼，从官方给出的测试结果来看，用 “令人震惊的、革命性的 ZGC” 来形容都不为过。\n图3-23 和图3-24是 ZGC 与 Parallel Scavenge 、G1 三款收集器通过 SPECjbb 2015的测试结果。\n在 ZGC 的 “弱项” 吞吐量方面，以低延迟为首要目标的 ZGC 已经达到了以高吞吐量为目标 Parallel Scavenge 的 99%，直接超越了G1。如果将吞吐量测试设定为面向 SLA（Service Level Agreements）应用的 “Critical Throughput” 的话，ZGC 的表现甚至还反超了 Parallel Scavenge 收集器。而在 ZGC 的强项停顿时间测试上，它就毫不留情地与 Parallel Scavenge、G1 拉开了两个数量级的差距。不论是平均停顿，还是95%停顿、99%停顿、99.9%停顿，抑或是最大停顿时间，ZGC 均能毫不费劲地控制在十毫秒之内，以至于把它和另外两款停顿数百近千毫秒的收集器放到一起对比，就几乎显示不了 ZGC 的柱状条(图3-24a)，必须把结果的纵坐标从线性尺度调整成对数尺度（图 3-24b，纵坐标轴的尺度是对数增长的）才能观察到 ZGC 的测试结果。\n\n","categories":["Java","JVM"],"tags":["深入理解JVM虚拟机（第三版）"]},{"title":"运行时数据区","url":"/posts/53185/","content":"\n1.程序计数器程序计数器 是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。\n在 Java 虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。\n由于 Java 虚拟机的多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。\n因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为 “线程私有” 的内存。\n如果线程正在执行的是一个 Java 方法，这个计数器记录的是 正在执行的虚拟机字节码指令的地址。\n如果正在执行的是本地方法，这个计数器值为空（Undefined）。\n异常状况此内存区域是唯一一个在《Java虚拟机规范》中没有规定任何 OutOfMemoryError 情况的区域。\n2.Java 虚拟机栈Java 虚拟机栈是线程私有的，生命周期与线程相同。\n虚拟机栈描述的是 Java 方法执行的线程内存模型：\n每个方法被执行的时候，Java 虚拟机都会同步创建一个 栈帧 用于存储 局部变量表、操作数栈、动态链接、方法出口 等信息。\n每一个栈帧被调用直至执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。\n局部变量表局部变量表 存放了编译期可知的各种 Java 虚拟机基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它并不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或者其他与此对象相关的位置）和 returnAddress 类型 （指向了一条字节码指令的地址）。\n这些数据类型在局部变量表中的存储空间以 局部变量槽（Slot） 来表示。\nlong 和 double 类型的数据会占用两个变量槽，其余的类型只占用一个。\n局部变量所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。\n这里说的 “大小” 是指变量槽的数量。\n虚拟机真正使用多大的内存空间来实现一个变量槽，譬如按照1个变量槽占用32个比特、64个比特，或者更多，这完全由具体的虚拟机实现自行决定。\n异常状况在《Java虚拟机规范》中，对这个内存区域规定了两类异常状况：\n\nStackOverflowError：线程请求的栈深度大于虚拟机所允许的深度；\nOutOfMemoryError：如果Java虚拟机栈容量可以动态扩展，当栈扩展时无法申请到足够的内存会抛出。\n\n\nHotSpot 虚拟机的栈容量是不可以动态扩展的，以前的 Classic 虚拟机倒是可以。\n所以在 HotSpot 虚拟机上是不会由于虚拟机栈无法扩展而导致 OutOfMemoryError 异常 ——只要线程申请栈空间成功了就不会有 OOM。\n但是如果申请时就失败了，仍然会出现 OOM异常。\n\n3.Java 堆Java 堆是虚拟机所管理的内存中最大的一块。\nJava 堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。\n此内存区域的唯一目的就是存放对象实例，“几乎”所有的 对象实例都在这里分配内存。\n在《Java虚拟机规范》中对Java堆的描述是：所有的对象实例以及数组都应当在堆上分配。\n这里的 几乎 是指从实现角度看，随着Java语言的发展，现在已经能看到些许迹象表明日后可能出现值类型的支持，即使值考虑现在，由于即时编译技术的进步，尤其是逃逸分析技术的日渐强大，栈上分配、标量替换优化手段已经导致一些微妙的变化悄然发生，所以说Java对象实例都分配在堆上也渐渐变得不是那么绝对了。\n根据《Java虚拟机规范》的规定，Java堆可以处于物理上不连续的内存空间中，但在逻辑上它应该被视为连续的。\n这点就像我们用磁盘空间去存储文件一样，并不要求每个文件都连续存放。\n但对于大对象（如数组对象），多数虚拟机实现出于实现简单、存储高效的考虑，很可能会要求连续的内存空间。\n异常状况如果在Java堆中没有内存完成实例分配，并且堆也无法再扩展时，Java虚拟机将会抛出 OutOfMemoryError 异常。\n4.方法区方法区是各个线程共享的内存区域，用于存储 已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存 等数据。\n《Java虚拟机规范》中把方法区描述为 堆的一个逻辑分区，但他却有一个别名叫作 “非堆”（Non-Heap），目的是与Java堆区分开。\n与永久代的区别永久代与方法区本质上并不是等价的，因为仅仅是当时的 HotSpot 虚拟机设计团队选择把收集器的分代设计扩展至方法区，或者说使用永久代来实现方法区而已。\n这样做是的 HotSpot 的垃圾收集器能够像管理Java堆一样管理这部分内存，省去专门为方法区编写内存管理代码的工作。\n在 JDK 6 的时候 HotSpot 开发团队就有放弃永久代，逐步改为采用本地内存（Native Memory）来实现方法区的计划了。\nJDK 7 时，把原本放在永久代的 字符串常量池、静态变量 等移至移至Java堆中。\nJDK 8 时，完全放弃了永久代的概念，用在本地内存中实现的 元空间（Meta-space）来代替，把 JDK 7 中永久代还剩余的内容（主要是类型信息）全部移到元空间中了。\n运行时常量池运行时常量池是方法区的一部分。\nClass 文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是 常量池表，用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池。\n除了保存 Class 文件中描述的符号引用外，还会把 由符号引用翻译出来的直接引用也存储在运行时常量池中。\n异常状况根据《Java虚拟机规范》的规定，如果方法区无法满足新的内存分配需求时，将抛出 OutOfMemoryError 异常。\n5.直接内存直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是《Java虚拟机规范》中定义的内存区域。\n但是这部分内存也被频繁使用，而且也可能导致 OutOfMemoryError 异常出现。\n在 JDK 1.4 中新加入的 NIO 类，引入了一种基于通道（Channel）对于缓冲区（Buffer）的 I/O 方式，它可以使用 Native 函数库直接分配对外内存，然后通过一个存储在Java堆里面的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样能在一些场景总显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。\n显然，本机直接内存的分配不会受到Java堆大小的限制，但是，既然是内存，则肯定会收到本机总内存（包括物理内存、SWAP分区或者分页文件）大小以及处理器寻址空间的限制，一般服务器管理员配置虚拟机参数时，会根据实际内存去设置 -Xmx 等参数信息，但经常忽略掉直接内存，使得各个内存区域总和大于物理内存限制（包括物理的和操作系统级的限制），从而导致动态扩展时出现 OutOfMemoryError 异常。\n","categories":["Java","JVM"],"tags":["深入理解JVM虚拟机（第三版）"]},{"title":"127.0.0.1和0.0.0.0地址的区别","url":"/posts/4930/","content":"一、共同点\n都属于特殊地址。\n都属于A类地址。\n都是IPV4地址。\n\n接下来我们分别看下这两个地址：\n二、0.0.0.0首先，0.0.0.0是不能被ping通的。0.0.0.0称为“unspecified”，即未指定（即无效的，无意义的）地址。从功能上看，一般用于某些程序/网络协议中不便使用具体ip的特殊情况（说白了就是一个用于某些比较坑的情况的“占位符”），比如DHCP客户端还未获取到ip的时候规定使用0.0.0.0作“源地址”，或者在服务器中，0.0.0.0并不是一个真实的的IP地址，它表示本机中所有的IPV4地址。服务器不指定在哪个网卡上监听时，也使用0.0.0.0,这个时候监听本机中所有IP的端口。\n用途\n用作服务端，表示本机上的任意IPV4地址。\n\n三、127.0.0.1首先我们要先知道一个概念，凡是以127开头的IP地址，都是回环地址（Loop back address），其所在的回环接口一般被理解为虚拟网卡，并不是真正的路由器接口。\n所谓的回环地址，通俗的讲，就是我们在主机上发送给127开头的IP地址的数据包会被发送的主机自己接收，根本传不出去，外部设备也无法通过回环地址访问到本机。\n最后127.0.0.1属于{127,}集合中的一个，而所有网络号为127的地址都被称之为回环地址，所以回环地址！=127.0.0.1,它们是包含关系，即回环地址包含127.0.0.1。\n用途\n回环测试,通过使用ping 127.0.0.1 测试某台机器上的网络设备，操作系统或者TCP/IP实现是否工作正常。\nDDos攻击防御：网站收到DDos攻击之后，将域名A记录到127.0.0.1，即让攻击者自己攻击自己。\n大部分Web容器测试的时候绑定的本机地址。\n\n四、localhost相比127.0.0.1，localhost具有更多的意义。localhost是个域名，而不是一个ip地址。之所以我们经常把localhost与127.0.0.1认为是同一个是因为我们使用的大多数电脑上都讲localhost指向了127.0.0.1这个地址。\n但是localhost的意义并不局限于127.0.0.1。\nlocalhost是一个域名，用于指代this computer或者this host,可以用它来获取运行在本机上的网络服务。\n在大多数系统中，localhost被指向了IPV4的127.0.0.1和IPV6的::1。\n127.0.0.1    localhost::1          localhost\n\n所以，在使用的时候要注意确认IPV4还是IPV6\n五、总结127.0.0.1 是一个环回地址。并不表示“本机”。0.0.0.0才是真正表示“本网络中的本机”。\n在实际应用中，一般我们在服务端绑定端口的时候可以选择绑定到0.0.0.0，这样我的服务访问方就可以通过我的多个ip地址访问我的服务。\n比如我有一台服务器，一个外放地址A,一个内网地址B，如果我绑定的端口指定了0.0.0.0，那么通过内网地址或外网地址都可以访问我的应用。但是如果我之绑定了内网地址，那么通过外网地址就不能访问。 所以如果绑定0.0.0.0,也有一定安全隐患，对于只需要内网访问的服务，可以只绑定内网地址。\n","categories":["运维","Linux"]},{"title":"Linux 系统是如何收发网络包的","url":"/posts/1271/","content":"一、网络模型为了使得多种设备能通过网络相互通信，和为了解决各种不同设备在网络互联中的兼容性问题，国际标准化组织制定了开放式系统互联通信参考模型（Open System Interconnection Reference Model），也就是 OSI 网络模型，该模型主要有 7 层，分别是应用层、表示层、会话层、传输层、网络层、数据链路层以及物理层。\n每一层负责的职能都不同，如下：\n\n应用层，负责给应用程序提供统一的接口；\n表示层，负责把数据转换成兼容另一个系统能识别的格式；\n会话层，负责建立、管理和终止表示层实体之间的通信会话；\n传输层，负责端到端的数据传输；\n网络层，负责数据的路由、转发、分片；\n数据链路层，负责数据的封帧和差错检测，以及 MAC 寻址；\n物理层，负责在物理网络中传输数据帧；\n\n由于 OSI 模型实在太复杂，提出的也只是概念理论上的分层，并没有提供具体的实现方案。\n事实上，我们比较常见，也比较实用的是四层模型，即 TCP/IP 网络模型，Linux 系统正是按照这套网络模型来实现网络协议栈的。\nTCP/IP 网络模型共有 4 层，分别是应用层、传输层、网络层和网络接口层，每一层负责的职能如下：\n\n应用层，负责向用户提供一组应用程序，比如 HTTP、DNS、FTP 等;\n传输层，负责端到端的通信，比如 TCP、UDP 等；\n网络层，负责网络包的封装、分片、路由、转发，比如 IP、ICMP 等；\n网络接口层，负责网络包在物理网络中的传输，比如网络包的封帧、 MAC 寻址、差错检测，以及通过网卡传输网络帧等；\n\nTCP/IP 网络模型相比 OSI 网络模型简化了不少，也更加易记，它们之间的关系如下图：\n\n不过，我们常说的七层和四层负载均衡，是用 OSI 网络模型来描述的，七层对应的是应用层，四层对应的是传输层。\n二、Linux 网络协议栈我们可以把自己的身体比作应用层中的数据，打底衣服比作传输层中的 TCP 头，外套比作网络层中 IP 头，帽子和鞋子分别比作网络接口层的帧头和帧尾。\n在冬天这个季节，当我们要从家里出去玩的时候，自然要先穿个打底衣服，再套上保暖外套，最后穿上帽子和鞋子才出门，这个过程就好像我们把 TCP 协议通信的网络包发出去的时候，会把应用层的数据按照网络协议栈层层封装和处理。\n你从下面这张图可以看到，应用层数据在每一层的封装格式。\n\n其中：\n\n传输层，给应用数据前面增加了 TCP 头；\n网络层，给 TCP 数据包前面增加了 IP 头；\n网络接口层，给 IP 数据包前后分别增加了帧头和帧尾；\n\n这些新增的头部和尾部，都有各自的作用，也都是按照特定的协议格式填充，这每一层都增加了各自的协议头，那自然网络包的大小就增大了，但物理链路并不能传输任意大小的数据包，所以在以太网中，规定了最大传输单元（MTU）是 1500 字节，也就是规定了单次传输的最大 IP 包大小。\n当网络包超过 MTU 的大小，就会在网络层分片，以确保分片后的 IP 包不会超过 MTU 大小，如果 MTU 越小，需要的分包就越多，那么网络吞吐能力就越差，相反的，如果 MTU 越大，需要的分包就越少，那么网络吞吐能力就越好。\n知道了 TCP/IP 网络模型，以及网络包的封装原理后，那么 Linux 网络协议栈的样子，你想必猜到了大概，它其实就类似于 TCP/IP 的四层结构：\n\n从上图的的网络协议栈，你可以看到：\n\n应用程序需要通过系统调用，来跟 Socket 层进行数据交互；\nSocket 层的下面就是传输层、网络层和网络接口层；\n最下面的一层，则是网卡驱动程序和硬件网卡设备；\n\n三、Linux 接收网络包的流程网卡是计算机里的一个硬件，专门负责接收和发送网络包，当网卡接收到一个网络包后，会通过 DMA 技术，将网络包写入到指定的内存地址，也就是写入到 Ring Buffer ，这个是一个环形缓冲区，接着就会告诉操作系统这个网络包已经到达。\n\n那应该怎么告诉操作系统这个网络包已经到达了呢？\n\n最简单的一种方式就是触发中断，也就是每当网卡收到一个网络包，就触发一个中断告诉操作系统。\n但是，这存在一个问题，在高性能网络场景下，网络包的数量会非常多，那么就会触发非常多的中断，要知道当 CPU 收到了中断，就会停下手里的事情，而去处理这些网络包，处理完毕后，才会回去继续其他事情，那么频繁地触发中断，则会导致 CPU 一直没完没了的处理中断，而导致其他任务可能无法继续前进，从而影响系统的整体效率。\n所以为了解决频繁中断带来的性能开销，Linux 内核在 2.6 版本中引入了 NAPI 机制，它是混合「中断和轮询」的方式来接收网络包，它的核心概念就是不采用中断的方式读取数据，而是首先采用中断唤醒数据接收的服务程序，然后 poll 的方法来轮询数据。\n因此，当有网络包到达时，会通过 DMA 技术，将网络包写入到指定的内存地址，接着网卡向 CPU 发起硬件中断，当 CPU 收到硬件中断请求后，根据中断表，调用已经注册的中断处理函数。\n硬件中断处理函数会做如下的事情：\n\n需要先「暂时屏蔽中断」，表示已经知道内存中有数据了，告诉网卡下次再收到数据包直接写内存就可以了，不要再通知 CPU 了，这样可以提高效率，避免 CPU 不停的被中断。\n接着，发起「软中断」，然后恢复刚才屏蔽的中断。\n\n至此，硬件中断处理函数的工作就已经完成。\n硬件中断处理函数做的事情很少，主要耗时的工作都交给软中断处理函数了。\n\n软中断的处理\n\n内核中的 ksoftirqd 线程专门负责软中断的处理，当 ksoftirqd 内核线程收到软中断后，就会来轮询处理数据。\nksoftirqd 线程会从 Ring Buffer 中获取一个数据帧，用 sk_buff 表示，从而可以作为一个网络包交给网络协议栈进行逐层处理。\n\n网络协议栈\n\n首先，会先进入到网络接口层，在这一层会检查报文的合法性，如果不合法则丢弃，合法则会找出该网络包的上层协议的类型，比如是 IPv4，还是 IPv6，接着再去掉帧头和帧尾，然后交给网络层。\n到了网络层，则取出 IP 包，判断网络包下一步的走向，比如是交给上层处理还是转发出去。当确认这个网络包要发送给本机后，就会从 IP 头里看看上一层协议的类型是 TCP 还是 UDP，接着去掉 IP 头，然后交给传输层。\n传输层取出 TCP 头或 UDP 头，根据四元组「源 IP、源端口、目的 IP、目的端口」 作为标识，找出对应的 Socket，并把数据放到 Socket 的接收缓冲区。\n最后，应用层程序调用 Socket 接口，将内核的 Socket 接收缓冲区的数据「拷贝」到应用层的缓冲区，然后唤醒用户进程。\n至此，一个网络包的接收过程就已经结束了，你也可以从下图左边部分看到网络包接收的流程，右边部分刚好反过来，它是网络包发送的流程。\n\n四、Linux 发送网络包的流程如上图的右半部分，发送网络包的流程正好和接收流程相反。\n首先，应用程序会调用 Socket 发送数据包的接口，由于这个是系统调用，所以会从用户态陷入到内核态中的 Socket 层，内核会申请一个内核态的 sk_buff 内存，将用户待发送的数据拷贝到 sk_buff 内存，并将其加入到发送缓冲区。\n接下来，网络协议栈从 Socket 发送缓冲区中取出 sk_buff，并按照 TCP/IP 协议栈从上到下逐层处理。\n如果使用的是 TCP 传输协议发送数据，那么先拷贝一个新的 sk_buff 副本 ，这是因为 sk_buff 后续在调用网络层，最后到达网卡发送完成的时候，这个 sk_buff 会被释放掉。而 TCP 协议是支持丢失重传的，在收到对方的 ACK 之前，这个 sk_buff 不能被删除。所以内核的做法就是每次调用网卡发送的时候，实际上传递出去的是 sk_buff 的一个拷贝，等收到 ACK 再真正删除。\n接着，对 sk_buff 填充 TCP 头。这里提一下，sk_buff 可以表示各个层的数据包，在应用层数据包叫 data，在 TCP 层我们称为 segment，在 IP 层我们叫 packet，在数据链路层称为 frame。\n你可能会好奇，为什么全部数据包只用一个结构体来描述呢？协议栈采用的是分层结构，上层向下层传递数据时需要增加包头，下层向上层数据时又需要去掉包头，如果每一层都用一个结构体，那在层之间传递数据的时候，就要发生多次拷贝，这将大大降低 CPU 效率。\n于是，为了在层级之间传递数据时，不发生拷贝，只用 sk_buff 一个结构体来描述所有的网络包，那它是如何做到的呢？是通过调整 sk_buff 中 data 的指针，比如：\n\n当接收报文时，从网卡驱动开始，通过协议栈层层往上传送数据报，通过增加 skb-&gt;data 的值，来逐步剥离协议首部。\n当要发送报文时，创建 sk_buff 结构体，数据缓存区的头部预留足够的空间，用来填充各层首部，在经过各下层协议时，通过减少 skb-&gt;data 的值来增加协议首部。\n\n你可以从下面这张图看到，当发送报文时，data 指针的移动过程。\n\n至此，传输层的工作也就都完成了。\n然后交给网络层，在网络层里会做这些工作：选取路由（确认下一跳的 IP）、填充 IP 头、netfilter 过滤、对超过 MTU 大小的数据包进行分片。处理完这些工作后会交给网络接口层处理。\n网络接口层会通过 ARP 协议获得下一跳的 MAC 地址，然后对 sk_buff 填充帧头和帧尾，接着将 sk_buff 放到网卡的发送队列中。\n这一些工作准备好后，会触发「软中断」告诉网卡驱动程序，这里有新的网络包需要发送，驱动程序会从发送队列中读取 sk_buff，将这个 sk_buff 挂到 RingBuffer 中，接着将 sk_buff 数据映射到网卡可访问的内存 DMA 区域，最后触发真实的发送。\n当数据发送完成以后，其实工作并没有结束，因为内存还没有清理。当发送完成的时候，网卡设备会触发一个硬中断来释放内存，主要是释放 sk_buff 内存和清理 RingBuffer 内存。\n最后，当收到这个 TCP 报文的 ACK 应答时，传输层就会释放原始的 sk_buff 。\n\n发送网络数据的时候，涉及几次内存拷贝操作？\n\n第一次，调用发送数据的系统调用的时候，内核会申请一个内核态的 sk_buff 内存，将用户待发送的数据拷贝到 sk_buff 内存，并将其加入到发送缓冲区。\n第二次，在使用 TCP 传输协议的情况下，从传输层进入网络层的时候，每一个 sk_buff 都会被克隆一个新的副本出来。副本 sk_buff 会被送往网络层，等它发送完的时候就会释放掉，然后原始的 sk_buff 还保留在传输层，目的是为了实现 TCP 的可靠传输，等收到这个数据包的 ACK 时，才会释放原始的 sk_buff 。\n第三次，当 IP 层发现 sk_buff 大于 MTU 时才需要进行。会再申请额外的 sk_buff，并将原来的 sk_buff 拷贝为多个小的 sk_buff。\n五、总结电脑与电脑之间通常都是通过网卡、交换机、路由器等网络设备连接到一起，那由于网络设备的异构性，国际标准化组织定义了一个七层的 OSI 网络模型，但是这个模型由于比较复杂，实际应用中并没有采用，而是采用了更为简化的 TCP/IP 模型，Linux 网络协议栈就是按照了该模型来实现的。\nTCP/IP 模型主要分为应用层、传输层、网络层、网络接口层四层，每一层负责的职责都不同，这也是 Linux 网络协议栈主要构成部分。\n当应用程序通过 Socket 接口发送数据包，数据包会被网络协议栈从上到下进行逐层处理后，才会被送到网卡队列中，随后由网卡将网络包发送出去。\n而在接收网络包时，同样也要先经过网络协议栈从下到上的逐层处理，最后才会被送到应用程序。\n\n参考资料：\n\nLinux 网络包发送过程：https://mp.weixin.qq.com/s/wThfD9th9e_-YGHJJ3HXNQ\nLinux 网络数据接收流程（TCP）- NAPI：https://wenfh2020.com/2021/12/29/kernel-tcp-receive/\nLinux网络-数据包接收过程：https://blog.csdn.net/frank_jb/article/details/115841622\n\n","categories":["运维","Linux"]},{"title":"TCP 和 HTTP 中的 KeepAlive 机制总结","url":"/posts/55114/","content":"一、什么是 KeepAlive\nKeepAlive 可以简单理解为一种状态保持或重用机制，比如当一条连接建立后，我们不想它立刻被关闭，如果实现了 KeepAlive 机制，就可以通过它来实现连接的保持\nHTTP 的 KeepAlive 在 HTTP 1.0 版本默认是关闭的，但在 HTTP1.1 是默认开启的；操作系统里 TCP 的 KeepAlive 默认也是关闭，但一般应用都会修改设置来开启。因此网上 TCP 流量中基于 KeepAlive 的是主流\nHTTP 的 KeepAlive 和 TCP 的 KeepAlive 有一定的依赖关系，名称又一样，因此经常被混淆，但其实是不同的东西，下面具体分析一下\n\n二、TCP 为什么要做 KeepAlive\n我们都知道 TCP 的三次握手和四次挥手。当两端通过三次握手建立 TCP 连接后，就可以传输数据了，数据传输完毕，连接并不会自动关闭，而是一直保持。只有两端分别通过发送各自的 FIN 报文时，才会关闭自己侧的连接。\n这个关闭机制看起来简单明了，但实际网络环境千变万化，衍生出了各种问题。假设因为实现缺陷、突然崩溃、恶意攻击或网络丢包等原因，一方一直没有发送 FIN 报文，则连接会一直保持并消耗着资源，为了防止这种情况，一般接收方都会主动中断一段时间没有数据传输的 TCP 连接，比如 LVS 会默认中断 90 秒内没有数据传输的 TCP 连接，F5 会中断 5 分钟内没有数据传输的 TCP 连接\n但有的时候我们的确不希望中断空闲的 TCP 连接，因为建立一次 TCP 连接需要经过一到两次的网络交互，且由于 TCP 的 slow start 机制，新的 TCP 连接开始数据传输速度是比较慢的，我们希望通过连接池模式，保持一部分空闲连接，当需要传输数据时，可以从连接池中直接拿一个空闲的 TCP 连接来全速使用，这样对性能有很大提升\n为了支持这种情况，TCP 实现了 KeepAlive 机制。KeepAlive 机制并不是 TCP 规范的一部分，但无论 Linux 和 Windows 都实现实现了该机制。TCP 实现里 KeepAlive 默认都是关闭的，且是每个连接单独设置的，而不是全局设置\n\n\nImplementors MAY include “keep-alives” in their TCP implementations, although this practice is not universally accepted.  If keep-alives are included, the application MUST  be able to turn them on or off for each TCP connection, and they MUST default to off.\n\n\n另外有一个特殊情况就是，当某应用进程关闭后，如果还有该进程相关的 TCP 连接，一般来说操作系统会自动关闭这些连接\n\n三、如何开启 TCP 的 KeepAlive\nTCP 的 KeepAlive 默认不是开启的，如果想使用，需要在自己的应用中为每个 TCP 连接设置SO_KEEPALIVE 才会生效\n在 Java 中，应用程序一般通过设置 java.net.SocketOptions 来开启 TCP 连接的 KeepAlive\n\n/** *  When the keepalive option is set for a TCP socket and no data *  has been exchanged across the socket in either direction for *  2 hours (NOTE: the actual value is implementation dependent), *  TCP automatically sends a keepalive probe to the peer. This probe is a *  TCP segment to which the peer must respond. *  One of three responses is expected: *  1. The peer responds with the expected ACK. The application is not *     notified (since everything is OK). TCP will send another probe *     following another 2 hours of inactivity. *  2. The peer responds with an RST, which tells the local TCP that *     the peer host has crashed and rebooted. The socket is closed. *  3. There is no response from the peer. The socket is closed. * *  The purpose of this option is to detect if the peer host crashes. * *  Valid only for TCP socket: SocketImpl * * @see Socket#setKeepAlive * @see Socket#getKeepAlive */@Native public final static int SO_KEEPALIVE = 0x0008;\n\n\nJava Docs 里对 SO_KEEPALIVE 的工作机制做了比较详细的说明，具体来说就是，如果某连接开启了 TCP KeepAlive，当连接空闲了两个小时（依赖操作系统的 net.ipv4.tcp_keepalive_time 设置），TCP 会自动发送一个 KeepAlive 探测报文给对端。对端必须回复这个探测报文，假设对端正常，就可以回复 ACK 报文，收到 ACK 后该连接就会继续维持，直到再次出现两个小时空闲然后探测；假设对端不正常，比如重启了，应该回复一个 RST 报文来关闭该连接。假设对端没有任何响应，TCP 会每隔 75 秒（依赖操作系统的 net.ipv4.tcp_keepalive_intvl 设置）再次重试，重试 9 次（依赖 OS 的 net.ipv4.tcp*keepalive*probes 设置）后如果依然没有回复则关闭连接\nLinux 中 KeepAlive 相关的配置可以通过如下方式查看\n\nchendw@chendw-PC:~$ sysctl -a | grep keepalivenet.ipv4.tcp_keepalive_time = 7200net.ipv4.tcp_keepalive_intvl = 75net.ipv4.tcp_keepalive_probes = 9\n\n\n\n四、HTTP 为什么要做 KeepAlive\nHTTP 虽然是基于有连接状态的 TCP，但本身却是一个无连接状态的协议，客户端建立连接，发出请求，获取响应，关闭连接，然后整个流程就结束了；当有新的 HTTP 请求，则使用新建立的 TCP 连接。老的连接一般会被客户端浏览器或服务器关闭，此时由于是两端主动发的 FIN 报文，因此即使 TCP 已经设置了 KeepAlive，TCP 连接也会被正常关闭\n这种模式下每个 HTTP 请求都会经过三次握手创建新的 TCP，再加上 TCP 慢启动的影响，以及单个网页里包含越来越多的资源请求，因此效果并不理想。为了提升性能，HTTP 规范也提出了 KeepAlive 机制，HTTP 请求携带头部 Connection: Keep-Alive 信息，告知服务器不要关闭该 TCP 连接，当服务器收到该请求，完成响应后，不会主动主动关闭该 TCP 连接。而浏览器当然也不会主动关闭，而是在后续请求里复用该 TCP 连接来发送下一个 HTTP 请求\nHTTP1.0 默认不开启 KeepAlive，因此要使用的话需要浏览器支持，在发送 HTTP 请求时主动携带 \n\nConnection: Keep-Alive``头部，应用服务器同样也要支持；而HTTP1.1规范明确规定了要默认开启KeepAlive，所以支持HTTP1.1的浏览器不需要显式指定，发送请求时会自动携带该头部，只有在想关闭时可以通过设置 ``Connection: Close 头部告知对端\n\n另外，HTTP 的 KeepAlive 机制还提供了头部 Keep-Alive: max=5, timeout=120 来控制连接关闭时间，比如如上头部就表示该 TCP 连接还会保持 120 秒，max 表示可以发送的请求数，不过在非管道连接下会被忽略，我们基本都是非管道连接，因此可以忽略\nHTTP/2 为每个域名使用单个 TCP 连接，本身就是连接复用，因此请求不再需要携带头部来开启 KeepAlive\n\n五、HTTP 的 KeepAlive 和 TCP 的 KeepAlive 的关系\n从上面可以看出，虽然都叫 KeepAlive 且有依赖关系，但 HTTP 的 KeepAlive 和 TCP 的 KeepAlive 是两个完全不同的概念\nTCP 的 KeepAlive 是由操作系统内核来控制，通过 keep-alive 报文来防止 TCP 连接被对端、防火墙或其他中间设备意外中断，和上层应用没有任何关系，只负责维护单个 TCP 连接的状态，其上层应用可以复用该 TCP 长连接，也可以关闭该 TCP 长连接\nHTTP 的 KeepAlive 机制则是和自己的业务密切相关的，浏览器通过头部告知服务器要复用这个 TCP 连接，请不要随意关闭。只有到了 keepalive 头部规定的 timeout 才会关闭该 TCP 连接，不过这具体依赖应用服务器，应用服务器也可以根据自己的设置在响应后主动关闭这个 TCP 连接，只要在响应的时候携带 Connection: Close 告知对方\n所以很多时候我们可以把 HTTP 连接理解为 TCP 连接，但 HTTP KeepAlive 则不能当成 TCP 的 KeepAlive 看待\n假设我们不开启 TCP 长连接而只开启 HTTP 长连接，是不是 HTTP 的 KeepAlive 就不起作用了？并不是的，此时 HTTP 的 KeepAlive 还会正常起作用，TCP 连接还会被复用，但被复用的 TCP 连接出现故障的概率就高很多。由于没有开启 TCP 的 KeepAlive，防火墙或负载转发服务等中间设备可能因为该 TCP 空闲太长而悄悄关闭该连接，当 HTTP 从自己的连接池拿出该 TCP 连接时，可能并不知道该连接被关闭，继续使用就会出现错误\n为了减少错误，一般来说开启 HTTP 的 KeepAlive 的应用都会开启 TCP 的 KeepAlive\n默认的 net.ipv4.tcp_keepalive_time 为 2 个小时，是不是太长了？感觉太长了，2 小时监测一次感觉黄花菜都凉了。我们公司 F5 后面的 Nginx 服务器配置了 30 分钟，但应该也是太长了吧，F5 维持空闲连接 5 分钟，那超时监测不应该低于这个值吗 ？？？，比如Google Cloud说其防火墙允许 10 分钟空闲连接，因此建议 net.ipv4.tcp_keepalive_time 设置为 6 分钟\n\n六、如何使用 HTTP 的 KeepAlive\n很明显，开启 HTTP KeepAlive 不需要用户做任何操作，只要浏览器和应用服务器支持即可，不过需要注意的是，HTTP KeepAlive 的相关头部都是 hop-by-hop 类型的\n和 TCP 连接不同，一个完整的 HTTP 事务，可能会横跨多个 TCP 连接，比如浏览器请求某个网页，请求可能先通过浏览器与负载均衡之间的 TCP 连接传输，再经过负载均衡到 Nginx 的 TCP 连接，最后在经过 Nginx 与业务 Tomcat 服务器的 TCP 连接，Tomcat 处理完请求并返回响应后，响应沿着同样的 TCP 连接路线返回\n因此 HTTP 的头部被分为了两部分：End-to-end 头部和 Hop-by-hop 头部，End-to-end 头部会被中间的代理原样转发，比如浏览器请求报文中的 host 头部，会被负载均衡、反向代理原样转发到 Tomcat 里，除非特意修改。而 Hop-by-hop 头部则只在当前 TCP 连接里有效，大部分头部都是 End-to-end ，但 KeepAlive 相关头部很明显和 TCP 连接有密切关系，因此是 Hop-by-hop 的\n\n\n * End-to-end headers which are transmitted to the ultimate recipient of a request or response. End-to-end headers in responses MUST be stored as part of a cache entry and MUST be transmitted in any response formed from a cache entry.\n * Hop-by-hop headers which are meaningful only for a single transport-level connection and are not stored by caches or forwarded by proxies.\n\n\n也就是说，即使浏览器请求时携带了 Connection: Keep-Alive ，也只表示浏览器到负载均衡之间是长连接，但负载均衡到 nginx、nginx 到 tomcat 是否是长连接则需要具体分析。比如 Nginx 虽然支持 HTTP 的 Keep-Alive，但由 Nginx 发起的 HTTP 请求默认不是长连接\n由于这种 Hop-by-hop  的特性，HTTP 长连接中的 timeout 设置就十分可疑了，不过一般来说应用服务器都是根据自己的设置来管理 TCP 连接的，因此 HTTP 长连接中 Connection 头部每个请求都携带， keepalive 头部用的就比较少\n\n七、Nginx 的 KeepAlive 配置\nNginx 与客户端的长连接\nNginx 是支持 HTTP KeepAlive 的，因此只要 client 发送的 http 请求携带了 KeepAlive 头部，客户端和 Nginx 的长连接就能正常保持\n可以使用 keepaliverequests 和 keepalivetimeout 调整对 client 的长连接的单个连接承受的最大请求数，以及长连接最大空闲时长\n从上面可知，服务端可以根据客户端的 keepalive 头部来管理 TCP 连接，也可以根据自己的设置来管理，Nginx 一般根据自己的设置来管理\n\n\n  Syntax:    keepalive_requests number;\n  Default:    keepalive_requests 100;\n  Context:    http, server, location\n  This directive appeared in version 0.8.0.\n  Sets the maximum number of requests that can be served through one keep-alive connection. After the maximum number of requests are made, the connection is closed.\n Closing connections periodically is necessary to free per-connection memory allocations. Therefore, using too high maximum number of requests could result in excessive memory usage and not recommended.\n  Syntax:    keepalivetimeout timeout [headertimeout];\n  Default:    keepalive_timeout 75s;\n  Context:    http, server, location\n The first parameter sets a timeout during which a keep-alive client connection will stay open on the server side. The zero value disables keep-alive client connections. The optional second parameter sets a value in the “Keep-Alive: timeout=time” response header field. Two parameters may differ.\n\n\n客户端修改默认值具体配置如下\n\nhttp &#123;    keepalive_requests 100;    keepalive_timeout 75s;    upstream backend &#123;        server 192.167.61.1:8080;    &#125;&#125;\n\n\nNginx 与 Upstream Server 的长连接\nNginx 作为发起方的时候，默认还是不开启 HTTP 的 KeepAlive 的，因此需要主动设置\n在 upstream 区块使用 keepalive 开启，数字表示每个 work 开启的最大长连接数\nNginx 和上游交互时，默认 proxy_http_version 为 1.0，因此需要配置 proxy_http_version ，并清空 connection，这样即使前一跳是短连接，Nginx 与上游也可以是长连接\n另外 upstream 里的 keepalive_requests 和 http 区块里的一样是 100，但 keepalive_timeout 默认为 60 秒，比 http 区块里的少 15 秒，不过也正常，毕竟是里层，这个设置是比较合理的，使用默认的就可以\n\nupstream backend &#123;    server 192.167.61.1:8080;    server 192.167.61.1:8082 back;    keepalive 100;    # keepalive_requests 100;    # keepalive_timeout 60s;&#125;local /test &#123;    proxy_http_version 1.1;    proxy_set_header Connection &quot;&quot;; // 传递给上游服务器的头信息    proxy_pass http://backend;&#125;\n\n\n\n\n另外，Nginx 还在 listner 指令上提供了一个 so_keepalive 选项，来开启 Nginx 对 TCP 长连接的支持，应该开启的是客户端与 Nginx 之间的 TCP 长连接，但一般没有人使用，那负载均衡和 Nginx、Nginx 和 Tomcat 之间是不需要 TCP 长连接吗？因为中间没有网络设备？否则 TCP 长连接是由谁来做检测？\n长连接的资源占用问题\n 长连接带来的一个很明显的问题就是资源的占用，浏览器对同一个域名一般能并发建立 6 个连接，一般这些都是长连接，而这些连接会维护 75 秒，但客户端获得响应以后一般就结束了，下一次的客户是不同的源地址，因此无法复用前一个浏览器与服务器之间维护的长连接，这会造成服务端维护了大量不再被使用的连接，所以长连接的意义在于有大量资源持续请求的场景\n 假设你就一个静态页面，里面包含几个资源，使用短连接对服务器并发更好\n 另外，注意 Nginx 中 keepalive_requests 默认的 100 表示的是单个长连接能处理的最大请求数，而并不是 Nginx 能维护的长连接数。Nginx 能维护的 TCP 连接数，为工作进程个数 worker_processes 乘以每个工作进程允许维护的最大连接数 worker_connections（默认 512）；如果想计算 Nginx 能服务的最大请求数，还需要在最大 TCP 连接数外，加上操作系统允许的排队等待数 net.core.somaxconn，默认 128\n Nginx 通过事件驱动来实现大量长连接的维护，具体可以查看 Nginx 文档\n端口号与文件数\n由于端口在传输层使用 16 位来传输，因此取值范围只能是 0 到 65535，再加上 TCP 连接关闭后端口并不能立刻被重用，而是要经过 2MSL 的 TIME_WAIT 闲置，所以经常有人以为一个服务器同时最大能维持的 TCP 数是 65000/2*60 ，大约 500 左右\n 这个理解是有偏颇的。端口的限制只是对发起方来说的，即源端口。比如 Nginx 作为反向代理，和上游 Tomcat 建立连接时，源 IP 和目的 IP 肯定是固定的，目的端口也是固定的，比如 Tomcat 的 8080 端口，只有源端口可变，所以 Nginx 和上游 Tomcat 最多只能建立 500 左右的 TCP 连接，不过两端 IP 都是固定的，所以 TCP 连接重用效果非常好，并不会造成性能问题\n 当 Nginx 作为接收方和客户端浏览器建立连接时，Nginx 服务器提供固定的 IP 和端口，而客户端浏览器 IP 和端口都会正常变动，因此 Nginx 服务器上维护的与客户端的长连接是不受端口限制的，不过此时服务器又会遇到著名的 C10K 问题\n 此时限制服务器维持 TCP 连接数的是操作系统允许打开的最大文件数，要修改的主要有以下几处\n\n\n/proc/sys/fs/file-max：操作系统所有进程一共可以打开的文件数\n/proc/sys/fs/nr_open：单个进程能分配的最大文件数\nulimit 的 open files：当前 shell 以及由它启动的进程可以打开的最大文件数，如果超过了 nropen，要先调整 nropen 的值\n\n八、Tomcat 的 KeepAlive 配置\nTomcat7 以上都默认开启了 keepalive 支持。两个主要参数 maxKeepAliveRequest 和 KeepAliveTimeout\n maxKeepAliveRequest：一个长连接能接受的最大请求数，默认 100\n KeepAliveTimeout：一个长连接最长空闲时间，否则被关闭，默认为 connectionTimeout 的值，默认 60s\nTomcat 里的应用作为发起方的时候，是否支持 KeepAlive 是由应用自行决定的，和 Tomcat 无关\n\n参考资料\nRFC1122 - TCP Keep-Alives\nHTTP Header Type\nNginx ngxhttpcore_module\nHTTP Keepalive Connections and Web Performance\nTuning NGINX for Performance\nThe C10K problem\nulimit的探讨\n\n","categories":["运维","Linux"]},{"title":"TCP 状态转换","url":"/posts/43054/","content":"一、TCP 状态转换图概念\nSYN: (同步序列编号，Synchronize Sequence Numbers)\nACK: (确认编号，Acknowledgement Number)\nFIN: (结束标志，FINish)\n\n\n二、状态说明CLOSED表示初始状态。\nLISTEN表示服务器端的某个 socket 处于监听状态，可以接受连接。\nSYN_SEND在服务端监听后，客户端 socket 执行 connect 时，客户端发送 SYN 报文，此时客户端就进入 SYN_SENT 状态，等待服务端的确认。也就是TCP三次握手中的第1步之后，注意是客户端状态。\nsysctl -w net.ipv4.tcp_syn_retries = 2\n\n做为客户端可以设置 SYN 包的重试次数，默认 5 次（大约180s）引用校长的话：仅仅重试 2 次，现代网络够了。\nSYN_RCVD表示服务端接受到了SYN报文，在正常情况下，这个状态是服务器端的 socket 在建立TCP连接时的三次握手会话过程中的一个中间状态，很短暂，基本上用 netstat 你是很难看到这种状态的，除非你特意写了一个客户端测试程序，故意将三次TCP握手过程中最后一个 ACK 报文不予发送。因此这种状态时，当收到客户端的 ACK 报文后，它会进入到 ESTABLISHED 状态。\n注意是服务端状态，一般 15 个左右正常，如果很大，怀疑遭受 SYN_FLOOD 攻击。\nsysctl -w net.ipv4.tcp_max_syn_backlog=4096\n\n设置该状态的等待队列数，默认1024，调大后可适当防止syn-flood，可参见man 7 tcp\nsysctl -w net.ipv4.tcp_syncookies=1 ,　打开syncookie，在 syn backlog 队列不足的时候，提供一种机制临时将syn链接换出。\nsysctl -w net.ipv4.tcp_synack_retries = 2 ,做为服务端返回 ACK 包的重试次数，默认 5 次（大约180s）引用校长的话：仅仅重试2次，现代网络够了\nESTABLISHED客户端接受到服务端的 ACK 包后的状态，服务端在发出 ACK 在一定时间后即为 ESTABLISHED。\nsysctl -w net.ipv4.tcp_keepalive_time = 1200 ，默认为7200秒(2小时)，系统针对空闲链接会进行心跳检查，如果超过net.ipv4.tcp_keepalive_probes * net.ipv4.tcp_keepalive_intvl = 默认11分，终止对应的tcp链接，可适当调整心跳检查频率目前线上的监控 waring:600 , critial : 800\nFIN_WAIT_1这个是已经建立连接之后，其中一方请求终止连接，等待对方的FIN报文。FIN_WAIT_1状态是当 socket 在 ESTABLISHED 状态时，它想主动关闭连接，向对方发送了 FIN 报文，此时该 socket 即进入到 FIN_WAIT_1 状态。而当对方回应 ACK 报文后，则进入到 FIN_WAIT_2 状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以 FIN_WAIT_1 状态一般是比较难见到的，而 FIN_WAIT_2 状态还有时常常可以用 netstat 看到。也是在TCP四次握手的第1步。\nCLOSE_WAIT这种状态的含义其实是表示在等待关闭。怎么理解呢？\n当对方 close 一个 socket 后发送 FIN 报文给你，你系统毫无疑问地会回应一个 ACK 报文给对方，此时则进入到 CLOSE_WAIT 状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以 close 这个 socket，发送 FIN 报文给对方，也即关闭连接。所以你在 CLOSE_WAIT 状态下，需要完成的事情是等待你去关闭连接。\nFIN_WAIT_2主动关闭的一方，在接受到被动关闭一方的ACK后，也就是TCP四次握手的第2步。\nsysctl -w net.ipv4.tcp_fin_timeout=30, 可以设定被动关闭方返回FIN后的超时时间，有效回收链接，避免syn-flood.\nLASK_ACK被动关闭的一方，在发送 ACK 后，被动一方 close socket 时，会再发起一个FIN请求，在等待主动关闭方的 ACK 时就是 LASK_ACK 状态。当收到 ACK 后也即可以进入到 CLOSED 可用状态了。\nTIME_WAIT主动关闭的一方，在收到被动关闭的FIN包后，发送ACK。也就是TCP四次握手的第4步。\nsysctl -w net.ipv4.tcp_tw_recycle = 1 , 打开快速回收 TIME_WAIT，Enabling this option is not recommended since this causes problems when working with NAT (Network Address Translation)\nsysctl -w net.ipv4.tcp_tw_reuse =1, 快速回收并重用TIME_WAIT的链接, 貌似和tw_recycle有冲突，不能重用就回收?\nnet.ipv4.tcp_max_tw_buckets: 处于time_wait状态的最多链接数，默认为180000.\n","categories":["运维","Linux"]},{"title":"Containerd 命令","url":"/posts/6486/","content":"ctr 是 containerd 的一个客户端工具。crictl 是 CRI 兼容的容器运行时命令行接口，可以使用它来检查和调试 k8s 节点上的容器运行时和应用程序。\n一、ctr 命令\n\n\n命令\n作用\n\n\n\nctr image ls\n查看镜像\n\n\nctr task ls\n正在运行的镜像\n\n\nctr run\n运行一个容器\n\n\n二、crictl 命令镜像相关\n\n\n命令\n作用\n\n\n\ncrictl images\n查看镜像\n\n\ncrictl pull\n拉取镜像\n\n\ncrictl rmi\n删除镜像\n\n\ncrictl inspecti -o yaml\n镜像详情\n\n\n容器相关\n\n\n命令\n作用\n\n\n\ncrictl ps\n正在运行的容器列表\n\n\ncrictl inspect -o yaml\n容器详情\n\n\ncrictl create\n创建容器\n\n\ncrictl start\n启动容器\n\n\ncrictl stop\n停止容器\n\n\ncrictl rm\n删除容器\n\n\ncrictl attach\nattach\n\n\ncrictl exec\nexec\n\n\ncrictl logs\n日志\n\n\ncrictl stats\nstats\n\n\nPOD 相关\n\n\n命令\n作用\n\n\n\ncrictl pods\nPOD 列表\n\n\ncrictl inspectp -o yaml\nPOD 详情\n\n\ncrictl runp\n运行 POD\n\n\ncrictl stopp\n停止 POD\n\n\n","categories":["运维","Containerd"]},{"title":"Alpine","url":"/posts/36793/","content":"简介Alpine 操作系统是一个面向安全的轻型 Linux 发行版。\n它不同于通常 Linux 发行版，Alpine 采用了 musl libc 和 busybox 以减小系统的体积和运行时资源消耗，但功能上比 busybox 又完善的多，因此得到开源社区越来越多的青睐。\n在保持瘦身的同时，Alpine 还提供了自己的包管理工具 apk，可以通过 https://pkgs.alpinelinux.org/packages 网站上查询包信息，也可以直接通过 apk 命令直接查询和安装各种软件。\nAlpine 由非商业组织维护的，支持广泛场景的 Linux发行版，它特别为资深/重度Linux用户而优化，关注安全，性能和资源效能。Alpine 镜像可以适用于更多常用场景，并且是一个优秀的可以适用于生产的基础系统/环境。\nAlpine Docker 镜像也继承了 Alpine Linux 发行版的这些优势。相比于其他 Docker 镜像，它的容量非常小，仅仅只有 5 MB 左右（对比 Ubuntu 系列镜像接近 200 MB），且拥有非常友好的包管理机制。官方镜像来自 docker-alpine 项目。\n目前 Docker 官方已开始推荐使用 Alpine 替代之前的 Ubuntu 做为基础镜像环境。这样会带来多个好处。包括镜像下载速度加快，镜像安全性提高，主机之间的切换更方便，占用更少磁盘空间等。\n下表是官方镜像的大小比较：\nREPOSITORY          TAG           IMAGE ID          VIRTUAL SIZEalpine              latest        4e38e38c8ce0      4.799 MBdebian              latest        4d6ce913b130      84.98 MBubuntu              latest        b39b81afc8ca      188.3 MBcentos              latest        8efe422e6104      210 MB\n\n获取并使用官方镜像由于镜像很小，下载时间往往很短，读者可以直接使用 docker run 指令直接运行一个 Alpine 容器，并指定运行的 Linux 指令，例如：\ndocker run alpine echo &#x27;123&#x27;123\n\n\n\ndocker run --rm -it --entrypoint /bin/sh alpine\n\n迁移至 Alpine 基础镜像目前，大部分 Docker 官方镜像都已经支持 Alpine 作为基础镜像，可以很容易进行迁移。\n例如：\n\nubuntu/debian -&gt; alpine\npython:3 -&gt; python:3-alpine\nruby:2.6 -&gt; ruby:2.6-alpine\n\n另外，如果使用 Alpine 镜像替换 Ubuntu 基础镜像，安装软件包时需要用 apk 包管理器替换 apt 工具，如\napk add --no-cache &lt;package&gt;\n\nAlpine 中软件安装包的名字可能会与其他发行版有所不同，可以在 https://pkgs.alpinelinux.org/packages 网站搜索并确定安装包名称。如果需要的安装包不在主索引内，但是在测试或社区索引中。那么可以按照以下方法使用这些安装包。\n$ echo &quot;http://dl-cdn.alpinelinux.org/alpine/edge/testing&quot; &gt;&gt; /etc/apk/repositories$ apk --update add --no-cache &lt;package&gt;\n\n由于在国内访问 apk 仓库较缓慢，建议在使用 apk 之前先替换仓库地址为国内镜像。\nRUN sed -i &quot;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g&quot; /etc/apk/repositories \\      &amp;&amp; apk add --no-cache &lt;package&gt;\n\n相关资源\nAlpine 官网：https://www.alpinelinux.org/\nAlpine 官方仓库：https://github.com/alpinelinux\nAlpine 官方镜像：https://hub.docker.com/_/alpine/\nAlpine 官方镜像仓库：https://github.com/gliderlabs/docker-alpine\n\n","categories":["运维","Docker"]},{"title":"Docker Desktop for Mac 容器访问宿主机IP","url":"/posts/62365/","content":"文档地址：https://dockerdocs.cn/docker-for-mac/networking/\n主机的IP地址正在更改（如果没有网络访问权限，则没有IP地址）。我们建议您连接到特殊的DNS名称 host.docker.internal\n该名称 解析为主机使用的内部IP地址\n这是出于开发目的，不适用于Docker Desktop for Mac以外的生产环境。\n","categories":["运维","Docker"]},{"title":"Dockerfile 环境变量配置Java启动参数","url":"/posts/6088/","content":"ENTRYPOINT [&quot;sh&quot;,&quot;-c&quot;,&quot;java $&#123;JAVA_OPTS&#125; -jar /opt/app.jar&quot;]\n或者\nENTRYPOINT java $&#123;JAVA_OPTS&#125; -jar /opt/app.jar\n\n这种方式不会替换环境变量\nENTRYPOINT [&quot;java&quot;,&quot;$&#123;JAVA_OPTS&#125;&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n","categories":["运维","Docker"]},{"title":"Nginx location 匹配规则","url":"/posts/13357/","content":"一、url匹配规则location [=|~|~*|^~|@] /uri/ &#123;  ...&#125; \n\n\n=：表示精确匹配后面的url\n~：表示正则匹配，但是区分大小写\n~*：正则匹配，不区分大小写\n^~：表示普通字符匹配，如果该选项匹配，只匹配该选项，不匹配别的选项，一般用来匹配目录\n@：”@” 定义一个命名的 location，使用在内部定向时，例如 error_page\n\n上述匹配规则的优先匹配顺序：\n\n= 前缀的指令严格匹配这个查询。如果找到，停止搜索；\n所有剩下的常规字符串，最长的匹配。如果这个匹配使用 ^~ 前缀，搜索停止；\n正则表达式，在配置文件中定义的顺序；\n如果第 3 条规则产生匹配的话，结果被使用。否则，使用第 2 条规则的结果。\n\n目标地址处理规则匹配到uri后，接下来要代理到目标服务地址。\nupstream api_server &#123;  server 10.0.101.62:8081;  server 10.0.101.61:8082;&#125;location / &#123;        rewrite ^(.*)$ http://10.0.101.62:8000/my-module$1 break;&#125;location ^~ /my-module/ &#123;    root   /data/my-module/dist;    rewrite ^/my-module/(.*)$  /$1 break;    index  index.html index.htm;&#125;location /my-module/api &#123;    proxy_pass  http://api_server;    proxy_set_header Host $host;    proxy_set_header  X-Real-IP        $remote_addr;    proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;    proxy_set_header  your-custome-header    &quot;myHeader&quot;;    proxy_set_header X-NginX-Proxy true;&#125;\n\n上述配置，默认访问/会重定向到/my-module, 然后直接返回/data/my-module/dist下的html等静态文件。\n访问/my-module/api则会代理到我们api服务器地址，是一个默认的round-robin负载均衡配置。\n下面是访问localhost的日志, 访问首页一共进行了2次重定向。\nRequest URL: http://10.0.101.62:8000/Request Method: GETStatus Code: 302 Moved TemporarilyLocation: http://10.0.101.62:8000/flash/Request URL: http://10.0.101.62:8000/flash/Request Method: GETStatus Code: 302 Moved TemporarilyLocation: http://10.0.101.62:8000/flash/index.htmlRequest URL: http://10.0.101.62:8000/flash/index.htmlRequest Method: GETStatus Code: 304 Not Modified\n\nalias与root的区别\nroot 实际访问文件路径会拼接URL中的路径\nalias 实际访问文件路径不会拼接URL中的路径\n\nalias 示例：\nlocation ^~ /sta/ &#123;     alias /usr/local/nginx/html/static/;  &#125;\n\n请求：http://test.com/sta/sta1.html实际访问：/usr/local/nginx/html/static/sta1.html 文件\nroot 示例：\nlocation ^~ /tea/ &#123;     root /usr/local/nginx/html/;  &#125;\n\n请求：http://test.com/tea/tea1.html实际访问：/usr/local/nginx/html/tea/tea1.html 文件\n显然，第二次重定向是不需要的，本意是访问/flash/的时候，直接访问对应目录下的html静态文件。 但因为root拼接flash导致找不到对应文件，要重写url，去掉flash这个模块前缀，使用了rewrite, 而rewrite会返回302重定向。\n接下来，我们修改root为alias\n location ^~ /flash/ &#123;    alias   /data/flash/dist/;    #rewrite ^/flash/(.*)$  /$1 break;    index  index.html index.htm;&#125;\n\n直接重定向1次后返回html\nRequest URL: http://10.0.101.62:8000/Request Method: GETStatus Code: 302 Moved TemporarilyRequest URL: http://10.0.101.62:8000/flash/Request Method: GETStatus Code: 200 OK (from disk cache)\n\nlast 和 break 关键字的区别只用到了break，即匹配到此处后不会继续跳。\npermanent 和 redirect 关键字的区别\nrewrite … permanent 永久性重定向，请求日志中的状态码为301\nrewrite … redirect 临时重定向，请求日志中的状态码为302\n\n我们常用的80端口转443，即http转https的一种配置方案为：\nserver &#123;    listen 80;    server_name demo.com;    rewrite ^(.*)$ https://$&#123;server_name&#125;$1 permanent; &#125;\n\n会返回301永久重定向到对应的https：\nRequest URL: http://demo.com/flash/index.htmlRequest Method: GETStatus Code: 301 Moved PermanentlyLocation: https://demo/flash/index.html\n\n二、一些使用场景上述demo差不多就是我平时用的前后端分离的代理配置方案。下面是一些遇到过的场景。\n配置一个静态文件下载服务，我们下面软件会经常看到index /的页面。\nserver &#123;        listen       8888;        #端口        server_name  _;   #服务名        charset utf-8,gbk;        root    /data/download;  #显示的根索引目录        autoindex on;             #开启索引功能        autoindex_exact_size off; # 关闭计算文件确切大小（单位bytes），只显示大概大小（单位kb、mb、gb）        autoindex_localtime on;   # 显示本机时间而非 GMT 时间&#125;\n\n配置http重定向到https\nserver &#123;    listen 80;    server_name demo.com;    rewrite ^(.*)$ https://$&#123;server_name&#125;$1 permanent; &#125;                   server &#123;    listen       443;    server_name  demo.com;    charset utf-8;    location / &#123;       alias   /data/web;       index  index.html index.htm;    &#125;&#125;\n\n配置静态前端页面\nlocation / &#123;    alias   /data/web;    index  index.html index.htm;&#125;\n\n配置反向代理, 比如我们访问http://demo.com/api/aaa/bbb，我们想要代理到http://api.com/api/aaa/bbb, 只切换了域名，uri相同。\nupstream api_server &#123;  server 10.0.101.62:8081;  server 10.0.101.61:8082;&#125;location /api &#123;    proxy_pass  http://api_server;    proxy_set_header Host $host;    proxy_set_header  X-Real-IP        $remote_addr;    proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;    proxy_set_header  your-custome-header    &quot;myHeader&quot;;    proxy_set_header X-NginX-Proxy true;&#125;\n\n配置反向代理时，移除前缀。比如我们的服务http://demo.com/users/aaa/bbb, 我们想要代理到http://users.com/aaa/bbb，即切换域名的同时，去掉users前缀。区别是proxy_pass 结尾的/.\nlocation ^~/users/ &#123;    proxy_set_header Host $host;    proxy_set_header  X-Real-IP        $remote_addr;    proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;    proxy_set_header X-NginX-Proxy true;    proxy_pass http://users.com/;&#125;\n\n反向代理时，想要自定义修改uri。使用rewrite正则修改。\n# 修改uri，去掉了flash的前缀，$1表示正则匹配到的字符串内容。location ^~ /flash/ &#123;    root   /data/flash/dist/;    rewrite ^/flash/(.*)$  /$1 break;    index  index.html index.htm;&#125;# 修改uri, 重新代理到新的地址location ^~/order/ &#123;    proxy_set_header Host $host;    proxy_set_header  X-Real-IP        $remote_addr;    proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;    proxy_set_header X-NginX-Proxy true;    rewrite ^/order/(.*)$ /$1 break;    proxy_pass http://order;&#125;\n\n代理跨域, 比如bing每日一图，不支持我们ajax获取图片地址，我们可以自己写一个支持的接口。\nhttp://101.200.218.760/proxy/bing/HPImageArchive.aspx?format=js&amp;idx=0&amp;n=1\n代理对象为：https://cn.bing.com/HPImageArchive.aspx?format=js&amp;idx=0&amp;n=1\nlocation ^~/proxy/bing/ &#123;    add_header &#x27;Access-Control-Allow-Origin&#x27; &#x27;http://localhost:8088&#x27;;    add_header &#x27;Cache-Control&#x27; &#x27;public, max-age=604800&#x27;;    add_header &#x27;Access-Control-Allow-Credentials&#x27; &#x27;true&#x27;;    add_header &#x27;Access-Control-Allow-Methods&#x27; &#x27;GET, POST, OPTIONS&#x27;;    rewrite ^/proxy/bing/(.*)$ /$1 break;    proxy_pass https://cn.bing.com/; &#125;\n","categories":["运维","Nginx"]},{"title":"Nginx proxy_pass 转发规则","url":"/posts/23488/","content":"一、proxy_pass 只包含 HOST例如：\n\nhttp://host - √\nhttps://host - √\nhttp://host:port - √\nhttps://host:port - √\nhttp://host/ - x\nhttp://host:port/ - x\n\n这种情况下，会把匹配到的所有路径直接穿透转发。比如以下的配置\nlocation /api/ &#123;  proxy_pass http://127.0.0.1:3000;&#125;\n\n访问 http://127.0.0.1:80/api/test, 最终的请求地址是 http://127.0.0.1:3000/api/test\n二、proxy_pass 包含路径这里的路径哪怕只是一个 / 也是存在的，如：\n\nhttp://host - x\n\nhttp://host:port- x\n\nhttps://host/ - √\n\nhttps://host:port/ - √\n\nhttp://host/api - √\n\nhttp://host/api/ - √\n\n\n这种情况下，url 里面会 去掉 location 匹配的字符串，拼接到 proxy_pass 再进行转发。\nlocation /api/ &#123;  proxy_pass http://127.0.0.1:3000/;&#125;\n\n访问 http://127.0.0.1:80/api/test，最终的请求地址是 http://127.0.0.1:3000/test\n三、重写代理链接 - url rewrite使用 rewrite 指令并且生效后，proxy_pass url 链接中的路径会被忽略，如：\nserver &#123;  listen       83;  location / &#123;  rewrite ^/api/(.*) /fixpath=$1 break;  proxy_pass http://127.0.0.1:3000/node/;&#125;location ^/api/ &#123;  rewrite ^/api/(.*) /fixpath=$1 break   # $1表示正则匹配到的字符串内容。  proxy_pass http://127.0.0.1:3000/node/;  &#125;&#125;\n\n访问 http://127.0.0.1:83/bb/cc，最终的请求地址是 http://127.0.0.1:3000/node/bb/cc，因为匹配上 / 了，没有匹配 rewrite 。\n访问 http://127.0.0.1:83/api/cc，最终的请求地址是 http://127.0.0.1:3000/fixpath=cc，我们写的 proxy_pass http://127.0.0.1:3000/node/ 里面的 node 路径丢失了。\n","categories":["运维","Nginx"]},{"title":"Nginx upstream 配置","url":"/posts/9876/","content":"upstream 文档\nhealth_check 模块 文档\n","categories":["运维","Nginx"]},{"title":"Nginx 替换请求Body的字符","url":"/posts/44729/","content":"ngx_http_sub_module 文档\nproxy_set_header Accept-Encoding &quot;&quot;; #防止后端服务器在返回gzip后的内容时模块不起作用subs_filter_types text/html text/css text/xml; #替换html、css、xml内容sub_filter  xxx.xxx.com 100.100.101.111;#sub_filter  host: &quot;xxx.hxxx.com&quot; host: &quot;100.100.101.111&quot;;sub_filter_last_modified on;sub_filter_types * ;sub_filter_once off;\n\n","categories":["运维","Nginx"]},{"title":"Nginx 配置限流","url":"/posts/38208/","content":"限流官方文档\n一、控制速率正常限流ngx_http_limit_req_module 模块提供限制请求处理速率能力，使用了漏桶算法(leaky bucket)。下面例子使用 nginx limit_req_zone 和 limit_req 两个指令，限制单个IP的请求处理速率。\n在 nginx.conf http 中添加限流配置：\n\n格式：limit_req_zone key zone rate\n\nhttp &#123;    limit_req_zone $binary_remote_addr zone=myRateLimit:10m rate=10r/s;&#125;\n\n配置 server，使用 limit_req 指令应用限流。\nserver &#123;    location / &#123;        limit_req zone=myRateLimit;        proxy_pass http://my_upstream;    &#125;&#125;\n\n\nkey ：定义限流对象，binary_remote_addr 是一种key，表示基于 remote_addr(客户端IP) 来做限流，binary_ 的目的是压缩内存占用量。\nzone：定义共享内存区来存储访问信息， myRateLimit:10m 表示一个大小为10M，名字为myRateLimit的内存区域。1M能存储16000 IP地址的访问信息，10M可以存储16W IP地址访问信息。\nrate 用于设置最大访问速率，rate=10r/s 表示每秒最多处理10个请求。Nginx 实际上以毫秒为粒度来跟踪请求信息，因此 10r/s 实际上是限制：每100毫秒处理一个请求。这意味着，自上一个请求处理完后，若后续100毫秒内又有请求到达，将拒绝处理该请求。\n\n处理突发流量上面例子限制 10r/s，如果有时正常流量突然增大，超出的请求将被拒绝，无法处理突发流量，可以结合 burst 参数使用来解决该问题。\nserver &#123;    location / &#123;        limit_req zone=myRateLimit burst=20;        proxy_pass http://my_upstream;    &#125;&#125;\n\nburst 译为突发、爆发，表示在超过设定的处理速率后能额外处理的请求数。当 rate=10r/s 时，将1s拆成10份，即每100ms可处理1个请求。\n此处，burst=20 ，若同时有21个请求到达，Nginx 会处理第一个请求，剩余20个请求将放入队列，然后每隔100ms从队列中获取一个请求进行处理。若请求数大于21，将拒绝处理多余的请求，直接返回503.\n不过，单独使用 burst 参数并不实用。假设 burst=50 ，rate依然为10r/s，排队中的50个请求虽然每100ms会处理一个，但第50个请求却需要等待 50 * 100ms即 5s，这么长的处理时间自然难以接受。\n因此，burst 往往结合 nodelay 一起使用。\nserver &#123;    location / &#123;        limit_req zone=myRateLimit burst=20 nodelay;        proxy_pass http://my_upstream;    &#125;&#125;\n\nnodelay 针对的是 burst 参数，burst=20 nodelay 表示这20个请求立马处理，不能延迟，相当于特事特办。不过，即使这20个突发请求立马处理结束，后续来了请求也不会立马处理。burst=20 相当于缓存队列中占了20个坑，即使请求被处理了，这20个位置这只能按 100ms一个来释放。\n这就达到了速率稳定，但突然流量也能正常处理的效果。\n二、限制连接数ngx_http_limit_conn_module 提供了限制连接数的能力，利用 limit_conn_zone 和 limit_conn 两个指令即可。下面是 Nginx 官方例子：\nlimit_conn_zone $binary_remote_addr zone=perip:10m;limit_conn_zone $server_name zone=perserver:10m;server &#123;    ...    limit_conn perip 10;    limit_conn perserver 100;&#125;\n\nlimit_conn perip 10 作用的key 是 $binary_remote_addr，表示限制单个IP同时最多能持有10个连接。\nlimit_conn perserver 100 作用的key是 $server_name，表示虚拟主机(server) 同时能处理并发连接的总数。\n需要注意的是：只有当 request header 被后端server处理后，这个连接才进行计数。\n设置白名单限流主要针对外部访问，内网访问相对安全，可以不做限流，通过设置白名单即可。利用 Nginx ngx_http_geo_module 和 ngx_http_map_module 两个工具模块即可搞定。\n在 nginx.conf 的 http 部分中配置白名单：\ngeo $limit &#123;    default 1;    10.0.0.0/8 0;    192.168.0.0/24 0;    172.20.0.35 0;&#125;map $limit $limit_key &#123;    0 &quot;&quot;;    1 $binary_remote_addr;&#125;limit_req_zone $limit_key zone=myRateLimit:10m rate=10r/s;\n\ngeo 对于白名单(子网或IP都可以) 将返回0，其他IP将返回1。\nmap 将 $limit 转换为 $limit_key，如果是 $limit 是0(白名单)，则返回空字符串；如果是1，则返回客户端实际IP。\nlimit_req_zone 限流的key不再使用 $binary_remote_addr，而是 $limit_key 来动态获取值。如果是白名单，limit_req_zone 的限流key则为空字符串，将不会限流；若不是白名单，将会对客户端真实IP进行限流。\n三、拓展阅读除限流外，ngx_http_core_module 还提供了限制数据传输速度的能力(即常说的下载速度)。\n例如：\nlocation /flv/ &#123;    flv;    limit_rate_after 20m;    limit_rate       100k;&#125;\n\n这个限制是针对每个请求的，表示客户端下载前20M时不限速，后续限制100kb/s。\n","categories":["运维","Nginx"]},{"title":"nginx-proxy-manager","url":"/posts/57986/","content":"官方文档\ndocker run -d --name ngm \\   -p 81:81 \\   -p 80:80 \\   -p 443:443 \\   jc21/nginx-proxy-manager:latest\n\n","categories":["运维","Nginx"]},{"title":"K8S 部署 Grafana","url":"/posts/64015/","content":"这里没有对 Grafana 进行挂载。\ncat &lt;&lt;EOF | kubectl apply -f -kind: DeploymentapiVersion: apps/v1metadata:  labels:    app: grafana  name: grafana  namespace: monitorspec:  replicas: 1  revisionHistoryLimit: 10  selector:    matchLabels:      app: grafana  template:    metadata:      labels:        app: grafana    spec:      securityContext:        runAsUser: 0      containers:        - name: grafana          image: grafana/grafana:latest          imagePullPolicy: IfNotPresent          env:            - name: GF_AUTH_BASIC_ENABLED              value: &quot;true&quot;            - name: GF_AUTH_ANONYMOUS_ENABLED              value: &quot;false&quot;            - name: GF_SECURITY_ADMIN_USER              value: &quot;admin&quot;            - name: GF_SECURITY_ADMIN_PASSWORD              value: &quot;admin&quot;          readinessProbe:            httpGet:              path: /login              port: 3000          ports:            - containerPort: 3000              protocol: TCP---kind: ServiceapiVersion: v1metadata:  labels:    app: grafana  name: grafana  namespace: monitorspec:  ports:    - port: 3000      targetPort: 3000  selector:    app: grafanaEOF\n\nGrafana 中配置 prometheus 时地址填 http://&#123;prometheus的service name&#125;.&#123;prometheus所在namespace&#125;:9090 即可。\nK8S 集群仪表盘仪表盘github\n","categories":["运维","Prometheus"]},{"title":"K8S 部署 Prometheus","url":"/posts/4309/","content":"一、授权对 K8S 的访问想要配置 prometheus 能够收集各个 namespace 或整个集群的指标，需要对其授权。\ncat &lt;&lt;EOF | kubectl apply -f -apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: prometheusrules:- apiGroups: [&quot;&quot;]  resources:  - nodes  - nodes/proxy  - services  - endpoints  - pods  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- apiGroups:  - extensions  resources:  - ingresses  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- nonResourceURLs: [&quot;/metrics&quot;]  verbs: [&quot;get&quot;]---apiVersion: v1kind: ServiceAccountmetadata:  name: prometheus  namespace: monitor---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: prometheusroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: prometheussubjects:- kind: ServiceAccount  name: prometheus  namespace: monitorEOF\n\n二、使用 ConfigMap 管理配置创建 ConfigMap\ncat &lt;&lt;\\EOF | kubectl apply -f -apiVersion: v1kind: ConfigMapmetadata:  name: prometheus-config  namespace: monitordata:  prometheus.yml: |-    global:      scrape_interval:     15s       evaluation_interval: 15s    scrape_configs:      - job_name: &#x27;prometheus&#x27;        static_configs:        - targets: [&#x27;localhost:9090&#x27;]EOF\n\n三、部署 Prometheuscat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: Deploymentmetadata:  name: prometheus  namespace: monitor  labels:    app: prometheusspec:  replicas: 1  selector:    matchLabels:      app: prometheus  template:    metadata:      labels:        app: prometheus    spec:      serviceAccountName: prometheus      serviceAccount: prometheus      containers:        - name: prometheus          image: prom/prometheus          command:          - &quot;/bin/prometheus&quot;          args:          - &quot;--config.file=/etc/prometheus/prometheus.yml&quot;          ports:            - containerPort: 9090              protocol: TCP          volumeMounts:            - mountPath: &quot;/etc/prometheus&quot;              name: prometheus-config      volumes:        - name: prometheus-config          configMap:            name: prometheus-config---apiVersion: v1kind: &quot;Service&quot;metadata:  name: prometheus  namespace: monitorspec:  ports:    - name: prometheus      protocol: TCP      port: 9090      targetPort: 9090  selector:    app: prometheus  type: NodePortEOF\n\n","categories":["运维","Prometheus"]},{"title":"K8S 部署 kube-state-metrics","url":"/posts/38027/","content":"Github 仓库\n部署 yaml\nK8S 各种资源指标\n一、创建访问权限cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1automountServiceAccountToken: falsekind: ServiceAccountmetadata:  labels:    app.kubernetes.io/component: exporter    app.kubernetes.io/name: kube-state-metrics    app.kubernetes.io/version: 2.8.1  name: kube-state-metrics  namespace: monitor---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  labels:    app.kubernetes.io/component: exporter    app.kubernetes.io/name: kube-state-metrics    app.kubernetes.io/version: 2.8.1  name: kube-state-metricsrules:- apiGroups:  - &quot;&quot;  resources:  - configmaps  - secrets  - nodes  - pods  - services  - serviceaccounts  - resourcequotas  - replicationcontrollers  - limitranges  - persistentvolumeclaims  - persistentvolumes  - namespaces  - endpoints  verbs:  - list  - watch- apiGroups:  - apps  resources:  - statefulsets  - daemonsets  - deployments  - replicasets  verbs:  - list  - watch- apiGroups:  - batch  resources:  - cronjobs  - jobs  verbs:  - list  - watch- apiGroups:  - autoscaling  resources:  - horizontalpodautoscalers  verbs:  - list  - watch- apiGroups:  - authentication.k8s.io  resources:  - tokenreviews  verbs:  - create- apiGroups:  - authorization.k8s.io  resources:  - subjectaccessreviews  verbs:  - create- apiGroups:  - policy  resources:  - poddisruptionbudgets  verbs:  - list  - watch- apiGroups:  - certificates.k8s.io  resources:  - certificatesigningrequests  verbs:  - list  - watch- apiGroups:  - discovery.k8s.io  resources:  - endpointslices  verbs:  - list  - watch- apiGroups:  - storage.k8s.io  resources:  - storageclasses  - volumeattachments  verbs:  - list  - watch- apiGroups:  - admissionregistration.k8s.io  resources:  - mutatingwebhookconfigurations  - validatingwebhookconfigurations  verbs:  - list  - watch- apiGroups:  - networking.k8s.io  resources:  - networkpolicies  - ingressclasses  - ingresses  verbs:  - list  - watch- apiGroups:  - coordination.k8s.io  resources:  - leases  verbs:  - list  - watch- apiGroups:  - rbac.authorization.k8s.io  resources:  - clusterrolebindings  - clusterroles  - rolebindings  - roles  verbs:  - list  - watch---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: kube-state-metricsroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: kube-state-metricssubjects:- kind: ServiceAccount  name: kube-state-metrics  namespace: monitorEOF\n\n二、部署在原始 service 的 yaml 上增加了注解，使 prometheus 能够发现 endpoint。\nannotations:  prometheus.io/scrape: &#x27;true&#x27;\n\n部署脚本：\ncat &lt;&lt;EOF | kubectl apply -f -apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app.kubernetes.io/component: exporter    app.kubernetes.io/name: kube-state-metrics    app.kubernetes.io/version: 2.8.1  name: kube-state-metrics  namespace: monitorspec:  replicas: 1  selector:    matchLabels:      app.kubernetes.io/name: kube-state-metrics  template:    metadata:      labels:        app.kubernetes.io/component: exporter        app.kubernetes.io/name: kube-state-metrics        app.kubernetes.io/version: 2.8.1    spec:      automountServiceAccountToken: true      containers:      - image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.8.1        livenessProbe:          httpGet:            path: /healthz            port: 8080          initialDelaySeconds: 5          timeoutSeconds: 5        name: kube-state-metrics        ports:        - containerPort: 8080          name: http-metrics        - containerPort: 8081          name: telemetry        readinessProbe:          httpGet:            path: /            port: 8081          initialDelaySeconds: 5          timeoutSeconds: 5        securityContext:          allowPrivilegeEscalation: false          capabilities:            drop:            - ALL          readOnlyRootFilesystem: true          runAsUser: 65534      serviceAccountName: kube-state-metrics---apiVersion: v1kind: Servicemetadata:  annotations:    prometheus.io/scrape: &#x27;true&#x27;  labels:    app.kubernetes.io/component: exporter    app.kubernetes.io/name: kube-state-metrics    app.kubernetes.io/version: 2.8.1  name: kube-state-metrics  namespace: monitorspec:  clusterIP: None  ports:  - name: http-metrics    port: 8080    targetPort: http-metrics  - name: telemetry    port: 8081    targetPort: telemetry  selector:    app.kubernetes.io/name: kube-state-metricsEOF\n\n三、Prometheus 监控 kube-stae-metrics- job_name: &#x27;kubernetes-service-endpoints&#x27;  kubernetes_sd_configs:  - role: endpoints  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]    action: keep    regex: true  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]    action: replace    target_label: __scheme__    regex: (https?)  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]    action: replace    target_label: __metrics_path__    regex: (.+)  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]    action: replace    target_label: __address__    regex: ([^:]+)(?::\\d+)?;(\\d+)    replacement: $1:$2  - action: labelmap    regex: __meta_kubernetes_service_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_service_name]    action: replace    target_label: kubernetes_name\n\n","categories":["运维","Prometheus"]},{"title":"Prometheus Operator 监控 K8S应用","url":"/posts/64877/","content":"一、Prometheus Operator 能做什么Github\napi 文档\n要了解 Prometheus Operator 能做什么，其实就是要了解 Prometheus Operator 为我们提供了哪些自定义的 Kubernetes 资源，列出了 Prometheus Operator 目前提供的️4类资源：\n\nPrometheus：声明式创建和管理 Prometheus Server实例；\nPodMonitor：配置 Prometheus 需要监控的 pod；\nServiceMonitor：配置 Prometheus 需要监控的 service；\nPrometheusRule：负责声明式的管理告警配置；\nAlertmanager：声明式的创建和管理 Alertmanager 实例。\n\n简言之，Prometheus Operator能够帮助用户自动化的创建以及管理 Prometheus Server 以及其相应的配置。\n二、部署 Prometheus Operator在 Kubernetes 中安装 Prometheus Operator 非常简单，用户可以从以下地址中获取 Prometheus Operator的源码：\ngit clone https://github.com/coreos/prometheus-operator.git\n\n这里，我们为 Promethues Operator 创建一个单独的命名空间 monitor：\nkubectl create namespace monitor\n\n由于需要对 Prometheus Operator 进行 RBAC 授权，而默认的 bundle.yaml 中使用了 default 命名空间，因此，在安装 Prometheus Operator 之前需要先替换一下 bundle.yaml 文件中所有 namespace 定义，由 default 修改为 monitor。 通过运行一下命令安装Prometheus Operator的Deployment实例：\nkubectl -n monitor create -f bundle.yaml\n\nPrometheus Operator 通过 Deployment 的形式进行部署，为了能够让 Prometheus Operator 能够监听和管理 Kubernetes 资源同时也创建了单独的 ServiceAccount 以及相关的授权动作。\n查看Prometheus Operator部署状态，以确保已正常运行：\nkubectl get pods -n monitorNAME                                   READY   STATUS    RESTARTS     AGEprometheus-operator-64d6cb9c4d-xpx7f   1/1     Running   8 (2m ago)   24h\n\n三、部署 Prometheus 服务1. 创建具有集群权限的 ServiceAccount由于默认创建的 Prometheus 实例使用的是其所在命名空间（本文中是 monitor）下的 default 账号，该账号并没有权限能够获取其他命名空间下的任何资源信息。\n为了修复这个问题，我们需要在 monitor 命名空间下为创建一个名为 Prometheus 的 ServiceAccount，并且为该账号赋予相应的集群访问权限。\ncat &lt;&lt;EOF | kubectl -n monitor create -f -apiVersion: v1kind: ServiceAccountmetadata:  name: prometheus  namespace: monitor---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: prometheusrules:- apiGroups: [&quot;&quot;]  resources:  - nodes  - services  - endpoints  - pods  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- apiGroups: [&quot;&quot;]  resources:  - configmaps  verbs: [&quot;get&quot;]- nonResourceURLs: [&quot;/metrics&quot;]  verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: prometheusroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: prometheussubjects:- kind: ServiceAccount  name: prometheus  namespace: monitorEOF\n\n2. 创建 Prometheus 实例当集群中已经安装 Prometheus Operator 之后，对于部署 Prometheus Server 实例就变成了声明一个 Prometheus 资源，如下所示，我们在 monitor 命名空间下创建一个 Prometheus 实例：\ncat &lt;&lt;EOF | kubectl -n monitor apply -f -apiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata:  name: prometheus  namespace: monitorspec:  serviceAccountName: prometheus  serviceMonitorSelector:    matchLabels:      evn: test  resources:    requests:      memory: 400Mi      cpu: 1EOF\n\nPrometheus 通过  serviceMonitorSelector 选择具有指定标签的 ServiceMonitor 监控配置。\npodMonitorSelector 选择具有指定标签的 PodMonitor 监控配置。\n此时，查看 monitor 命名空间下的 statefulsets 资源，可以看到 Prometheus Operator 自动通过 Statefulset 创建的 Prometheus 实例：\nkubectl get statefulsets -n monitorNAME              READY   AGEprometheus-inst   0/1     2m58s\n\n查看 Pod 实例：\nkubectl get pods -n monitorNAME                                   READY   STATUS    RESTARTS   AGEprometheus-prometheus-0                      2/2     Running   0          64sprometheus-operator-64d6cb9c4d-j2n9t   1/1     Running   0          32m\n\n3. 访问 Prometheus 界面通过 port-forward 访问 Prometheus 实例:\nkubectl -n monitor port-forward statefulsets/prometheus-prometheus 9090:9090\n\n通过 http://localhost:9090 可以在本地直接访问 Prometheus Operator 创建的 Prometheus 实例。\n四、使用 ServiceMonitor 管理监控配置修改监控配置项也是 Prometheus 下常用的运维操作之一，为了能够自动化的管理 Prometheus 的配置，Prometheus Operator 使用了自定义资源类型 ServiceMonitor 和 PodMonitor 分别来描述监控的 service 和 pod。\n1. 部署示例应用cat &lt;&lt;EOF | kubectl create -f -apiVersion: apps/v1kind: Deploymentmetadata:  name: example-app  namespace: defaultspec:  replicas: 3  selector:    matchLabels:      app: example-app  template:    metadata:      labels:        app: example-app    spec:      containers:      - name: example-app        image: fabxc/instrumented_app        ports:        - name: web          containerPort: 8080EOF\n\n示例应用会通过 Deployment 创建3个Pod实例，并且通过 Service 暴露应用访问信息。\nkubectl get podsNAME                           READY   STATUS    RESTARTS   AGEexample-app-7d95cbf666-755k4   1/1     Running   0          2m6sexample-app-7d95cbf666-grvxp   1/1     Running   0          2m6sexample-app-7d95cbf666-z245c   1/1     Running   0          2m6s\n\n在本地同样通过 port-forward 访问任意Pod实例\nkubectl port-forward deployments/example-app 8080:8080\n\n访问本地的 http://localhost:8080/metrics 实例应用程序会返回指标数据。\n2. 给服务添加监控在原生的 Prometheus 配置方式中，我们在 Prometheus 配置文件中静态的定义 job ，为了能够让 Prometheus 能够采集部署在 Kubernetes 下应用的监控数据，可以通过部署 ServiceMonitor 或 PodMonitor 来实现，配置后的实际 job_name 名称就是 namespace/name 的形式\n方式一：ServiceMonitor而在 Prometheus Operator 中，则可以直接声明一个 ServiceMonitor 对象，如下所示：\ncat &lt;&lt;EOF | kubectl apply -f -apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  name: example-app  namespace: monitor  labels:    evn: testspec:  namespaceSelector:    matchNames:    - default  selector:    matchLabels:      app: example-app    # 给 service 打的标签  endpoints:  - port: web        # 这里是 service 中的name    path: /metrics   # 路径，没有默认 /metrics    interval: 15s    # Prometheus对当前endpoints采集的周期，单位为秒EOF\n\n通过定义 selector 中的标签定义选择监控目标的 service 对象，同时在 endpoints 中指定 port 名称为 web 的端口。\n默认情况下 ServiceMonitor 和监控对象必须是在相同 Namespace 下的。\n在本示例中由于 Prometheus 是部署在 monitor 命名空间下，因此为了能够关联 default 命名空间下的 example 对象，需要使用 namespaceSelector 定义让其可以跨命名空间关联 ServiceMonitor 资源。\n如果希望 ServiceMonitor 可以关联任意命名空间下的标签，则通过以下方式定义：\nspec:  namespaceSelector:    any: true\n\n如果监控的 Target 对象启用了 BasicAuth 认证，那在定义 ServiceMonitor 对象时，可以使用 endpoints 配置中定义 basicAuth 如下所示：\napiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  name: example-app-service-monitor  namespace: monitor  labels:    team: frontendspec:  namespaceSelector:    matchNames:    - default  selector:    matchLabels:      app: example-app  endpoints:  - basicAuth:      password:        name: basic-auth        key: password      username:        name: basic-auth        key: user    port: web\n\n其中 basicAuth 中关联了名为 basic-auth 的 Secret 对象，用户需要手动将认证信息保存到Secret中:\napiVersion: v1kind: Secretmetadata:  name: basic-authdata:  password: dG9vcg== # base64编码后的密码  user: YWRtaW4= # base64编码后的用户名type: Opaque\n\n方式二：PodMonitorcat &lt;&lt;EOF | kubectl apply -f -apiVersion: monitoring.coreos.com/v1kind: PodMonitormetadata:  name: example-app-pod-monitor  namespace: monitor  labels:    env: testspec:  namespaceSelector:    matchNames:      - has-yum-test  selector:    matchLabels:      app: example-app  podMetricsEndpoints:    - path: /metrics      port: webEOF\n","categories":["运维","Prometheus"]},{"title":"Prometheus 实现 K8S POD 服务发现","url":"/posts/23567/","content":"一、添加配置在 prometheus.yml 中添加如下配置：\n- job_name: &#x27;kubernetes-pods&#x27;  kubernetes_sd_configs:  - role: pod  relabel_configs:  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]    action: keep    regex: true  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]    action: replace    target_label: __metrics_path__    regex: (.+)  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]    action: replace    regex: ([^:]+)(?::\\d+)?;(\\d+)    replacement: $1:$2    target_label: __address__  - action: labelmap    regex: __meta_kubernetes_pod_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_pod_name]    action: replace    target_label: kubernetes_pod_name\n\n二、使 pod 能被发现pod 的服务发现在上边配置的基础上，需在pod中添加下边注解：\nannotations:  prometheus.io/scrape: &#x27;true&#x27;     # 区分是否可以可供 prometheus 采集  prometheus.io/port: &#x27;9100&#x27;      # 区分 pod 中哪个端口可以采集指标  prometheus.io/path: /actuator/prometheus   # Pod中的容器可能并没有使用默认的/metrics作为监控采集路径\n\n样例：\napiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx      annotations:        prometheus.io/path: /actuator/prometheus        prometheus.io/port: &#x27;9100&#x27;        prometheus.io/scrape: &#x27;true&#x27;    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80\n\n","categories":["运维","Prometheus"]},{"title":"Prometheus 监控 K8S 集群","url":"/posts/4836/","content":"下表中，梳理了监控Kubernetes集群监控的各个维度以及策略：\n\n\n\n目标\n服务发现模式\n监控方法\n数据源\n\n\n\n从集群各节点kubelet组件中获取节点kubelet的基本运行状态的监控指标\nnode\n白盒监控\nkubelet\n\n\n从集群各节点kubelet内置的cAdvisor中获取，节点中运行的容器的监控指标\nnode\n白盒监控\nkubelet\n\n\n从部署到各个节点的Node Exporter中采集主机资源相关的运行资源\nnode\n白盒监控\nnode exporter\n\n\n对于内置了Promthues支持的应用，需要从Pod实例中采集其自定义监控指标\npod\n白盒监控\ncustom pod\n\n\n获取API Server组件的访问地址，并从中获取Kubernetes集群相关的运行监控指标\nendpoints\n白盒监控\napi server\n\n\n获取集群中Service的访问地址，并通过Blackbox Exporter获取网络探测指标\nservice\n黑盒监控\nblackbox exporter\n\n\n获取集群中Ingress的访问信息，并通过Blackbox Exporter获取网络探测指标\ningress\n黑盒监控\nblackbox exporter\n\n\n下文中的 /var/run/secrets/kubernetes.io/serviceaccount/ca.crt 和 /var/run/secrets/kubernetes.io/serviceaccount/token 都是 Prometheus 的 pod 启动后自动注入进来的，通过这两个文件我们可以在 Pod 中访问 apiserver。\n一、从Kubelet获取节点运行状态Kubelet 组件运行在 Kubernetes 集群的各个节点中，其负责维护和管理节点上Pod的运行状态。kubelet组件的正常运行直接关系到该节点是否能够正常的被 Kubernetes 集群正常使用。\n方式一：基于Node模式，Prometheus 会自动发现 Kubernetes 中所有 Node 节点的信息并作为监控的目标Target。 而这些Target的访问地址实际上就是Kubelet的访问地址，并且Kubelet实际上直接内置了对Promtheus的支持。\n修改 prometheus.yml 配置文件，并添加以下采集任务配置：\n- job_name: &#x27;kubernetes-kubelet&#x27;  scheme: https  tls_config:  ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token  kubernetes_sd_configs:  - role: node  relabel_configs:  - action: labelmap    regex: __meta_kubernetes_node_label_(.+)\n\n这里使用 Node 模式自动发现集群中所有 Kubelet 作为监控的数据采集目标，同时通过 labelmap 步骤，将 Node 节点上的标签，作为样本的标签保存到时间序列当中。\n重新加载 promethues 配置文件，并重建 Promthues 的 Pod 实例后，查看 kubernetes-kubelet 任务采集状态，我们会看到以下错误提示信息：\n\nGet https://192.168.99.100:10250/metrics: x509: cannot validate certificate for 192.168.99.100 because it doesn’t contain any IP SANs\n\n这是由于当前使用的ca证书中，并不包含 192.168.99.100 的地址信息。为了解决该问题，第一种方法是直接跳过ca证书校验过程，通过在 tls_config 中设置 insecure_skip_verify 为 true 即可。 这样 Promthues 在采集样本数据时，将会自动跳过 ca 证书的校验过程，从而从 kubelet 采集到监控数据：\n- job_name: &#x27;kubernetes-kubelet&#x27;    scheme: https    tls_config:      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt      insecure_skip_verify: true    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token    kubernetes_sd_configs:    - role: node    relabel_configs:    - action: labelmap      regex: __meta_kubernetes_node_label_(.+)\n\n\n\n\n方式二：不直接通过 kubelet 的 metrics 服务采集监控数据，而通过 Kubernetes 的 api-server 提供的代理 API 访问各个节点中kubelet 的 metrics 服务，如下所示：\n- job_name: &#x27;kubernetes-kubelet&#x27;    scheme: https    tls_config:      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token    kubernetes_sd_configs:    - role: node    relabel_configs:    - action: labelmap      regex: __meta_kubernetes_node_label_(.+)    - target_label: __address__      replacement: kubernetes.default.svc:443    - source_labels: [__meta_kubernetes_node_name]      regex: (.+)      target_label: __metrics_path__      replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics\n\n\n\n通过  relabel_configs，将从 Kubernetes 获取到的默认地址 __address__ 替换为 kubernetes.default.svc:443。同时将 __metrics_path__ 替换为 api-server 的代理地址 /api/v1/nodes/$&#123;1&#125;/proxy/metrics。\n\n通过获取各个节点中 kubelet 的监控指标，用户可以评估集群中各节点的性能表现。例如，通过指标 kubelet_pod_start_latency_microseconds 可以获得当前节点中Pod启动时间相关的统计数据。\n# 99%的Pod启动时间kubelet_pod_start_latency_microseconds&#123;quantile=&quot;0.99&quot;&#125;\n\n\nPod平均启动时间大致为42s左右（包含镜像下载时间）：\nkubelet_pod_start_latency_microseconds_sum / kubelet_pod_start_latency_microseconds_count\n\n\n除此以外，监控指标kubelet docker 还可以体现出kubelet与当前节点的docker服务的调用情况，从而可以反映出docker本身是否会影响kubelet的性能表现等问题。\n二、从Kubelet获取节点容器资源使用情况各节点的 kubelet 组件中除了包含自身的监控指标信息以外，kubelet 组件还内置了对 cAdvisor 的支持。cAdvisor 能够获取当前节点上运行的所有容器的资源使用情况，通过访问 kubelet 的 /metrics/cadvisor 地址可以获取到 cadvisor 的监控指标，因此和获取 kubelet 监控指标类似，这里同样通过 node 模式自动发现所有的 kubelet 信息，并通过适当的 relabel 过程，修改监控采集任务的配置。 与采集kubelet 自身监控指标相似，这里也有两种方式采集 cadvisor 中的监控指标：\ncadvisor prometheus 指标\n方式一：直接访问 kubelet 的 /metrics/cadvisor 地址，需要跳过 ca 证书认证：\n- job_name: &#x27;kubernetes-cadvisor&#x27;  scheme: https  tls_config:    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt    insecure_skip_verify: true  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token  kubernetes_sd_configs:  - role: node  relabel_configs:  - source_labels: [__meta_kubernetes_node_name]    regex: (.+)    target_label: __metrics_path__    replacement: /metrics/cadvisor  - action: labelmap    regex: __meta_kubernetes_node_label_(.+)\n\n\n\n\n方式二：通过 api-server 提供的代理地址访问 kubelet 的 /metrics/cadvisor 地址：\n- job_name: &#x27;kubernetes-cadvisor&#x27;  scheme: https  tls_config:    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token  kubernetes_sd_configs:  - role: node  relabel_configs:  - target_label: __address__    replacement: kubernetes.default.svc:443  - source_labels: [__meta_kubernetes_node_name]    regex: (.+)    target_label: __metrics_path__    replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor  - action: labelmap    regex: __meta_kubernetes_node_label_(.+)\n\n\n三、使用 NodeExporter 监控集群资源使用情况为了能够采集集群中各个节点的资源使用情况，我们需要在各节点中部署一个 Node Exporter 实例。在本章的“部署Prometheus”小节，我们使用了 Kubernetes 内置的控制器之一 Deployment。Deployment 能够确保 Prometheus 的 Pod 能够按照预期的状态在集群中运行，而 Pod 实例可能随机运行在任意节点上。而与 Prometheus 的部署不同的是，对于Node Exporter 而言每个节点只需要运行一个唯一的实例，此时，就需要使用 Kubernetes 的另外一种控制器 Daemonset。顾名思义，Daemonset 的管理方式类似于操作系统中的守护进程。Daemonset 会确保在集群中所有（也可以指定）节点上运行一个唯一的 Pod 实例。\n创建 node-exporter-daemonset.yml 文件，并写入以下内容：\ncat &lt;&lt;EOF | kubectl apply -f -apiVersion: extensions/v1kind: DaemonSetmetadata:  name: node-exporterspec:  template:    metadata:      annotations:        prometheus.io/scrape: &#x27;true&#x27;        prometheus.io/port: &#x27;9100&#x27;        prometheus.io/path: &#x27;metrics&#x27;      labels:        app: node-exporter      name: node-exporter    spec:      containers:      - image: prom/node-exporter        imagePullPolicy: IfNotPresent        name: node-exporter        ports:        - containerPort: 9100          hostPort: 9100          name: scrape      hostNetwork: true      hostPID: trueEOF\n\n由于 Node Exporter 需要能够访问宿主机，因此这里指定了 hostNetwork 和 hostPID，让 Pod 实例能够以主机网络以及系统进程的形式运行。同时 YAML 文件中也创建了 NodeExporter 相应的 Service。这样通过 Service 就可以访问到对应的 NodeExporter 实例。\n查看Daemonset以及Pod的运行状态\n$ kubectl get daemonsetsNAME            DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGEnode-exporter   1         1         1         1            1           &lt;none&gt;          15s$ kubectl get podsNAME                               READY     STATUS    RESTARTS   AGE...node-exporter-9h56z                1/1       Running   0          51s\n\n由于 Node Exporter 是以主机网络的形式运行，因此直接访问MiniKube的虚拟机IP加上Pod的端口即可访问当前节点上运行的Node Exporter实例:\n$ minikube ip192.168.99.100$ curl http://192.168.99.100:9100/metrics...process_start_time_seconds 1.5251401593e+09# HELP process_virtual_memory_bytes Virtual memory size in bytes.# TYPE process_virtual_memory_bytes gaugeprocess_virtual_memory_bytes 1.1984896e+08\n\n目前为止，通过 Daemonset 的形式将Node Exporter部署到了集群中的各个节点中。接下来，我们只需要通过 Prometheus 的pod服务发现模式，找到当前集群中部署的Node Exporter实例即可。 需要注意的是，由于 Kubernetes 中并非所有的 Pod 都提供了对Prometheus的支持，有些可能只是一些简单的用户应用，为了区分哪些Pod实例是可以供Prometheus进行采集的，这里我们为Node Exporter添加了注解：\nprometheus.io/scrape: &#x27;true&#x27;\n\n由于Kubernetes中Pod可能会包含多个容器，还需要用户通过注解指定用户提供监控指标的采集端口：\nprometheus.io/port: &#x27;9100&#x27;\n\n而有些情况下，Pod中的容器可能并没有使用默认的/metrics作为监控采集路径，因此还需要支持用户指定采集路径：\nprometheus.io/path: &#x27;metrics&#x27;\n\n为 Prometheus 创建监控采集任务 kubernetes-pods，如下所示：\n- job_name: &#x27;kubernetes-pods&#x27;  kubernetes_sd_configs:  - role: pod  relabel_configs:  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]    action: keep    regex: true  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]    action: replace    target_label: __metrics_path__    regex: (.+)  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]    action: replace    regex: ([^:]+)(?::\\d+)?;(\\d+)    replacement: $1:$2    target_label: __address__  - action: labelmap    regex: __meta_kubernetes_pod_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    action: replace    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_pod_name]    action: replace    target_label: kubernetes_pod_name\n\n\n\n\n通过以上 relabel 过程实现对Pod实例的过滤，以及采集任务地址替换，从而实现对特定Pod实例监控指标的采集。需要说明的是kubernetes-pods并不是只针对Node Exporter而言，对于用户任意部署的Pod实例，只要其提供了对Prometheus的支持，用户都可以通过为Pod添加注解的形式为其添加监控指标采集的支持。\n四 、从 kube-apiserver 获取集群运行监控指标在开始正式内容之前，我们需要先了解一下Kubernetes中Service是如何实现负载均衡的，如下图所示，一般来说Service有两个主要的使用场景：\n\n\n代理对集群内部应用Pod实例的请求：当创建Service时如果指定了标签选择器，Kubernetes会监听集群中所有的Pod变化情况，通过Endpoints自动维护满足标签选择器的Pod实例的访问信息；\n代理对集群外部服务的请求：当创建Service时如果不指定任何的标签选择器，此时需要用户手动创建Service对应的Endpoint资源。例如，一般来说，为了确保数据的安全，我们通常讲数据库服务部署到集群外。 这是为了避免集群内的应用硬编码数据库的访问信息，这是就可以通过在集群内创建Service，并指向外部的数据库服务实例。\n\nkube-apiserver扮演了整个Kubernetes集群管理的入口的角色，负责对外暴露Kubernetes API。kube-apiserver组件一般是独立部署在集群外的，为了能够让部署在集群内的应用（kubernetes插件或者用户应用）能够与kube-apiserver交互，Kubernetes会默认在命名空间下创建一个名为kubernetes的服务，如下所示：\n$ kubectl get svc kubernetes -o wideNAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE       SELECTORkubernetes            ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          166d      &lt;none&gt;\n\n而该kubernetes服务代理的后端实际地址通过endpoints进行维护，如下所示：\n$ kubectl get endpoints kubernetesNAME         ENDPOINTS        AGEkubernetes   10.0.2.15:8443   166d\n\n通过这种方式集群内的应用或者系统主机就可以通过集群内部的DNS域名kubernetes.default.svc访问到部署外部的kube-apiserver实例。\n因此，如果我们想要监控kube-apiserver相关的指标，只需要通过endpoints资源找到kubernetes对应的所有后端地址即可。\n如下所示，创建监控任务kubernetes-apiservers，这里指定了服务发现模式为endpoints。Promtheus会查找当前集群中所有的endpoints配置，并通过relabel进行判断是否为apiserver对应的访问地址：\n- job_name: &#x27;kubernetes-apiservers&#x27;  kubernetes_sd_configs:  - role: endpoints  scheme: https  tls_config:    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token  relabel_configs:  - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]    action: keep    regex: default;kubernetes;https  - target_label: __address__    replacement: kubernetes.default.svc:443\n\n在 relabel_configs 配置中第一步用于判断当前endpoints是否为 kube-apiserver 对用的地址。第二步，替换监控采集地址到kubernetes.default.svc:443即可。重新加载配置文件，重建Promthues实例，得到以下结果。\n\n五、对Ingress和Service进行网络探测为了能够对Ingress和Service进行探测，我们需要在集群部署Blackbox Exporter实例。 如下所示，创建blackbox-exporter.yaml用于描述部署相关的内容:\napiVersion: v1kind: Servicemetadata:  labels:    app: blackbox-exporter  name: blackbox-exporterspec:  ports:  - name: blackbox    port: 9115    protocol: TCP  selector:    app: blackbox-exporter  type: ClusterIP---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  labels:    app: blackbox-exporter  name: blackbox-exporterspec:  replicas: 1  selector:    matchLabels:      app: blackbox-exporter  template:    metadata:      labels:        app: blackbox-exporter    spec:      containers:      - image: prom/blackbox-exporter        imagePullPolicy: IfNotPresent        name: blackbox-exporter\n\n通过kubectl命令部署Blackbox Exporter实例，这里将部署一个Blackbox Exporter的Pod实例，同时通过服务blackbox-exporter在集群内暴露访问地址blackbox-exporter.default.svc.cluster.local，对于集群内的任意服务都可以通过该内部DNS域名访问Blackbox Exporter实例：\n$ kubectl get podsNAME                                        READY     STATUS        RESTARTS   AGEblackbox-exporter-f77fc78b6-72bl5           1/1       Running       0          4s$ kubectl get svcNAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGEblackbox-exporter           ClusterIP   10.109.144.192   &lt;none&gt;        9115/TCP         3m\n\n为了能够让Prometheus能够自动的对Service进行探测，我们需要通过服务发现自动找到所有的Service信息。 如下所示，在Prometheus的配置文件中添加名为kubernetes-services的监控采集任务：\n- job_name: &#x27;kubernetes-services&#x27;  metrics_path: /probe  params:    module: [http_2xx]  kubernetes_sd_configs:  - role: service  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]    action: keep    regex: true  - source_labels: [__address__]    target_label: __param_target  - target_label: __address__    replacement: blackbox-exporter.default.svc.cluster.local:9115  - source_labels: [__param_target]    target_label: instance  - action: labelmap    regex: __meta_kubernetes_service_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_service_name]    target_label: kubernetes_name\n\n\n\n在该任务配置中，通过指定kubernetes_sd_config的role为service指定服务发现模式：\nkubernetes_sd_configs:\t- role: service\n\n为了区分集群中需要进行探测的Service实例，我们通过标签‘prometheus.io/probe: true’进行判断，从而过滤出需要探测的所有Service实例：\n- source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\taction: keep\tregex: true\n\n并且将通过服务发现获取到的 Service 实例地址__address__转换为获取监控数据的请求参数。同时将__address执行Blackbox Exporter实例的访问地址，并且重写了标签instance的内容：\n- source_labels: [__address__]  target_label: __param_target- target_label: __address__  replacement: blackbox-exporter.default.svc.cluster.local:9115- source_labels: [__param_target]  target_label: instance\n\n最后，为监控样本添加了额外的标签信息：\n- action: labelmap  regex: __meta_kubernetes_service_label_(.+)- source_labels: [__meta_kubernetes_namespace]  target_label: kubernetes_namespace- source_labels: [__meta_kubernetes_service_name]  target_label: kubernetes_name\n\n对于Ingress而言，也是一个相对类似的过程，这里给出对Ingress探测的Promthues任务配置作为参考：\n- job_name: &#x27;kubernetes-ingresses&#x27;  metrics_path: /probe  params:    module: [http_2xx]  kubernetes_sd_configs:  - role: ingress  relabel_configs:  - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe]    action: keep    regex: true  - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]    regex: (.+);(.+);(.+)    replacement: $&#123;1&#125;://$&#123;2&#125;$&#123;3&#125;    target_label: __param_target  - target_label: __address__    replacement: blackbox-exporter.default.svc.cluster.local:9115  - source_labels: [__param_target]    target_label: instance  - action: labelmap    regex: __meta_kubernetes_ingress_label_(.+)  - source_labels: [__meta_kubernetes_namespace]    target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_ingress_name]    target_label: kubernetes_name\n\n参考文章：https://yunlzheng.gitbook.io/prometheus-book/part-iii-prometheus-shi-zhan/readmd/use-prometheus-monitor-kubernetes\n","categories":["运维","Prometheus"]},{"title":"Prometheus 配置文件","url":"/posts/42741/","content":"Prometheus 配置方式有两种：\n\n命令行，用来配置不可变命令参数，主要是Prometheus运行参数，比如数据存储位置\n配置文件，用来配置Prometheus应用参数，比如数据采集，报警对接\n\n不重启进程配置生效方式也有两种：\n\n对进程发送信号SIGHUP\nHTTP POST请求，需要开启 –web.enable-lifecycle选项 curl -X POST http://192.168.66.112:9091/-/reload\n\n一、命令行命令行可用配置可通过prometheus -h来查看。\n-h, --help                     Show context-sensitive help (also try --help-long and --help-man).    --version                  Show application version.    --config.file=&quot;prometheus.yml&quot;                               Prometheus configuration file path.    --web.listen-address=&quot;0.0.0.0:9090&quot;                               Address to listen on for UI, API, and telemetry.    --web.read-timeout=5m      Maximum duration before timing out read of the request, and closing idle                               connections.    --web.max-connections=512  Maximum number of simultaneous connections.    --web.route-prefix=&lt;path&gt;  Prefix for the internal routes of web endpoints. Defaults to path of                               --web.external-url.    --web.user-assets=&lt;path&gt;   Path to static asset directory, available at /user.    --web.enable-lifecycle     Enable shutdown and reload via HTTP request.\n\n二、配置文件以下说明中， 方括号表示参数是可选的。 对于非列表参数，该值设置为指定的默认值。通用占位符定义如下：\n&lt;boolean&gt;: 布尔值，可以采用true或false值&lt;duration&gt;: 与正则表达式[0-9]+(ms|[smhdwy])匹配的持续时间&lt;labelname&gt;: 与正则表达式[a-zA-Z_][a-zA-Z0-9_]*匹配的字符串&lt;labelvalue&gt;: 一串unicode字符&lt;filename&gt;: 当前工作目录中的有效路径&lt;host&gt;: 由主机名或IP后跟可选端口号组成的有效字符串&lt;path&gt;: 有效的URL路径&lt;scheme&gt;: 一个字符串，可以使用值http或https&lt;string&gt;: 常规字符串&lt;secret&gt;: 包含密码的常规字符串，例如密码&lt;tmpl_string&gt;: 使用前已模板扩展的字符串\n\nprometheus 配置样例\n配置文件中配置项如下：\n# 全局配置 （如果有内部单独设定，会覆盖这个参数）global:  #默认情况下收集目标的频率。  [ scrape_interval: &lt;duration&gt; | default = 1m ]  #收集数据请求超时之前的时间。  [ scrape_timeout: &lt;duration&gt; | default = 10s ]  #评估规则的频率。  [ evaluation_interval: &lt;duration&gt; | default = 1m ]  #与外部系统（federation, remote storage, Alertmanage）通信时添加到任何时间序列或警报的标签。  external_labels:    [ &lt;labelname&gt;: &lt;labelvalue&gt; ... ]# 采集配置。配置数据源，包含分组job_name以及具体target。又分为静态配置和服务发现scrape_configs:  [ - &lt;scrape_config&gt; ... ]  # 告警规则。 按照设定参数进行扫描加载，用于自定义报警规则，其报警媒介和route路由由alertmanager插件实现。rule_files:  [ - &lt;filepath_glob&gt; ... ]# 告警插件定义。这里会设定alertmanager这个报警插件alerting:  alert_relabel_configs:    [ - &lt;relabel_config&gt; ... ]  alertmanagers:    [ - &lt;alertmanager_config&gt; ... ]# 用于远程存储写配置。remote_write:  [ - &lt;remote_write&gt; ... ]# 用于远程读配置。remote_read:  [ - &lt;remote_read&gt; ... ]\n\n1. scrape_configs 字段全局配置指定在所有其他配置上下文中有效的参数。 它们还用作其他配置部分的默认设置。\n# 默认情况下，作业名称分配给抓取的指标。job_name: &lt;job_name&gt;# 从这项工作中scrape目标的频率。[ scrape_interval: &lt;duration&gt; | default = &lt;global_config.scrape_interval&gt; ]# 抓取此作业时的每次抓取超时。[ scrape_timeout: &lt;duration&gt; | default = &lt;global_config.scrape_timeout&gt; ]# 从目标获取指标的HTTP资源路径。[ metrics_path: &lt;path&gt; | default = /metrics ]#honor_labels控制Prometheus如何处理已存在于抓取数据中的标签与Prometheus将在服务器端附加的标签（“作业”和“实例”标签，手动配置的目标标签以及由服务发现实现生成的标签）之间的冲突。#如果将honor_labels设置为“true”，则通过保留已抓取数据中的标签值并忽略冲突的服务器端标签来解决标签冲突。#如果将honor_labels设置为“false”，则通过将已抓取数据中的冲突标签重命名为“exported_ &lt;original-label&gt;”（例如“exported_instance”，“exported_job”），然后附加服务器端标签来解决标签冲突。#将honor_labels设置为“true”对于诸如联合和刮除Pushgateway的用例很有用，在这种情况下应保留目标中指定的所有标签。#请注意，此设置不会影响任何全局配置的“external_labels”。在与外部系统通信时，仅在时间序列尚无给定标签时才始终应用它们，否则将忽略它们。[ honor_labels: &lt;boolean&gt; | default = false ]#Honor_timestamps控制Prometheus是否尊重抓取数据中的时间戳。#如果将honor_timestamps设置为“true”，则将使用目标公开的指标的时间戳。#如果将honor_timestamps设置为“ false”，则目标忽略的度量标准的时间戳将被忽略。[ honor_timestamps: &lt;boolean&gt; | default = true ]# 配置用于请求的网络协议方案。[ scheme: &lt;scheme&gt; | default = http ]# 可选的 HTTP URL 参数。params:  [ &lt;string&gt;: [&lt;string&gt;, ...] ]# 使用配置的用户名和密码，在每个抓取请求上设置“ Authorization”标头。password和password_file是互斥的。basic_auth:  [ username: &lt;string&gt; ]  [ password: &lt;secret&gt; ]  [ password_file: &lt;string&gt; ]# 使用配置的载体令牌在每个抓取请求上设置“Authorization”标头。 它与`bearer_token_file`互斥。[ bearer_token: &lt;secret&gt; ]# 使用从配置文件中读取的承载令牌，在每个抓取请求上设置“Authorization”标头。 它与`bearer_token`互斥。[ bearer_token_file: /path/to/bearer/token/file ]# 配置抓取请求的TLS设置。tls_config:  [ &lt;tls_config&gt; ]#可选的代理URL。[ proxy_url: &lt;string&gt; ]# Azure服务发现配置列表。azure_sd_configs:  [ - &lt;azure_sd_config&gt; ... ]# Consul服务发现配置列表。consul_sd_configs:  [ - &lt;consul_sd_config&gt; ... ]# DNS服务发现配置列表。dns_sd_configs:  [ - &lt;dns_sd_config&gt; ... ]# EC2服务发现配置列表。ec2_sd_configs:  [ - &lt;ec2_sd_config&gt; ... ]# OpenStack服务发现配置列表。openstack_sd_configs:  [ - &lt;openstack_sd_config&gt; ... ]# 文件服务发现配置列表。file_sd_configs:  [ - &lt;file_sd_config&gt; ... ]# GCE服务发现配置列表。gce_sd_configs:  [ - &lt;gce_sd_config&gt; ... ]# Kubernetes服务发现配置列表。kubernetes_sd_configs:  [ - &lt;kubernetes_sd_config&gt; ... ]# Marathon服务发现配置列表。marathon_sd_configs:  [ - &lt;marathon_sd_config&gt; ... ]# AirBnB的Nerve服务发现配置列表。nerve_sd_configs:  [ - &lt;nerve_sd_config&gt; ... ]# Zookeeper Serverset服务发现配置列表。serverset_sd_configs:  [ - &lt;serverset_sd_config&gt; ... ]# Triton服务发现配置列表。triton_sd_configs:  [ - &lt;triton_sd_config&gt; ... ]# 此作业的带标签的静态配置目标列表。static_configs:  [ - &lt;static_config&gt; ... ]# 目标重新标记配置的列表。relabel_configs:  [ - &lt;relabel_config&gt; ... ]# metric重新标记配置列表。metric_relabel_configs:  [ - &lt;relabel_config&gt; ... ]# 为了防止Prometheus服务过载，使用该字段限制经过relabel之后的数据采集数量，超过该数字拉取的数据就会被忽略。0表示没有限制。[ sample_limit: &lt;int&gt; | default = 0 ]\n\ntls_configtls_config允许配置TLS连接。\n#用于验证API服务器证书的CA证书。[ ca_file: &lt;filename&gt; ]#用于服务器的客户端证书身份验证的证书和密钥文件。[ cert_file: &lt;filename&gt; ][ key_file: &lt;filename&gt; ]#ServerName扩展名，指示服务器的名称。 https://tools.ietf.org/html/rfc4366#section-3.1[ server_name: &lt;string&gt; ]#禁用服务器证书的验证。[ insecure_skip_verify: &lt;boolean&gt; ]\n\nstatic_configs服务发现来获取抓取目标为动态配置，这个配置项目为静态配置，静态配置为典型的targets配置，在改配置字段可以直接添加标签\nstatic_configs:- targets: [&#x27;localhost:9090&#x27;, &#x27;localhost:9191&#x27;]  labels:    my:   label    your: label\n\n采集器所采集的数据都会带有label，当使用服务发现时，比如consul所携带的label如下:\n__meta_consul_address: consul地址__meta_consul_dc: consul中服务所在的数据中心__meta_consul_metadata_: 服务的metadata__meta_consul_node: 服务所在consul节点的信息__meta_consul_service_address: 服务访问地址__meta_consul_service_id: 服务ID__meta_consul_service_port: 服务端口__meta_consul_service: 服务名称__meta_consul_tags: 服务包含的标签信息\n\n这些lable是数据筛选与聚合计算的基础。\nrelabel_configs 和 metric_relabel_configs抓取数据很繁杂，尤其是通过服务发现添加的target。所以过滤就显得尤为重要，我们知道抓取数据就是抓取 target 的一些列 metrics，Prometheus 过滤是通过对标签操作操现的，在字段 relabel_configs 和 metric_relabel_configs 里面配置，两者的配置都需要relabel_config 字段。该字段需要配置项如下：\n[ source_labels: &#x27;[&#x27; &lt;labelname&gt; [, ...] &#x27;]&#x27; ][ separator: &lt;string&gt; | default = ; ][ target_label: &lt;labelname&gt; ][ regex: &lt;regex&gt; | default = (.*) ][ modulus: &lt;uint64&gt; ][ replacement: &lt;string&gt; | default = $1 ]#action除了默认动作还有keep、drop、hashmod、labelmap、labeldrop、labelkeep[ action: &lt;relabel_action&gt; | default = replace ]\n\n示例：\nscrape_configs:  - job_name: consul_sd    relabel_configs:    - source_labels:  [&quot;__meta_consul_dc&quot;]      regex: &quot;(.*)&quot;      replacement: $1      action: replace      target_label: &quot;dc&quot;    metric_relabel_configs:    - source_labels: [id]      regex: &#x27;/system.slice/var-lib-docker-containers.*-shm.mount&#x27;      action: drop    - source_labels: [container_label_JenkinsId]      regex: &#x27;.+&#x27;      action: drop\n\n其他服务发现的 config官方配置文档\n2. alerting 字段该字段配置与Alertmanager进行对接的配置 样例：\nalerting:  alert_relabel_configs: # 动态修改 alert 属性的规则配置。    - source_labels: [dc]       regex: (.+)\\d+      target_label: dc1  alertmanagers:    - static_configs:        - targets: [&#x27;127.0.0.1:9093&#x27;] # 单实例配置        #- targets: [&#x27;172.31.10.167:19093&#x27;,&#x27;172.31.10.167:29093&#x27;,&#x27;172.31.10.167:39093&#x27;] # 集群配置    - job_name: &#x27;Alertmanager&#x27;    # metrics_path defaults to &#x27;/metrics&#x27;    # scheme defaults to &#x27;http&#x27;.    - static_configs:      - targets: [&#x27;localhost:19093&#x27;]\n\nalert_relabel_configs警报重新标记在发送到 alertmanager 之前应用于警报。 它具有与目标重新标记相同的配置格式和操作，外部标签标记后应用警报重新标记，主要是针对集群配置。\n这个设置的用途是确保具有不同外部 label 的 HA 对 Prometheus 服务端发送相同的警报信息。\nalertmanager该项目主要用来配置不同的 alertmanagers 服务，以及 Prometheus 服务和他们的链接参数。alertmanagers 服务可以静态配置也可以使用服务发现配置。Prometheus 以 pushing 的方式向 alertmanager 传递数据。\nalertmanager 可以通过 static_configs 参数静态配置，也可以使用其中一种支持的服务发现机制动态发现，我们上面的配置是静态的单实例。\n[ timeout: &lt;duration&gt; | default = 10s ][ path_prefix: &lt;path&gt; | default = / ][ scheme: &lt;scheme&gt; | default = http ]basic_auth:  [ username: &lt;string&gt; ]  [ password: &lt;string&gt; ]  [ password_file: &lt;string&gt; ][ bearer_token: &lt;string&gt; ][ bearer_token_file: /path/to/bearer/token/file ]tls_config:  [ &lt;tls_config&gt; ][ proxy_url: &lt;string&gt; ]azure_sd_configs:  [ - &lt;azure_sd_config&gt; ... ]consul_sd_configs:  [ - &lt;consul_sd_config&gt; ... ]dns_sd_configs:  [ - &lt;dns_sd_config&gt; ... ]ec2_sd_configs:  [ - &lt;ec2_sd_config&gt; ... ]file_sd_configs:  [ - &lt;file_sd_config&gt; ... ]gce_sd_configs:  [ - &lt;gce_sd_config&gt; ... ]kubernetes_sd_configs:  [ - &lt;kubernetes_sd_config&gt; ... ]marathon_sd_configs:  [ - &lt;marathon_sd_config&gt; ... ]nerve_sd_configs:  [ - &lt;nerve_sd_config&gt; ... ]serverset_sd_configs:  [ - &lt;serverset_sd_config&gt; ... ]triton_sd_configs:  [ - &lt;triton_sd_config&gt; ... ]static_configs:  [ - &lt;static_config&gt; ... ]relabel_configs:  [ - &lt;relabel_config&gt; ... ]\n\n此外，relabel_configs 允许从发现的实体中选择 Alertmanager，并对使用的API路径提供高级修改，该路径通过 __alerts_path__ 标签公开。\n完成以上配置后，重启Prometheus服务，用以加载生效，也可以使用热加载功能，使其配置生效。然后通过浏览器，访问 http://192.168.1.220:19090/alerts 就可以看 inactive pending firing 三个状态，没有警报信息是因为我们还没有配置警报规则 rules。\n这里定义和prometheus集成的alertmanager插件，用于监控报警。后续会单独进行alertmanger插件的配置、配置说明、报警媒介以及route路由规则记录。\n3. rule_files 字段这个主要是用来设置告警规则，基于设定什么指标进行报警（类似触发器trigger）。这里设定好规则以后，prometheus 会根据全局global 设定的 evaluation_interval 参数进行扫描加载，规则改动后会自动加载。其报警媒介和route路由由alertmanager插件实现。 样例：\n# Load rules once and periodically evaluate them according to the global &#x27;evaluation_interval&#x27;.rule_files:  # - &quot;first_rules.yml&quot;  # - &quot;second_rules.yml&quot; \n\nfirst_rules.yml 样例：\ngroups: - name: test-rules   rules:   - alert: InstanceDown # 告警名称     expr: up == 0 # 告警的判定条件，参考Prometheus高级查询来设定     for: 10s # 满足告警条件持续时间多久后，才会发送告警     labels: #标签项      severity: error     annotations: # 解析项，详细解释告警信息      summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: has been down&quot;      description: &quot;&#123;&#123;$labels.instance&#125;&#125;: job &#123;&#123;$labels.job&#125;&#125; has been down &quot;\n\nPrometheus 支持两种类型的 Rules ，可以对其进行配置，然后定期进行运算：recording rules 记录规则 与 alerting rules 警报规则，规则文件的计算频率与警报规则计算频率一致，都是通过全局配置中的 evaluation_interval 定义。\n规则分组不论是recording rules还是alerting rules都要在组里面。\ngroups:    - name: example    #该组下的规则    rules:      [ - &lt;rule&gt; ... ]\n\nalerting rules要在Prometheus中使用Rules规则，就必须创建一个包含必要规则语句的文件，并让Prometheus通过Prometheus配置中的rule_files字段加载该文件，前面我们已经讲过了。 其实语法都一样，除了 recording rules 中的收集的指标名称 record: 字段配置方式略有不同，其他都是一样的。\n配置范例：\n- alert: ServiceDown  expr: avg_over_time(up[5m]) * 100 &lt; 50  annotations:      description: The service &#123;&#123; $labels.job &#125;&#125; instance &#123;&#123; $labels.instance &#125;&#125; is        not responding for more than 50% of the time for 5 minutes.      summary: The service &#123;&#123; $labels.job &#125;&#125; is not responding- alert: RedisDown  expr: avg_over_time(redis_up[5m]) * 100 &lt; 50  annotations:      description: The Redis service &#123;&#123; $labels.job &#125;&#125; instance &#123;&#123; $labels.instance        &#125;&#125; is not responding for more than 50% of the time for 5 minutes.      summary: The Redis service &#123;&#123; $labels.job &#125;&#125; is not responding- alert: PostgresDown  expr: avg_over_time(pg_up[5m]) * 100 &lt; 50  annotations:      description: The Postgres service &#123;&#123; $labels.job &#125;&#125; instance &#123;&#123; $labels.instance        &#125;&#125; is not responding for more than 50% of the time for 5 minutes.      summary: The Postgres service &#123;&#123; $labels.job &#125;&#125; is not responding\n\n定义 Recording rulesrecording rules 是提前设置好一个比较花费大量时间运算或经常运算的表达式，其结果保存成一组新的时间序列数据。当需要查询的时候直接会返回已经计算好的结果，这样会比直接查询快，同时也减轻了PromQl的计算压力，同时对可视化查询的时候也很有用，可视化展示每次只需要刷新重复查询相同的表达式即可。\n在配置的时候，除却 record: 需要注意，其他的基本上是一样的，一个 groups 下可以包含多条规则 rules ，Recording 和 Rules 保存在 group 内，Group 中的规则以规则的配置时间间隔顺序运算，也就是全局中的 evaluation_interval 设置。\n配置示例：\ngroups:- name: http_requests_total  rules:  - record: job:http_requests_total:rate10m    expr: sum by (job)(rate(http_requests_total[10m]))    lables:      team: operations  - record: job:http_requests_total:rate30m    expr: sum by (job)(rate(http_requests_total[30m]))    lables:      team: operations\n\n上面的规则其实就是根据 record 规则中的定义，Prometheus 会在后台完成 expr 中定义的 PromQL 表达式周期性运算，以 job 为维度使用 sum 聚合运算符 计算 函数rate 对http_requests_total 指标区间 10m 内的增长率，并且将计算结果保存到新的时间序列 job:http_requests_total:rate 10m 中， 同时还可以通过 labels 为样本数据添加额外的自定义标签，但是要注意的是这个 lables 一定存在当前表达式 Metrics 中。\n使用模板模板是在警报中使用时间序列标签和值展示的一种方法，可以用于警报规则中的注释（annotation）与标签（lable）。模板其实使用的go语言的标准模板语法，并公开一些包含时间序列标签和值的变量。这样查询的时候，更具有可读性，也可以执行其他PromQL查询 来向警报添加额外内容，ALertmanager Web UI中会根据标签值显示器警报信息。\n&#123;&#123; $lable.&#125;&#125; 可以获取当前警报实例中的指定标签值\n&#123;&#123; $value &#125;&#125; 变量可以获取当前PromQL表达式的计算样本值。\ngroups:- name: operations  rules:# monitor node memory usage  - alert: node-memory-usage    expr: (1 - (node_memory_MemAvailable_bytes&#123;env=&quot;operations&quot;,job!=&#x27;atlassian&#x27;&#125; / (node_memory_MemTotal_bytes&#123;env=&quot;operations&quot;&#125;)))* 100 &gt; 90    for: 1m    labels:      status: Warning      team: operations    annotations:      description: &quot;Environment: &#123;&#123; $labels.env &#125;&#125; Instance: &#123;&#123; $labels.instance &#125;&#125; memory usage above &#123;&#123; $value &#125;&#125; ! ! !&quot;      summary:  &quot;node os memory usage status&quot;\n\n调整好rules以后，我们可以使用 curl -XPOST http://localhost:9090/-/reload 或者 对Prometheus服务重启，让警报规则生效。\n这个时候，我们可以把阈值调整为 50 来进行故障模拟操作，这时在去访问UI的时候，当持续1分钟满足警报条件，实际警报状态已转换为 Firing，可以在 Annotations中看到模板信息 summary 与 description 已经成功显示。\n规则检查\n#打镜像后使用FROM golang:1.10RUN GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go get -u github.com/prometheus/prometheus/cmd/promtoolFROM alpine:latest  COPY --from=0 /go/bin/promtool /binENTRYPOINT [&quot;/bin/promtool&quot;]  # 编译docker build -t promtool:0.1 .#使用docker run --rm -v /root/test/prom:/opt promtool:0.1 check rules /opt/rule.yml#返回Checking /opt/rule.yml  SUCCESS: 1 rules found\n\n4. remote_read 字段#远程读取的urlurl: &lt;string&gt;#通过标签来过滤读取的数据required_matchers:  [ &lt;labelname&gt;: &lt;labelvalue&gt; ... ][ remote_timeout: &lt;duration&gt; | default = 1m ]#当远端不是存储的时候激活该项[ read_recent: &lt;boolean&gt; | default = false ]basic_auth:  [ username: &lt;string&gt; ]  [ password: &lt;string&gt; ]  [ password_file: &lt;string&gt; ][ bearer_token: &lt;string&gt; ][ bearer_token_file: /path/to/bearer/token/file ]tls_config:  [ &lt;tls_config&gt; ][ proxy_url: &lt;string&gt; ]\n\n5. remote_write 字段url: &lt;string&gt;[ remote_timeout: &lt;duration&gt; | default = 30s ]#写入数据时候进行标签过滤write_relabel_configs:  [ - &lt;relabel_config&gt; ... ]basic_auth:  [ username: &lt;string&gt; ]  [ password: &lt;string&gt; ]  [ password_file: &lt;string&gt; ][ bearer_token: &lt;string&gt; ][ bearer_token_file: /path/to/bearer/token/file ]tls_config:  [ &lt;tls_config&gt; ][ proxy_url: &lt;string&gt; ]#远端写细粒度配置，这里暂时仅仅列出官方注释queue_config:  # Number of samples to buffer per shard before we start dropping them.  [ capacity: &lt;int&gt; | default = 10000 ]  # Maximum number of shards, i.e. amount of concurrency.  [ max_shards: &lt;int&gt; | default = 1000 ]  # Maximum number of samples per send.  [ max_samples_per_send: &lt;int&gt; | default = 100]  # Maximum time a sample will wait in buffer.  [ batch_send_deadline: &lt;duration&gt; | default = 5s ]  # Maximum number of times to retry a batch on recoverable errors.  [ max_retries: &lt;int&gt; | default = 3 ]  # Initial retry delay. Gets doubled for every retry.  [ min_backoff: &lt;duration&gt; | default = 30ms ]  # Maximum retry delay.  [ max_backoff: &lt;duration&gt; | default = 100ms ]\n\n参考（1）Prometheus 配置详解 https://www.dazhuanlan.com/2019/12/12/5df11ada207ce/\n（2）Prometheus配置文件prometheus.yml 四个模块详解 http://www.21yunwei.com/archives/7321\n（3）官方文档说明 https://prometheus.io/docs/prometheus/latest/configuration/configuration/\n（4）Prometheus监控神器-Rules篇 https://zhuanlan.zhihu.com/p/179295676\n（5）Prometheus监控神器-Alertmanager篇(1) https://zhuanlan.zhihu.com/p/179292686\n（6）Prometheus监控神器-Alertmanager篇(2) https://zhuanlan.zhihu.com/p/179294441\n","categories":["运维","Prometheus"]},{"title":"Docker 搭建 Redis 集群以及哈希槽动态扩容","url":"/posts/29004/","content":"一、创建网络docker network create --subnet=172.10.1.0/24 redis\n\n二、创建 Redis 容器创建6个redis实例\ndocker create --name redis1 --net host --privileged=true -v /data/redis/share/redis1:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6381docker create --name redis2 --net host --privileged=true -v /data/redis/share/redis2:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6382docker create --name redis3 --net host --privileged=true -v /data/redis/share/redis3:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6383docker create --name redis4 --net host --privileged=true -v /data/redis/share/redis4:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6384docker create --name redis5 --net host --privileged=true -v /data/redis/share/redis5:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6385docker create --name redis6 --net host --privileged=true -v /data/redis/share/redis6:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6386\n\n命令说明：\n\ndocker create                          // 创建容器\n–name redis1                         // 容器名\n–net host                                // docker网络，使用宿主机的ip和端口\n–privileged=true                   // docker容器，获取宿主机root权限\n-v /data/redis/share/redis1:/data     // 容器的/data 挂载到宿主机 /data/redis/share/redis-node-1\nredis:5.0.7                              // redis 镜像名称和版本号\n–cluster-enabled yes           // redis.conf的配置：开启redis集群\n–appendonly yes                 // redis.conf的配置：开启数据持久化\n–port 6381                           // redis.conf的配置：redis端口号\n\n三、启动容器docker start redis1 redis2 redis3 redis4 redis5 redis6\n\n四、创建集群1.进入redis1容器中docker exec -it redis1 /bin/bash\n\n2.配置一主一从redis-cli --cluster create 192.168.20.10:6381 192.168.20.10:6382 192.168.20.10:6383 192.168.20.10:6384 192.168.20.10:6385 192.168.20.10:6386 --cluster-replicas 1\n\n3.查看集群信息（1）进入redis客户端redis-cli -h 192.168.20.10 -p 6381 -c\n\n（2）查看集群信息cluster info\n\n（3）查看节点信息cluster nodes\n\n（4）查看集群槽位信息退出redis客户端执行命令\nredis-cli --cluster check 192.168.20.10:6381\n\n五、redis集群哈希槽扩容1.新增两台redisdocker create --name redis-node-7 --net host --privileged=true -v /data/redis/share/redis-node-7:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6387docker create --name redis-node-8 --net host --privileged=true -v /data/redis/share/redis-node-8:/data redis:5.0.7 --cluster-enabled yes --appendonly yes --port 6388\n\n2.启动容器docker start redis-node-7 redis-node-8\n\n3.添加6387为master节点（1）进入容器内docker exec -it redis-node-7 /bin/bash\n\n（2）添加节点redis-cli --cluster add-node 192.168.20.10:6387 192.168.20.10:6381\n\n说明：\n\n192.168.20.10:6387：被添加节点\n192.168.20.10:6381：集群内任意节点\n\n执行结果：\nroot@dev:/data# redis-cli --cluster add-node 192.168.20.10:6387 192.168.20.10:6381&gt;&gt;&gt; Adding node 192.168.20.10:6387 to cluster 192.168.20.10:6381&gt;&gt;&gt; Performing Cluster Check (using node 192.168.20.10:6381)M: e3b687667d24189134183edac32bf9a4f69c6033 192.168.20.10:6381   slots:[0-5460] (5461 slots) master   1 additional replica(s)M: 981842c6e96abe8abc34326400565e0b2c44dd0f 192.168.20.10:6382   slots:[5461-10922] (5462 slots) master   1 additional replica(s)S: 4f65496819a0490760d4e1375fb7a371732a1ba8 192.168.20.10:6386   slots: (0 slots) slave   replicates 1b826b5348a69af7054e04c3908ee066e9649ae0S: c450e9af3b9d84c4e8cee6e70e7317628c4a9960 192.168.20.10:6384   slots: (0 slots) slave   replicates e3b687667d24189134183edac32bf9a4f69c6033M: 1b826b5348a69af7054e04c3908ee066e9649ae0 192.168.20.10:6383   slots:[10923-16383] (5461 slots) master   1 additional replica(s)S: 88be3f5ac4cd4c468004e829f3857666e9ff9bc5 192.168.20.10:6385   slots: (0 slots) slave   replicates 981842c6e96abe8abc34326400565e0b2c44dd0f[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 192.168.20.10:6387 to make it join the cluster.[OK] New node added correctly.123456789101112131415161718192021222324252627\n\n（3）查看节点信息redis-cli --cluster check 192.168.20.10:6381\n\n执行结果：\nroot@dev:/data# redis-cli --cluster check 192.168.20.10:6381192.168.20.10:6381 (e3b68766...) -&gt; 0 keys | 5461 slots | 1 slaves.192.168.20.10:6382 (981842c6...) -&gt; 1 keys | 5462 slots | 1 slaves.192.168.20.10:6383 (1b826b53...) -&gt; 0 keys | 5461 slots | 1 slaves.192.168.20.10:6387 (c4629e20...) -&gt; 0 keys | 0 slots | 0 slaves.[OK] 1 keys in 4 masters.0.00 keys per slot on average.&gt;&gt;&gt; Performing Cluster Check (using node 192.168.20.10:6381)M: e3b687667d24189134183edac32bf9a4f69c6033 192.168.20.10:6381   slots:[0-5460] (5461 slots) master   1 additional replica(s)M: 981842c6e96abe8abc34326400565e0b2c44dd0f 192.168.20.10:6382   slots:[5461-10922] (5462 slots) master   1 additional replica(s)S: 4f65496819a0490760d4e1375fb7a371732a1ba8 192.168.20.10:6386   slots: (0 slots) slave   replicates 1b826b5348a69af7054e04c3908ee066e9649ae0S: c450e9af3b9d84c4e8cee6e70e7317628c4a9960 192.168.20.10:6384   slots: (0 slots) slave   replicates e3b687667d24189134183edac32bf9a4f69c6033M: 1b826b5348a69af7054e04c3908ee066e9649ae0 192.168.20.10:6383   slots:[10923-16383] (5461 slots) master   1 additional replica(s)S: 88be3f5ac4cd4c468004e829f3857666e9ff9bc5 192.168.20.10:6385   slots: (0 slots) slave   replicates 981842c6e96abe8abc34326400565e0b2c44dd0fM: c4629e200d17cfa3625d5769e279e02948a1386b 192.168.20.10:6387   slots: (0 slots) master[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.\n\n通过信息可以发现，6387 节点虽然已成功加入到集群，但并未分配槽号。\n（4）分配槽号redis-cli --cluster reshard 192.168.20.10:6381\n\n说明：\n\n192.168.20.10:6381：集群中任意节点 ip:port\n\n执行结果\nroot@dev:/data# redis-cli --cluster reshard 192.168.20.10:6381&gt;&gt;&gt; Performing Cluster Check (using node 192.168.20.10:6381)M: e3b687667d24189134183edac32bf9a4f69c6033 192.168.20.10:6381   slots:[0-5460] (5461 slots) master   1 additional replica(s)M: 981842c6e96abe8abc34326400565e0b2c44dd0f 192.168.20.10:6382   slots:[5461-10922] (5462 slots) master   1 additional replica(s)S: 4f65496819a0490760d4e1375fb7a371732a1ba8 192.168.20.10:6386   slots: (0 slots) slave   replicates 1b826b5348a69af7054e04c3908ee066e9649ae0S: c450e9af3b9d84c4e8cee6e70e7317628c4a9960 192.168.20.10:6384   slots: (0 slots) slave   replicates e3b687667d24189134183edac32bf9a4f69c6033M: 1b826b5348a69af7054e04c3908ee066e9649ae0 192.168.20.10:6383   slots:[10923-16383] (5461 slots) master   1 additional replica(s)S: 88be3f5ac4cd4c468004e829f3857666e9ff9bc5 192.168.20.10:6385   slots: (0 slots) slave   replicates 981842c6e96abe8abc34326400565e0b2c44dd0fM: c4629e200d17cfa3625d5769e279e02948a1386b 192.168.20.10:6387   slots: (0 slots) master[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 4096        # 设置槽数据 16384/master 台数What is the receiving node ID? c4629e200d17cfa3625d5769e279e02948a1386b    # 输入新节点 node idPlease enter all the source node IDs.    Type &#x27;all&#x27; to user all the nodes as source nodes for the hash slots.    Type &#x27;done&#x27; once you entered all the source nodes IDs.Source node #1: all                  # 表示全部节点重新分配槽号\n\n（5）再次查看节点信息redis-cli --cluster check 192.168.20.10:6381\n\n执行结果：\nroot@dev:/data# redis-cli --cluster check 192.168.20.10:6381 192.168.20.10:6381 (e3b68766...) -&gt; 0 keys | 4096 slots | 1 slaves.192.168.20.10:6382 (981842c6...) -&gt; 1 keys | 4096 slots | 1 slaves.192.168.20.10:6383 (1b826b53...) -&gt; 0 keys | 4096 slots | 1 slaves.192.168.20.10:6387 (c4629e20...) -&gt; 0 keys | 4096 slots | 0 slaves.[OK] 1 keys in 4 masters.0.00 keys per slot on average.&gt;&gt;&gt; Performing Cluster Check (using node 192.168.20.10:6381)M: e3b687667d24189134183edac32bf9a4f69c6033 192.168.20.10:6381   slots:[1365-5460] (4096 slots) master   1 additional replica(s)M: 981842c6e96abe8abc34326400565e0b2c44dd0f 192.168.20.10:6382   slots:[6827-10922] (4096 slots) master   1 additional replica(s)S: 4f65496819a0490760d4e1375fb7a371732a1ba8 192.168.20.10:6386   slots: (0 slots) slave   replicates 1b826b5348a69af7054e04c3908ee066e9649ae0S: c450e9af3b9d84c4e8cee6e70e7317628c4a9960 192.168.20.10:6384   slots: (0 slots) slave   replicates e3b687667d24189134183edac32bf9a4f69c6033M: 1b826b5348a69af7054e04c3908ee066e9649ae0 192.168.20.10:6383   slots:[12288-16383] (4096 slots) master   1 additional replica(s)S: 88be3f5ac4cd4c468004e829f3857666e9ff9bc5 192.168.20.10:6385   slots: (0 slots) slave   replicates 981842c6e96abe8abc34326400565e0b2c44dd0fM: c4629e200d17cfa3625d5769e279e02948a1386b 192.168.20.10:6387   slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.\n\n可以看出每个节点都分配了 4096 个槽号，但是 新加入的节点所分配的槽号并不是连续的。\n因为对于新加入的节点，要重新分配槽号成本较大，故从三个旧节点移动部分槽号给新节点。\n4.为新加入的master节点分配从节点（1）将 6388 设置为 6387 从节点redis-cli --cluster add-node 192.168.20.10:6388 192.168.20.10:6387 --cluster-slave --cluster-master-id c4629e200d17cfa3625d5769e279e02948a1386b\n\n说明：\n\nadd-node：后边分别为新加入的节点和节点对应的mastercluster-slave：表示新加入的节点是slave节点–cluster-master-id：表示slave对应的master的node ID\n\n执行结果：\nroot@dev:/data# redis-cli --cluster add-node 192.168.20.10:6388 192.168.20.10:6387 --cluster-slave --cluster-master-id c4629e200d17cfa3625d5769e279e02948a1386b&gt;&gt;&gt; Adding node 192.168.20.10:6388 to cluster 192.168.20.10:6387&gt;&gt;&gt; Performing Cluster Check (using node 192.168.20.10:6387)M: c4629e200d17cfa3625d5769e279e02948a1386b 192.168.20.10:6387   slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) masterM: 1b826b5348a69af7054e04c3908ee066e9649ae0 192.168.20.10:6383   slots:[12288-16383] (4096 slots) master   1 additional replica(s)S: c450e9af3b9d84c4e8cee6e70e7317628c4a9960 192.168.20.10:6384   slots: (0 slots) slave   replicates e3b687667d24189134183edac32bf9a4f69c6033S: 88be3f5ac4cd4c468004e829f3857666e9ff9bc5 192.168.20.10:6385   slots: (0 slots) slave   replicates 981842c6e96abe8abc34326400565e0b2c44dd0fM: e3b687667d24189134183edac32bf9a4f69c6033 192.168.20.10:6381   slots:[1365-5460] (4096 slots) master   1 additional replica(s)M: 981842c6e96abe8abc34326400565e0b2c44dd0f 192.168.20.10:6382   slots:[6827-10922] (4096 slots) master   1 additional replica(s)S: 4f65496819a0490760d4e1375fb7a371732a1ba8 192.168.20.10:6386   slots: (0 slots) slave   replicates 1b826b5348a69af7054e04c3908ee066e9649ae0[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.&gt;&gt;&gt; Send CLUSTER MEET to node 192.168.20.10:6388 to make it join the cluster.Waiting for the cluster to join&gt;&gt;&gt; Configure node as replica of 192.168.20.10:6387.[OK] New node added correctly.\n\n（2）查看节点信息redis-cli --cluster check 192.168.20.10:6381\n\n执行结果：\nroot@dev:/data# redis-cli --cluster check 192.168.20.10:6381192.168.20.10:6381 (e3b68766...) -&gt; 0 keys | 4096 slots | 1 slaves.192.168.20.10:6382 (981842c6...) -&gt; 1 keys | 4096 slots | 1 slaves.192.168.20.10:6383 (1b826b53...) -&gt; 0 keys | 4096 slots | 1 slaves.192.168.20.10:6387 (c4629e20...) -&gt; 0 keys | 4096 slots | 1 slaves.[OK] 1 keys in 4 masters.0.00 keys per slot on average.&gt;&gt;&gt; Performing Cluster Check (using node 192.168.20.10:6381)M: e3b687667d24189134183edac32bf9a4f69c6033 192.168.20.10:6381   slots:[1365-5460] (4096 slots) master   1 additional replica(s)S: 7f81ef37a7ae89311f0c58542a3b92b87879e8a5 192.168.20.10:6388   slots: (0 slots) slave   replicates c4629e200d17cfa3625d5769e279e02948a1386bM: 981842c6e96abe8abc34326400565e0b2c44dd0f 192.168.20.10:6382   slots:[6827-10922] (4096 slots) master   1 additional replica(s)S: 4f65496819a0490760d4e1375fb7a371732a1ba8 192.168.20.10:6386   slots: (0 slots) slave   replicates 1b826b5348a69af7054e04c3908ee066e9649ae0S: c450e9af3b9d84c4e8cee6e70e7317628c4a9960 192.168.20.10:6384   slots: (0 slots) slave   replicates e3b687667d24189134183edac32bf9a4f69c6033M: 1b826b5348a69af7054e04c3908ee066e9649ae0 192.168.20.10:6383   slots:[12288-16383] (4096 slots) master   1 additional replica(s)S: 88be3f5ac4cd4c468004e829f3857666e9ff9bc5 192.168.20.10:6385   slots: (0 slots) slave   replicates 981842c6e96abe8abc34326400565e0b2c44dd0fM: c4629e200d17cfa3625d5769e279e02948a1386b 192.168.20.10:6387   slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master   1 additional replica(s)[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.\n\n可以看出从节点已加入成功。\n六、redis集群哈希槽收缩（1）删除master对应slave节点redis-cli --cluster del-node 192.168.20.10:6388 7f81ef37a7ae89311f0c58542a3b92b87879e8a5\n\n说明：\n\n192.168.20.10:6388：对应被删除节点7f81ef37a7ae89311f0c58542a3b92b87879e8a5：对应被删除节点 node id\n\n执行结果：\nroot@dev:/data# redis-cli --cluster del-node 192.168.20.10:6388 7f81ef37a7ae89311f0c58542a3b92b87879e8a5&gt;&gt;&gt; Removing node 7f81ef37a7ae89311f0c58542a3b92b87879e8a5 from cluster 192.168.20.10:6388&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node.\n\n（2）查看节点信息redis-cli --cluster check 192.168.20.10:6381\n\n执行结果：\nroot@dev:/data# redis-cli --cluster check 192.168.20.10:6381192.168.20.10:6381 (e3b68766...) -&gt; 0 keys | 4096 slots | 1 slaves.192.168.20.10:6382 (981842c6...) -&gt; 1 keys | 4096 slots | 1 slaves.192.168.20.10:6383 (1b826b53...) -&gt; 0 keys | 4096 slots | 1 slaves.192.168.20.10:6387 (c4629e20...) -&gt; 0 keys | 4096 slots | 0 slaves.[OK] 1 keys in 4 masters.0.00 keys per slot on average.&gt;&gt;&gt; Performing Cluster Check (using node 192.168.20.10:6381)M: e3b687667d24189134183edac32bf9a4f69c6033 192.168.20.10:6381   slots:[1365-5460] (4096 slots) master   1 additional replica(s)M: 981842c6e96abe8abc34326400565e0b2c44dd0f 192.168.20.10:6382   slots:[6827-10922] (4096 slots) master   1 additional replica(s)S: 4f65496819a0490760d4e1375fb7a371732a1ba8 192.168.20.10:6386   slots: (0 slots) slave   replicates 1b826b5348a69af7054e04c3908ee066e9649ae0S: c450e9af3b9d84c4e8cee6e70e7317628c4a9960 192.168.20.10:6384   slots: (0 slots) slave   replicates e3b687667d24189134183edac32bf9a4f69c6033M: 1b826b5348a69af7054e04c3908ee066e9649ae0 192.168.20.10:6383   slots:[12288-16383] (4096 slots) master   1 additional replica(s)S: 88be3f5ac4cd4c468004e829f3857666e9ff9bc5 192.168.20.10:6385   slots: (0 slots) slave   replicates 981842c6e96abe8abc34326400565e0b2c44dd0fM: c4629e200d17cfa3625d5769e279e02948a1386b 192.168.20.10:6387   slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.\n\n可以看出集群中已无 6388 节点。\n（3）查看容器退出容器，查看容器状态，可以看到 redis-node-8 容器已被停用\nroot@dev:~# docker ps CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMESc9827168c61d        redis:5.0.7         &quot;docker-entrypoint.s…&quot;   2 hours ago         Up 2 hours                              redis-node-7d1ad96f4a4ee        redis:5.0.7         &quot;docker-entrypoint.s…&quot;   9 days ago          Up 3 hours                              redis6db82428f5225        redis:5.0.7         &quot;docker-entrypoint.s…&quot;   9 days ago          Up 3 hours                              redis55a0c7a3516f5        redis:5.0.7         &quot;docker-entrypoint.s…&quot;   9 days ago          Up 2 hours                              redis4cf9863611d0b        redis:5.0.7         &quot;docker-entrypoint.s…&quot;   9 days ago          Up 3 hours                              redis37fa64b2a8071        redis:5.0.7         &quot;docker-entrypoint.s…&quot;   9 days ago          Up 3 hours                              redis2eae771794f0c        redis:5.0.7         &quot;docker-entrypoint.s…&quot;   9 days ago          Up 2 hours                              redis1\n\n（4）重新分配 6387 的槽号需先进入任意一台redis容器\n执行命令\nredis-cli --cluster reshard 192.168.20.10:6381\n\n执行结果：\nroot@dev:/data# redis-cli --cluster reshard 192.168.20.10:6381&gt;&gt;&gt; Performing Cluster Check (using node 192.168.20.10:6381)M: e3b687667d24189134183edac32bf9a4f69c6033 192.168.20.10:6381   slots:[1365-5460] (4096 slots) master   1 additional replica(s)M: 981842c6e96abe8abc34326400565e0b2c44dd0f 192.168.20.10:6382   slots:[6827-10922] (4096 slots) master   1 additional replica(s)S: 4f65496819a0490760d4e1375fb7a371732a1ba8 192.168.20.10:6386   slots: (0 slots) slave   replicates 1b826b5348a69af7054e04c3908ee066e9649ae0S: c450e9af3b9d84c4e8cee6e70e7317628c4a9960 192.168.20.10:6384   slots: (0 slots) slave   replicates e3b687667d24189134183edac32bf9a4f69c6033M: 1b826b5348a69af7054e04c3908ee066e9649ae0 192.168.20.10:6383   slots:[12288-16383] (4096 slots) master   1 additional replica(s)S: 88be3f5ac4cd4c468004e829f3857666e9ff9bc5 192.168.20.10:6385   slots: (0 slots) slave   replicates 981842c6e96abe8abc34326400565e0b2c44dd0fM: c4629e200d17cfa3625d5769e279e02948a1386b 192.168.20.10:6387   slots:[0-1364],[5461-6826],[10923-12287] (4096 slots) master[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.How many slots do you want to move (from 1 to 16384)? 4096      # 被删除的槽的数量What is the receiving node ID? e3b687667d24189134183edac32bf9a4f69c6033   # 接手被删除槽号的 node id，这里指定给 6381。这里只能输入一个node id，若想将槽号分配给多个 node 需要多次执行该命令Please enter all the source node IDs.  Type &#x27;all&#x27; to use all the nodes as source nodes for the hash slots.  Type &#x27;done&#x27; once you entered all the source nodes IDs.Source node #1: c4629e200d17cfa3625d5769e279e02948a1386b     # 删除槽号的node idSource node #2: done          # done 表示输入node id 结束（这里可以输入多个node id）Do you want to proceed with the proposed reshard plan (yes/no)? yes\n\n（5）再次查看节点信息redis-cli --cluster check 192.168.20.10:6381\n\n执行结果：\nroot@dev:/data# redis-cli --cluster check 192.168.20.10:6381192.168.20.10:6381 (e3b68766...) -&gt; 0 keys | 8192 slots | 1 slaves.192.168.20.10:6382 (981842c6...) -&gt; 1 keys | 4096 slots | 1 slaves.192.168.20.10:6383 (1b826b53...) -&gt; 0 keys | 4096 slots | 1 slaves.192.168.20.10:6387 (c4629e20...) -&gt; 0 keys | 0 slots | 0 slaves.[OK] 1 keys in 4 masters.0.00 keys per slot on average.&gt;&gt;&gt; Performing Cluster Check (using node 192.168.20.10:6381)M: e3b687667d24189134183edac32bf9a4f69c6033 192.168.20.10:6381   slots:[0-6826],[10923-12287] (8192 slots) master   1 additional replica(s)M: 981842c6e96abe8abc34326400565e0b2c44dd0f 192.168.20.10:6382   slots:[6827-10922] (4096 slots) master   1 additional replica(s)S: 4f65496819a0490760d4e1375fb7a371732a1ba8 192.168.20.10:6386   slots: (0 slots) slave   replicates 1b826b5348a69af7054e04c3908ee066e9649ae0S: c450e9af3b9d84c4e8cee6e70e7317628c4a9960 192.168.20.10:6384   slots: (0 slots) slave   replicates e3b687667d24189134183edac32bf9a4f69c6033M: 1b826b5348a69af7054e04c3908ee066e9649ae0 192.168.20.10:6383   slots:[12288-16383] (4096 slots) master   1 additional replica(s)S: 88be3f5ac4cd4c468004e829f3857666e9ff9bc5 192.168.20.10:6385   slots: (0 slots) slave   replicates 981842c6e96abe8abc34326400565e0b2c44dd0fM: c4629e200d17cfa3625d5769e279e02948a1386b 192.168.20.10:6387   slots: (0 slots) master[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.\n\n这里可以看到 6387 已不包含任何槽号。\n（6）删除节点 6387redis-cli --cluster del-node 192.168.20.10:6387 c4629e200d17cfa3625d5769e279e02948a1386b\n\n执行结果：\nroot@dev:/data# redis-cli --cluster del-node 192.168.20.10:6387 c4629e200d17cfa3625d5769e279e02948a1386b&gt;&gt;&gt; Removing node c4629e200d17cfa3625d5769e279e02948a1386b from cluster 192.168.20.10:6387&gt;&gt;&gt; Sending CLUSTER FORGET messages to the cluster...&gt;&gt;&gt; SHUTDOWN the node.\n\n（7）再次查看节点信息redis-cli --cluster check 192.168.20.10:6381\n\n执行结果：\nroot@dev:/data# redis-cli --cluster check 192.168.20.10:6381192.168.20.10:6381 (e3b68766...) -&gt; 0 keys | 8192 slots | 1 slaves.192.168.20.10:6382 (981842c6...) -&gt; 1 keys | 4096 slots | 1 slaves.192.168.20.10:6383 (1b826b53...) -&gt; 0 keys | 4096 slots | 1 slaves.[OK] 1 keys in 3 masters.0.00 keys per slot on average.&gt;&gt;&gt; Performing Cluster Check (using node 192.168.20.10:6381)M: e3b687667d24189134183edac32bf9a4f69c6033 192.168.20.10:6381   slots:[0-6826],[10923-12287] (8192 slots) master   1 additional replica(s)M: 981842c6e96abe8abc34326400565e0b2c44dd0f 192.168.20.10:6382   slots:[6827-10922] (4096 slots) master   1 additional replica(s)S: 4f65496819a0490760d4e1375fb7a371732a1ba8 192.168.20.10:6386   slots: (0 slots) slave   replicates 1b826b5348a69af7054e04c3908ee066e9649ae0S: c450e9af3b9d84c4e8cee6e70e7317628c4a9960 192.168.20.10:6384   slots: (0 slots) slave   replicates e3b687667d24189134183edac32bf9a4f69c6033M: 1b826b5348a69af7054e04c3908ee066e9649ae0 192.168.20.10:6383   slots:[12288-16383] (4096 slots) master   1 additional replica(s)S: 88be3f5ac4cd4c468004e829f3857666e9ff9bc5 192.168.20.10:6385   slots: (0 slots) slave   replicates 981842c6e96abe8abc34326400565e0b2c44dd0f[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered.\n\n可以看出 6387 节点已被删除。\n","categories":["运维","Redis"]},{"title":"Redis Scan 原理解析与踩坑","url":"/posts/19043/","content":"1. 概述由于 Redis 是单线程在处理用户的命令，而 Keys 命令会一次性遍历所有 Key，于是在 命令执行过程中，无法执行其他命令。这就导致如果 Redis 中的 key 比较多，那么 Keys 命令执行时间就会比较长，从而阻塞 Redis。\n所以很多教程都推荐使用 Scan 命令来代替 Keys，因为 Scan 可以限制每次遍历的 key 数量。\nKeys 的缺点：\n\n1）没有limit，我们只能一次性获取所有符合条件的key，如果结果有上百万条，那么等待你的就是“无穷无尽”的字符串输出。\n2）keys命令是遍历算法，时间复杂度是O(N)。如我们刚才所说，这个命令非常容易导致Redis服务卡顿。因此，我们要尽量避免在生产环境使用该命令。\n\n相比于keys命令，Scan命令有两个比较明显的优势：\n\n1）Scan命令的时间复杂度虽然也是O(N)，但它是分次进行的，不会阻塞线程。\n2）Scan命令提供了 count 参数，可以控制每次遍历的集合数。\n\n\n可以理解为 Scan 是渐进式的 Keys。\n\nScan 命令语法如下：\nSCAN cursor [MATCH pattern] [COUNT count] \n\n\ncursor - 游标。\npattern - 匹配的模式。\ncount - 指定每次遍历多少个集合。\n可以简单理解为每次遍历多少个元素\n根据测试，推荐 Count大小为 1W。\n\n\n\nScan 返回值为数组，会返回一个游标+一系列的 Key\n大致用法如下：\nSCAN命令是基于游标的，每次调用后，都会返回一个游标，用于下一次迭代。当游标返回0时，表示迭代结束。\n\n第一次 Scan 时指定游标为 0，表示开启新的一轮迭代，然后 Scan 命令返回一个新的游标，作为第二次 Scan 时的游标值继续迭代，一直到 Scan 返回游标为0，表示本轮迭代结束。\n\n通过这个就可以看出，Scan 完成一次迭代，需要和 Redis 进行多次交互。\nScan 命令注意事项：\n\n返回的结果可能会有重复，需要客户端去重复，这点非常重要;\n遍历的过程中如果有数据修改，改动后的数据能不能遍历到是不确定的;\n单次返回的结果是空的并不意味着遍历结束，而要看返回的游标值是否为零;\n\n2. Scan 踩坑使用时遇到一个 特殊场景，跨区域远程连接 Redis 并进行模糊查询，扫描所有指定前缀的 Key。\n最开始也没多想，直接就是开始 Scan，然后 Count 参数指定的是 1000。\n\nRedis 中大概几百万 Key。\n\n最后发现这个接口需要几十上百秒才返回。\n什么原因呢？\nScan 命令中的 Count 指定一次扫描多少 Key，这里指定为 1000，几百万Key就需要几千次迭代，即和 Redis 交互几千次，然后因为是远程连接，网络延迟比较大，所以耗时特别长。\n最后将 Count 参数调大后，减少了交互次数，就好多了。\n\nCount 参数越大，Redis 阻塞时间也会越长，需要取舍。\n极限一点，Count 参数和总 Key 数一致时，Scan 命令就和 Keys 效果一样了。\n\nCount 大小和 Scan 总耗时的关系如下图：\n\n可以发现 Count 越大，总耗时就越短，不过越后面提升就越不明显了。\n\n所以推荐的 Count 大小为 1W 左右。\n\n如果不考虑 Redis 的阻塞，其实 Keys 比 Scan 会快很多，毕竟一次性处理，省去了多余的交互。\n3. Scan原理Redis使用了Hash表作为底层实现，原因不外乎高效且实现简单。类似于HashMap那样数组+链表的结构。其中第一维的数组大小为2n(n&gt;=0)。每次扩容数组长度扩大一倍。\nScan命令就是对这个一维数组进行遍历。每次返回的游标值也都是这个数组的索引。Count 参数表示遍历多少个数组的元素，将这些元素下挂接的符合条件的结果都返回。因为每个元素下挂接的链表大小不同，所以每次返回的结果数量也就不同。\n演示关于 Scan 命令的遍历顺序，我们可以用一个小栗子来具体看一下：\n127.0.0.1:6379&gt; keys *1) &quot;db_number&quot;2) &quot;key1&quot;3) &quot;myKey&quot;127.0.0.1:6379&gt; scan 0 MATCH * COUNT 11) &quot;2&quot;2) 1) &quot;db_number&quot;127.0.0.1:6379&gt; scan 2 MATCH * COUNT 11) &quot;1&quot;2) 1) &quot;myKey&quot;127.0.0.1:6379&gt; scan 1 MATCH * COUNT 11) &quot;3&quot;2) 1) &quot;key1&quot;127.0.0.1:6379&gt; scan 3 MATCH * COUNT 11) &quot;0&quot;2) (empty list or set)\n\n如上所示，SCAN命令的遍历顺序是：0-&gt;2-&gt;1-&gt;3\n这个顺序看起来有些奇怪，我们把它转换成二进制：00-&gt;10-&gt;01-&gt;11\n可以看到每次这个序列是高位加1的。\n\n普通二进制的加法，是从右往左相加、进位。而这个序列是从左往右相加、进位的。\n\n相关源码：\nv = rev(v); v++; v = rev(v); \n\n将游标倒置，加一后，再倒置，也就是我们所说的“高位加1”的操作。\n相关源码先贴一下代码：\nunsigned long dictScan(dict *d,                       unsigned long v,                       dictScanFunction *fn,                       void *privdata)&#123;    dictht *t0, *t1;    const dictEntry *de;    unsigned long m0, m1;    if (dictSize(d) == 0) return 0;    if (!dictIsRehashing(d)) &#123;//没有在做rehash，所以只有第一个表有数据的        t0 = &amp;(d-&gt;ht[0]);        m0 = t0-&gt;sizemask;        //槽位大小-1,因为大小总是2^N,所以sizemask的二进制总是后面都为1,        //比如16个slot的字典，sizemask为00001111        /* Emit entries at cursor */        de = t0-&gt;table[v &amp; m0];//找到当前这个槽位，然后处理数据        while (de) &#123;            fn(privdata, de);//将这个slot的链表数据全部入队，准备返回给客户端。            de = de-&gt;next;        &#125;    &#125; else &#123;        t0 = &amp;d-&gt;ht[0];        t1 = &amp;d-&gt;ht[1];        /* Make sure t0 is the smaller and t1 is the bigger table */        if (t0-&gt;size &gt; t1-&gt;size) &#123;//将地位设置为            t0 = &amp;d-&gt;ht[1];            t1 = &amp;d-&gt;ht[0];        &#125;        m0 = t0-&gt;sizemask;        m1 = t1-&gt;sizemask;        /* Emit entries at cursor */        de = t0-&gt;table[v &amp; m0];//处理小一点的表。        while (de) &#123;            fn(privdata, de);            de = de-&gt;next;        &#125;        /* Iterate over indices in larger table that are the expansion\t         * of the index pointed to by the cursor in the smaller table */        do &#123;//扫描大点的表里面的槽位，注意这里是个循环，会将小表没有覆盖的slot全部扫描一次的            /* Emit entries at cursor */            de = t1-&gt;table[v &amp; m1];            while (de) &#123;                fn(privdata, de);                de = de-&gt;next;            &#125;            /* Increment bits not covered by the smaller mask */            //下面的意思是，还需要扩展小点的表，将其后缀固定，然后看高位可以怎么扩充。            //其实就是想扫描一下小表里面的元素可能会扩充到哪些地方，需要将那些地方处理一遍。            //后面的(v &amp; m0)是保留v在小表里面的后缀。            //((v | m0) + 1) &amp; ~m0) 是想给v的扩展部分的二进制位不断的加1，来造成高位不断增加的效果。            v = (((v | m0) + 1) &amp; ~m0) | (v &amp; m0);            /* Continue while bits covered by mask difference is non-zero */        &#125; while (v &amp; (m0 ^ m1));//终止条件是 v的高位区别位没有1了，其实就是说到头了。    &#125;    /* Set unmasked bits so incrementing the reversed cursor\t     * operates on the masked bits of the smaller table */    v |= ~m0;    //按位取反，其实相当于v |= m0-1 , ~m0也就是11110000,    //这里相当于将v的不相干的高位全部置为1，待会再进行翻转二进制位，然后加1，然后再转回来    /* Increment the reverse cursor */    v = rev(v);    v++;    v = rev(v);    //下面将v的每一位倒过来再加1，再倒回去，这是什么意思呢，    //其实就是要将有效二进制位里面的高位第一个0位设置置为1，因为现在是0嘛    return v;&#125;\n\nreverse binary iterationRedis Scan 命令最终使用的是 reverse binary iteration 算法，大概可以翻译为 逆二进制迭代，具体算法细节可以看一下这个Github 相关讨论\n这个算法简单来说就是：\n依次从高位（有效位）开始，不断尝试将当前高位设置为1，然后变动更高位为不同组合，以此来扫描整个字典数组。\n其最大的优势在于，从高位扫描的时候，如果槽位是2^N个,扫描的临近的2个元素都是与2^(N-1)相关的就是说同模的，比如槽位8时，0%4 == 4%4， 1%4 == 5%4 ， 因此想到其实hash的时候，跟模是很相关的。\n比如当整个字典大小只有4的时候，一个元素计算出的整数为5， 那么计算他的hash值需要模4，也就是hash(n) == 5%4 == 1 , 元素存放在第1个槽位中。当字典扩容的时候，字典大小变为8， 此时计算hash的时候为5%8 == 5 ， 该元素从1号slot迁移到了5号，1和5是对应的，我们称之为同模或者对应。\n同模的槽位的元素最容易出现合并或者拆分了。因此在迭代的时候只要及时的扫描这些相关的槽位，这样就不会造成大面积的重复扫描。\n3 种情况迭代哈希表时，有以下三种情况：\n\n从迭代开始到结束，哈希表不 Rehash；\n从迭代开始到结束，哈希表Rehash，但每次迭代，哈希表要么不开始 Rehash，要么已经结束 Rehash；\n从一次迭代开始到结束，哈希表在一次或多次迭代中 Rehash。\n即再 Rehash 过程中，执行 Scan 命令，这时数据可能只迁移了一部分。\n\n\n\n因此，游标的实现需要兼顾以上三种情况。上述三种情况下游标实现的要求如下：\n第一种情况比较简单。假设redis的hash表大小为4，第一个游标为0，读取第一个bucket的数据，然后游标返回2，下次读取bucket 2 ，依次遍历。\n第二种情况更复杂。假设redis的hash表大小为4，如果rehash后大小变成8。如果如上返回游标(即返回2)，则显示下图：\n\n假设bucket 0读取后返回到cursor 2，当客户端再次Scan cursor 2时，hash表已经被rehash，大小翻倍到8，redis计算一个key bucket如下：\nhash(key)&amp;(size-1) \n\n即如果大小为4，hash(key)&amp;11，如果大小为8，hash(key)&amp;111。所以当size从4扩大到8时，2 号bucket中的原始数据会被分散到2 (010) 和 6 (110) 这两个 bucket中。\n\n从二进制来看，size为4时，在hash(key)之后，取低两位，即hash(key)&amp;11，如果size为8，bucket位置为hash(key) &amp; 111，即取低三个位。\n\n所以依旧不会出现漏掉数据的情况。\n第三种情况，如果返回游标2时正在进行rehash，则Hash表1的bucket 2中的一些数据可能已经rehash到了的Hash表2 的bucket[2]或bucket[6]，那么必须完全遍历 哈希表2的 bucket 2 和 6，否则可能会丢失数据。\n\nRedis 全局有两个Hash表，扩容时会渐进式的将表1的数据迁移到表2，查询时程序会先在 ht[0] 里面进行查找， 如果没找到的话， 就会继续到 ht[1] 里面进行查找。\n详细信息可以查看：Redis教程(四)—全局数据结构\n\n游标计算具体游标计算代码如下：\n\nScan 命令中的游标，其实就是 Redis 内部的 bucket。\n\nv |= ~m0; // 将游标v的unmarsked 比特都置为1v = rev(v);// 反转vv++; //这个是关键，加1，对一个数加1，其实就是将这个数的低位的连续1变为0，然后将最低的一个0变为1，其实就是将最低的一个0变为1v = rev(v);//再次反转，即得到下一个游标值\n\n代码逻辑非常简单，计算过程如下：\n\n\n大小为 4 时，游标状态转换为 0-2-1-3。\n当大小为 8 时，游标状态转换为 0-4-2-6-1-5-3-7。\n\n可以看出，当size由小变大时，所有原来的游标都能在大hashTable中找到对应的位置，并且顺序一致，不会重复读取，也不会被遗漏。\n总结一下：redis在rehash 扩容的时候，不会重复或者漏掉数据。但缩容，可能会造成重复但不会漏掉数据。\n缩容处理之所以会出现重复数据，其实就是为了保证缩容后数据不丢。\n假设当前 hash 大小为 8：\n\n1）第一次先遍历了 0 号槽，返回游标为 4；\n2）准备遍历 4 号槽，然后此时发生了缩容，4 号槽的元素也进到 0 号槽了。\n3）但是0 号槽之前已经被遍历过了，此时会丢数据吗？\n\n答案就在源码中：\ndo &#123;//扫描大点的表里面的槽位，注意这里是个循环，会将小表没有覆盖的slot全部扫描一次的    /* Emit entries at cursor */    de = t1-&gt;table[v &amp; m1];    while (de) &#123;        fn(privdata, de);        de = de-&gt;next;    &#125;    /* Increment bits not covered by the smaller mask */    //下面的意思是，还需要扩展小点的表，将其后缀固定，然后看高位可以怎么扩充。    //其实就是想扫描一下小表里面的元素可能会扩充到哪些地方，需要将那些地方处理一遍。    //后面的(v &amp; m0)是保留v在小表里面的后缀。    //((v | m0) + 1) &amp; ~m0) 是想给v的扩展部分的二进制位不断的加1，来造成高位不断增加的效果。    v = (((v | m0) + 1) &amp; ~m0) | (v &amp; m0);    /* Continue while bits covered by mask difference is non-zero */&#125; while (v &amp; (m0 ^ m1));//终止条件是 v的高位区别位没有1了，其实就是说到头了。\n\n具体计算方法：\nv = (((v | m0) + 1) &amp; ~m0) | (v &amp; m0); \n\n右边的下半部分是v，左边的上半部分是v。 (v&amp;m0) 取出v的低位，例如size=4时v&amp;00000011\n左半边(v|m0) + 1 将V 的低位设置为1，然后+1 将进位到v 的高位，再次&amp;m0，V 的高位将被取出。\n假设游标返回2并且正在rehashing，大小从4变为8，那么M0 = 00000011 v = 00000010\n根据公式计算的下一个光标是 ((00000010 | 00000011) +1) &amp; (11111111100) | (00000010 &amp; 00000011) = (00000100) &amp; (11111111100) | (00000000010) = (000000000110) 正好是 6。\n4. 小结\nScan Count 参数限制的是遍历的 bucket 数，而不是限制的返回的元素个数\n由于不同 bucket 中的元素个数不同，其中满足条件的个数也不同，所以每次 Scan 返回元素也不一定相同\n\n\nCount 越大，Scan 总耗时越短，但是单次耗时越大，即阻塞Redis 时间边长\n推荐 Count 大小为 1W左右\n当 Count = Redis Key 总数时，Scan 和 Keys 效果一致\n\n\nScan 采用 逆二进制迭代法来计算游标，主要为了兼容Rehash的情况\nScan 为了兼容缩容后不漏掉数据，会出现重复遍历。\n即客户端需要做去重处理\n\n\n\n核心就是 逆二进制迭代法，比较复杂，而且算法作者也没有具体证明，为什么这样就能实现，只是测试发现没有问题，各种情况都能兼容。\n具体算法细节可以看一下这个Github 相关讨论\n\nantirez: Hello @pietern! I’m starting to re-evaluate the idea of an iterator for Redis, and the first item in this task is definitely to understand better your pull request and implementation. I don’t understand exactly the implementation with the reversed bits counter… I wonder if there is a way to make that more intuitive… so investing some more time into this, and if I fail I’ll just merge your code trying to augment it with more comments… Hard to explain but awesome.pietern： Although I don’t have a formal proof for these guarantees, I’m reasonably confident they hold. I worked through every hash table state (stable, grow, shrink) and it appears to work everywhere by means of the reverse binary iteration (for lack of a better word).\n\n所以只能说这个算法很巧妙。就像卡马克快速逆平方根算法：\nfloat Q_rsqrt( float number ) &#123;     long i;     float x2, y;     const float threehalfs = 1.5F ;    x2 = number * 0.5F ;     y = number ;     i = * ( long * ) &amp;y; // evil floating point bit level hacking     i = 0x5f3759df - ( i &gt;&gt; 1 ); // what the fuck?     y = * ( float * ) &amp;i;     y = y * ( threehalfs - ( x2 * y * y ) ); // 1st iteration //  y = y * ( threehalfs - ( x2 * y * y ) ); // 2nd iteration, this can be removed    return y ;&#125;\n\n其中的这个0x5f3759df数就很巧妙。\n","categories":["运维","Redis"]},{"title":"Redis string常用命令","url":"/posts/54115/","content":"set语法： SET key value [NX] [XX] [EX &lt;seconds&gt;] [PX [millseconds]] 设置一对key value必选参数说明  \nSET：命令key：待设置的keyvalue: 设置的key的value\n可选参数说明NX：表示key不存在才设置，如果存在则返回NULLXX：表示key存在时才设置，如果不存在则返回NULLEX seconds：设置过期时间，过期时间精确为秒PX millsecond：设置过期时间，过期时间精确为毫秒\nSET：命令\n127.0.0.1:6379&gt; set user1 &#x27;&#123;&quot;id&quot;:1,&quot;username&quot;:&quot;agan01&quot;,&quot;password&quot;:&quot;123456&quot;,&quot;sex&quot;:1&#125;&#x27;OK127.0.0.1:6379&gt; get user1&quot;&#123;\\&quot;id\\&quot;:1,\\&quot;username\\&quot;:\\&quot;agan01\\&quot;,\\&quot;password\\&quot;:\\&quot;123456\\&quot;,\\&quot;sex\\&quot;:1&#125;&quot;\n\nNX：表示key不存在才设置，如果存在则返回NULL\n127.0.0.1:6379&gt; set user1 &#x27;&#123;&quot;id&quot;:1,&quot;username&quot;:&quot;agan01&quot;,&quot;password&quot;:&quot;123456&quot;,&quot;sex&quot;:1&#125;&#x27; NX(nil)因为user1已经存在，所以设置失败，返回nil\n\nXX：表示key存在时才设置，如果不存在则返回NULL\n127.0.0.1:6379&gt; set user1 &#x27;&#123;&quot;id&quot;:2,&quot;username&quot;:&quot;agan01&quot;,&quot;password&quot;:&quot;123456&quot;,&quot;sex&quot;:1&#125;&#x27; XXOK127.0.0.1:6379&gt; get user1&quot;&#123;\\&quot;id\\&quot;:2,\\&quot;username\\&quot;:\\&quot;agan01\\&quot;,\\&quot;password\\&quot;:\\&quot;123456\\&quot;,\\&quot;sex\\&quot;:1&#125;&quot;\n\nEX seconds：设置过期时间，过期时间精确为秒采用ttl查看剩余过期时间\n127.0.0.1:6379&gt; set user1 &#x27;&#123;&quot;id&quot;:2,&quot;username&quot;:&quot;agan01&quot;,&quot;password&quot;:&quot;123456&quot;,&quot;sex&quot;:1&#125;&#x27; EX 600OK\n\nPX millsecond：设置过期时间，过期时间精确为毫秒\n127.0.0.1:6379&gt; set user1 &#x27;&#123;&quot;id&quot;:2,&quot;username&quot;:&quot;agan01&quot;,&quot;password&quot;:&quot;123456&quot;,&quot;sex&quot;:1&#125;&#x27; PX 600000OK\nsetnx语法：SETNX key value所有参数为必选参数,设置一对key value，如果key存在，则设置失败，等同于 SET key value NX\nsetex语法：SETEX key expire value所有参数为必选参数，设置一对 key value，并设过期时间,单位为秒，等同于 SET key value EX expire\npsetex语法：PSETEX key expire value所有参数为必选参数，设置一对 key value，并设过期时间,单位为毫秒，等同于 SET key value PX expire\nmset作用：批量设值语法：MSET key1 value1 [key2 value2 key3 value3 …]所有参数为必选，key、value对至少为一对。该命令功能是设置多对key-value值。\nmget作用：批量取值语法：MGET key1 [key2 key3 …]所有参数为必选，key值至少为一个，获取多个key的value值，key值存的返回对应的value，不存在的返回NULL\n127.0.0.1:6379&gt; mset k1 v1 k2 v2 k3 v3OK127.0.0.1:6379&gt; mget k1 k2 k31) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;\ngetset作用：先查key出value的值，然后再修改新值语法：GETSET key value所有参数为必选参数，获取指定key的value，并设置key的值为新值value\n127.0.0.1:6379&gt; getset user1 agan1&quot;&#123;\\&quot;id\\&quot;:2,\\&quot;username\\&quot;:\\&quot;agan01\\&quot;,\\&quot;password\\&quot;:\\&quot;123456\\&quot;,\\&quot;sex\\&quot;:1&#125;&quot;127.0.0.1:6379&gt; get user1&quot;agan1&quot;\nsetrange作用：为某个key，修改偏移量offset后的值为value语法：SETRANGE key offset value所有参数为必选参数，设置指定key，偏移量offset后的值为value，影响范围为value的长度， offset不能小于0\n127.0.0.1:6379&gt; set user 123456OK127.0.0.1:6379&gt; setrange user 2 agan(integer) 6127.0.0.1:6379&gt; get user&quot;12agan&quot;\n\ngetrange作用：截取字符串语法：GETRANGE key start end所有参数为必选参数，获取指定key指定区间的value值,start、end可以为负数，如果为负数则反向取区间\n127.0.0.1:6379&gt; get user&quot;12agan&quot;127.0.0.1:6379&gt; getrange user 1 3&quot;2ag&quot;127.0.0.1:6379&gt; getrange user 0 3&quot;12ag&quot;\nappend作用：字符串拼接语法：APPEND key str\n127.0.0.1:6379&gt; set user 123OK127.0.0.1:6379&gt; append user abc(integer) 6127.0.0.1:6379&gt; get user&quot;123abc&quot;\nsubstr作用：字符串截取语法：SUBSTR key str\n127.0.0.1:6379&gt; get user&quot;123abc&quot;127.0.0.1:6379&gt; substr user 0 3&quot;123a&quot;\n\n数字操作incr作用：计数器语法： incr key所有参数为必选，指定key做加1操作。指定key对应的值必须为整型，否则返回错误,操作成功后返回操作后的值\n127.0.0.1:6379&gt; incr product01(integer) 1127.0.0.1:6379&gt; get product01&quot;1&quot;127.0.0.1:6379&gt; incr product01(integer) 2127.0.0.1:6379&gt; incr product01(integer) 3127.0.0.1:6379&gt; get product01&quot;3&quot;\n\ndecr语法： decr key所有参数为必选，指定key做减1操作。指定key对应的值必须为整型，否则返回错误,操作成功后返回操作后的值。为DECR的逆操作。\n127.0.0.1:6379&gt; get product01&quot;3&quot;127.0.0.1:6379&gt; decr product01(integer) 2127.0.0.1:6379&gt; decr product01(integer) 1127.0.0.1:6379&gt; get product01&quot;1&quot;\n\nincrby作用：加法语法：incrby key data所有参数为必选参数，指定key做加data操作,指定key对应的值和data必须为整型，否则返回错误,操作成功后返回操作后的值\n127.0.0.1:6379&gt; set product01 100OK127.0.0.1:6379&gt; get product01&quot;100&quot;127.0.0.1:6379&gt; incrby product01 20(integer) 120127.0.0.1:6379&gt; get product01&quot;120&quot;\n\ndecrby作用：减法语法：DECRBY key data所有参数为必选参数，指定key做减data操作,指定key对应的值和data必须为整型，否则返回错误,操作成功后返回操作后的值\n127.0.0.1:6379&gt; get product01&quot;120&quot;127.0.0.1:6379&gt; decrby product01 30(integer) 90127.0.0.1:6379&gt; get product01&quot;90&quot;\nincrbyfloat作用：在原有的key上加上浮点数\n语法：incrbyfloat key num\n127.0.0.1:6379&gt; set money 10.5OK127.0.0.1:6379&gt; incrbyfloat money 2.2&quot;12.7&quot;","categories":["运维","Redis"]},{"title":"Redis 内存碎片化率","url":"/posts/3431/","content":"一、查看 Redis 内存占用情况redis-cli -p 6379 info memory# Memoryused_memory:1073741736used_memory_human:1024.00Mused_memory_rss:1650737152used_memory_rss_human:1.54Gused_memory_peak:1608721680used_memory_peak_human:1.50Gused_memory_peak_perc:66.75%used_memory_overhead:253906398used_memory_startup:766152used_memory_dataset:819835338used_memory_dataset_perc:76.41%total_system_memory:67535904768total_system_memory_human:62.90Gused_memory_lua:37888used_memory_lua_human:37.00Kmaxmemory:1073741824maxmemory_human:1.00Gmaxmemory_policy:allkeys-lrumem_fragmentation_ratio:1.54mem_allocator:jemalloc-4.0.3active_defrag_running:0lazyfree_pending_objects:0\n\n内存碎片化率计算公式mem_fragmentation_ratio = used_memory_rss/used_memory\n\nused_memory：使用Redis服务自带的分配器分配的内存空间大小。\nused_memory_rss：操作系统分配给Redis实例的内存大小，表示该进程所占物理内存的大小。其中包含了内存碎片的开销。\n\n不同内存碎片化率含义\nmem_fragmentation_ratio &lt; 1：表示 Redis 内存分配超出了物理内存，操作系统正在进行内存交换，内存交换会引起非常明显的响应延迟；\n1 &lt; mem_fragmentation_ratio &lt; 1.5：是合理的；\nmem_fragmentation_ratio &gt; 1.5：说明 Redis 消耗了实际需要物理内存的150%以上，其中50%是内存碎片率，可能是操作系统或Redis实例中内存管理变差的表现。\n\n二、内存碎片化相关配置# Enabled active defragmentation# 自动碎片整理总开关# activedefrag yes # Minimum amount of fragmentation waste to start active defrag# 内存碎片达到多少的时候开启整理active-defrag-ignore-bytes 100mb # Minimum percentage of fragmentation to start active defrag# 碎片率达到百分之多少开启整理active-defrag-threshold-lower 10 # Maximum percentage of fragmentation at which we use maximum effort# 尽最大努力保证的碎片化率active-defrag-threshold-upper 100 # Minimal effort for defrag in CPU percentage# 整理碎片最少占用的 CUP 百分比active-defrag-cycle-min 25 # Maximal effort for defrag in CPU percentage# 整理碎片最多占用 CPU 的百分比active-defrag-cycle-max 75# 自动清理过程中长度小于1000的set/hash/zset/list才会进行自动清理active-defrag-max-scan-fields 1000\n\n注： active-defrag-ignore-bytes与active-defrag-threshold-lower两个条件同时满足会触发内存碎片清理，当有一个不满足则停止清理。\n命令开始自动内存碎片化整理set activedefrag yesconfig rewrite\n\n三、手动释放碎片化内存memory purge\n\n","categories":["运维","Redis"]},{"title":"Redis 压缩列表ziplist","url":"/posts/30991/","content":"1. Ziplist 结构\nzlbytes无符号 4 字节整数，保存 ziplist 内存大小。\n通过 zlbytes 可以直接对 ziplist 的内存大小进行调整，无需遍历整个列表。\nzltail压缩列表最后一个 entry 距离起始地址的偏移量，占 4 个字节。\n这个偏移量使得对表尾的 pop 操作可以在无需遍历整个列表的情况下进行。\nzllen压缩列表的 entry 数量，占 2 个字节。\n当压缩列表的元素数量操作 2^16 - 2 时，zllen 会设置为 2^16 - 1，此时获取元素数量需要遍历整个压缩列表才能获取到。\n故 zllen 不能替代 zltail。\nentry压缩表存储数据的节点，\n（1）结构\nprevlen：前一个 entry 的大小，用于反向遍历；\nencoding：编码，表示不同长度的字符串或整数；\ndata： 存储 entry 中实际数据。\n\n（2）prevlen以字节为单位，记录了前一个节点的长度。长度可以是 1 字节 或 5 字节。\n\n前节点长度小于254：prevlen长度 1 字节，用于存储前节点长度；\n前节点长度大于254：prevlen长度为 5 字节，第一个字节设置为 254，后面 4 个字节存储前节点长度。\n\n\nprevlen 属性主要用于 反向遍历。\n通过 zltial，可以得到最后一个节点位置，获取到前一个节点 prevlen ，指针向前移动即可指向倒数第二个节点。以此类推。\n（3）encoding记录了节点中 data 的 数据类型 和 长度。\n类型主要有两种：字符串 和 整数。\n字符串如果 encoding 以 00、01 或者 10开头，表示数据类型是字符串。\n#define ZIP_STR_06B (0 &lt;&lt; 6)#define ZIP_STR_14B (1 &lt;&lt; 6)#define ZIP_STR_32B (2 &lt;&lt; 6)\n\n字符串有三种编码：\n\n长度 &lt; 2^6：以 00 开头，后 6 位表示 data 数据的长度；\n2^6 &lt;= 长度 &lt; 2^14：以 01 开头，后续 6 位 + 下一个字节的 8 位（共14位），表示 data 长度；\n2^14 &lt;= 长度 &lt; 2^32：以 10 开头，后续 6 位不用，从下一个字节开始，连续 32 位 表示 data 长度。\n\n\n整数如果 encoding 以 11 开头，表示数据类型为整数。\n\n看了上图的最后一个类型，可能有小伙伴就有疑问：为啥没有 11111111 ？答：因为 11111111 表示 zlend (十进制的 255，十六进制的 oxff)\n（4）datadata 表示真实存的数据的 byte数组，可以是 字符串 或者 整数，从编码可以得知类型和长度。知道长度，就知道 data 的起始位置了。\n比较特殊的是，整数 1 ~ 13 (0001 ~ 1101)，因为比较短，刚好可以塞在 encoding 字段里面，所以就没有 data。\n2. 连锁更新通过上面的分析，我们知道：\n\n前个节点的长度小于 254 的时候，用 1 个字节保存 prevlen\n前个字节的长度大于等于 254 的时候，用 5 个字节保存 prevlen\n\n现在我们来考虑一种情况：假设一个压缩列表中，有多个长度 250 ~ 253 的节点，假设是 entry1 ~ entryN。\n因为都是小于 254，所以都是用 1 个字节 保存 prevlen。\n如果此时，在压缩列表最前面，插入一个 254 长度的节点，此时它的长度需要 5 个字节。\n也就是说 entry1.prevlen 会从 1 个字节变为 5 个字节，因为 **prevlen **变长，entry1 的长度超过 254 了。\n这下就糟糕了，entry2.prevlen 也会因为 entry1 而变长，entry2 长度也会超过 254 了。\n然后接着 entry3 也会连锁更新。。。直到节点不超过 254， 噩梦终止。。。\n这种由于一个节点的增删，后续节点变长而导致的连续重新分配内存的现象，就是连锁更新。最坏情况下，会导致整个压缩列表的所有节点都重新分配内存。\n每次分配空间的最坏时间复杂度是 O(n)，所以连锁更新的最坏时间复杂度高达 O(n2) !\n影响虽然说，连锁更新的时间复杂度高，但是它造成大的性能影响的概率很低，原因如下：\n\n压缩列表中需要需要有连续多个长度刚好为 250 ~ 253 的节点，才有可能发生连锁更新。实际上，这种情况并不多见。\n即使有连续多个长度刚好为 250 ~ 253 的节点，连续的个数也不多，不会对性能造成很大影响。\n\n因此，压缩列表插入操作，平均复杂度还是 O(n)\n总结：\n压缩列表是一种为节约内存而开发的顺序型数据结构，是 ZSET、HASH 和 LIST 的底层实现之一。\n压缩列表有 3 种字符串类型编码、6 种整数类型编码\n压缩列表的增删，可能会引发连锁更新操作，但这种操作出现的几率并不高。\n\n","categories":["运维","Redis"]},{"title":"Redis 官方可视化工具 RedisInsight","url":"/posts/10722/","content":"RedisInsight 是一个直观高效的 Redis GUI 管理工具，它可以对 Redis 的内存、连接数、命中率以及正常运行时间进行监控，并且可以在界面上使用 CLI 和连接的 Redis 进行交互（RedisInsight 内置对 Redis 模块支持）：\n官方文档：https://docs.redis.com/latest/ri/\nRedisInsight 提供的功能：\n\n唯一支持 Redis Cluster 的 GUI 工具；\n可以基于 Browser 的界面来进行搜索键、查看和编辑数据；\n支持基于 SSL/TLS 的连接，同时还可以在界面上进行内存分析；\n\n","categories":["运维","Redis"]},{"title":"Redis 实用命令合集","url":"/posts/49997/","content":"1、查看当前内存占用redis-cli info | grep used_memory:\n\n可用于计算某种数据占用内存大小。\n","categories":["运维","Redis"]},{"title":"Redis 延迟排查","url":"/posts/2191/","content":"官方延迟问题排查文档：https://redis.io/topics/latency\n一、Redis延迟1.网络延迟redis-cli 提供了通过ping测量延时的命令，延迟指客户端发出命令到客户端接收到对命令的响应之间的时间。\n–latencyping redis实例，并测试响应时间。\n默认统计每秒的延时情况，可通过 -i 修改时间间隔。\n默认会一直显示最新统计结果，可通过 --raw  或 --csv 只显示一次的统计结果。\n统计结果时间单位为ms。\nroot@b86b4d13b636:/data# redis-cli -p 7001 --latencymin: 0, max: 7, avg: 0.09 (927 samples)root@b86b4d13b636:/data# redis-cli -p 7001 --latency --raw -i 10   0 17 0.70 772root@b86b4d13b636:/data# redis-cli -p 7001 --latency --csv /data/aa -i 100,11,0.58,77\n\n–latency-history与 --latency 一样，但会保留历史结果，时间间隔默认为15s，可通过 -i 改变时间间隔。\nroot@b86b4d13b636:/data# redis-cli -p 7001 --latency-historymin: 0, max: 1, avg: 0.12 (1303 samples) -- 15.01 seconds rangemin: 0, max: 1, avg: 0.11 (1297 samples) -- 15.01 seconds rangemin: 0, max: 14, avg: 0.15 (1275 samples) -- 15.01 seconds range\n\n–latency-dist将延迟结果以彩色显示，可以查看延迟时间所占比例。\nroot@b86b4d13b636:/data# redis-cli -p 7001 --latency-dist\n\n\n2.固有延迟–intrinsic-latency &lt;sec&gt;&lt;sec&gt; 表示检测的时间长度。\n有一种延迟是运行 Redis 的环境的固有部分，即由您的操作系统内核提供的延迟，如果您使用虚拟化，则由您使用的管理程序提供。虽然这种延迟无法消除，但研究它很重要，因为它是基线。\n换句话说，由于内核或管理程序的实现或设置，redis无法实现比该延迟更低的延迟时间。\n参数100是测试将执行的秒数。我们运行测试的时间越长，我们就越有可能发现延迟峰值。100 秒通常是合适的。\n该测试是 CPU 密集型的，并且可能会使系统中的单个内核饱和。\n固有延迟可能会随着时间的推移而变化，具体取决于系统的负载。\nroot@b86b4d13b636:/data# redis-cli -p 7001 --intrinsic-latency 100Max latency so far: 1 microseconds.Max latency so far: 95 microseconds.Max latency so far: 134 microseconds.Max latency so far: 180 microseconds.Max latency so far: 189 microseconds.Max latency so far: 249 microseconds.Max latency so far: 271 microseconds.Max latency so far: 433 microseconds.Max latency so far: 529 microseconds.2503422416 total runs (avg latency: 0.0399 microseconds / 39.95 nanoseconds per run).Worst run took 13243x longer than the average latency.\n\n3.监控延时（latency）Latency 有5种命令查看监控报告，而不同的被监控代码路径被称为事件。\n例如，command是一个测量可能缓慢命令执行的延迟峰值fast-command的事件，而是监视O(1)和O(log N)命令的事件名称。其他事件不太通用，并监视由 Redis 执行的非常具体的操作。例如fork事件只监控Redis执行fork(2)系统调用所用的时间。\n具体可查看官方文档：https://redis.io/topics/latency-monitor\n开启延时监控官方建议虽然这个监控消耗的内存较少，但是如果redis集群运行良好，没有充分的理由不要开启监控提高redis集群对内存的占用率。\n超过100毫秒的响应将会被监控，默认为0，表示关闭状态。\nCONFIG SET latency-monitor-threshold 100\n\nLatency Latest返回所有事件的最新延迟样本\n127.0.0.1:6379&gt; latency latest1) 1) &quot;command&quot; #事件名称   2) (integer) 1405067976 #延迟发生的时间戳   3) (integer) 251 #延迟 毫秒   4) (integer) 1001 #事件的最大延迟\n\nLatency History返回给定事件的延迟时间序列。\n127.0.0.1:6379&gt; latency history command #查看commend事件的历史延迟1) 1) (integer) 1405067822 #时间戳   2) (integer) 251 #延迟2) 1) (integer) 1405067941   2) (integer) 1001\n\nLatency Reset重置一个或多个事件的延迟时间序列数据。\nLatency Graph呈现事件延迟样本的 ASCII 艺术图。\nLatency Doctor回复人类可读的延迟分析报告。\n4.延迟原因解决排查延迟问题，有时还需要知道延迟产生的原因，官网总结可能如下：\n\nLatency induced by network and communication\nSingle threaded nature of Redis\nLatency generated by slow commands\nLatency generated by fork\nFork time in different systems\nLatency induced by transparent huge pages\nLatency induced by swapping (operating system paging)\nLatency due to AOF and disk I/O\nLatency generated by expires\n\n二、慢查询slowlog-log-slower-than慢查询时间阈值，超过这个阈值的查询将会被记录，默认 10000 微秒，即10ms。\n127.0.0.1:7001&gt; config get slowlog-log-slower-than1) &quot;slowlog-log-slower-than&quot;2) &quot;10000&quot;\n\n设置阈值\n127.0.0.1:7001&gt; config set slowlog-log-slower-than 2000OK\n\nslowlog-max-len慢查询日志最大条数，默认128条。\n127.0.0.1:7001&gt; config get slowlog-max-len1) &quot;slowlog-max-len&quot;2) &quot;128&quot;\n\n设置最大条数\n127.0.0.1:7001&gt; config set slowlog-max-len 64OK\n\n配置持久化到配置文件config rewrite\n\n获取前 n 条慢查询日志命令格式：slowlog get [n]\n127.0.0.1:7001&gt; slowlog get 10 1) 1) (integer) 1             # 日志唯一id    2) (integer) 1640439905    # 命令执行时的 UNIX 时间戳    3) (integer) 1             # 命令执行时才，单位 微秒    4) 1) &quot;REPLCONF&quot;           # 命令及参数       2) &quot;ACK&quot;       3) &quot;484&quot;    5) &quot;127.0.0.1:38877&quot;    6) &quot;&quot; 7) 1) (integer) 0    2) (integer) 1640439899    3) (integer) 6    4) 1) &quot;get&quot;       2) &quot;k&quot;    5) &quot;127.0.0.1:46228&quot;    6) &quot;&quot;\n\n慢查询日志重置127.0.0.1:7001&gt; slowlog resetOK\n\n建议slowlog-max-len 配置建议：\n\n线上建议调大慢查询列表，记录慢查询时Redis会对长命令做截断操作，并不会占用大量内存。增大慢查询列表可以减缓慢查询被剔除的可能，例如线上可设置为1000以上。\n\nslowlog-log-slower-than 配置建议：\n\n默认值超过10毫秒判定为慢查询，需要根据Redis并发量调整该值。由于Redis采用单线程响应命令，对于高流量的场景，如果命令执行时间在1毫秒以上，那么Redis最多可支撑OPS不到1000。因此对于高OPS场景的Redis建议设置为1毫秒。\n慢查询只记录命令执行时间，并不包括命令排队和网络传输时间。因此客户端执行命令的时间会大于命令实际执行时间。因为命令执行排队机制，慢查询会导致其他命令级联阻塞，因此当客户端出现请求超时，需要检查该时间点是否有对应的慢查询，从而分析出是否为慢查询导致的命令级联阻塞。\n由于慢查询日志是一个先进先出的队列，也就是说如果慢查询比较多的情况下，可能会丢失部分慢查询命令，为了防止这种情况发生，可以定期执行slow get命令将慢查询日志持久化到其他存储中（例如MySQL），然后可以制作可视化界面进行查询。\n\n三、操作BigKey如果你查询慢日志发现，并不是复杂度过高的命令导致的，而都是 SET / DEL 这种简单命令出现在慢日志中，那么你就要怀疑你的实例否写入了 bigkey。\nRedis 在写入数据时，需要为新的数据分配内存，相对应的，当从 Redis 中删除数据时，它会释放对应的内存空间。\n如果一个 key 写入的 value 非常大，那么 Redis 在分配内存时就会比较耗时。同样的，当删除这个 key 时，释放内存也会比较耗时，这种类型的 key 我们一般称之为 bigkey。\n查看bigkeyroot@redis-1:/data# redis-cli -h 127.0.0.1 --bigkeys -i 0.01                # Scanning the entire keyspace to find biggest keys as well as# average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec# per 100 SCAN commands (not usually needed).[00.00%] Biggest string found so far &#x27;&quot;quota_de_202112271630_9427&quot;&#x27; with 1 bytes-------- summary -------Sampled 188 keys in the keyspace!Total key length in bytes is 4897 (avg len 26.05)Biggest string found &#x27;&quot;quota_de_202112271630_9427&quot;&#x27; has 1 bytes0 lists with 0 items (00.00% of keys, avg size 0.00)0 hashs with 0 fields (00.00% of keys, avg size 0.00)188 strings with 188 bytes (100.00% of keys, avg size 1.00)0 streams with 0 entries (00.00% of keys, avg size 0.00)0 sets with 0 members (00.00% of keys, avg size 0.00)0 zsets with 0 members (00.00% of keys, avg size 0.00)\n\n从输出结果我们可以很清晰地看到，每种数据类型所占用的最大内存 / 拥有最多元素的 key 是哪一个，以及每种数据类型在整个实例中的占比和平均大小 / 元素数量。\n其实，使用这个命令的原理，就是 Redis 在内部执行了 SCAN 命令，遍历整个实例中所有的 key，然后针对 key 的类型，分别执行 STRLEN、LLEN、HLEN、SCARD、ZCARD 命令，来获取 String 类型的长度、容器类型（List、Hash、Set、ZSet）的元素个数。\n要注意 2 个问题：\n\n对线上实例进行 bigkey 扫描时，Redis 的 OPS 会突增，为了降低扫描过程中对 Redis 的影响，最好控制一下扫描的频率，指定 -i 参数即可，它表示扫描过程中每次扫描后休息的时间间隔，单位是秒\n扫描结果中，对于容器类型（List、Hash、Set、ZSet）的 key，只能扫描出元素最多的 key。但一个 key 的元素多，不一定表示占用内存也多，你还需要根据业务情况，进一步评估内存占用情况\n\nbigkey 优化方向\n业务应用尽量避免写入 bigkey\n如果你使用的 Redis 是 4.0 以上版本，用 UNLINK 命令替代 DEL，此命令可以把释放 key 内存的操作，放到后台线程中去执行，从而降低对 Redis 的影响\n如果你使用的 Redis 是 6.0 以上版本，可以开启 lazy-free 机制（lazyfree-lazy-user-del = yes），在执行 DEL 命令时，释放内存也会放到后台线程中执行\n\n但即便可以使用方案 2，也不建议在实例中存入 bigkey。\n这是因为 bigkey 在很多场景下，依旧会产生性能问题。例如，bigkey 在分片集群模式下，对于数据的迁移也会有性能影响，以及数据过期、数据淘汰、透明大页，都会受到 bigkey 的影响。\n四、集中过期如果平时在操作 Redis 时，并没有延迟很大的情况发生，但在某个时间点突然出现一波延时，其现象表现为：变慢的时间点很有规律，例如某个整点，或者每间隔多久就会发生一波延迟。如果是出现这种情况，那么需要排查一下，业务代码中是否存在设置大量 key 集中过期的情况。\n如果有大量的 key 在某个固定时间点集中过期，在这个时间点访问 Redis 时，就有可能导致延时变大。\n为什么集中过期会导致 Redis 延迟变大？这就需要我们了解 Redis 的过期策略是怎样的。\nRedis 的过期数据采用被动过期 + 主动过期两种策略：\n\n被动过期：只有当访问某个 key 时，才判断这个 key 是否已过期，如果已过期，则从实例中删除\n主动过期：Redis 内部维护了一个定时任务，默认每隔 100 毫秒（1秒10次）就会从全局的过期哈希表中随机取出 20 个 key，然后删除其中过期的 key，如果过期 key 的比例超过了 25%，则继续重复此过程，直到过期 key 的比例下降到 25% 以下，或者这次任务的执行耗时超过了 25 毫秒，才会退出循环\n\n注意，这个主动过期 key 的定时任务，是在 Redis 主线程中执行的。\n也就是说如果在执行主动过期的过程中，出现了需要大量删除过期 key 的情况，那么此时应用程序在访问 Redis 时，必须要等待这个过期任务执行结束，Redis 才可以服务这个客户端请求。\n此时就会出现，应用访问 Redis 延时变大。\n如果此时需要过期删除的是一个 bigkey，那么这个耗时会更久。而且，这个操作延迟的命令并不会记录在慢日志中。\n因为慢日志中只记录一个命令真正操作内存数据的耗时，而 Redis 主动删除过期 key 的逻辑，是在命令真正执行之前执行的。\n所以，此时你会看到，慢日志中没有操作耗时的命令，但我们的应用程序却感知到了延迟变大，其实时间都花费在了删除过期 key 上，这种情况我们需要尤为注意。\n\n分析和排查需要检查你的业务代码，是否存在集中过期 key 的逻辑。\n一般集中过期使用的是 expireat / pexpireat 命令，你需要在代码中搜索这个关键字。\n排查代码后，如果确实存在集中过期 key 的逻辑存在，但这种逻辑又是业务所必须的，那此时如何优化，同时又不对 Redis 有性能影响呢？\n一般有两种方案来规避这个问题：\n\n集中过期 key 增加一个随机过期时间，把集中过期的时间打散，降低 Redis 清理过期 key 的压力\n如果你使用的 Redis 是 4.0 以上版本，可以开启 lazy-free 机制，当删除过期 key 时，把释放内存的操作放到后台线程中执行，避免阻塞主线程\n\n第一种方案，在设置 key 的过期时间时，增加一个随机时间，伪代码可以这么写：\n# 在过期时间点之后的 5 分钟内随机过期掉redis.expireat(key, expire_time + random(300))    \n\n这样一来，Redis 在处理过期时，不会因为集中删除过多的 key 导致压力过大，从而避免阻塞主线程。\n第二种方案，Redis 4.0 以上版本，开启 lazy-free 机制：\n# 释放过期 key 的内存，放到后台线程执行lazyfree-lazy-expire yes\n\n另外，除了业务层面的优化和修改配置之外，你还可以通过运维手段及时发现这种情况。\n运维层面，你需要把 Redis 的各项运行状态数据监控起来，在 Redis 上执行 INFO 命令就可以拿到这个实例所有的运行状态数据。\n在这里我们需要重点关注 expired_keys 这一项，它代表整个实例到目前为止，累计删除过期 key 的数量。\n你需要把这个指标监控起来，当这个指标在很短时间内出现了突增，需要及时报警出来，然后与业务应用报慢的时间点进行对比分析，确认时间是否一致，如果一致，则可以确认确实是因为集中过期 key 导致的延迟变大。\n五、实例内存达到上限如果你的 Redis 实例设置了内存上限 maxmemory，那么也有可能导致 Redis 变慢。\n当我们把 Redis 当做纯缓存使用时，通常会给这个实例设置一个内存上限 maxmemory，然后设置一个数据淘汰策略。\n而当实例的内存达到了 maxmemory 后，你可能会发现，在此之后每次写入新数据，操作延迟变大了。\n这是为什么？\n原因在于，当 Redis 内存达到 maxmemory 后，每次写入新的数据之前，Redis 必须先从实例中踢出一部分数据，让整个实例的内存维持在 maxmemory 之下，然后才能把新数据写进来。\n这个踢出旧数据的逻辑也是需要消耗时间的，而具体耗时的长短，要取决于你配置的淘汰策略：\n\nallkeys-lru：不管 key 是否设置了过期，淘汰最近最少访问的 key\nvolatile-lru：只淘汰最近最少访问、并设置了过期时间的 key\nallkeys-random：不管 key 是否设置了过期，随机淘汰 key\nvolatile-random：只随机淘汰设置了过期时间的 key\nallkeys-ttl：不管 key 是否设置了过期，淘汰即将过期的 key\nnoeviction：不淘汰任何 key，实例内存达到 maxmeory 后，再写入新数据直接返回错误\nallkeys-lfu：不管 key 是否设置了过期，淘汰访问频率最低的 key（4.0+版本支持）\nvolatile-lfu：只淘汰访问频率最低、并设置了过期时间 key（4.0+版本支持）\n\n具体使用哪种策略，我们需要根据具体的业务场景来配置。\n一般最常使用的是 allkeys-lru / volatile-lru 淘汰策略，它们的处理逻辑是，每次从实例中随机取出一批 key（这个数量可配置），然后淘汰一个最少访问的 key，之后把剩下的 key 暂存到一个池子中，继续随机取一批 key，并与之前池子中的 key 比较，再淘汰一个最少访问的 key。以此往复，直到实例内存降到 maxmemory 之下。\n需要注意的是，Redis 的淘汰数据的逻辑与删除过期 key 的一样，也是在命令真正执行之前执行的，也就是说它也会增加我们操作 Redis 的延迟，而且，写 OPS 越高，延迟也会越明显。\n\n另外，如果此时你的 Redis 实例中还存储了 bigkey，那么在淘汰删除 bigkey 释放内存时，也会耗时比较久。\n看到了么？bigkey 的危害到处都是，这也是前面我提醒你尽量不存储 bigkey 的原因。\n优化建议\n避免存储 bigkey，降低释放内存的耗时\n淘汰策略改为随机淘汰，随机淘汰比 LRU 要快很多（视业务情况调整）\n拆分实例，把淘汰 key 的压力分摊到多个实例上\n如果使用的是 Redis 4.0 以上版本，开启 layz-free 机制，把淘汰 key 释放内存的操作放到后台线程中执行（配置 lazyfree-lazy-eviction = yes）\n\n六、fork耗时严重为了保证 Redis 数据的安全性，我们可能会开启后台定时 RDB 和 AOF rewrite 功能。但如果你发现，操作 Redis 延迟变大，都发生在 Redis 后台 RDB 和 AOF rewrite 期间，那就需要排查，在这期间有可能导致变慢的情况。\n当 Redis 开启了后台 RDB 和 AOF rewrite 后，在执行时，它们都需要主进程创建出一个子进程进行数据的持久化。\n主进程创建子进程，会调用操作系统提供的 fork 函数。\n而 fork 在执行过程中，主进程需要拷贝自己的内存页表给子进程，如果这个实例很大，那么这个拷贝的过程也会比较耗时。\n而且这个 fork 过程会消耗大量的 CPU 资源，在完成 fork 之前，整个 Redis 实例会被阻塞住，无法处理任何客户端请求。\n如果此时你的 CPU 资源本来就很紧张，那么 fork 的耗时会更长，甚至达到秒级，这会严重影响 Redis 的性能。\n那如何确认确实是因为 fork 耗时导致的 Redis 延迟变大呢？\n你可以在 Redis 上执行 INFO 命令，查看 latest_fork_usec 项，单位微秒。\n# 上一次 fork 耗时，单位微秒latest_fork_usec:59477\n\n这个时间就是主进程在 fork 子进程期间，整个实例阻塞无法处理客户端请求的时间。\n如果这个耗时很久，就要警惕起来了，这意味在这期间，你的整个 Redis 实例都处于不可用的状态。\n除了数据持久化会生成 RDB 之外，当主从节点第一次建立数据同步时，主节点也创建子进程生成 RDB，然后发给从节点进行一次全量同步，所以，这个过程也会对 Redis 产生性能影响。\n\n优化方案\n控制 Redis 实例的内存：尽量在 10G 以下，执行 fork 的耗时与实例大小有关，实例越大，耗时越久\n合理配置数据持久化策略：在 slave 节点执行 RDB 备份，推荐在低峰期执行，而对于丢失数据不敏感的业务（例如把 Redis 当做纯缓存使用），可以关闭 AOF 和 AOF rewrite\nRedis 实例不要部署在虚拟机上：fork 的耗时也与系统也有关，虚拟机比物理机耗时更久\n降低主从库全量同步的概率：适当调大 repl-backlog-size 参数，避免主从全量同步\n\n七、开启内存大页除了上面讲到的子进程 RDB 和 AOF rewrite 期间，fork 耗时导致的延时变大之外，这里还有一个方面也会导致性能问题，这就是操作系统是否开启了内存大页机制。\n什么是内存大页？我们都知道，应用程序向操作系统申请内存时，是按内存页进行申请的，而常规的内存页大小是 4KB。\nLinux 内核从 2.6.38 开始，支持了内存大页机制，该机制允许应用程序以 2MB 大小为单位，向操作系统申请内存。\n应用程序每次向操作系统申请的内存单位变大了，但这也意味着申请内存的耗时变长。\n这对 Redis 会有什么影响呢？当 Redis 在执行后台 RDB 和 AOF rewrite 时，采用 fork 子进程的方式来处理。但主进程 fork 子进程后，此时的主进程依旧是可以接收写请求的，而进来的写请求，会采用 Copy On Write（写时复制）的方式操作内存数据。\n也就是说，主进程一旦有数据需要修改，Redis 并不会直接修改现有内存中的数据，而是先将这块内存数据拷贝出来，再修改这块新内存的数据，这就是所谓的「写时复制」。\n写时复制你也可以理解成，谁需要发生写操作，谁就需要先拷贝，再修改。\n这样做的好处是，父进程有任何写操作，并不会影响子进程的数据持久化（子进程只持久化 fork 这一瞬间整个实例中的所有数据即可，不关心新的数据变更，因为子进程只需要一份内存快照，然后持久化到磁盘上）。\n但是请注意，主进程在拷贝内存数据时，这个阶段就涉及到新内存的申请，如果此时操作系统开启了内存大页，那么在此期间，客户端即便只修改 10B 的数据，Redis 在申请内存时也会以 2MB 为单位向操作系统申请，申请内存的耗时变长，进而导致每个写请求的延迟增加，影响到 Redis 性能。\n同样地，如果这个写请求操作的是一个 bigkey，那主进程在拷贝这个 bigkey 内存块时，一次申请的内存会更大，时间也会更久。可见，bigkey 在这里又一次影响到了性能。\n\n那如何解决这个问题？很简单，你只需要关闭内存大页机制就可以了。\n首先，你需要查看 Redis 机器是否开启了内存大页：\n$ cat /sys/kernel/mm/transparent_hugepage/enabled[always] madvise never\n\n如果输出选项是 always，就表示目前开启了内存大页机制，我们需要关掉它：\n$ echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled\n\n其实，操作系统提供的内存大页机制，其优势是，可以在一定程序上降低应用程序申请内存的次数。\n但是对于 Redis 这种对性能和延迟极其敏感的数据库来说，我们希望 Redis 在每次申请内存时，耗时尽量短，所以我不建议你在 Redis 机器上开启这个机制。\n八、开启AOF前面我们分析了 RDB 和 AOF rewrite 对 Redis 性能的影响，主要关注点在 fork 上。\n其实，关于数据持久化方面，还有影响 Redis 性能的因素，这次我们重点来看 AOF 数据持久化。\n如果你的 AOF 配置不合理，还是有可能会导致性能问题。\n当 Redis 开启 AOF 后，其工作原理如下：\n\nRedis 执行写命令后，把这个命令写入到 AOF 文件内存中（write 系统调用）\nRedis 根据配置的 AOF 刷盘策略，把 AOF 内存数据刷到磁盘上（fsync 系统调用）\n\n为了保证 AOF 文件数据的安全性，Redis 提供了 3 种刷盘机制：\n\nappendfsync always：主线程每次执行写操作后立即刷盘，此方案会占用比较大的磁盘 IO 资源，但数据安全性最高\nappendfsync no：主线程每次写操作只写内存就返回，内存数据什么时候刷到磁盘，交由操作系统决定，此方案对性能影响最小，但数据安全性也最低，Redis 宕机时丢失的数据取决于操作系统刷盘时机\nappendfsync everysec：主线程每次写操作只写内存就返回，然后由后台线程每隔 1 秒执行一次刷盘操作（触发fsync系统调用），此方案对性能影响相对较小，但当 Redis 宕机时会丢失 1 秒的数据\n\nAOF刷盘机制对性能的影响如果你的 AOF 配置为 appendfsync always，那么 Redis 每处理一次写操作，都会把这个命令写入到磁盘中才返回，整个过程都是在主线程执行的，这个过程必然会加重 Redis 写负担。\n原因也很简单，操作磁盘要比操作内存慢几百倍，采用这个配置会严重拖慢 Redis 的性能，因此我不建议你把 AOF 刷盘方式配置为 always。\n我们接着来看 appendfsync no 配置项。\n在这种配置下，Redis 每次写操作只写内存，什么时候把内存中的数据刷到磁盘，交给操作系统决定，此方案对 Redis 的性能影响最小，但当 Redis 宕机时，会丢失一部分数据，为了数据的安全性，一般我们也不采取这种配置。\n\n如果你的 Redis 只用作纯缓存，对于数据丢失不敏感，采用配置 appendfsync no 也是可以的。\n\n看到这里，我猜你肯定和大多数人的想法一样，选比较折中的方案 appendfsync everysec 就没问题了吧？\n这个方案优势在于，Redis 主线程写完内存后就返回，具体的刷盘操作是放到后台线程中执行的，后台线程每隔 1 秒把内存中的数据刷到磁盘中。\n这种方案既兼顾了性能，又尽可能地保证了数据安全，是不是觉得很完美？\n但是，采用这种方案你也要警惕一下，因为这种方案还是存在导致 Redis 延迟变大的情况发生，甚至会阻塞整个 Redis。\n这是为什么？我把 AOF 最耗时的刷盘操作，放到后台线程中也会影响到 Redis 主线程？\n你试想这样一种情况：当 Redis 后台线程在执行 AOF 文件刷盘时，如果此时磁盘的 IO 负载很高，那这个后台线程在执行刷盘操作（fsync系统调用）时就会被阻塞住。\n此时的主线程依旧会接收写请求，紧接着，主线程又需要把数据写到文件内存中（write 系统调用），但此时的后台子线程由于磁盘负载过高，导致 fsync 发生阻塞，迟迟不能返回，那主线程在执行 write 系统调用时，也会被阻塞住，直到后台线程 fsync 执行完成后，主线程执行 write 才能成功返回。\n看到了么？在这个过程中，主线程依旧有阻塞的风险。\n\n所以，尽管你的 AOF 配置为 appendfsync everysec，也不能掉以轻心，要警惕磁盘压力过大导致的 Redis 有性能问题。\n那什么情况下会导致磁盘 IO 负载过大？以及如何解决这个问题呢？\n排查方向\n子进程正在执行 AOF rewrite，这个过程会占用大量的磁盘 IO 资源\n有其他应用程序在执行大量的写文件操作，也会占用磁盘 IO 资源\n\n对于情况1，说白了就是，Redis 的 AOF 后台子线程刷盘操作，撞上了子进程 AOF rewrite！\n这怎么办？难道要关闭 AOF rewrite 才行？\n幸运的是，Redis 提供了一个配置项，当子进程在 AOF rewrite 期间，可以让后台子线程不执行刷盘（不触发 fsync 系统调用）操作。\n这相当于在 AOF rewrite 期间，临时把 appendfsync 设置为了 none，配置如下：\n# AOF rewrite 期间，AOF 后台子线程不进行刷盘操作# 相当于在这期间，临时把 appendfsync 设置为了 noneno-appendfsync-on-rewrite yes\n\n当然，开启这个配置项，在 AOF rewrite 期间，如果实例发生宕机，那么此时会丢失更多的数据，性能和数据安全性，你需要权衡后进行选择。\n如果占用磁盘资源的是其他应用程序，那就比较简单了，你需要定位到是哪个应用程序在大量写磁盘，然后把这个应用程序迁移到其他机器上执行就好了，避免对 Redis 产生影响。\n当然，如果你对 Redis 的性能和数据安全都有很高的要求，那么我建议从硬件层面来优化，更换为 SSD 磁盘，提高磁盘的 IO 能力，保证 AOF 期间有充足的磁盘资源可以使用。\n九、绑定CPU很多时候，我们在部署服务时，为了提高服务性能，降低应用程序在多个 CPU 核心之间的上下文切换带来的性能损耗，通常采用的方案是进程绑定 CPU 的方式提高性能。\n但在部署 Redis 时，如果你需要绑定 CPU 来提高其性能，建议仔细斟酌后再做操作。\n为什么？因为 Redis 在绑定 CPU 时，是有很多考究的，如果你不了解 Redis 的运行原理，随意绑定 CPU 不仅不会提高性能，甚至有可能会带来相反的效果。\n我们都知道，一般现代的服务器会有多个 CPU，而每个 CPU 又包含多个物理核心，每个物理核心又分为多个逻辑核心，每个物理核下的逻辑核共用 L1/L2 Cache。\n而 Redis Server 除了主线程服务客户端请求之外，还会创建子进程、子线程。\n其中子进程用于数据持久化，而子线程用于执行一些比较耗时操作，例如异步释放 fd、异步 AOF 刷盘、异步 lazy-free 等等。\n如果你把 Redis 进程只绑定了一个 CPU 逻辑核心上，那么当 Redis 在进行数据持久化时，fork 出的子进程会继承父进程的 CPU 使用偏好。\n而此时的子进程会消耗大量的 CPU 资源进行数据持久化（把实例数据全部扫描出来需要耗费CPU），这就会导致子进程会与主进程发生 CPU 争抢，进而影响到主进程服务客户端请求，访问延迟变大。\n这就是 Redis 绑定 CPU 带来的性能问题。\n那如何解决这个问题呢？如果你确实想要绑定 CPU，可以优化的方案是，不要让 Redis 进程只绑定在一个 CPU 逻辑核上，而是绑定在多个逻辑核心上，而且，绑定的多个逻辑核心最好是同一个物理核心，这样它们还可以共用 L1/L2 Cache。\n当然，即便我们把 Redis 绑定在多个逻辑核心上，也只能在一定程度上缓解主线程、子进程、后台线程在 CPU 资源上的竞争。\n因为这些子进程、子线程还是会在这多个逻辑核心上进行切换，存在性能损耗。\n如何再进一步优化？可能你已经想到了，我们是否可以让主线程、子进程、后台线程，分别绑定在固定的 CPU 核心上，不让它们来回切换，这样一来，他们各自使用的 CPU 资源互不影响。\n其实，这个方案 Redis 官方已经想到了。\nRedis 在 6.0 版本已经推出了这个功能，我们可以通过以下配置，对主线程、后台线程、后台 RDB 进程、AOF rewrite 进程，绑定固定的 CPU 逻辑核心：\n# Redis Server 和 IO 线程绑定到 CPU核心 0,2,4,6server_cpulist 0-7:2 # 后台子线程绑定到 CPU核心 1,3bio_cpulist 1,3 # 后台 AOF rewrite 进程绑定到 CPU 核心 8,9,10,11aof_rewrite_cpulist 8-11 # 后台 RDB 进程绑定到 CPU 核心 1,10,11# bgsave_cpulist 1,10-1\n\n如果你使用的正好是 Redis 6.0 版本，就可以通过以上配置，来进一步提高 Redis 性能。\n这里我需要提醒你的是，一般来说，Redis 的性能已经足够优秀，除非你对 Redis 的性能有更加严苛的要求，否则不建议你绑定 CPU。\n从上面的分析你也能看出，绑定 CPU 需要你对计算机体系结构有非常清晰的了解，否则谨慎操作。\n十、使用Swap如果你发现 Redis 突然变得非常慢，每次的操作耗时都达到了几百毫秒甚至秒级，那此时你就需要检查 Redis 是否使用到了 Swap，在这种情况下 Redis 基本上已经无法提供高性能的服务了。\n什么是 Swap？为什么使用 Swap 会导致 Redis 的性能下降？\n如果你对操作系统有些了解，就会知道操作系统为了缓解内存不足对应用程序的影响，允许把一部分内存中的数据换到磁盘上，以达到应用程序对内存使用的缓冲，这些内存数据被换到磁盘上的区域，就是 Swap。\n问题就在于，当内存中的数据被换到磁盘上后，Redis 再访问这些数据时，就需要从磁盘上读取，访问磁盘的速度要比访问内存慢几百倍！\n尤其是针对 Redis 这种对性能要求极高、性能极其敏感的数据库来说，这个操作延时是无法接受的。\n此时，你需要检查 Redis 机器的内存使用情况，确认是否存在使用了 Swap。\n你可以通过以下方式来查看 Redis 进程是否使用到了 Swap：\n# 先找到 Redis 的进程 ID$ ps -aux | grep redis-server # 查看 Redis Swap 使用情况$ cat /proc/$pid/smaps | egrep &#x27;^(Swap|Size)&#x27;\n\n输出结果如下：\nSize:               1256 kBSwap:                  0 kBSize:                  4 kBSwap:                  0 kBSize:                132 kBSwap:                  0 kBSize:              63488 kBSwap:                  0 kBSize:                132 kBSwap:                  0 kBSize:              65404 kBSwap:                  0 kBSize:            1921024 kBSwap:                  0 kB...\n\n这个结果会列出 Redis 进程的内存使用情况。\n每一行 Size 表示 Redis 所用的一块内存大小，Size 下面的 Swap 就表示这块 Size 大小的内存，有多少数据已经被换到磁盘上了，如果这两个值相等，说明这块内存的数据都已经完全被换到磁盘上了。\n如果只是少量数据被换到磁盘上，例如每一块 Swap 占对应 Size 的比例很小，那影响并不是很大。如果是几百兆甚至上 GB 的内存被换到了磁盘上，那么你就需要警惕了，这种情况 Redis 的性能肯定会急剧下降。\n解决方案\n增加机器的内存，让 Redis 有足够的内存可以使用\n整理内存空间，释放出足够的内存供 Redis 使用，然后释放 Redis 的 Swap，让 Redis 重新使用内存\n\n可见，当 Redis 使用到 Swap 后，此时的 Redis 性能基本已达不到高性能的要求（你可以理解为武功被废），所以你也需要提前预防这种情况。\n预防的办法就是，你需要对 Redis 机器的内存和 Swap 使用情况进行监控，在内存不足或使用到 Swap 时报警出来，及时处理。\n十一、碎片整理Redis 的数据都存储在内存中，当我们的应用程序频繁修改 Redis 中的数据时，就有可能会导致 Redis 产生内存碎片。\n内存碎片会降低 Redis 的内存使用率，我们可以通过执行 INFO 命令，得到这个实例的内存碎片率：\n# Memoryused_memory:5709194824used_memory_human:5.32Gused_memory_rss:8264855552used_memory_rss_human:7.70G...mem_fragmentation_ratio:1.45\n\n这个内存碎片率是怎么计算的？mem_fragmentation_ratio = used_memory_rss / used_memory\n其中 used_memory 表示 Redis 存储数据的内存大小，而 used_memory_rss 表示操作系统实际分配给 Redis 进程的大小。\n如果 mem_fragmentation_ratio &gt; 1.5，说明内存碎片率已经超过了 50%，这时我们就需要采取一些措施来降低内存碎片了。\n解决的方案\n如果你使用的是 Redis 4.0 以下版本，只能通过重启实例来解决\n如果你使用的是 Redis 4.0 版本，它正好提供了自动碎片整理的功能，可以通过配置开启碎片自动整理\n\n但是，开启内存碎片整理，它也有可能会导致 Redis 性能下降。\n原因在于，Redis 的碎片整理工作是也在主线程中执行的，当其进行碎片整理时，必然会消耗 CPU 资源，产生更多的耗时，从而影响到客户端的请求。\n所以，当你需要开启这个功能时，最好提前测试评估它对 Redis 的影响。\nRedis 碎片整理的参数配置如下：\n# 开启自动内存碎片整理（总开关）activedefrag yes # 内存使用 100MB 以下，不进行碎片整理active-defrag-ignore-bytes 100mb # 内存碎片率超过 10%，开始碎片整理active-defrag-threshold-lower 10# 内存碎片率超过 100%，尽最大努力碎片整理active-defrag-threshold-upper 100 # 内存碎片整理占用 CPU 资源最小百分比active-defrag-cycle-min 1# 内存碎片整理占用 CPU 资源最大百分比active-defrag-cycle-max 25 # 碎片整理期间，对于 List/Set/Hash/ZSet 类型元素一次 Scan 的数量active-defrag-max-scan-fields 1000\n\n需要结合 Redis 机器的负载情况，以及应用程序可接受的延迟范围进行评估，合理调整碎片整理的参数，尽可能降低碎片整理期间对 Redis 的影响。\n十二、 网络带宽过载如果以上产生性能问题的场景，你都规避掉了，而且 Redis 也稳定运行了很长时间，但在某个时间点之后开始，操作 Redis 突然开始变慢了，而且一直持续下去，这种情况又是什么原因导致？\n此时你需要排查一下 Redis 机器的网络带宽是否过载，是否存在某个实例把整个机器的网路带宽占满的情况。\n网络带宽过载的情况下，服务器在 TCP 层和网络层就会出现数据包发送延迟、丢包等情况。\nRedis 的高性能，除了操作内存之外，就在于网络 IO 了，如果网络 IO 存在瓶颈，那么也会严重影响 Redis 的性能。\n如果确实出现这种情况，你需要及时确认占满网络带宽 Redis 实例，如果属于正常的业务访问，那就需要及时扩容或迁移实例了，避免因为这个实例流量过大，影响这个机器的其他实例。\n运维层面，你需要对 Redis 机器的各项指标增加监控，包括网络流量，在网络流量达到一定阈值时提前报警，及时确认和扩容。\n十三、其他原因除了以上这些，还有一些比较小的点，也需要注意一下：\n\n频繁短连接\n\n你的业务应用，应该使用长连接操作 Redis，避免频繁的短连接。\n频繁的短连接会导致 Redis 大量时间耗费在连接的建立和释放上，TCP 的三次握手和四次挥手同样也会增加访问延迟。\n\n运维监控\n\n前面我也提到了，要想提前预知 Redis 变慢的情况发生，必不可少的就是做好完善的监控。\n监控其实就是对采集 Redis 的各项运行时指标，通常的做法是监控程序定时采集 Redis 的 INFO 信息，然后根据 INFO 信息中的状态数据做数据展示和报警。\n这里我需要提醒你的是，在写一些监控脚本，或使用开源的监控组件时，也不能掉以轻心。\n在写监控脚本访问 Redis 时，尽量采用长连接的方式采集状态信息，避免频繁短连接。同时，你还要注意控制访问 Redis 的频率，避免影响到业务请求。\n在使用一些开源的监控组件时，最好了解一下这些组件的实现原理，以及正确配置这些组件，防止出现监控组件发生 Bug，导致短时大量操作 Redis，影响 Redis 性能的情况发生。\n我们当时就发生过，DBA 在使用一些开源组件时，因为配置和使用问题，导致监控程序频繁地与 Redis 建立和断开连接，导致 Redis 响应变慢。\n\n其它程序争抢资源\n\n最后需要提醒你的是，你的 Redis 机器最好专项专用，只用来部署 Redis 实例，不要部署其他应用程序，尽量给 Redis 提供一个相对「安静」的环境，避免其它程序占用 CPU、内存、磁盘资源，导致分配给 Redis 的资源不足而受到影响。\n","categories":["运维","Redis"]},{"title":"Redis 持久化","url":"/posts/15405/","content":"Redis支持两种方式的持久化：RDB快照和AOF。\nRDB持久化RDB快照用官方的话来说：RDB持久化方案是按照指定时间间隔对你的数据集生成的时间点快照（point-to-time snapshot）。它以紧缩的二进制文件保存Redis数据库某一时刻所有数据对象的内存快照，可用于Redis的数据备份、转移与恢复。到目前为止，仍是官方的默认支持方案。\nRDB工作原理既然说RDB是Redis中数据集的时间点快照，那我们先简单了解一下Redis内的数据对象在内存中是如何存储与组织的。\n默认情况下，Redis中有16个数据库，编号从0-15，每个Redis数据库使用一个redisDb对象来表示，redisDb使用hashtable存储K-V对象。为方便理解，我以其中一个db为例绘制Redis内部数据的存储结构示意图。时间点快照也就是某一时刻Redis内每个DB中每个数据对象的状态，先假设在这一时刻所有的数据对象不再改变，我们就可以按照上图中的数据结构关系，把这些数据对象依次读取出来并写入到文件中，以此实现Redis的持久化。然后，当Redis重启时按照规则读取这个文件中的内容，再写入到Redis内存即可恢复至持久化时的状态。\n当然，这个前提时我们上面的假设成立，否则面对一个时刻变化的数据集，我们无从下手。我们知道Redis中客户端命令处理是单线程模型，如果把持久化作为一个命令处理，那数据集肯定时处于静止状态。另外，操作系统提供的fork()函数创建的子进程可获得与父进程一致的内存数据，相当于获取了内存数据副本；fork完成后，父进程该干嘛干嘛，持久化状态的工作交给子进程就行了。\n很显然，第一种情况不可取，持久化备份会导致短时间内Redis服务不可用，这对于高HA的系统来讲是无法容忍的。所以，第二种方式是RDB持久化的主要实践方式。由于fork子进程后，父进程数据一直在变化，子进程并不与父进程同步，RDB持久化必然无法保证实时性；RDB持久化完成后发生断电或宕机，会导致部分数据丢失；备份频率决定了丢失数据量的大小，提高备份频率，意味着fork过程消耗较多的CPU资源，也会导致较大的磁盘I/O。\n持久化流程在Redis内完成RDB持久化的方法有rdbSave和rdbSaveBackground两个函数方法（源码文件rdb.c中），先简单说下两者差别：\n\nrdbSave：是同步执行的，方法调用后就会立刻启动持久化流程。由于Redis是单线程模型，持久化过程中会阻塞，Redis无法对外提供服务；\nrdbSaveBackground：是后台（异步）执行的，该方法会fork出子进程，真正的持久化过程是在子进程中执行的（调用rdbSave），主进程会继续提供服务；\n\nRDB持久化的触发必然离不开以上两个方法，触发的方式分为手动和自动。手动触发容易理解，是指我们通过Redis客户端人为的对Redis服务端发起持久化备份指令，然后Redis服务端开始执行持久化流程，这里的指令有save和bgsave。自动触发是Redis根据自身运行要求，在满足预设条件时自动触发的持久化流程，自动触发的场景有如下几个（摘自这篇文章）：\n\nserverCron中save m n配置规则自动触发；\n从节点全量复制时，主节点发送rdb文件给从节点完成复制操作，主节点会出发bgsave；\n执行debug reload命令重新加载redis时；\n默认情况下（未开启AOF）执行shutdown命令时，自动执行bgsave；\n\n结合源码及参考文章，我整理了RDB持久化流程来帮助大家有个整体的了解，然后再从一些细节进行说明。从上图可以知道：\n\n自动触发的RDB持久化是通过rdbSaveBackground以子进程方式执行的持久化策略；\n手动触发是以客户端命令方式触发的，包含save和bgsave两个命令，其中save命令是在Redis的命令处理线程以阻塞的方式调用rdbSave方法完成的。\n\n自动触发流程是一个完整的链路，涵盖了rdbSaveBackground、rdbSave等，接下来我以serverCron为例分析一下整个流程。\nsave规则及检查serverCron是Redis内的一个周期性函数，每隔100毫秒执行一次，它的其中一项工作就是：根据配置文件中save规则来判断当前需要进行自动持久化流程，如果满足条件则尝试开始持久化。了解一下这部分的实现。\n在redisServer中有几个与RDB持久化有关的字段，我从代码中摘出来，中英文对照着看下：\nstruct redisServer &#123;    /* 省略其他字段 */     /* RDB persistence */    long long dirty;                /* Changes to DB from the last save                                     * 上次持久化后修改key的次数 */    struct saveparam *saveparams;   /* Save points array for RDB，                                     * 对应配置文件多个save参数 */    int saveparamslen;              /* Number of saving points，                                     * save参数的数量 */    time_t lastsave;                /* Unix time of last successful save                                      * 上次持久化时间*/    /* 省略其他字段 */&#125;/* 对应redis.conf中的save参数 */struct saveparam &#123;    time_t seconds;                    /* 统计时间范围 */       int changes;                    /* 数据修改次数 */&#125;;\n\nsaveparams对应redis.conf下的save规则，save参数是Redis触发自动备份的触发策略，seconds为统计时间（单位：秒）， changes为在统计时间内发生写入的次数。save m n的意思是：m秒内有n条写入就触发一次快照，即备份一次。save参数可以配置多组，满足在不同条件的备份要求。如果需要关闭RDB的自动备份策略，可以使用save &quot;&quot;。以下为几种配置的说明：\n# 表示900秒（15分钟）内至少有1个key的值发生变化，则执行save 900 1# 表示300秒（5分钟）内至少有1个key的值发生变化，则执行save 300 10# 表示60秒（1分钟）内至少有10000个key的值发生变化，则执行save 60 10000# 该配置将会关闭RDB方式的持久化save &quot;&quot;\n\nserverCron对RDB save规则的检测代码如下所示：\nint serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) &#123;    /* 省略其他逻辑 */        /* 如果用户请求进行AOF文件重写时，Redis正在执行RDB持久化，Redis会安排在RDB持久化完成后执行AOF文件重写，     * 如果aof_rewrite_scheduled为true，说明需要执行用户的请求 */    /* Check if a background saving or AOF rewrite in progress terminated. */    if (hasActiveChildProcess() || ldbPendingChildren())    &#123;        run_with_period(1000) receiveChildInfo();        checkChildrenDone();    &#125; else &#123;        /* 后台无 saving/rewrite 子进程才会进行，逐个检查每个save规则*/        for (j = 0; j &lt; server.saveparamslen; j++) &#123;            struct saveparam *sp = server.saveparams+j;                        /* 检查规则有几个：满足修改次数，满足统计周期，达到重试时间间隔或者上次持久化完成*/            if (server.dirty &gt;= sp-&gt;changes                 &amp;&amp; server.unixtime-server.lastsave &gt; sp-&gt;seconds                 &amp;&amp;(server.unixtime-server.lastbgsave_try &gt; CONFIG_BGSAVE_RETRY_DELAY || server.lastbgsave_status == C_OK))            &#123;                serverLog(LL_NOTICE,&quot;%d changes in %d seconds. Saving...&quot;, sp-&gt;changes, (int)sp-&gt;seconds);                rdbSaveInfo rsi, *rsiptr;                rsiptr = rdbPopulateSaveInfo(&amp;rsi);                /* 执行bgsave过程 */                rdbSaveBackground(server.rdb_filename,rsiptr);                break;            &#125;        &#125;        /* 省略：Trigger an AOF rewrite if needed. */    &#125;    /* 省略其他逻辑 */&#125;\n\n如果没有后台的RDB持久化或AOF重写进程，serverCron会根据以上配置及状态判断是否需要执行持久化操作，判断依据就是看lastsave、dirty是否满足saveparams数组中的其中一个条件。如果有一个条件匹配，则调用rdbSaveBackground方法，执行异步持久化流程。\nrdbSaveBackgroundrdbSaveBackground是RDB持久化的辅助性方法，主要工作是fork子进程，然后根据调用方（父进程或者子进程）不同，有两种不同的执行逻辑。\n\n如果调用方是父进程，则fork出子进程，保存子进程信息后直接返回。\n如果调用方是子进程则调用rdbSave执行RDB持久化逻辑，持久化完成后退出子进程。\n\nint rdbSaveBackground(char *filename, rdbSaveInfo *rsi) &#123;    pid_t childpid;    if (hasActiveChildProcess()) return C_ERR;    server.dirty_before_bgsave = server.dirty;    server.lastbgsave_try = time(NULL);    // fork子进程    if ((childpid = redisFork(CHILD_TYPE_RDB)) == 0) &#123;        int retval;        /* Child 子进程：修改进程标题 */        redisSetProcTitle(&quot;redis-rdb-bgsave&quot;);        redisSetCpuAffinity(server.bgsave_cpulist);        // 执行rdb持久化        retval = rdbSave(filename,rsi);        if (retval == C_OK) &#123;            sendChildCOWInfo(CHILD_TYPE_RDB, 1, &quot;RDB&quot;);        &#125;        // 持久化完成后，退出子进程        exitFromChild((retval == C_OK) ? 0 : 1);    &#125; else &#123;        /* Parent 父进程：记录fork子进程的时间等信息*/        if (childpid == -1) &#123;            server.lastbgsave_status = C_ERR;            serverLog(LL_WARNING,&quot;Can&#x27;t save in background: fork: %s&quot;,                strerror(errno));            return C_ERR;        &#125;        serverLog(LL_NOTICE,&quot;Background saving started by pid %ld&quot;,(long) childpid);        // 记录子进程开始的时间、类型等。        server.rdb_save_time_start = time(NULL);        server.rdb_child_type = RDB_CHILD_TYPE_DISK;        return C_OK;    &#125;    return C_OK; /* unreached */&#125;\n\nrdbSave是真正执行持久化的方法，它在执行时存在大量的I/O、计算操作，耗时、CPU占用较大，在Redis的单线程模型中持久化过程会持续占用线程资源，进而导致Redis无法提供其他服务。为了解决这一问题Redis在rdbSaveBackground中fork出子进程，由子进程完成持久化工作，避免了占用父进程过多的资源。\n需要注意的是，如果父进程内存占用过大，fork过程会比较耗时，在这个过程中父进程无法对外提供服务；另外，需要综合考虑计算机内存使用量，fork子进程后会占用双倍的内存资源，需要确保内存够用。通过info stats命令查看latest_fork_usec选项，可以获取最近一个fork以操作的耗时。\nrdbSaveRedis的rdbSave函数是真正进行RDB持久化的函数，流程、细节贼多，整体流程可以总结为：创建并打开临时文件、Redis内存数据写入临时文件、临时文件写入磁盘、临时文件重命名为正式RDB文件、更新持久化状态信息（dirty、lastsave）。其中“Redis内存数据写入临时文件”最为核心和复杂，写入过程直接体现了RDB文件的文件格式，本着一图胜千言的理念，我按照源码流程绘制了下图。补充说明一下，上图右下角“遍历当前数据库的键值对并写入”这个环节会根据不同类型的Redis数据类型及底层数据结构采用不同的格式写入到RDB文件中，不再展开了。我觉得大家对整个过程有个直观的理解就好，这对于我们理解Redis内部的运作机制大有裨益。\nAOF持久化上一节我们知道RDB是一种时间点（point-to-time）快照，适合数据备份及灾难恢复，由于工作原理的“先天性缺陷”无法保证实时性持久化，这对于缓存丢失零容忍的系统来说是个硬伤，于是就有了AOF。\nAOF工作原理AOF是Append Only File的缩写，它是Redis的完全持久化策略，从1.1版本开始支持；这里的file存储的是引起Redis数据修改的命令集合（比如：set/hset/del等），这些集合按照Redis Server的处理顺序追加到文件中。当重启Redis时，Redis就可以从头读取AOF中的指令并重放，进而恢复关闭前的数据状态。\nAOF持久化默认是关闭的，修改redis.conf以下信息并重启，即可开启AOF持久化功能。\n# no-关闭，yes-开启，默认noappendonly yesappendfilename appendonly.aof\n\nAOF本质是为了持久化，持久化对象是Redis内每一个key的状态，持久化的目的是为了在Reids发生故障重启后能够恢复至重启前或故障前的状态。相比于RDB，AOF采取的策略是按照执行顺序持久化每一条能够引起Redis中对象状态变更的命令，命令是有序的、有选择的。把aof文件转移至任何一台Redis Server，从头到尾按序重放这些命令即可恢复如初。举个例子：\n首先执行指令set number 0，然后随机调用incr number、get number 各5次，最后再执行一次get number ，我们得到的结果肯定是5。\n因为在这个过程中，能够引起number状态变更的只有set/incr类型的指令，并且它们执行的先后顺序是已知的，无论执行多少次get都不会影响number的状态。所以，保留所有set/incr命令并持久化至aof文件即可。按照aof的设计原理，aof文件中的内容应该是这样的（这里是假设，实际为RESP协议）：\nset number 0incr numberincr numberincr numberincr numberincr number\n\n最本质的原理用“命令重放”四个字就可以概括。但是，考虑实际生产环境的复杂性及操作系统等方面的限制，Redis所要考虑的工作要比这个例子复杂的多：\n\nRedis Server启动后，aof文件一直在追加命令，文件会越来越大。文件越大，Redis重启后恢复耗时越久；文件太大，转移工作就越难；不加管理，可能撑爆硬盘。很显然，需要在合适的时机对文件进行精简。例子中的5条incr指令很明显的可以替换为为一条set命令，存在很大的压缩空间。\n众所周知，文件I/O是操作系统性能的短板，为了提高效率，文件系统设计了一套复杂的缓存机制，Redis操作命令的追加操作只是把数据写入了缓冲区（aof_buf），从缓冲区到写入物理文件在性能与安全之间权衡会有不同的选择。\n文件压缩即意味着重写，重写时即可依据已有的aof文件做命令整合，也可以先根据当前Redis内数据的状态做快照，再把存储快照过程中的新增的命令做追加。\naof备份后的文件是为了恢复数据，结合aof文件的格式、完整性等因素，Redis也要设计一套完整的方案做支持。\n\n持久化流程从流程上来看，AOF的工作原理可以概括为几个步骤：命令追加（append）、文件写入与同步（fsync）、文件重写（rewrite）、重启加载（load），接下来依次了解每个步骤的细节及背后的设计哲学。\n命令追加当 AOF 持久化功能处于打开状态时，Redis 在执行完一个写命令之后，会以协议格式(也就是RESP，即 Redis 客户端和服务器交互的通信协议 )把被执行的写命令追加到 Redis 服务端维护的 AOF 缓冲区末尾。对AOF文件只有单线程的追加操作，没有seek等复杂的操作，即使断电或宕机也不存在文件损坏风险。另外，使用文本协议好处多多：\n\n文本协议有很好的兼容性；\n文本协议就是客户端的请求命令，不需要二次处理，节省了存储及加载时的处理开销；\n文本协议具有可读性，方便查看、修改等处理。\n\nAOF缓冲区类型为Redis自主设计的数据结构sds，Redis会根据命令的类型采用不同的方法（catAppendOnlyGenericCommand、catAppendOnlyExpireAtCommand等）对命令内容进行处理，最后写入缓冲区。\n需要注意的是：如果命令追加时正在进行AOF重写，这些命令还会追加到重写缓冲区（aof_rewrite_buffer）。\n文件写入与同步AOF文件的写入与同步离不开操作系统的支持，开始介绍之前，我们需要补充一下Linux I/O缓冲区相关知识。硬盘I/O性能较差，文件读写速度远远比不上CPU的处理速度，如果每次文件写入都等待数据写入硬盘，会整体拉低操作系统的性能。为了解决这个问题，操作系统提供了延迟写（delayed write）机制来提高硬盘的I/O性能。\n\n传统的UNIX实现在内核中设有缓冲区高速缓存或页面高速缓存，大多数磁盘I/O都通过缓冲进行。 当将数据写入文件时，内核通常先将该数据复制到其中一个缓冲区中，如果该缓冲区尚未写满，则并不将其排入输出队列，而是等待其写满或者当内核需要重用该缓冲区以便存放其他磁盘块数据时， 再将该缓冲排入到输出队列，然后待其到达队首时，才进行实际的I/O操作。这种输出方式就被称为延迟写。\n\n延迟写减少了磁盘读写次数，但是却降低了文件内容的更新速度，使得欲写到文件中的数据在一段时间内并没有写到磁盘上。当系统发生故障时，这种延迟可能造成文件更新内容的丢失。为了保证磁盘上实际文件系统与缓冲区高速缓存中内容的一致性，UNIX系统提供了sync、fsync和fdatasync三个函数为强制写入硬盘提供支持。\nRedis每次事件轮训结束前（beforeSleep）都会调用函数flushAppendOnlyFile，flushAppendOnlyFile会把AOF缓冲区（aof_buf）中的数据写入内核缓冲区，并且根据appendfsync配置来决定采用何种策略把内核缓冲区中的数据写入磁盘，即调用fsync()。该配置有三个可选项always、no、everysec，具体说明如下：\n\nalways：每处理一个命令都将 aof_buf 缓冲区中的所有内容写入并同步到AOF 文件，即每个命令都调用fsync()，是安全性最高、性能最差的一种策略。\nno：将 aof_buf 缓冲区中的所有内容写入到 AOF 文件， 但并不对 AOF 文件进行同步， 何时同步由操作系统来决定。即不执行刷盘，让操作系统自己执行刷盘。性能最好，安全性最差。\neverysec：将 aof_buf 缓冲区中的所有内容写入到 AOF 文件，如果上次同步 AOF 文件的时间距离现在超过一秒钟， 那么再次对 AOF 文件进行同步， 并且这个同步操作是异步的，由一个后台线程专门负责执行，即每秒刷盘1次。这是官方建议的同步策略，也是默认配置，做到兼顾性能和数据安全性，理论上只有在系统突然宕机的情况下丢失1秒的数据。\n\n注意：上面介绍的策略受配置项no-appendfsync-on-rewrite的影响，它的作用是告知Redis：AOF文件重写期间是否禁止调用fsync()，默认是no。\n如果appendfsync设置为always或everysec，后台正在进行的BGSAVE或者BGREWRITEAOF消耗过多的磁盘I/O，在某些Linux系统配置下，Redis对fsync()的调用可能阻塞很长时间。然而这个问题还没有修复，因为即使是在不同的线程中执行fsync()，同步写入操作也会被阻塞。\n为了缓解此问题，可以使用该选项，以防止在进行BGSAVE或BGREWRITEAOF时在主进程中调用fsync(）。\n\n设置为yes意味着，如果子进程正在进行BGSAVE或BGREWRITEAOF，AOF的持久化能力就与appendfsync设置为no有着相同的效果。最糟糕的情况下，这可能会导致30秒的缓存数据丢失。\n如果你的系统有上面描述的延迟问题，就把这个选项设置为yes，否则保持为no。\n\n文件重写如前面提到的，Redis长时间运行，命令不断写入AOF，文件会越来越大，不加控制可能影响宿主机的安全。\n为了解决AOF文件体积问题，Redis引入了AOF文件重写功能，它会根据Redis内数据对象的最新状态生成新的AOF文件，新旧文件对应的数据状态一致，但是新文件会具有较小的体积。重写既减少了AOF文件对磁盘空间的占用，又可以提高Redis重启时数据恢复的速度。还是下面这个例子，旧文件中的6条命令等同于新文件中的1条命令，压缩效果显而易见。我们说，AOF文件太大时会触发AOF文件重写，那到底是多大呢？有哪些情况会触发重写操作呢？\n与RDB方式一样，AOF文件重写既可以手动触发，也会自动触发。手动触发直接调用bgrewriteaof命令，如果当时无子进程执行会立刻执行，否则安排在子进程结束后执行。自动触发由Redis的周期性方法serverCron检查在满足一定条件时触发。先了解两个配置项：\n\nauto-aof-rewrite-percentage：代表当前AOF文件大小（aof_current_size）和上一次重写后AOF文件大小（aof_base_size）相比，增长的比例。\nauto-aof-rewrite-min-size：表示运行BGREWRITEAOF时AOF文件占用空间最小值，默认为64MB；\n\nRedis启动时把aof_base_size初始化为当时aof文件的大小，Redis运行过程中，当AOF文件重写操作完成时，会对其进行更新；aof_current_size为serverCron执行时AOF文件的实时大小。当满足以下两个条件时，AOF文件重写就会触发：\n增长比例：(aof_current_size - aof_base_size) / aof_base_size &gt; auto-aof-rewrite-percentage文件大小：aof_current_size &gt; auto-aof-rewrite-min-size\n\n手动触发与自动触发的代码如下，同样在周期性方法serverCron中：\nint serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) &#123;    /* 省略其他逻辑 */        /* 如果用户请求进行AOF文件重写时，Redis正在执行RDB持久化，Redis会安排在RDB持久化完成后执行AOF文件重写，     * 如果aof_rewrite_scheduled为true，说明需要执行用户的请求 */    if (!hasActiveChildProcess() &amp;&amp;        server.aof_rewrite_scheduled)    &#123;        rewriteAppendOnlyFileBackground();    &#125;    /* Check if a background saving or AOF rewrite in progress terminated. */    if (hasActiveChildProcess() || ldbPendingChildren())    &#123;        run_with_period(1000) receiveChildInfo();        checkChildrenDone();    &#125; else &#123;        /* 省略rdb持久化条件检查 */        /* AOF重写条件检查：aof开启、无子进程运行、增长百分比已设置、当前文件大小超过阈值 */        if (server.aof_state == AOF_ON &amp;&amp;            !hasActiveChildProcess() &amp;&amp;            server.aof_rewrite_perc &amp;&amp;            server.aof_current_size &gt; server.aof_rewrite_min_size)        &#123;            long long base = server.aof_rewrite_base_size ?                server.aof_rewrite_base_size : 1;            /* 计算增长百分比 */            long long growth = (server.aof_current_size*100/base) - 100;            if (growth &gt;= server.aof_rewrite_perc) &#123;                serverLog(LL_NOTICE,&quot;Starting automatic rewriting of AOF on %lld%% growth&quot;,growth);                rewriteAppendOnlyFileBackground();            &#125;        &#125;    &#125;    /**/&#125;\n\nAOF文件重写的流程是什么？听说Redis支持混合持久化，对AOF文件重写有什么影响？\n从4.0版本开始，Redis在AOF模式中引入了混合持久化方案，即：纯AOF方式、RDB+AOF方式，这一策略由配置参数aof-use-rdb-preamble（使用RDB作为AOF文件的前半段）控制，默认关闭(no)，设置为yes可开启。所以，在AOF重写过程中文件的写入会有两种不同的方式。当aof-use-rdb-preamble的值是：\n\nno：按照AOF格式写入命令，与4.0前版本无差别；\nyes：先按照RDB格式写入数据状态，然后把重写期间AOF缓冲区的内容以AOF格式写入，文件前半部分为RDB格式，后半部分为AOF格式。\n\n结合源码（6.0版本，源码太多这里不贴出，可参考aof.c）及参考资料，绘制AOF重写（BGREWRITEAOF）流程图：结合上图，总结一下AOF文件重写的流程：\n\nrewriteAppendOnlyFileBackground开始执行，检查是否有正在进行的AOF重写或RDB持久化子进程：如果有，则退出该流程；如果没有，则继续创建接下来父子进程间数据传输的通信管道。执行fork()操作，成功后父子进程分别执行不同的流程。\n父进程：\n记录子进程信息（pid）、时间戳等；\n继续响应其他客户端请求；\n收集AOF重写期间的命令，追加至aof_rewrite_buffer；\n等待并向子进程同步aof_rewrite_buffer的内容；\n\n\n子进程：\n修改当前进程名称，创建重写所需的临时文件，调用rewriteAppendOnlyFile函数；\n根据aof-use-rdb-preamble配置，以RDB或AOF方式写入前半部分，并同步至硬盘；\n从父进程接收增量AOF命令，以AOF方式写入后半部分，并同步至硬盘；\n重命名AOF文件，子进程退出。\n\n\n\n数据加载Redis启动后通过loadDataFromDisk函数执行数据加载工作。这里需要注意，虽然持久化方式可以选择AOF、RDB或者两者兼用，但是数据加载时必须做出选择，两种方式各自加载一遍就乱套了。\n理论上，AOF持久化比RDB具有更好的实时性，当开启了AOF持久化方式，Redis在数据加载时优先考虑AOF方式。而且，Redis 4.0版本后AOF支持了混合持久化，加载AOF文件需要考虑版本兼容性。Redis数据加载流程如下图所示：在AOF方式下，开启混合持久化机制生成的文件是“RDB头+AOF尾”，未开启时生成的文件全部为AOF格式。考虑两种文件格式的兼容性，如果Redis发现AOF文件为RDB头，会使用RDB数据加载的方法读取并恢复前半部分；然后再使用AOF方式读取并恢复后半部分。由于AOF格式存储的数据为RESP协议命令，Redis采用伪客户端执行命令的方式来恢复数据。\n如果在AOF命令追加过程中发生宕机，由于延迟写的技术特点，AOF的RESP命令可能不完整（被截断）。遇到这种情况时，Redis会按照配置项aof-load-truncated执行不同的处理策略。这个配置是告诉Redis启动时读取aof文件，如果发现文件被截断（不完整）时该如何处理：\n\nyes：则尽可能多的加载数据，并以日志的方式通知用户；\nno：则以系统错误的方式崩溃，并禁止启动，需要用户修复文件后再重启。\n\n总结Redis提供了两种持久化的选择：RDB支持以特定的实践间隔为数据集生成时间点快照；AOF把Redis Server收到的每条写指令持久化到日志中，待Redis重启时通过重放命令恢复数据。日志格式为RESP协议，对日志文件只做append操作，无损坏风险。并且当AOF文件过大时可以自动重写压缩文件。\n当然，如果你不需要对数据进行持久化，也可以禁用Redis的持久化功能，但是大多数情况并非如此。实际上，我们时有可能同时使用RDB和AOF两种方式的，最重要的就是我们要理解两者的区别，以便合理使用。\nRDB vs AOFRDB优点\nRDB是一个紧凑压缩的二进制文件，代表Redis在某一个时间点上的数据快照，非常适合用于备份、全量复制等场景。\nRDB对灾难恢复、数据迁移非常友好，RDB文件可以转移至任何需要的地方并重新加载。\nRDB是Redis数据的内存快照，数据恢复速度较快，相比于AOF的命令重放有着更高的性能。\n\nRDB缺点\nRDB方式无法做到实时或秒级持久化。因为持久化过程是通过fork子进程后由子进程完成的，子进程的内存只是在fork操作那一时刻父进程的数据快照，而fork操作后父进程持续对外服务，内部数据时刻变更，子进程的数据不再更新，两者始终存在差异，所以无法做到实时性。\nRDB持久化过程中的fork操作，会导致内存占用加倍，而且父进程数据越多，fork过程越长。\nRedis请求高并发可能会频繁命中save规则，导致fork操作及持久化备份的频率不可控；\nRDB文件有文件格式要求，不同版本的Redis会对文件格式进行调整，存在老版本无法兼容新版本的问题。\n\nAOF优点\nAOF持久化有更好的实时性，我们可以选择三种不同的方式（appendfsync）：no、every second、always，every second作为默认的策略具有最好的性能，极端情况下可能会丢失一秒的数据。\nAOF文件只有append操作，无复杂的seek等文件操作，没有损坏风险。即使最后写入数据被截断，也很容易使用redis-check-aof工具修复；\n当AOF文件变大时，Redis可在后台自动重写。重写过程中旧文件会持续写入，重写完成后新文件将变得更小，并且重写过程中的增量命令也会append到新文件。\nAOF文件以已于理解与解析的方式包含了对Redis中数据的所有操作命令。即使不小心错误的清除了所有数据，只要没有对AOF文件重写，我们就可以通过移除最后一条命令找回所有数据。\nAOF已经支持混合持久化，文件大小可以有效控制，并提高了数据加载时的效率。\n\nAOF缺点\n对于相同的数据集合，AOF文件通常会比RDB文件大；\n在特定的fsync策略下，AOF会比RDB略慢。一般来讲，fsync_every_second的性能仍然很高，fsync_no的性能与RDB相当。但是在巨大的写压力下，RDB更能提供最大的低延时保障。\n在AOF上，Redis曾经遇到一些几乎不可能在RDB上遇到的罕见bug。一些特殊的指令（如BRPOPLPUSH）导致重新加载的数据与持久化之前不一致，Redis官方曾经在相同的条件下进行测试，但是无法复现问题。\n\n使用建议对RDB和AOF两种持久化方式的工作原理、执行流程及优缺点了解后，我们来思考下，实际场景中应该怎么权衡利弊，合理的使用两种持久化方式。如果仅仅是使用Redis作为缓存工具，所有数据可以根据持久化数据库进行重建，则可关闭持久化功能，做好预热、缓存穿透、击穿、雪崩之类的防护工作即可。\n一般情况下，Redis会承担更多的工作，如分布式锁、排行榜、注册中心等，持久化功能在灾难恢复、数据迁移方面将发挥较大的作用。建议遵循几个原则：\n\n不要把Redis作为数据库，所有数据尽可能可由应用服务自动重建。\n使用4.0以上版本Redis，使用AOF+RDB混合持久化功能。\n合理规划Redis最大占用内存，防止AOF重写或save过程中资源不足。\n避免单机部署多实例。\n生产环境多为集群化部署，可在slave开启持久化能力，让master更好的对外提供写服务。\n备份文件应自动上传至异地机房或云存储，做好灾难备份。\n\n关于fork()通过上面的分析，我们都知道RDB的快照、AOF的重写都需要fork，这是一个重量级操作，会对Redis造成阻塞。因此为了不影响Redis主进程响应，我们需要尽可能降低阻塞。\n\n降低fork的频率，比如可以手动来触发RDB生成快照、与AOF重写；\n控制Redis最大使用内存，防止fork耗时过长；\n使用更高性能的硬件；\n合理配置Linux的内存分配策略，避免因为物理内存不足导致fork失败。\n\n","categories":["运维","Redis"]},{"title":"Redis 跳表","url":"/posts/44045/","content":"理解跳表，从单链表开始说起下图是一个简单的有序单链表，单链表的特性就是每个元素存放下一个元素的引用。即：通过第一个元素可以找到第二个元素，通过第二个元素可以找到第三个元素，依次类推，直到找到最后一个元素。\n\n现在我们有个场景，想快速找到上图链表中的 10 这个元素，只能从头开始遍历链表，直到找到我们需要找的元素。查找路径：1、3、4、5、7、8、9、10。这样的查找效率很低，平均时间复杂度很高O(n)。那有没有办法提高链表的查找速度呢？如下图所示，我们从链表中每两个元素抽出来，加一级索引，一级索引指向了原始链表，即：通过一级索引 7 的down指针可以找到原始链表的 7 。那现在怎么查找 10 这个元素呢？\n\n先在索引找 1、4、7、9，遍历到一级索引的 9 时，发现 9 的后继节点是 13，比 10 大，于是不往后找了，而是通过 9 找到原始链表的 9，然后再往后遍历找到了我们要找的 10，遍历结束。\n有没有发现，加了一级索引后，查找路径：1、4、7、9、10，查找节点需要遍历的元素相对少了，我们不需要对 10 之前的所有数据都遍历，查找的效率提升了。\n那如果加二级索引呢？如下图所示，查找路径：1、7、9、10。是不是找 10 的效率更高了？这就是跳表的思想，用“空间换时间”，通过给链表建立索引，提高了查找的效率。\n\n可能同学们会想，从上面案例来看，提升的效率并不明显，本来需要遍历8个元素，优化了半天，还需要遍历 4 个元素，其实是因为我们的数据量太少了，当数据量足够大时，效率提升会很大。\n如下图所示，假如有序单链表现在有1万个元素，分别是 0~9999。现在我们建了很多级索引，最高级的索引，就两个元素 0、5000，次高级索引四个元素 0、2500、5000、7500，依次类推，当我们查找 7890 这个元素时，查找路径为 0、5000、7500 … 7890，通过最高级索引直接跳过了5000个元素，次高层索引直接跳过了2500个元素，从而使得链表能够实现二分查找。由此可以看出，当元素数量较多时，索引提高的效率比较大，近似于二分查找。\n\n到这里大家应该已经明白了什么是跳表。跳表是可以实现二分查找的有序链表。\n查找的时间复杂度既然跳表可以提升链表查找元素的效率，那查找一个元素的时间复杂度到底是多少呢？查找元素的过程是从最高级索引开始，一层一层遍历最后下沉到原始链表。所以，时间复杂度 = 索引的高度 * 每层索引遍历元素的个数。\n先来求跳表的索引高度。如下图所示，假设每两个结点会抽出一个结点作为上一级索引的结点，原始的链表有n个元素，则一级索引有 n/2 个元素、二级索引有 n/4 个元素、k级索引就有 n/2k 个元素。最高级索引一般有2个元素，即：最高级索引 h 满足 2 = n/2h，即 h = log2n - 1，最高级索引 h 为索引层的高度加上原始数据一层，跳表的总高度 h = log2n。\n\n我们看上图中加粗的箭头，表示查找元素 x 的路径，那查找过程中每一层索引最多遍历几个元素呢？\n图中所示，现在到达第 k 级索引，我们发现要查找的元素 x 比 y 大比 z 小，所以，我们需要从 y 处下降到 k-1 级索引继续查找，k-1级索引中比 y 大比 z 小的只有一个 w，所以在 k-1 级索引中，我们遍历的元素最多就是 y、w、z，发现 x 比 w大比 z 小之后，再下降到 k-2 级索引。所以，k-2 级索引最多遍历的元素为 w、u、z。\n其实每级索引都是类似的道理，每级索引中都是两个结点抽出一个结点作为上一级索引的结点。 现在我们得出结论：当每级索引都是两个结点抽出一个结点作为上一级索引的结点时，每一层最多遍历3个结点。\n跳表的索引高度 h = log2n，且每层索引最多遍历 3 个元素。所以跳表中查找一个元素的时间复杂度为 O(3*logn)，省略常数即：O(logn)。\n空间复杂度跳表通过建立索引，来提高查找元素的效率，就是典型的“空间换时间”的思想，所以在空间上做了一些牺牲，那空间复杂度到底是多少呢？\n假如原始链表包含 n 个元素，则一级索引元素个数为 n/2、二级索引元素个数为 n/4、三级索引元素个数为 n/8 以此类推。所以，索引节点的总和是：n/2 + n/4 + n/8 + … + 8 + 4 + 2 = n-2，**空间复杂度是 O(n)**。\n如下图所示：如果每三个结点抽一个结点做为索引，索引总和数就是 n/3 + n/9 + n/27 + … + 9 + 3 + 1= n/2，减少了一半。所以我们可以通过较少索引数来减少空间复杂度，但是相应的肯定会造成查找效率有一定下降，我们可以根据我们的应用场景来控制这个阈值，看我们更注重时间还是空间。\n\nBut，索引结点往往只需要存储 key 和几个指针，并不需要存储完整的对象，所以当对象比索引结点大很多时，索引占用的额外空间就可以忽略了。举个例子：我们现在需要用跳表来给所有学生建索引，学生有很多属性：学号、姓名、性别、身份证号、年龄、家庭住址、身高、体重等。学生的各种属性只需要在原始链表中存储一份即可，我们只需要用学生的学号（int 类型的数据）建立索引，所以索引相对原始数据而言，占用的空间可以忽略。\n插入数据插入数据看起来也很简单，跳表的原始链表需要保持有序，所以我们会向查找元素一样，找到元素应该插入的位置。如下图所示，要插入数据6，整个过程类似于查找6，整个的查找路径为 1、1、1、4、4、5。查找到第底层原始链表的元素 5 时，发现 5 小于 6 但是后继节点 7 大于 6，所以应该把 6 插入到 5 之后 7 之前。整个时间复杂度为查找元素的时间复杂度 O(logn)。\n\n如下图所示，假如一直往原始列表中添加数据，但是不更新索引，就可能出现两个索引节点之间数据非常多的情况，极端情况，跳表退化为单链表，从而使得查找效率从 O(logn) 退化为 O(n)。那这种问题该怎么解决呢？我们需要在插入数据的时候，索引节点也需要相应的增加、或者重建索引，来避免查找效率的退化。那我们该如何去维护这个索引呢？\n\n比较容易理解的做法就是完全重建索引，我们每次插入数据后，都把这个跳表的索引删掉全部重建，重建索引的时间复杂度是多少呢？因为索引的空间复杂度是 O(n)，即：索引节点的个数是 O(n) 级别，每次完全重新建一个 O(n) 级别的索引，时间复杂度也是 O(n) 。造成的后果是：为了维护索引，导致每次插入数据的时间复杂度变成了 O(n)。\n那有没有其他效率比较高的方式来维护索引呢？\n假如跳表每一层的晋升概率是 1/2，最理想的索引就是在原始链表中每隔一个元素抽取一个元素做为一级索引。换种说法，我们在原始链表中随机的选 n/2 个元素做为一级索引是不是也能通过索引提高查找的效率呢？ \n当然可以了，因为一般随机选的元素相对来说都是比较均匀的。\n如下图所示，随机选择了n/2 个元素做为一级索引，虽然不是每隔一个元素抽取一个，但是对于查找效率来讲，影响不大，比如我们想找元素 16，仍然可以通过一级索引，使得遍历路径较少了将近一半。\n如果抽取的一级索引的元素恰好是前一半的元素 1、3、4、5、7、8，那么查找效率确实没有提升，但是这样的概率太小了。我们可以认为：当原始链表中元素数量足够大，且抽取足够随机的话，我们得到的索引是均匀的。我们要清楚设计良好的数据结构都是为了应对大数据量的场景，如果原始链表只有 5 个元素，那么依次遍历 5 个元素也没有关系，因为数据量太少了。所以，我们可以维护一个这样的索引：随机选 n/2 个元素做为一级索引、随机选 n/4 个元素做为二级索引、随机选 n/8 个元素做为三级索引，依次类推，一直到最顶层索引。这里每层索引的元素个数已经确定，且每层索引元素选取的足够随机，所以可以通过索引来提升跳表的查找效率。\n\n那代码该如何实现，才能使跳表满足上述这个样子呢？可以在每次新插入元素的时候，尽量让该元素有 1/2 的几率建立一级索引、1/4 的几率建立二级索引、1/8 的几率建立三级索引，以此类推，就能满足我们上面的条件。现在我们就需要一个概率算法帮我们把控这个 1/2、1/4、1/8 … ，当每次有数据要插入时，先通过概率算法告诉我们这个元素需要插入到几级索引中，然后开始维护索引并把数据插入到原始链表中。下面开始讲解这个概率算法代码如何实现。\n我们可以实现一个 randomLevel() 方法，该方法会随机生成 1~MAX_LEVEL 之间的数（MAX_LEVEL表示索引的最高层数），且该方法有 1/2 的概率返回 1、1/4 的概率返回 2、1/8的概率返回 3，以此类推。\n\nrandomLevel() 方法返回 1 表示当前插入的该元素不需要建索引，只需要存储数据到原始链表即可（概率 1/2）\nrandomLevel() 方法返回 2 表示当前插入的该元素需要建一级索引（概率 1/4）\nrandomLevel() 方法返回 3 表示当前插入的该元素需要建二级索引（概率 1/8）\nrandomLevel() 方法返回 4 表示当前插入的该元素需要建三级索引（概率 1/16）\n。。。以此类推\n\n所以，通过 randomLevel() 方法，我们可以控制整个跳表各级索引中元素的个数。重点来了：randomLevel() 方法返回 2 的时候会建立一级索引，我们想要一级索引中元素个数占原始数据的 1/2，但是 randomLevel() 方法返回 2 的概率为 1/4，那是不是有矛盾呢？明明说好的 1/2，结果一级索引元素个数怎么变成了原始链表的 1/4？我们先看下图，应该就明白了。\n\n假设我们在插入元素 6 的时候，randomLevel() 方法返回 1，则我们不会为 6 建立索引。插入 7 的时候，randomLevel() 方法返回3 ，所以我们需要为元素 7 建立二级索引。这里我们发现了一个特点：当建立二级索引的时候，同时也会建立一级索引；当建立三级索引时，同时也会建立一级、二级索引。所以，一级索引中元素的个数等于 [ 原始链表元素个数 ] * *[ randomLevel() 方法返回值 &gt; 1 的概率 ]*。因为 randomLevel() 方法返回值 &gt; 1就会建索引，凡是建索引，无论几级索引必然有一级索引，所以一级索引中元素个数占原始数据个数的比率为 randomLevel() 方法返回值 &gt; 1 的概率。那 randomLevel() 方法返回值 &gt; 1 的概率是多少呢？因为 randomLevel() 方法随机生成 1~MAX_LEVEL 的数字，且 randomLevel() 方法返回值 1 的概率为 1/2，则 randomLevel() 方法返回值 &gt; 1 的概率为 1 - 1/2 = 1/2。即通过上述流程实现了一级索引中元素个数占原始数据个数的 1/2。\n同理，当 randomLevel() 方法返回值 &gt; 2 时，会建立二级或二级以上索引，都会在二级索引中增加元素，因此二级索引中元素个数占原始数据的比率为 randomLevel() 方法返回值 &gt; 2 的概率。 randomLevel() 方法返回值 &gt; 2 的概率为 1 减去 randomLevel() = 1 或 =2 的概率，即 1 - 1/2 - 1/4 = 1/4。OK，达到了我们设计的目标：二级索引中元素个数占原始数据的 1/4。\n以此类推，可以得出，遵守以下两个条件：\n\nrandomLevel() 方法，随机生成 1~MAX_LEVEL 之间的数（MAX_LEVEL表示索引的最高层数），且有 1/2的概率返回 1、1/4的概率返回 2、1/8的概率返回 3 …\nrandomLevel() 方法返回 1 不建索引、返回2建一级索引、返回 3 建二级索引、返回 4 建三级索引 …\n\n就可以满足我们想要的结果，即：一级索引中元素个数应该占原始数据的 1/2，二级索引中元素个数占原始数据的 1/4，三级索引中元素个数占原始数据的 1/8 ，依次类推，一直到最顶层索引。\n但是问题又来了，怎么设计这么一个 randomLevel() 方法呢？直接撸代码：\n// 该 randomLevel 方法会随机生成 1~MAX_LEVEL 之间的数，且 ：//        1/2 的概率返回 1//        1/4 的概率返回 2//        1/8 的概率返回 3 以此类推private int randomLevel() &#123;  int level = 1;  // 当 level &lt; MAX_LEVEL，且随机数小于设定的晋升概率时，level + 1  while (Math.random() &lt; SKIPLIST_P &amp;&amp; level &lt; MAX_LEVEL)    level += 1;  return level;&#125;\n\n上述代码可以实现我们的功能，而且，我们的案例中晋升概率 SKIPLIST_P 设置的 1/2，即：每两个结点抽出一个结点作为上一级索引的结点。如果我们想节省空间利用率，可以适当的降低代码中的 SKIPLIST_P，从而减少索引元素个数，Redis 的 zset 中 SKIPLIST_P 设定的 0.25。下图所示，是Redis t_zset.c 中 zslRandomLevel 函数的实现：\n\nRedis 源码中 (random()&amp;0xFFFF) &lt; (ZSKIPLIST_P * 0xFFFF)  在功能上等价于我代码中的 Math.random() &lt; SKIPLIST_P ，只不过 Redis 作者 antirez 使用位运算来提高浮点数比较的效率。\n整体思路大家应该明白了，那插入数据时维护索引的时间复杂度是多少呢？**元素插入到单链表的时间复杂度为 O(1)**，我们索引的高度最多为 logn，当插入一个元素 x 时，最坏的情况就是元素 x 需要插入到每层索引中，所以插入数据到各层索引中，最坏时间复杂度是 O(logn)。\n过程大概理解了，再通过一个例子描述一下跳表插入数据的全流程。现在我们要插入数据 6 到跳表中，首先 randomLevel() 返回 3，表示需要建二级索引，即：一级索引和二级索引需要增加元素 6。该跳表目前最高三级索引，首先找到三级索引的 1，发现 6 比 1大比 13小，所以，从 1 下沉到二级索引。\n\n下沉到二级索引后，发现 6 比 1 大比 7 小，此时需要在二级索引中 1 和 7 之间加一个元素6 ，并从元素 1 继续下沉到一级索引。\n\n下沉到一级索引后，发现 6 比 1 大比 4 大，所以往后查找，发现 6 比 4 大比 7 小，此时需要在一级索引中 4 和 7 之间加一个元素 6 ，并把二级索引的 6 指向 一级索引的 6，最后，从元素 4 继续下沉到原始链表。\n\n下沉到原始链表后，就比较简单了，发现 4、5 比 6小，7比6大，所以将6插入到 5 和 7 之间即可，整个插入过程结束。\n\n整个插入过程的路径与查找元素路径类似， 每层索引中插入元素的时间复杂度 O(1)，所以整个插入的时间复杂度是 O(logn)。\n删除数据跳表删除数据时，要把索引中对应节点也要删掉。如下图所示，如果要删除元素 9，需要把原始链表中的 9 和第一级索引的 9 都删除掉。\n\n跳表中，删除元素的时间复杂度是多少呢？\n删除元素的过程跟查找元素的过程类似，只不过在查找的路径上如果发现了要删除的元素 x，则执行删除操作。跳表中，每一层索引其实都是一个有序的单链表，单链表删除元素的时间复杂度为 O(1)，索引层数为 logn 表示最多需要删除 logn 个元素，所以删除元素的总时间包含 查找元素的时间 加 删除 logn个元素的时间 为 O(logn) + O(logn) = 2 O(logn)，忽略常数部分，删除元素的时间复杂度为 O(logn)。\n总结\n跳表是可以实现二分查找的有序链表；\n每个元素插入时随机生成它的level；\n最底层包含所有的元素；\n如果一个元素出现在level(x)，那么它肯定出现在x以下的level中；\n每个索引节点包含两个指针，一个向下，一个向右；（笔记目前看过的各种跳表源码实现包括Redis 的zset 都没有向下的指针，那怎么从二级索引跳到一级索引呢？留个悬念，看源码吧，文末有跳表实现源码）\n跳表查询、插入、删除的时间复杂度为O(log n)，与平衡二叉树接近；\n\n为什么Redis选择使用跳表而不是红黑树来实现有序集合？Redis 中的有序集合(zset) 支持的操作：\n\n插入一个元素\n删除一个元素\n查找一个元素\n有序输出所有元素\n按照范围区间查找元素（比如查找值在 [100, 356] 之间的数据）\n\n其中，前四个操作红黑树也可以完成，且时间复杂度跟跳表是一样的。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。按照区间查找数据时，跳表可以做到 O(logn) 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了，非常高效。\n工业上其他使用跳表的场景在博客上从来没有见过有同学讲述 HBase MemStore 的数据结构，其实 HBase MemStore 内部存储数据就使用的跳表。为什么呢？HBase 属于 LSM Tree 结构的数据库，LSM Tree 结构的数据库有个特点，实时写入的数据先写入到内存，内存达到阈值往磁盘 flush 的时候，会生成类似于 StoreFile 的 有序文件，而跳表恰好就是天然有序的，所以在 flush 的时候效率很高，而且跳表查找、插入、删除性能都很高，这应该是 HBase MemStore 内部存储数据使用跳表的原因之一。HBase 使用的是 java.util.concurrent 下的 ConcurrentSkipListMap()。\nGoogle 开源的 key/value 存储引擎 LevelDB 以及 Facebook 基于 LevelDB 优化的 RocksDB 都是 LSM Tree 结构的数据库，他们内部的 MemTable 都是使用了跳表这种数据结构。\n","categories":["运维","Redis"]},{"title":"Redis 高级客户端Lettuce详解","url":"/posts/46477/","content":"前提Lettuce是一个Redis的Java驱动包，初识她的时候是使用RedisTemplate的时候遇到点问题Debug到底层的一些源码，发现spring-data-redis的驱动包在某个版本之后替换为Lettuce。Lettuce翻译为生菜，没错，就是吃的那种生菜，所以它的Logo长这样：\n[\n既然能被Spring生态所认可，Lettuce想必有过人之处，于是笔者花时间阅读她的官方文档，整理测试示例，写下这篇文章。编写本文时所使用的版本为Lettuce 5.1.8.RELEASE，SpringBoot 2.1.8.RELEASE，JDK [8,11]。超长警告：这篇文章断断续续花了两周完成，超过4万字…..\nLettuce简介Lettuce是一个高性能基于Java编写的Redis驱动框架，底层集成了Project Reactor提供天然的反应式编程，通信框架集成了Netty使用了非阻塞IO，5.x版本之后融合了JDK1.8的异步编程特性，在保证高性能的同时提供了十分丰富易用的API，5.1版本的新特性如下：\n\n支持Redis的新增命令ZPOPMIN, ZPOPMAX, BZPOPMIN, BZPOPMAX。\n支持通过Brave模块跟踪Redis命令执行。\n支持Redis Streams。\n支持异步的主从连接。\n支持异步连接池。\n新增命令最多执行一次模式（禁止自动重连）。\n全局命令超时设置（对异步和反应式命令也有效）。\n……等等\n\n注意一点：Redis的版本至少需要2.6，当然越高越好，API的兼容性比较强大。\n只需要引入单个依赖就可以开始愉快地使用Lettuce：\n\nMaven\n&lt;dependency&gt;    &lt;groupId&gt;io.lettuce&lt;/groupId&gt;    &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;    &lt;version&gt;5.1.8.RELEASE&lt;/version&gt;&lt;/dependency&gt;\nGradle\ndependencies &#123;  compile &#x27;io.lettuce:lettuce-core:5.1.8.RELEASE&#x27;&#125;\n\n连接Redis单机、哨兵、集群模式下连接Redis需要一个统一的标准去表示连接的细节信息，在Lettuce中这个统一的标准是RedisURI。可以通过三种方式构造一个RedisURI实例：\n\n定制的字符串URI语法：\n\nRedisURI uri = RedisURI.create(&quot;redis://localhost/&quot;);\n\n\n使用建造器（RedisURI.Builder）：\n\nRedisURI uri = RedisURI.builder().withHost(&quot;localhost&quot;).withPort(6379).build();\n\n\n直接通过构造函数实例化：\n\nRedisURI uri = new RedisURI(&quot;localhost&quot;, 6379, 60, TimeUnit.SECONDS);\n\n定制的连接URI语法\n单机（前缀为redis://）\n\n格式：redis://[password@]host[:port][/databaseNumber][?[timeout=timeout[d|h|m|s|ms|us|ns]]完整：redis://mypassword@127.0.0.1:6379/0?timeout=10s简单：redis://localhost\n\n\n单机并且使用SSL（前缀为rediss://） &lt;== 注意后面多了个s\n\n格式：rediss://[password@]host[:port][/databaseNumber][?[timeout=timeout[d|h|m|s|ms|us|ns]]完整：rediss://mypassword@127.0.0.1:6379/0?timeout=10s简单：rediss://localhost\n\n\n单机Unix Domain Sockets模式（前缀为redis-socket://）\n\n格式：redis-socket://path[?[timeout=timeout[d|h|m|s|ms|us|ns]][&amp;_database=database_]]完整：redis-socket:///tmp/redis?timeout=10s&amp;_database=0\n\n\n哨兵（前缀为redis-sentinel://）\n\n格式：redis-sentinel://[password@]host[:port][,host2[:port2]][/databaseNumber][?[timeout=timeout[d|h|m|s|ms|us|ns]]#sentinelMasterId完整：redis-sentinel://mypassword@127.0.0.1:6379,127.0.0.1:6380/0?timeout=10s#mymaster\n\n超时时间单位：\n\nd 天\nh 小时\nm 分钟\ns 秒钟\nms 毫秒\nus 微秒\nns 纳秒\n\n个人建议使用RedisURI提供的建造器，毕竟定制的URI虽然简洁，但是比较容易出现人为错误。鉴于笔者没有SSL和Unix Domain Socket的使用场景，下面不对这两种连接方式进行列举。\n基本使用Lettuce使用的时候依赖于四个主要组件：\n\nRedisURI：连接信息。\nRedisClient：Redis客户端，特殊地，集群连接有一个定制的RedisClusterClient。\nConnection：Redis连接，主要是StatefulConnection或者StatefulRedisConnection的子类，连接的类型主要由连接的具体方式（单机、哨兵、集群、订阅发布等等）选定，比较重要。\nRedisCommands：Redis命令API接口，基本上覆盖了Redis发行版本的所有命令，提供了同步（sync）、异步（async）、反应式（reative）的调用方式，对于使用者而言，会经常跟RedisCommands系列接口打交道。\n\n一个基本使用例子如下：\n@Testpublic void testSetGet() throws Exception &#123;    RedisURI redisUri = RedisURI.builder()                    // &lt;1&gt; 创建单机连接的连接信息            .withHost(&quot;localhost&quot;)            .withPort(6379)            .withTimeout(Duration.of(10, ChronoUnit.SECONDS))            .build();    RedisClient redisClient = RedisClient.create(redisUri);   // &lt;2&gt; 创建客户端    StatefulRedisConnection&lt;String, String&gt; connection = redisClient.connect();     // &lt;3&gt; 创建线程安全的连接    RedisCommands&lt;String, String&gt; redisCommands = connection.sync();                // &lt;4&gt; 创建同步命令    SetArgs setArgs = SetArgs.Builder.nx().ex(5);    String result = redisCommands.set(&quot;name&quot;, &quot;throwable&quot;, setArgs);    Assertions.assertThat(result).isEqualToIgnoringCase(&quot;OK&quot;);    result = redisCommands.get(&quot;name&quot;);    Assertions.assertThat(result).isEqualTo(&quot;throwable&quot;);    // ... 其他操作    connection.close();   // &lt;5&gt; 关闭连接    redisClient.shutdown();  // &lt;6&gt; 关闭客户端&#125;\n\n注意：\n\n**&lt;5&gt;**：关闭连接一般在应用程序停止之前操作，一个应用程序中的一个Redis驱动实例不需要太多的连接（一般情况下只需要一个连接实例就可以，如果有多个连接的需要可以考虑使用连接池，其实Redis目前处理命令的模块是单线程，在客户端多个连接多线程调用理论上没有效果）。\n&lt;6&gt;：关闭客户端一般应用程序停止之前操作，如果条件允许的话，基于后开先闭原则，客户端关闭应该在连接关闭之后操作。\n\nAPILettuce主要提供三种API：\n\n同步（sync）：RedisCommands。\n异步（async）：RedisAsyncCommands。\n反应式（reactive）：RedisReactiveCommands。\n\n先准备好一个单机Redis连接备用：\nprivate static StatefulRedisConnection&lt;String, String&gt; CONNECTION;private static RedisClient CLIENT;@BeforeClasspublic static void beforeClass() &#123;    RedisURI redisUri = RedisURI.builder()            .withHost(&quot;localhost&quot;)            .withPort(6379)            .withTimeout(Duration.of(10, ChronoUnit.SECONDS))            .build();    CLIENT = RedisClient.create(redisUri);    CONNECTION = CLIENT.connect();&#125;@AfterClasspublic static void afterClass() throws Exception &#123;    CONNECTION.close();    CLIENT.shutdown();&#125;\n\nRedis命令API的具体实现可以直接从StatefulRedisConnection实例获取，见其接口定义：\npublic interface StatefulRedisConnection&lt;K, V&gt; extends StatefulConnection&lt;K, V&gt; &#123;    boolean isMulti();    RedisCommands&lt;K, V&gt; sync();    RedisAsyncCommands&lt;K, V&gt; async();    RedisReactiveCommands&lt;K, V&gt; reactive();&#125;    \n\n值得注意的是，在不指定编码解码器RedisCodec的前提下，RedisClient创建的StatefulRedisConnection实例一般是泛型实例StatefulRedisConnection&lt;String,String&gt;，也就是所有命令API的KEY和VALUE都是String类型，这种使用方式能满足大部分的使用场景。当然，必要的时候可以定制编码解码器RedisCodec&lt;K,V&gt;。\n同步API先构建RedisCommands实例：\nprivate static RedisCommands&lt;String, String&gt; COMMAND;@BeforeClasspublic static void beforeClass() &#123;    COMMAND = CONNECTION.sync();&#125;\n\n基本使用：\n@Testpublic void testSyncPing() throws Exception &#123;   String pong = COMMAND.ping();   Assertions.assertThat(pong).isEqualToIgnoringCase(&quot;PONG&quot;);&#125;@Testpublic void testSyncSetAndGet() throws Exception &#123;    SetArgs setArgs = SetArgs.Builder.nx().ex(5);    COMMAND.set(&quot;name&quot;, &quot;throwable&quot;, setArgs);    String value = COMMAND.get(&quot;name&quot;);    log.info(&quot;Get value: &#123;&#125;&quot;, value);&#125;// Get value: throwable\n\n同步API在所有命令调用之后会立即返回结果。如果熟悉Jedis的话，RedisCommands的用法其实和它相差不大。\n异步API先构建RedisAsyncCommands实例：\nprivate static RedisAsyncCommands&lt;String, String&gt; ASYNC_COMMAND;@BeforeClasspublic static void beforeClass() &#123;    ASYNC_COMMAND = CONNECTION.async();&#125;\n\n基本使用：\n@Testpublic void testAsyncPing() throws Exception &#123;    RedisFuture&lt;String&gt; redisFuture = ASYNC_COMMAND.ping();    log.info(&quot;Ping result:&#123;&#125;&quot;, redisFuture.get());&#125;// Ping result:PONG\n\nRedisAsyncCommands所有方法执行返回结果都是RedisFuture实例，而RedisFuture接口的定义如下：\npublic interface RedisFuture&lt;V&gt; extends CompletionStage&lt;V&gt;, Future&lt;V&gt; &#123;    String getError();    boolean await(long timeout, TimeUnit unit) throws InterruptedException;&#125;    \n\n也就是，RedisFuture可以无缝使用Future或者JDK1.8中引入的CompletableFuture提供的方法。举个例子：\n@Testpublic void testAsyncSetAndGet1() throws Exception &#123;    SetArgs setArgs = SetArgs.Builder.nx().ex(5);    RedisFuture&lt;String&gt; future = ASYNC_COMMAND.set(&quot;name&quot;, &quot;throwable&quot;, setArgs);    // CompletableFuture#thenAccept()    future.thenAccept(value -&gt; log.info(&quot;Set命令返回:&#123;&#125;&quot;, value));    // Future#get()    future.get();&#125;// Set命令返回:OK@Testpublic void testAsyncSetAndGet2() throws Exception &#123;    SetArgs setArgs = SetArgs.Builder.nx().ex(5);    CompletableFuture&lt;Void&gt; result =            (CompletableFuture&lt;Void&gt;) ASYNC_COMMAND.set(&quot;name&quot;, &quot;throwable&quot;, setArgs)                    .thenAcceptBoth(ASYNC_COMMAND.get(&quot;name&quot;),                            (s, g) -&gt; &#123;                                log.info(&quot;Set命令返回:&#123;&#125;&quot;, s);                                log.info(&quot;Get命令返回:&#123;&#125;&quot;, g);                            &#125;);    result.get();&#125;// Set命令返回:OK// Get命令返回:throwable\n\n如果能熟练使用CompletableFuture和函数式编程技巧，可以组合多个RedisFuture完成一些列复杂的操作。\n反应式APILettuce引入的反应式编程框架是Project Reactor，如果没有反应式编程经验可以先自行了解一下Project Reactor。\n构建RedisReactiveCommands实例：\nprivate static RedisReactiveCommands&lt;String, String&gt; REACTIVE_COMMAND;@BeforeClasspublic static void beforeClass() &#123;    REACTIVE_COMMAND = CONNECTION.reactive();&#125;\n\n根据Project Reactor，RedisReactiveCommands的方法如果返回的结果只包含0或1个元素，那么返回值类型是Mono，如果返回的结果包含0到N（N大于0）个元素，那么返回值是Flux。举个例子：\n@Testpublic void testReactivePing() throws Exception &#123;    Mono&lt;String&gt; ping = REACTIVE_COMMAND.ping();    ping.subscribe(v -&gt; log.info(&quot;Ping result:&#123;&#125;&quot;, v));    Thread.sleep(1000);&#125;// Ping result:PONG@Testpublic void testReactiveSetAndGet() throws Exception &#123;    SetArgs setArgs = SetArgs.Builder.nx().ex(5);    REACTIVE_COMMAND.set(&quot;name&quot;, &quot;throwable&quot;, setArgs).block();    REACTIVE_COMMAND.get(&quot;name&quot;).subscribe(value -&gt; log.info(&quot;Get命令返回:&#123;&#125;&quot;, value));    Thread.sleep(1000);&#125;// Get命令返回:throwable@Testpublic void testReactiveSet() throws Exception &#123;    REACTIVE_COMMAND.sadd(&quot;food&quot;, &quot;bread&quot;, &quot;meat&quot;, &quot;fish&quot;).block();    Flux&lt;String&gt; flux = REACTIVE_COMMAND.smembers(&quot;food&quot;);    flux.subscribe(log::info);    REACTIVE_COMMAND.srem(&quot;food&quot;, &quot;bread&quot;, &quot;meat&quot;, &quot;fish&quot;).block();    Thread.sleep(1000);&#125;// meat// bread// fish\n\n举个更加复杂的例子，包含了事务、函数转换等：\n@Testpublic void testReactiveFunctional() throws Exception &#123;    REACTIVE_COMMAND.multi().doOnSuccess(r -&gt; &#123;        REACTIVE_COMMAND.set(&quot;counter&quot;, &quot;1&quot;).doOnNext(log::info).subscribe();        REACTIVE_COMMAND.incr(&quot;counter&quot;).doOnNext(c -&gt; log.info(String.valueOf(c))).subscribe();    &#125;).flatMap(s -&gt; REACTIVE_COMMAND.exec())            .doOnNext(transactionResult -&gt; log.info(&quot;Discarded:&#123;&#125;&quot;, transactionResult.wasDiscarded()))            .subscribe();    Thread.sleep(1000);&#125;// OK// 2// Discarded:false\n\n这个方法开启一个事务，先把counter设置为1，再将counter自增1。\n发布和订阅非集群模式下的发布订阅依赖于定制的连接StatefulRedisPubSubConnection，集群模式下的发布订阅依赖于定制的连接StatefulRedisClusterPubSubConnection，两者分别来源于RedisClient#connectPubSub()系列方法和RedisClusterClient#connectPubSub()：\n\n非集群模式：\n\n// 可能是单机、普通主从、哨兵等非集群模式的客户端RedisClient client = ...StatefulRedisPubSubConnection&lt;String, String&gt; connection = client.connectPubSub();connection.addListener(new RedisPubSubListener&lt;String, String&gt;() &#123; ... &#125;);// 同步命令RedisPubSubCommands&lt;String, String&gt; sync = connection.sync();sync.subscribe(&quot;channel&quot;);// 异步命令RedisPubSubAsyncCommands&lt;String, String&gt; async = connection.async();RedisFuture&lt;Void&gt; future = async.subscribe(&quot;channel&quot;);// 反应式命令RedisPubSubReactiveCommands&lt;String, String&gt; reactive = connection.reactive();reactive.subscribe(&quot;channel&quot;).subscribe();reactive.observeChannels().doOnNext(patternMessage -&gt; &#123;...&#125;).subscribe()\n\n\n集群模式：\n\n// 使用方式其实和非集群模式基本一致RedisClusterClient clusterClient = ...StatefulRedisClusterPubSubConnection&lt;String, String&gt; connection = clusterClient.connectPubSub();connection.addListener(new RedisPubSubListener&lt;String, String&gt;() &#123; ... &#125;);RedisPubSubCommands&lt;String, String&gt; sync = connection.sync();sync.subscribe(&quot;channel&quot;);// ...\n\n这里用单机同步命令的模式举一个Redis键空间通知（Redis Keyspace Notifications）的例子：\n@Testpublic void testSyncKeyspaceNotification() throws Exception &#123;    RedisURI redisUri = RedisURI.builder()            .withHost(&quot;localhost&quot;)            .withPort(6379)            // 注意这里只能是0号库            .withDatabase(0)            .withTimeout(Duration.of(10, ChronoUnit.SECONDS))            .build();    RedisClient redisClient = RedisClient.create(redisUri);    StatefulRedisConnection&lt;String, String&gt; redisConnection = redisClient.connect();    RedisCommands&lt;String, String&gt; redisCommands = redisConnection.sync();    // 只接收键过期的事件    redisCommands.configSet(&quot;notify-keyspace-events&quot;, &quot;Ex&quot;);    StatefulRedisPubSubConnection&lt;String, String&gt; connection = redisClient.connectPubSub();    connection.addListener(new RedisPubSubAdapter&lt;&gt;() &#123;        @Override        public void psubscribed(String pattern, long count) &#123;            log.info(&quot;pattern:&#123;&#125;,count:&#123;&#125;&quot;, pattern, count);        &#125;        @Override        public void message(String pattern, String channel, String message) &#123;            log.info(&quot;pattern:&#123;&#125;,channel:&#123;&#125;,message:&#123;&#125;&quot;, pattern, channel, message);        &#125;    &#125;);    RedisPubSubCommands&lt;String, String&gt; commands = connection.sync();    commands.psubscribe(&quot;__keyevent@0__:expired&quot;);    redisCommands.setex(&quot;name&quot;, 2, &quot;throwable&quot;);    Thread.sleep(10000);    redisConnection.close();    connection.close();    redisClient.shutdown();&#125;// pattern:__keyevent@0__:expired,count:1// pattern:__keyevent@0__:expired,channel:__keyevent@0__:expired,message:name\n\n实际上，在实现RedisPubSubListener的时候可以单独抽离，尽量不要设计成匿名内部类的形式。\n事务和批量命令执行事务相关的命令就是WATCH、UNWATCH、EXEC、MULTI和DISCARD，在RedisCommands系列接口中有对应的方法。举个例子：\n// 同步模式@Testpublic void testSyncMulti() throws Exception &#123;    COMMAND.multi();    COMMAND.setex(&quot;name-1&quot;, 2, &quot;throwable&quot;);    COMMAND.setex(&quot;name-2&quot;, 2, &quot;doge&quot;);    TransactionResult result = COMMAND.exec();    int index = 0;    for (Object r : result) &#123;        log.info(&quot;Result-&#123;&#125;:&#123;&#125;&quot;, index, r);        index++;    &#125;&#125;// Result-0:OK// Result-1:OK\n\nRedis的Pipeline也就是管道机制可以理解为把多个命令打包在一次请求发送到Redis服务端，然后Redis服务端把所有的响应结果打包好一次性返回，从而节省不必要的网络资源（最主要是减少网络请求次数）。Redis对于Pipeline机制如何实现并没有明确的规定，也没有提供特殊的命令支持Pipeline机制。Jedis中底层采用BIO（阻塞IO）通讯，所以它的做法是客户端缓存将要发送的命令，最后需要触发然后同步发送一个巨大的命令列表包，再接收和解析一个巨大的响应列表包。Pipeline在Lettuce中对使用者是透明的，由于底层的通讯框架是Netty，所以网络通讯层面的优化Lettuce不需要过多干预，换言之可以这样理解：Netty帮Lettuce从底层实现了Redis的Pipeline机制。但是，Lettuce的异步API也提供了手动Flush的方法：\n@Testpublic void testAsyncManualFlush() &#123;    // 取消自动flush    ASYNC_COMMAND.setAutoFlushCommands(false);    List&lt;RedisFuture&lt;?&gt;&gt; redisFutures = Lists.newArrayList();    int count = 5000;    for (int i = 0; i &lt; count; i++) &#123;        String key = &quot;key-&quot; + (i + 1);        String value = &quot;value-&quot; + (i + 1);        redisFutures.add(ASYNC_COMMAND.set(key, value));        redisFutures.add(ASYNC_COMMAND.expire(key, 2));    &#125;    long start = System.currentTimeMillis();    ASYNC_COMMAND.flushCommands();    boolean result = LettuceFutures.awaitAll(10, TimeUnit.SECONDS, redisFutures.toArray(new RedisFuture[0]));    Assertions.assertThat(result).isTrue();    log.info(&quot;Lettuce cost:&#123;&#125; ms&quot;, System.currentTimeMillis() - start);&#125;// Lettuce cost:1302 ms\n\n上面只是从文档看到的一些理论术语，但是现实是骨感的，对比了下Jedis的Pipeline提供的方法，发现了Jedis的Pipeline执行耗时比较低：\n@Testpublic void testJedisPipeline() throws Exception &#123;    Jedis jedis = new Jedis();    Pipeline pipeline = jedis.pipelined();    int count = 5000;    for (int i = 0; i &lt; count; i++) &#123;        String key = &quot;key-&quot; + (i + 1);        String value = &quot;value-&quot; + (i + 1);        pipeline.set(key, value);        pipeline.expire(key, 2);    &#125;    long start = System.currentTimeMillis();    pipeline.syncAndReturnAll();    log.info(&quot;Jedis cost:&#123;&#125; ms&quot;, System.currentTimeMillis()  - start);&#125;// Jedis cost:9 ms\n\n个人猜测Lettuce可能底层并非合并所有命令一次发送（甚至可能是单条发送），具体可能需要抓包才能定位。依此来看，如果真的有大量执行Redis命令的场景，不妨可以使用Jedis的Pipeline。\n注意：由上面的测试推断RedisTemplate的executePipelined()方法是** 假的 **Pipeline执行方法，使用RedisTemplate的时候请务必注意这一点。\nLua脚本执行Lettuce中执行Redis的Lua命令的同步接口如下：\npublic interface RedisScriptingCommands&lt;K, V&gt; &#123;    &lt;T&gt; T eval(String var1, ScriptOutputType var2, K... var3);    &lt;T&gt; T eval(String var1, ScriptOutputType var2, K[] var3, V... var4);    &lt;T&gt; T evalsha(String var1, ScriptOutputType var2, K... var3);    &lt;T&gt; T evalsha(String var1, ScriptOutputType var2, K[] var3, V... var4);    List&lt;Boolean&gt; scriptExists(String... var1);    String scriptFlush();    String scriptKill();    String scriptLoad(V var1);    String digest(V var1);&#125;\n\n异步和反应式的接口方法定义差不多，不同的地方就是返回值类型，一般我们常用的是eval()、evalsha()和scriptLoad()方法。举个简单的例子：\nprivate static RedisCommands&lt;String, String&gt; COMMANDS;private static String RAW_LUA = &quot;local key = KEYS[1]\\n&quot; +        &quot;local value = ARGV[1]\\n&quot; +        &quot;local timeout = ARGV[2]\\n&quot; +        &quot;redis.call(&#x27;SETEX&#x27;, key, tonumber(timeout), value)\\n&quot; +        &quot;local result = redis.call(&#x27;GET&#x27;, key)\\n&quot; +        &quot;return result;&quot;;private static AtomicReference&lt;String&gt; LUA_SHA = new AtomicReference&lt;&gt;();@Testpublic void testLua() throws Exception &#123;    LUA_SHA.compareAndSet(null, COMMANDS.scriptLoad(RAW_LUA));    String[] keys = new String[]&#123;&quot;name&quot;&#125;;    String[] args = new String[]&#123;&quot;throwable&quot;, &quot;5000&quot;&#125;;    String result = COMMANDS.evalsha(LUA_SHA.get(), ScriptOutputType.VALUE, keys, args);    log.info(&quot;Get value:&#123;&#125;&quot;, result);&#125;// Get value:throwable\n\n高可用和分片为了Redis的高可用，一般会采用普通主从（Master/Replica，这里笔者称为普通主从模式，也就是仅仅做了主从复制，故障需要手动切换）、哨兵和集群。普通主从模式可以独立运行，也可以配合哨兵运行，只是哨兵提供自动故障转移和主节点提升功能。普通主从和哨兵都可以使用MasterSlave，通过入参包括RedisClient、编码解码器以及一个或者多个RedisURI获取对应的Connection实例。\n这里注意一点，MasterSlave中提供的方法如果只要求传入一个RedisURI实例，那么Lettuce会进行拓扑发现机制，自动获取Redis主从节点信息；如果要求传入一个RedisURI集合，那么对于普通主从模式来说所有节点信息是静态的，不会进行发现和更新。\n拓扑发现的规则如下：\n\n对于普通主从（Master/Replica）模式，不需要感知RedisURI指向从节点还是主节点，只会进行一次性的拓扑查找所有节点信息，此后节点信息会保存在静态缓存中，不会更新。\n对于哨兵模式，会订阅所有哨兵实例并侦听订阅/发布消息以触发拓扑刷新机制，更新缓存的节点信息，也就是哨兵天然就是动态发现节点信息，不支持静态配置。\n\n拓扑发现机制的提供API为TopologyProvider，需要了解其原理的可以参考具体的实现。\n对于集群（Cluster）模式，Lettuce提供了一套独立的API。\n另外，如果Lettuce连接面向的是非单个Redis节点，连接实例提供了数据读取节点偏好（ReadFrom）设置，可选值有：\n\nMASTER：只从Master节点中读取。\nMASTER_PREFERRED：优先从Master节点中读取。\nSLAVE_PREFERRED：优先从Slavor节点中读取。\nSLAVE：只从Slavor节点中读取。\nNEAREST：使用最近一次连接的Redis实例读取。\n\n普通主从模式假设现在有三个Redis服务形成树状主从关系如下：\n\n节点一：localhost:6379，角色为Master。\n节点二：localhost:6380，角色为Slavor，节点一的从节点。\n节点三：localhost:6381，角色为Slavor，节点二的从节点。\n\n首次动态节点发现主从模式的节点信息需要如下构建连接：\n@Testpublic void testDynamicReplica() throws Exception &#123;    // 这里只需要配置一个节点的连接信息，不一定需要是主节点的信息，从节点也可以    RedisURI uri = RedisURI.builder().withHost(&quot;localhost&quot;).withPort(6379).build();    RedisClient redisClient = RedisClient.create(uri);    StatefulRedisMasterSlaveConnection&lt;String, String&gt; connection = MasterSlave.connect(redisClient, new Utf8StringCodec(), uri);    // 只从从节点读取数据    connection.setReadFrom(ReadFrom.SLAVE);    // 执行其他Redis命令    connection.close();    redisClient.shutdown();&#125;\n\n如果需要指定静态的Redis主从节点连接属性，那么可以这样构建连接：\n@Testpublic void testStaticReplica() throws Exception &#123;    List&lt;RedisURI&gt; uris = new ArrayList&lt;&gt;();    RedisURI uri1 = RedisURI.builder().withHost(&quot;localhost&quot;).withPort(6379).build();    RedisURI uri2 = RedisURI.builder().withHost(&quot;localhost&quot;).withPort(6380).build();    RedisURI uri3 = RedisURI.builder().withHost(&quot;localhost&quot;).withPort(6381).build();    uris.add(uri1);    uris.add(uri2);    uris.add(uri3);    RedisClient redisClient = RedisClient.create();    StatefulRedisMasterSlaveConnection&lt;String, String&gt; connection = MasterSlave.connect(redisClient,            new Utf8StringCodec(), uris);    // 只从主节点读取数据    connection.setReadFrom(ReadFrom.MASTER);    // 执行其他Redis命令    connection.close();    redisClient.shutdown();&#125;\n\n哨兵模式由于Lettuce自身提供了哨兵的拓扑发现机制，所以只需要随便配置一个哨兵节点的RedisURI实例即可：\n@Testpublic void testDynamicSentinel() throws Exception &#123;    RedisURI redisUri = RedisURI.builder()            .withPassword(&quot;你的密码&quot;)            .withSentinel(&quot;localhost&quot;, 26379)            .withSentinelMasterId(&quot;哨兵Master的ID&quot;)            .build();    RedisClient redisClient = RedisClient.create();    StatefulRedisMasterSlaveConnection&lt;String, String&gt; connection = MasterSlave.connect(redisClient, new Utf8StringCodec(), redisUri);    // 只允许从从节点读取数据    connection.setReadFrom(ReadFrom.SLAVE);    RedisCommands&lt;String, String&gt; command = connection.sync();    SetArgs setArgs = SetArgs.Builder.nx().ex(5);    command.set(&quot;name&quot;, &quot;throwable&quot;, setArgs);    String value = command.get(&quot;name&quot;);    log.info(&quot;Get value:&#123;&#125;&quot;, value);&#125;// Get value:throwable\n\n集群模式鉴于笔者对Redis集群模式并不熟悉，Cluster模式下的API使用本身就有比较多的限制，所以这里只简单介绍一下怎么用。先说几个特性：\n下面的API提供跨槽位（Slot）调用的功能：\n\nRedisAdvancedClusterCommands。\nRedisAdvancedClusterAsyncCommands。\nRedisAdvancedClusterReactiveCommands。\n\n静态节点选择功能：\n\nmasters：选择所有主节点执行命令。\nslaves：选择所有从节点执行命令，其实就是只读模式。\nall nodes：命令可以在所有节点执行。\n\n集群拓扑视图动态更新功能：\n\n手动更新，主动调用RedisClusterClient#reloadPartitions()。\n后台定时更新。\n自适应更新，基于连接断开和MOVED/ASK命令重定向自动更新。\n\nRedis集群搭建详细过程可以参考官方文档，假设已经搭建好集群如下（192.168.56.200是笔者的虚拟机Host）：\n\n192.168.56.200:7001 =&gt; 主节点，槽位0-5460。\n192.168.56.200:7002 =&gt; 主节点，槽位5461-10922。\n192.168.56.200:7003 =&gt; 主节点，槽位10923-16383。\n192.168.56.200:7004 =&gt; 7001的从节点。\n192.168.56.200:7005 =&gt; 7002的从节点。\n192.168.56.200:7006 =&gt; 7003的从节点。\n\n简单的集群连接和使用方式如下：\n@Testpublic void testSyncCluster()&#123;    RedisURI uri = RedisURI.builder().withHost(&quot;192.168.56.200&quot;).build();    RedisClusterClient redisClusterClient = RedisClusterClient.create(uri);    StatefulRedisClusterConnection&lt;String, String&gt; connection = redisClusterClient.connect();    RedisAdvancedClusterCommands&lt;String, String&gt; commands = connection.sync();    commands.setex(&quot;name&quot;,10, &quot;throwable&quot;);    String value = commands.get(&quot;name&quot;);    log.info(&quot;Get value:&#123;&#125;&quot;, value);&#125;// Get value:throwable\n\n节点选择：\n@Testpublic void testSyncNodeSelection() &#123;    RedisURI uri = RedisURI.builder().withHost(&quot;192.168.56.200&quot;).withPort(7001).build();    RedisClusterClient redisClusterClient = RedisClusterClient.create(uri);    StatefulRedisClusterConnection&lt;String, String&gt; connection = redisClusterClient.connect();    RedisAdvancedClusterCommands&lt;String, String&gt; commands = connection.sync();//  commands.all();  // 所有节点//  commands.masters();  // 主节点    // 从节点只读    NodeSelection&lt;String, String&gt; replicas = commands.slaves();    NodeSelectionCommands&lt;String, String&gt; nodeSelectionCommands = replicas.commands();    // 这里只是演示,一般应该禁用keys *命令    Executions&lt;List&lt;String&gt;&gt; keys = nodeSelectionCommands.keys(&quot;*&quot;);    keys.forEach(key -&gt; log.info(&quot;key: &#123;&#125;&quot;, key));    connection.close();    redisClusterClient.shutdown();&#125;\n\n定时更新集群拓扑视图（每隔十分钟更新一次，这个时间自行考量，不能太频繁）：\n@Testpublic void testPeriodicClusterTopology() throws Exception &#123;    RedisURI uri = RedisURI.builder().withHost(&quot;192.168.56.200&quot;).withPort(7001).build();    RedisClusterClient redisClusterClient = RedisClusterClient.create(uri);    ClusterTopologyRefreshOptions options = ClusterTopologyRefreshOptions            .builder()            .enablePeriodicRefresh(Duration.of(10, ChronoUnit.MINUTES))            .build();    redisClusterClient.setOptions(ClusterClientOptions.builder().topologyRefreshOptions(options).build());    StatefulRedisClusterConnection&lt;String, String&gt; connection = redisClusterClient.connect();    RedisAdvancedClusterCommands&lt;String, String&gt; commands = connection.sync();    commands.setex(&quot;name&quot;, 10, &quot;throwable&quot;);    String value = commands.get(&quot;name&quot;);    log.info(&quot;Get value:&#123;&#125;&quot;, value);    Thread.sleep(Integer.MAX_VALUE);    connection.close();    redisClusterClient.shutdown();&#125;\n\n自适应更新集群拓扑视图：\n@Testpublic void testAdaptiveClusterTopology() throws Exception &#123;    RedisURI uri = RedisURI.builder().withHost(&quot;192.168.56.200&quot;).withPort(7001).build();    RedisClusterClient redisClusterClient = RedisClusterClient.create(uri);    ClusterTopologyRefreshOptions options = ClusterTopologyRefreshOptions.builder()            .enableAdaptiveRefreshTrigger(                    ClusterTopologyRefreshOptions.RefreshTrigger.MOVED_REDIRECT,                    ClusterTopologyRefreshOptions.RefreshTrigger.PERSISTENT_RECONNECTS            )            .adaptiveRefreshTriggersTimeout(Duration.of(30, ChronoUnit.SECONDS))            .build();    redisClusterClient.setOptions(ClusterClientOptions.builder().topologyRefreshOptions(options).build());    StatefulRedisClusterConnection&lt;String, String&gt; connection = redisClusterClient.connect();    RedisAdvancedClusterCommands&lt;String, String&gt; commands = connection.sync();    commands.setex(&quot;name&quot;, 10, &quot;throwable&quot;);    String value = commands.get(&quot;name&quot;);    log.info(&quot;Get value:&#123;&#125;&quot;, value);    Thread.sleep(Integer.MAX_VALUE);    connection.close();    redisClusterClient.shutdown();&#125;\n\n动态命令和自定义命令自定义命令是Redis命令有限集，不过可以更细粒度指定KEY、ARGV、命令类型、编码解码器和返回值类型，依赖于dispatch()方法：\n// 自定义实现PING方法@Testpublic void testCustomPing() throws Exception &#123;    RedisURI redisUri = RedisURI.builder()            .withHost(&quot;localhost&quot;)            .withPort(6379)            .withTimeout(Duration.of(10, ChronoUnit.SECONDS))            .build();    RedisClient redisClient = RedisClient.create(redisUri);    StatefulRedisConnection&lt;String, String&gt; connect = redisClient.connect();    RedisCommands&lt;String, String&gt; sync = connect.sync();    RedisCodec&lt;String, String&gt; codec = StringCodec.UTF8;    String result = sync.dispatch(CommandType.PING, new StatusOutput&lt;&gt;(codec));    log.info(&quot;PING:&#123;&#125;&quot;, result);    connect.close();    redisClient.shutdown();&#125;// PING:PONG// 自定义实现Set方法@Testpublic void testCustomSet() throws Exception &#123;    RedisURI redisUri = RedisURI.builder()            .withHost(&quot;localhost&quot;)            .withPort(6379)            .withTimeout(Duration.of(10, ChronoUnit.SECONDS))            .build();    RedisClient redisClient = RedisClient.create(redisUri);    StatefulRedisConnection&lt;String, String&gt; connect = redisClient.connect();    RedisCommands&lt;String, String&gt; sync = connect.sync();    RedisCodec&lt;String, String&gt; codec = StringCodec.UTF8;    sync.dispatch(CommandType.SETEX, new StatusOutput&lt;&gt;(codec),            new CommandArgs&lt;&gt;(codec).addKey(&quot;name&quot;).add(5).addValue(&quot;throwable&quot;));    String result = sync.get(&quot;name&quot;);    log.info(&quot;Get value:&#123;&#125;&quot;, result);    connect.close();    redisClient.shutdown();&#125;// Get value:throwable\n\n动态命令是基于Redis命令有限集，并且通过注解和动态代理完成一些复杂命令组合的实现。主要注解在io.lettuce.core.dynamic.annotation包路径下。简单举个例子：\npublic interface CustomCommand extends Commands &#123;    // SET [key] [value]    @Command(&quot;SET ?0 ?1&quot;)    String setKey(String key, String value);    // SET [key] [value]    @Command(&quot;SET :key :value&quot;)    String setKeyNamed(@Param(&quot;key&quot;) String key, @Param(&quot;value&quot;) String value);    // MGET [key1] [key2]    @Command(&quot;MGET ?0 ?1&quot;)    List&lt;String&gt; mGet(String key1, String key2);    /**     * 方法名作为命令     */    @CommandNaming(strategy = CommandNaming.Strategy.METHOD_NAME)    String mSet(String key1, String value1, String key2, String value2);&#125;@Testpublic void testCustomDynamicSet() throws Exception &#123;    RedisURI redisUri = RedisURI.builder()            .withHost(&quot;localhost&quot;)            .withPort(6379)            .withTimeout(Duration.of(10, ChronoUnit.SECONDS))            .build();    RedisClient redisClient = RedisClient.create(redisUri);    StatefulRedisConnection&lt;String, String&gt; connect = redisClient.connect();    RedisCommandFactory commandFactory = new RedisCommandFactory(connect);    CustomCommand commands = commandFactory.getCommands(CustomCommand.class);    commands.setKey(&quot;name&quot;, &quot;throwable&quot;);    commands.setKeyNamed(&quot;throwable&quot;, &quot;doge&quot;);    log.info(&quot;MGET ===&gt; &quot; + commands.mGet(&quot;name&quot;, &quot;throwable&quot;));    commands.mSet(&quot;key1&quot;, &quot;value1&quot;,&quot;key2&quot;, &quot;value2&quot;);    log.info(&quot;MGET ===&gt; &quot; + commands.mGet(&quot;key1&quot;, &quot;key2&quot;));    connect.close();    redisClient.shutdown();&#125;// MGET ===&gt; [throwable, doge]// MGET ===&gt; [value1, value2]\n\n高阶特性Lettuce有很多高阶使用特性，这里只列举个人认为常用的两点：\n\n配置客户端资源。\n使用连接池。\n\n更多其他特性可以自行参看官方文档。\n配置客户端资源客户端资源的设置与Lettuce的性能、并发和事件处理相关。线程池或者线程组相关配置占据客户端资源配置的大部分（EventLoopGroups和EventExecutorGroup），这些线程池或者线程组是连接程序的基础组件。一般情况下，客户端资源应该在多个Redis客户端之间共享，并且在不再使用的时候需要自行关闭。笔者认为，客户端资源是面向Netty的。注意：除非特别熟悉或者花长时间去测试调整下面提到的参数，否则在没有经验的前提下凭直觉修改默认值，有可能会踩坑。\n客户端资源接口是ClientResources，实现类是DefaultClientResources。\n构建DefaultClientResources实例：\n// 默认ClientResources resources = DefaultClientResources.create();// 建造器ClientResources resources = DefaultClientResources.builder()                        .ioThreadPoolSize(4)                        .computationThreadPoolSize(4)                        .build()\n\n使用：\nClientResources resources = DefaultClientResources.create();// 非集群RedisClient client = RedisClient.create(resources, uri);// 集群RedisClusterClient clusterClient = RedisClusterClient.create(resources, uris);// ......client.shutdown();clusterClient.shutdown();// 关闭资源resources.shutdown();\n\n客户端资源基本配置：\n\n\n\n属性\n描述\n默认值\n\n\n\nioThreadPoolSize\nI/O线程数\nRuntime.getRuntime().availableProcessors()\n\n\ncomputationThreadPoolSize\n任务线程数\nRuntime.getRuntime().availableProcessors()\n\n\n客户端资源高级配置：\n\n\n\n属性\n描述\n默认值\n\n\n\neventLoopGroupProvider\nEventLoopGroup提供商\n-\n\n\neventExecutorGroupProvider\nEventExecutorGroup提供商\n-\n\n\neventBus\n事件总线\nDefaultEventBus\n\n\ncommandLatencyCollectorOptions\n命令延时收集器配置\nDefaultCommandLatencyCollectorOptions\n\n\ncommandLatencyCollector\n命令延时收集器\nDefaultCommandLatencyCollector\n\n\ncommandLatencyPublisherOptions\n命令延时发布器配置\nDefaultEventPublisherOptions\n\n\ndnsResolver\nDNS处理器\nJDK或者Netty提供\n\n\nreconnectDelay\n重连延时配置\nDelay.exponential()\n\n\nnettyCustomizer\nNetty自定义配置器\n-\n\n\ntracing\n轨迹记录器\n-\n\n\n非集群客户端RedisClient的属性配置：\nRedis非集群客户端RedisClient本身提供了配置属性方法：\nRedisClient client = RedisClient.create(uri);client.setOptions(ClientOptions.builder()                       .autoReconnect(false)                       .pingBeforeActivateConnection(true)                       .build());\n\n非集群客户端的配置属性列表：\n\n\n\n属性\n描述\n默认值\n\n\n\npingBeforeActivateConnection\n连接激活之前是否执行PING命令\nfalse\n\n\nautoReconnect\n是否自动重连\ntrue\n\n\ncancelCommandsOnReconnectFailure\n重连失败是否拒绝命令执行\nfalse\n\n\nsuspendReconnectOnProtocolFailure\n底层协议失败是否挂起重连操作\nfalse\n\n\nrequestQueueSize\n请求队列容量\n2147483647(Integer#MAX_VALUE)\n\n\ndisconnectedBehavior\n失去连接时候的行为\nDEFAULT\n\n\nsslOptions\nSSL配置\n-\n\n\nsocketOptions\nSocket配置\n10 seconds Connection-Timeout, no keep-alive, no TCP noDelay\n\n\ntimeoutOptions\n超时配置\n-\n\n\npublishOnScheduler\n发布反应式信号数据的调度器\n使用I/O线程\n\n\n集群客户端属性配置：\nRedis集群客户端RedisClusterClient本身提供了配置属性方法：\nRedisClusterClient client = RedisClusterClient.create(uri);ClusterTopologyRefreshOptions topologyRefreshOptions = ClusterTopologyRefreshOptions.builder()                .enablePeriodicRefresh(refreshPeriod(10, TimeUnit.MINUTES))                .enableAllAdaptiveRefreshTriggers()                .build();client.setOptions(ClusterClientOptions.builder()                       .topologyRefreshOptions(topologyRefreshOptions)                       .build());\n\n集群客户端的配置属性列表：\n\n\n\n属性\n描述\n默认值\n\n\n\nenablePeriodicRefresh\n是否允许周期性更新集群拓扑视图\nfalse\n\n\nrefreshPeriod\n更新集群拓扑视图周期\n60秒\n\n\nenableAdaptiveRefreshTrigger\n设置自适应更新集群拓扑视图触发器RefreshTrigger\n-\n\n\nadaptiveRefreshTriggersTimeout\n自适应更新集群拓扑视图触发器超时设置\n30秒\n\n\nrefreshTriggersReconnectAttempts\n自适应更新集群拓扑视图触发重连次数\n5\n\n\ndynamicRefreshSources\n是否允许动态刷新拓扑资源\ntrue\n\n\ncloseStaleConnections\n是否允许关闭陈旧的连接\ntrue\n\n\nmaxRedirects\n集群重定向次数上限\n5\n\n\nvalidateClusterNodeMembership\n是否校验集群节点的成员关系\ntrue\n\n\n使用连接池引入连接池依赖commons-pool2：\n&lt;dependency&gt;    &lt;groupId&gt;org.apache.commons&lt;/groupId&gt;    &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;    &lt;version&gt;2.7.0&lt;/version&gt;&lt;/dependency\n\n基本使用如下：\n@Testpublic void testUseConnectionPool() throws Exception &#123;    RedisURI redisUri = RedisURI.builder()            .withHost(&quot;localhost&quot;)            .withPort(6379)            .withTimeout(Duration.of(10, ChronoUnit.SECONDS))            .build();    RedisClient redisClient = RedisClient.create(redisUri);    GenericObjectPoolConfig poolConfig = new GenericObjectPoolConfig();    GenericObjectPool&lt;StatefulRedisConnection&lt;String, String&gt;&gt; pool            = ConnectionPoolSupport.createGenericObjectPool(redisClient::connect, poolConfig);    try (StatefulRedisConnection&lt;String, String&gt; connection = pool.borrowObject()) &#123;        RedisCommands&lt;String, String&gt; command = connection.sync();        SetArgs setArgs = SetArgs.Builder.nx().ex(5);        command.set(&quot;name&quot;, &quot;throwable&quot;, setArgs);        String n = command.get(&quot;name&quot;);        log.info(&quot;Get value:&#123;&#125;&quot;, n);    &#125;    pool.close();    redisClient.shutdown();&#125;\n\n其中，同步连接的池化支持需要用ConnectionPoolSupport，异步连接的池化支持需要用AsyncConnectionPoolSupport（Lettuce5.1之后才支持）。\n几个常见的渐进式删除例子渐进式删除Hash中的域-属性：\n@Testpublic void testDelBigHashKey() throws Exception &#123;    // SCAN参数    ScanArgs scanArgs = ScanArgs.Builder.limit(2);    // TEMP游标    ScanCursor cursor = ScanCursor.INITIAL;    // 目标KEY    String key = &quot;BIG_HASH_KEY&quot;;    prepareHashTestData(key);    log.info(&quot;开始渐进式删除Hash的元素...&quot;);    int counter = 0;    do &#123;        MapScanCursor&lt;String, String&gt; result = COMMAND.hscan(key, cursor, scanArgs);        // 重置TEMP游标        cursor = ScanCursor.of(result.getCursor());        cursor.setFinished(result.isFinished());        Collection&lt;String&gt; fields = result.getMap().values();        if (!fields.isEmpty()) &#123;            COMMAND.hdel(key, fields.toArray(new String[0]));        &#125;        counter++;    &#125; while (!(ScanCursor.FINISHED.getCursor().equals(cursor.getCursor()) &amp;&amp; ScanCursor.FINISHED.isFinished() == cursor.isFinished()));    log.info(&quot;渐进式删除Hash的元素完毕,迭代次数:&#123;&#125; ...&quot;, counter);&#125;private void prepareHashTestData(String key) throws Exception &#123;    COMMAND.hset(key, &quot;1&quot;, &quot;1&quot;);    COMMAND.hset(key, &quot;2&quot;, &quot;2&quot;);    COMMAND.hset(key, &quot;3&quot;, &quot;3&quot;);    COMMAND.hset(key, &quot;4&quot;, &quot;4&quot;);    COMMAND.hset(key, &quot;5&quot;, &quot;5&quot;);&#125;\n\n渐进式删除集合中的元素：\n@Testpublic void testDelBigSetKey() throws Exception &#123;    String key = &quot;BIG_SET_KEY&quot;;    prepareSetTestData(key);    // SCAN参数    ScanArgs scanArgs = ScanArgs.Builder.limit(2);    // TEMP游标    ScanCursor cursor = ScanCursor.INITIAL;    log.info(&quot;开始渐进式删除Set的元素...&quot;);    int counter = 0;    do &#123;        ValueScanCursor&lt;String&gt; result = COMMAND.sscan(key, cursor, scanArgs);        // 重置TEMP游标        cursor = ScanCursor.of(result.getCursor());        cursor.setFinished(result.isFinished());        List&lt;String&gt; values = result.getValues();        if (!values.isEmpty()) &#123;            COMMAND.srem(key, values.toArray(new String[0]));        &#125;        counter++;    &#125; while (!(ScanCursor.FINISHED.getCursor().equals(cursor.getCursor()) &amp;&amp; ScanCursor.FINISHED.isFinished() == cursor.isFinished()));    log.info(&quot;渐进式删除Set的元素完毕,迭代次数:&#123;&#125; ...&quot;, counter);&#125;private void prepareSetTestData(String key) throws Exception &#123;    COMMAND.sadd(key, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;);&#125;\n\n渐进式删除有序集合中的元素：\n@Testpublic void testDelBigZSetKey() throws Exception &#123;    // SCAN参数    ScanArgs scanArgs = ScanArgs.Builder.limit(2);    // TEMP游标    ScanCursor cursor = ScanCursor.INITIAL;    // 目标KEY    String key = &quot;BIG_ZSET_KEY&quot;;    prepareZSetTestData(key);    log.info(&quot;开始渐进式删除ZSet的元素...&quot;);    int counter = 0;    do &#123;        ScoredValueScanCursor&lt;String&gt; result = COMMAND.zscan(key, cursor, scanArgs);        // 重置TEMP游标        cursor = ScanCursor.of(result.getCursor());        cursor.setFinished(result.isFinished());        List&lt;ScoredValue&lt;String&gt;&gt; scoredValues = result.getValues();        if (!scoredValues.isEmpty()) &#123;            COMMAND.zrem(key, scoredValues.stream().map(ScoredValue&lt;String&gt;::getValue).toArray(String[]::new));        &#125;        counter++;    &#125; while (!(ScanCursor.FINISHED.getCursor().equals(cursor.getCursor()) &amp;&amp; ScanCursor.FINISHED.isFinished() == cursor.isFinished()));    log.info(&quot;渐进式删除ZSet的元素完毕,迭代次数:&#123;&#125; ...&quot;, counter);&#125;private void prepareZSetTestData(String key) throws Exception &#123;    COMMAND.zadd(key, 0, &quot;1&quot;);    COMMAND.zadd(key, 0, &quot;2&quot;);    COMMAND.zadd(key, 0, &quot;3&quot;);    COMMAND.zadd(key, 0, &quot;4&quot;);    COMMAND.zadd(key, 0, &quot;5&quot;);&#125;\n\n在SpringBoot中使用Lettuce个人认为，spring-data-redis中的API封装并不是很优秀，用起来比较重，不够灵活，这里结合前面的例子和代码，在SpringBoot脚手架项目中配置和整合Lettuce。先引入依赖：\n&lt;dependencyManagement&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt;            &lt;version&gt;2.1.8.RELEASE&lt;/version&gt;            &lt;type&gt;pom&lt;/type&gt;            &lt;scope&gt;import&lt;/scope&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;    &lt;/dependency&gt;            &lt;dependency&gt;        &lt;groupId&gt;io.lettuce&lt;/groupId&gt;        &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;        &lt;version&gt;5.1.8.RELEASE&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;        &lt;artifactId&gt;lombok&lt;/artifactId&gt;        &lt;version&gt;1.18.10&lt;/version&gt;        &lt;scope&gt;provided&lt;/scope&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;        \n\n一般情况下，每个应用应该使用单个Redis客户端实例和单个连接实例，这里设计一个脚手架，适配单机、普通主从、哨兵和集群四种使用场景。对于客户端资源，采用默认的实现即可。对于Redis的连接属性，比较主要的有Host、Port和Password，其他可以暂时忽略。基于约定大于配置的原则，先定制一系列属性配置类（其实有些配置是可以完全共用，但是考虑到要清晰描述类之间的关系，这里拆分多个配置属性类和多个配置方法）：\n@Data@ConfigurationProperties(prefix = &quot;lettuce&quot;)public class LettuceProperties &#123;    private LettuceSingleProperties single;    private LettuceReplicaProperties replica;    private LettuceSentinelProperties sentinel;    private LettuceClusterProperties cluster;&#125;@Datapublic class LettuceSingleProperties &#123;    private String host;    private Integer port;    private String password;&#125;@EqualsAndHashCode(callSuper = true)@Datapublic class LettuceReplicaProperties extends LettuceSingleProperties &#123;&#125;@EqualsAndHashCode(callSuper = true)@Datapublic class LettuceSentinelProperties extends LettuceSingleProperties &#123;    private String masterId;&#125;@EqualsAndHashCode(callSuper = true)@Datapublic class LettuceClusterProperties extends LettuceSingleProperties &#123;&#125;\n\n配置类如下，主要使用@ConditionalOnProperty做隔离，一般情况下，很少有人会在一个应用使用一种以上的Redis连接场景：\n@RequiredArgsConstructor@Configuration@ConditionalOnClass(name = &quot;io.lettuce.core.RedisURI&quot;)@EnableConfigurationProperties(value = LettuceProperties.class)public class LettuceAutoConfiguration &#123;    private final LettuceProperties lettuceProperties;    @Bean(destroyMethod = &quot;shutdown&quot;)    public ClientResources clientResources() &#123;        return DefaultClientResources.create();    &#125;    @Bean    @ConditionalOnProperty(name = &quot;lettuce.single.host&quot;)    public RedisURI singleRedisUri() &#123;        LettuceSingleProperties singleProperties = lettuceProperties.getSingle();        return RedisURI.builder()                .withHost(singleProperties.getHost())                .withPort(singleProperties.getPort())                .withPassword(singleProperties.getPassword())                .build();    &#125;    @Bean(destroyMethod = &quot;shutdown&quot;)    @ConditionalOnProperty(name = &quot;lettuce.single.host&quot;)    public RedisClient singleRedisClient(ClientResources clientResources, @Qualifier(&quot;singleRedisUri&quot;) RedisURI redisUri) &#123;        return RedisClient.create(clientResources, redisUri);    &#125;    @Bean(destroyMethod = &quot;close&quot;)    @ConditionalOnProperty(name = &quot;lettuce.single.host&quot;)    public StatefulRedisConnection&lt;String, String&gt; singleRedisConnection(@Qualifier(&quot;singleRedisClient&quot;) RedisClient singleRedisClient) &#123;        return singleRedisClient.connect();    &#125;    @Bean    @ConditionalOnProperty(name = &quot;lettuce.replica.host&quot;)    public RedisURI replicaRedisUri() &#123;        LettuceReplicaProperties replicaProperties = lettuceProperties.getReplica();        return RedisURI.builder()                .withHost(replicaProperties.getHost())                .withPort(replicaProperties.getPort())                .withPassword(replicaProperties.getPassword())                .build();    &#125;    @Bean(destroyMethod = &quot;shutdown&quot;)    @ConditionalOnProperty(name = &quot;lettuce.replica.host&quot;)    public RedisClient replicaRedisClient(ClientResources clientResources, @Qualifier(&quot;replicaRedisUri&quot;) RedisURI redisUri) &#123;        return RedisClient.create(clientResources, redisUri);    &#125;    @Bean(destroyMethod = &quot;close&quot;)    @ConditionalOnProperty(name = &quot;lettuce.replica.host&quot;)    public StatefulRedisMasterSlaveConnection&lt;String, String&gt; replicaRedisConnection(@Qualifier(&quot;replicaRedisClient&quot;) RedisClient replicaRedisClient,                                                                                     @Qualifier(&quot;replicaRedisUri&quot;) RedisURI redisUri) &#123;        return MasterSlave.connect(replicaRedisClient, new Utf8StringCodec(), redisUri);    &#125;    @Bean    @ConditionalOnProperty(name = &quot;lettuce.sentinel.host&quot;)    public RedisURI sentinelRedisUri() &#123;        LettuceSentinelProperties sentinelProperties = lettuceProperties.getSentinel();        return RedisURI.builder()                .withPassword(sentinelProperties.getPassword())                .withSentinel(sentinelProperties.getHost(), sentinelProperties.getPort())                .withSentinelMasterId(sentinelProperties.getMasterId())                .build();    &#125;    @Bean(destroyMethod = &quot;shutdown&quot;)    @ConditionalOnProperty(name = &quot;lettuce.sentinel.host&quot;)    public RedisClient sentinelRedisClient(ClientResources clientResources, @Qualifier(&quot;sentinelRedisUri&quot;) RedisURI redisUri) &#123;        return RedisClient.create(clientResources, redisUri);    &#125;    @Bean(destroyMethod = &quot;close&quot;)    @ConditionalOnProperty(name = &quot;lettuce.sentinel.host&quot;)    public StatefulRedisMasterSlaveConnection&lt;String, String&gt; sentinelRedisConnection(@Qualifier(&quot;sentinelRedisClient&quot;) RedisClient sentinelRedisClient,                                                                                      @Qualifier(&quot;sentinelRedisUri&quot;) RedisURI redisUri) &#123;        return MasterSlave.connect(sentinelRedisClient, new Utf8StringCodec(), redisUri);    &#125;    @Bean    @ConditionalOnProperty(name = &quot;lettuce.cluster.host&quot;)    public RedisURI clusterRedisUri() &#123;        LettuceClusterProperties clusterProperties = lettuceProperties.getCluster();        return RedisURI.builder()                .withHost(clusterProperties.getHost())                .withPort(clusterProperties.getPort())                .withPassword(clusterProperties.getPassword())                .build();    &#125;    @Bean(destroyMethod = &quot;shutdown&quot;)    @ConditionalOnProperty(name = &quot;lettuce.cluster.host&quot;)    public RedisClusterClient redisClusterClient(ClientResources clientResources, @Qualifier(&quot;clusterRedisUri&quot;) RedisURI redisUri) &#123;        return RedisClusterClient.create(clientResources, redisUri);    &#125;    @Bean(destroyMethod = &quot;close&quot;)    @ConditionalOnProperty(name = &quot;lettuce.cluster&quot;)    public StatefulRedisClusterConnection&lt;String, String&gt; clusterConnection(RedisClusterClient clusterClient) &#123;        return clusterClient.connect();    &#125;&#125;\n\n最后为了让IDE识别我们的配置，可以添加IDE亲缘性，/META-INF文件夹下新增一个文件spring-configuration-metadata.json，内容如下：\n&#123;  &quot;properties&quot;: [    &#123;      &quot;name&quot;: &quot;lettuce.single&quot;,      &quot;type&quot;: &quot;club.throwable.spring.lettuce.LettuceSingleProperties&quot;,      &quot;description&quot;: &quot;单机配置&quot;,      &quot;sourceType&quot;: &quot;club.throwable.spring.lettuce.LettuceProperties&quot;    &#125;,    &#123;      &quot;name&quot;: &quot;lettuce.replica&quot;,      &quot;type&quot;: &quot;club.throwable.spring.lettuce.LettuceReplicaProperties&quot;,      &quot;description&quot;: &quot;主从配置&quot;,      &quot;sourceType&quot;: &quot;club.throwable.spring.lettuce.LettuceProperties&quot;    &#125;,    &#123;      &quot;name&quot;: &quot;lettuce.sentinel&quot;,      &quot;type&quot;: &quot;club.throwable.spring.lettuce.LettuceSentinelProperties&quot;,      &quot;description&quot;: &quot;哨兵配置&quot;,      &quot;sourceType&quot;: &quot;club.throwable.spring.lettuce.LettuceProperties&quot;    &#125;,    &#123;      &quot;name&quot;: &quot;lettuce.single&quot;,      &quot;type&quot;: &quot;club.throwable.spring.lettuce.LettuceClusterProperties&quot;,      &quot;description&quot;: &quot;集群配置&quot;,      &quot;sourceType&quot;: &quot;club.throwable.spring.lettuce.LettuceProperties&quot;    &#125;  ]&#125;\n\n如果想IDE亲缘性做得更好，可以添加/META-INF/additional-spring-configuration-metadata.json进行更多细节定义。简单使用如下：\n@Slf4j@Componentpublic class RedisCommandLineRunner implements CommandLineRunner &#123;    @Autowired    @Qualifier(&quot;singleRedisConnection&quot;)    private StatefulRedisConnection&lt;String, String&gt; connection;    @Override    public void run(String... args) throws Exception &#123;        RedisCommands&lt;String, String&gt; redisCommands = connection.sync();        redisCommands.setex(&quot;name&quot;, 5, &quot;throwable&quot;);        log.info(&quot;Get value:&#123;&#125;&quot;, redisCommands.get(&quot;name&quot;));    &#125;&#125;// Get value:throwable\n\n小结本文算是基于Lettuce的官方文档，对它的使用进行全方位的分析，包括主要功能、配置都做了一些示例，限于篇幅部分特性和配置细节没有分析。Lettuce已经被spring-data-redis接纳作为官方的Redis客户端驱动，所以值得信赖，它的一些API设计确实比较合理，扩展性高的同时灵活性也高。个人建议，基于Lettuce包自行添加配置到SpringBoot应用用起来会得心应手，毕竟RedisTemplate实在太笨重，而且还屏蔽了Lettuce一些高级特性和灵活的API。\n参考资料：\n\nLettuce Reference Guide\n\n原文链接：https://www.cnblogs.com/throwable/p/11601538.html\n","categories":["运维","Redis"]},{"title":"快速部署 Redis","url":"/posts/9379/","content":"一、Docker 部署 Redis1. 单机 Redisdocker run -d -p 6379:6379 --name redis-s redis:latest redis-server --appendonly no --requirepass abc123\n\n\nappendonly：是否开启持久化\nrequirepass：密码\n\n2. 集群 Redisdocker run -d -e &quot;IP=0.0.0.0&quot; -e &quot;INITIAL_PORT=7000&quot; -p 7000-7005:7000-7005 --name redis grokzen/redis-cluster\n\nIP=0.0.0.0 设置的是集群中 Redis 的 IP，0.0.0.0 表示为 127.0.0.1。若配置错误会导致客户端在读取拓扑图时无法访问到各个redis节点。\n二、K8S 部署 Redis1. 集群 Rediscat &lt;&lt;EOF | kubectl -f -kind: ServiceapiVersion: v1metadata:  name: redis-clusterspec:  type: NodePort  ports:  - name: redis1    port: 7000    nodePort: 31070    targetPort: 7000    protocol: TCP  - name: redis2    port: 7001    nodePort: 31071    targetPort: 7001    protocol: TCP  - name: redis3    port: 7002    nodePort: 31072    targetPort: 7002    protocol: TCP  - name: redis4    port: 7003    nodePort: 31073    targetPort: 7003    protocol: TCP  - name: redis5    port: 7004    nodePort: 31074    targetPort: 7004    protocol: TCP  - name: redis6    port: 7005    nodePort: 31075    targetPort: 7005    protocol: TCP  selector:    name: redis-cluster---apiVersion: apps/v1kind: Deploymentmetadata:  name: redis-clusterspec:  replicas: 1  selector:    matchLabels:      name: redis-cluster  template:    metadata:      labels:        name: redis-cluster    spec:      containers:      - name: redis-cluster        image: grokzen/redis-cluster        imagePullPolicy: IfNotPresent        ports:        - containerPort: 7000          name: redis1          protocol: TCP        - containerPort: 7001          name: redis2          protocol: TCP        - containerPort: 7002          name: redis3          protocol: TCP        - containerPort: 7003          name: redis4          protocol: TCP        - containerPort: 7004          name: redis5          protocol: TCP        - containerPort: 7005          name: redis6          protocol: TCPEOF\n\n","categories":["运维","Redis"]},{"title":"快速部署 Zookeeper","url":"/posts/29635/","content":"Docker 部署 Zookeeperdocker run -d -e TZ=&quot;Asia/Shanghai&quot; -p 2181:2181 --name zk zookeeper\n\n","categories":["运维","Zookeeper"]},{"title":"Kafka 和 RabbitMQ 之间的区别","url":"/posts/51051/","content":"作为一个有丰富经验的微服务系统架构师，经常有人问我，“应该选择 RabbitMQ 还是 Kafka？”。基于某些原因， 许多开发者会把这两种技术当做等价的来看待。的确，在一些案例场景下选择 RabbitMQ 还是 Kafka 没什么差别，但是这两种技术在底层实现方面是有许多差异的。\n不同的场景需要不同的解决方案，选错一个方案能够严重的影响你对软件的设计，开发和维护的能力。\n这篇文章会先介绍 RabbitMQ 和 Apache Kafka 内部实现的相关概念。紧接着会主要介绍这两种技术的主要不同点以及他们各自的优缺点，最后我们会说明一下怎样选择这两种技术。\n一、异步消息模式异步消息可以作为解耦消息的生产和处理的一种解决方案。提到消息系统，我们通常会想到两种主要的消息模式——消息队列和发布/订阅模式。\n1、消息队列利用消息队列可以解耦生产者和消费者。多个生产者可以向同一个消息队列发送消息；但是，一个消息在被一个消息者处理的时候，这个消息在队列上会被锁住或者被移除并且其他消费者无法处理该消息。也就是说一个具体的消息只能由一个消费者消费。\n\n需要额外注意的是，如果消费者处理一个消息失败了，消息系统一般会把这个消息放回队列，这样其他消费者可以继续处理。消息队列除了提供解耦功能之外，它还能够对生产者和消费者进行独立的伸缩（scale），以及提供对错误处理的容错能力。\n2、发布/订阅\n例如，一个系统中产生的事件可以通过这种模式让发布者通知所有订阅者。在许多队列系统中常常用主题（topics）这个术语指代发布/订阅模式。在 RabbitMQ 中，主题就是发布/订阅模式的一种具体实现（更准确点说是交换器（exchange）的一种），但是在这篇文章中，我会把主题和发布/订阅当做等价来看待。\n一般来说，订阅有两种类型：\n1）临时（ephemeral）订阅，这种订阅只有在消费者启动并且运行的时候才存在。一旦消费者退出，相应的订阅以及尚未处理的消息就会丢失。\n2）持久（durable）订阅，这种订阅会一直存在，除非主动去删除。消费者退出后，消息系统会继续维护该订阅，并且后续消息可以被继续处理。\n二、RabbitMQ1、队列RabbitMQ 支持典型的开箱即用的消息队列。开发者可以定义一个命名队列，然后发布者可以向这个命名队列中发送消息。最后消费者可以通过这个命名队列获取待处理的消息。\n2、消息交换器RabbitMQ 使用消息交换器来实现发布/订阅模式。发布者可以把消息发布到消息交换器上而不用知道这些消息都有哪些订阅者。\n每一个订阅了交换器的消费者都会创建一个队列；然后消息交换器会把生产的消息放入队列以供消费者消费。消息交换器也可以基于各种路由规则为一些订阅者过滤消息。\n\n需要重点注意的是 RabbitMQ 支持临时和持久两种订阅类型。消费者可以调用 RabbitMQ 的 API 来选择他们想要的订阅类型。\n根据 RabbitMQ 的架构设计，我们也可以创建一种混合方法——订阅者以组队的方式然后在组内以竞争关系作为消费者去处理某个具体队列上的消息，这种由订阅者构成的组我们称为消费者组。按照这种方式，我们实现了发布/订阅模式，同时也能够很好的伸缩（scale-up）订阅者去处理收到的消息。\n\n三、Apache KafkaApache Kafka 不是消息中间件的一种实现。相反，它只是一种分布式流式系统。\n不同于基于队列和交换器的 RabbitMQ，Kafka 的存储层是使用分区事务日志来实现的。Kafka 也提供流式 API 用于实时的流处理以及连接器 API 用来更容易的和各种数据源集成；当然，这些已经超出了本篇文章的讨论范围。\n云厂商为 Kafka 存储层提供了可选的方案，比如 Azure Event Hubsy 以及 AWS Kinesis Data Streams 等。对于 Kafka 流式处理能力，还有一些特定的云方案和开源方案，不过，话说回来，它们也超出了本篇的范围。\n1、主题Kafka 没有实现队列这种东西。相应的，Kafka 按照类别存储记录集，并且把这种类别称为主题。\nKafka 为每个主题维护一个消息分区日志。每个分区都是由有序的不可变的记录序列组成，并且消息都是连续的被追加在尾部。\n当消息到达时，Kafka 就会把他们追加到分区尾部。默认情况下，Kafka 使用轮询分区器（partitioner）把消息一致的分配到多个分区上。\nKafka 可以改变创建消息逻辑流的行为。例如，在一个多租户的应用中，我们可以根据每个消息中的租户 ID 创建消息流。IoT 场景中，我们可以在常数级别下根据生产者的身份信息（identity）将其映射到一个具体的分区上。确保来自相同逻辑流上的消息映射到相同分区上，这就保证了消息能够按照顺序提供给消费者。\n\n消费者通过维护分区的偏移（或者说索引）来顺序的读出消息，然后消费消息。\n单个消费者可以消费多个不同的主题，并且消费者的数量可以伸缩到可获取的最大分区数量。\n所以在创建主题的时候，我们要认真的考虑一下在创建的主题上预期的消息吞吐量。消费同一个主题的多个消费者构成的组称为消费者组。通过 Kafka 提供的 API 可以处理同一消费者组中多个消费者之间的分区平衡以及消费者当前分区偏移的存储。\n\n2、Kafka 实现的消息模式Kafka 的实现很好地契合发布/订阅模式。\n生产者可以向一个具体的主题发送消息，然后多个消费者组可以消费相同的消息。每一个消费者组都可以独立的伸缩去处理相应的负载。由于消费者维护自己的分区偏移，所以他们可以选择持久订阅或者临时订阅，持久订阅在重启之后不会丢失偏移而临时订阅在重启之后会丢失偏移并且每次重启之后都会从分区中最新的记录开始读取。\n但是这种实现方案不能完全等价的当做典型的消息队列模式看待。当然，我们可以创建一个主题，这个主题和拥有一个消费者的消费组进行关联，这样我们就模拟出了一个典型的消息队列。不过这会有许多缺点，我们会在第二部分详细讨论。\n值得特别注意的是，Kafka 是按照预先配置好的时间保留分区中的消息，而不是根据消费者是否消费了这些消息。这种保留机制可以让消费者自由的重读之前的消息。另外，开发者也可以利用 Kafka 的存储层来实现诸如事件溯源和日志审计功能。\n尽管有时候 RabbitMQ 和 Kafka 可以当做等价来看，但是他们的实现是非常不同的。所以我们不能把他们当做同种类的工具来看待；一个是消息中间件，另一个是分布式流式系统。\n作为解决方案架构师，我们要能够认识到它们之间的差异并且尽可能的考虑在给定场景中使用哪种类型的解决方案。下面会指出这些差异并且提供什么时候使用哪种方案的指导建议。\n四、RabbitMQ 和 Kafka 的显著差异RabbitMQ 是一个消息代理，但是 Apache Kafka 是一个分布式流式系统。好像从语义上就可以看出差异，但是它们内部的一些特性会影响到我们是否能够很好的设计各种用例。\n例如，Kafka 最适用于数据的流式处理，但是 RabbitMQ 对流式中的消息就很难保持它们的顺序。\n另一方面，RabbitMQ 内置重试逻辑和死信（dead-letter）交换器，但是 Kafka 只是把这些实现逻辑交给用户来处理。\n这部分主要强调在不同系统之间它们的主要差异。\n1、消息顺序对于发送到队列或者交换器上的消息，RabbitMQ 不保证它们的顺序。尽管消费者按照顺序处理生产者发来的消息看上去很符合逻辑，但是这有很大误导性。\nRabbitMQ 文档中有关于消息顺序保证的说明：\n“发布到一个通道（channel）上的消息，用一个交换器和一个队列以及一个出口通道来传递，那么最终会按照它们发送的顺序接收到。” \n——RabbitMQ 代理语义（Broker Semantics）\n换话句话说，只要我们是单个消费者，那么接收到的消息就是有序的。然而，一旦有多个消费者从同一个队列中读取消息，那么消息的处理顺序就没法保证了。\n由于消费者读取消息之后可能会把消息放回（或者重传）到队列中（例如，处理失败的情况），这样就会导致消息的顺序无法保证。\n一旦一个消息被重新放回队列，另一个消费者可以继续处理它，即使这个消费者已经处理到了放回消息之后的消息。因此，消费者组处理消息是无序的，如下表所示：\n\n当然，我们可以通过限制消费者的并发数等于 1 来保证 RabbitMQ 中的消息有序性。更准确点说，限制单个消费者中的线程数为 1，因为任何的并行消息处理都会导致无序问题。\n不过，随着系统规模增长，单线程消费者模式会严重影响消息处理能力。所以，我们不要轻易的选择这种方案。\n另一方面，对于 Kafka 来说，它在消息处理方面提供了可靠的顺序保证。Kafka 能够保证发送到相同主题分区的所有消息都能够按照顺序处理。\n在前面说过，默认情况下，Kafka 会使用循环分区器（round-robin partitioner）把消息放到相应的分区上。不过，生产者可以给每个消息设置分区键（key）来创建数据逻辑流（比如来自同一个设备的消息，或者属于同一租户的消息）。\n所有来自相同流的消息都会被放到相同的分区中，这样消费者组就可以按照顺序处理它们。\n但是，我们也应该注意到，在同一个消费者组中，每个分区都是由一个消费者的一个线程来处理。结果就是我们没法伸缩（scale）单个分区的处理能力。\n不过，在 Kafka 中，我们可以伸缩一个主题中的分区数量，这样可以让每个分区分担更少的消息，然后增加更多的消费者来处理额外的分区。\n获胜者（Winner）：显而易见，Kafka 是获胜者，因为它可以保证按顺序处理消息。RabbitMQ 在这块就相对比较弱。\n2、消息路由RabbitMQ 可以基于定义的订阅者路由规则路由消息给一个消息交换器上的订阅者。一个主题交换器可以通过一个叫做 routing_key 的特定头来路由消息。\n或者，一个头部（headers）交换器可以基于任意的消息头来路由消息。这两种交换器都能够有效地让消费者设置他们感兴趣的消息类型，因此可以给解决方案架构师提供很好的灵活性。\n另一方面，Kafka 在处理消息之前是不允许消费者过滤一个主题中的消息。一个订阅的消费者在没有异常情况下会接受一个分区中的所有消息。\n作为一个开发者，你可能使用 Kafka 流式作业（job），它会从主题中读取消息，然后过滤，最后再把过滤的消息推送到另一个消费者可以订阅的主题。但是，这需要更多的工作量和维护，并且还涉及到更多的移动操作。\n获胜者：在消息路由和过滤方面，RabbitMQ 提供了更好的支持。\n3、消息时序（timing）在测定发送到一个队列的消息时间方面，RabbitMQ 提供了多种能力：\n1）消息存活时间（TTL）发送到 RabbitMQ 的每条消息都可以关联一个 TTL 属性。发布者可以直接设置 TTL 或者根据队列的策略来设置。\n系统可以根据设置的 TTL 来限制消息的有效期。如果消费者在预期时间内没有处理该消息，那么这条消息会自动的从队列上被移除（并且会被移到死信交换器上，同时在这之后的消息都会这样处理）。\nTTL 对于那些有时效性的命令特别有用，因为一段时间内没有处理的话，这些命令就没有什么意义了。\n2）延迟/预定的消息RabbitMQ 可以通过插件的方式来支持延迟或者预定的消息。当这个插件在消息交换器上启用的时候，生产者可以发送消息到 RabbitMQ 上，然后这个生产者可以延迟 RabbitMQ 路由这个消息到消费者队列的时间。\n这个功能允许开发者调度将来（future）的命令，也就是在那之前不应该被处理的命令。例如，当生产者遇到限流规则时，我们可能会把这些特定的命令延迟到之后的一个时间执行。\nKafka 没有提供这些功能。它在消息到达的时候就把它们写入分区中，这样消费者就可以立即获取到消息去处理。\nKafka 也没用为消息提供 TTL 的机制，不过我们可以在应用层实现。\n不过，我们必须要记住的一点是 Kafka 分区是一种追加模式的事务日志。所以，它是不能处理消息时间（或者分区中的位置）。\n获胜者：毫无疑问，RabbitMQ 是获胜者，因为这种实现天然的就限制 Kafka。\n4、消息留存（retention）当消费者成功消费消息之后，RabbitMQ 就会把对应的消息从存储中删除。这种行为没法修改。它几乎是所有消息代理设计的必备部分。\n相反，Kafka 会给每个主题配置超时时间，只要没有达到超时时间的消息都会保留下来。在消息留存方面，Kafka 仅仅把它当做消息日志来看待，并不关心消费者的消费状态。\n消费者可以不限次数的消费每条消息，并且他们可以操作分区偏移来“及时”往返的处理这些消息。Kafka 会周期的检查分区中消息的留存时间，一旦消息超过设定保留的时长，就会被删除。\nKafka 的性能不依赖于存储大小。所以，理论上，它存储消息几乎不会影响性能（只要你的节点有足够多的空间保存这些分区）。\n获胜者：Kafka 设计之初就是保存消息的，但是 RabbitMQ 并不是。所以这块没有可比性，Kafka 是获胜者。\n5、容错处理当处理消息，队列和事件时，开发者常常认为消息处理总是成功的。毕竟，生产者把每条消息放入队列或者主题后，即使消费者处理消息失败了，它仅仅需要做的就是重新尝试，直到成功为止。\n尽管表面上看这种方法是没错的，但是我们应该对这种处理方式多思考一下。首先我们应该承认，在某些场景下，消息处理会失败。所以，即使在解决方案部分需要人为干预的情况下，我们也要妥善地处理这些情况。\n消息处理存在两种可能的故障：\n1）瞬时故障——故障产生是由于临时问题导致，比如网络连接，CPU 负载，或者服务崩溃。我们可以通过一遍又一遍的尝试来减轻这种故障。\n2）持久故障——故障产生是由于永久的问题导致的，并且这种问题不能通过额外的重试来解决。比如常见的原因有软件 bug 或者无效的消息格式（例如，损坏（poison）的消息）。\n作为架构师和开发者，我们应该问问自己：“对于消息处理故障，我们应该重试多少次？每一次重试之间我们应该等多久？我们怎样区分瞬时和持久故障？”\n最重要的是：“所有重试都失败后或者遇到一个持久的故障，我们要做什么？”\n当然，不同业务领域有不同的回答，消息系统一般会给我们提供工具让我们自己实现解决方案。\nRabbitMQ 会给我们提供诸如交付重试和死信交换器（DLX）来处理消息处理故障。\nDLX 的主要思路是根据合适的配置信息自动地把路由失败的消息发送到 DLX，并且在交换器上根据规则来进一步的处理，比如异常重试，重试计数以及发送到“人为干预”的队列。\n查看下面篇文章，它在 RabbitMQ 处理重试上提供了额外的可能模式视角。\n\n链接：https://engineering.nanit.com/rabbitmq-retries-the-full-story-ca4cc6c5b493\n\n在 RabbitMQ 中我们需要记住最重要的事情是当一个消费者正在处理或者重试某个消息时（即使是在把它返回队列之前），其他消费者都可以并发的处理这个消息之后的其他消息。\n当某个消费者在重试处理某条消息时，作为一个整体的消息处理逻辑不会被阻塞。所以，一个消费者可以同步地去重试处理一条消息，不管花费多长时间都不会影响整个系统的运行。\n\n消费者 1 持续的在重试处理消息 1，同时其他消费者可以继续处理其他消息\n和 RabbitMQ 相反，Kafka 没有提供这种开箱即用的机制。在 Kafka 中，需要我们自己在应用层提供和实现消息重试机制。\n另外，我们需要注意的是当一个消费者正在同步地处理一个特定的消息时，那么同在这个分区上的其他消息是没法被处理的。\n由于消费者不能改变消息的顺序，所以我们不能够拒绝和重试一个特定的消息以及提交一个在这个消息之后的消息。你只要记住，分区仅仅是一个追加模式的日志。\n一个应用层解决方案可以把失败的消息提交到一个“重试主题”，并且从那个主题中处理重试；但是这样的话我们就会丢失消息的顺序。\n我们可以在 Uber.com 上找到 Uber 工程师实现的一个例子。如果消息处理的时延不是关注点，那么对错误有足够监控的 Kafka 方案可能就足够了。\n如果消费者阻塞在重试一个消息上，那么底部分区的消息就不会被处理\n获胜者：RabbitMQ 是获胜者，因为它提供了一个解决这个问题的开箱即用的机制。\n6、伸缩有多个基准测试，用于检查 RabbitMQ 和 Kafka 的性能。\n尽管通用的基准测试对一些特定的情况会有限制，但是 Kafka 通常被认为比 RabbitMQ 有更优越的性能。\nKafka 使用顺序磁盘 I / O 来提高性能。\n从 Kafka 使用分区的架构上看，它在横向扩展上会优于 RabbitMQ，当然 RabbitMQ 在纵向扩展上会有更多的优势。\nKafka 的大规模部署通常每秒可以处理数十万条消息，甚至每秒百万级别的消息。\n过去，Pivotal 记录了一个 Kafka 集群每秒处理一百万条消息的例子；但是，它是在一个有着 30 个节点集群上做的，并且这些消息负载被优化分散到多个队列和交换器上。\n\n链接：https://content.pivotal.io/blog/rabbitmq-hits-one-million-messages-per-second-on-google-compute-engine\n\n典型的 RabbitMQ 部署包含 3 到 7 个节点的集群，并且这些集群也不需要把负载分散到不同的队列上。这些典型的集群通常可以预期每秒处理几万条消息。\n获胜者：尽管这两个消息平台都可以处理大规模负载，但是 Kafka 在伸缩方面更优并且能够获得比 RabbitMQ 更高的吞吐量，因此这局 Kafka 获胜。\n但是，值得注意的是大部分系统都还没有达到这些极限！所以，除非你正在构建下一个非常受欢迎的百万级用户软件系统，否则你不需要太关心伸缩性问题，毕竟这两个消息平台都可以工作的很好。\n7、消费者复杂度RabbitMQ 使用的是智能代理和傻瓜式消费者模式。消费者注册到消费者队列，然后 RabbitMQ 把传进来的消息推送给消费者。RabbitMQ 也有拉取（pull）API；不过，一般很少被使用。\nRabbitMQ 管理消息的分发以及队列上消息的移除（也可能转移到 DLX）。消费者不需要考虑这块。\n根据 RabbitMQ 结构的设计，当负载增加的时候，一个队列上的消费者组可以有效的从仅仅一个消费者扩展到多个消费者，并且不需要对系统做任何的改变。\n\n相反，Kafka 使用的是傻瓜式代理和智能消费者模式。消费者组中的消费者需要协调他们之间的主题分区租约（以便一个具体的分区只由消费者组中一个消费者监听）。\n消费者也需要去管理和存储他们分区偏移索引。幸运的是 Kafka SDK 已经为我们封装了，所以我们不需要自己管理。\n另外，当我们有一个低负载时，单个消费者需要处理并且并行的管理多个分区，这在消费者端会消耗更多的资源。\n当然，随着负载增加，我们只需要伸缩消费者组使其消费者的数量等于主题中分区的数量。这就需要我们配置 Kafka 增加额外的分区。\n但是，随着负载再次降低，我们不能移除我们之前增加的分区，这需要给消费者增加更多的工作量。尽管这样，但是正如我们上面提到过，Kafka SDK 已经帮我们做了这个额外的工作。\n\nKafka 分区没法移除，向下伸缩后消费者会做更多的工作\n获胜者：根据设计，RabbitMQ 就是为了傻瓜式消费者而构建的。所以这轮 RabbitMQ 获胜。\n五、如何选择？现在我们就如面对百万美元问题一样：“什么时候使用 RabbitMQ 以及什么时候使用 Kafka？”概括上面的差异，我们不难得出下面的结论。\n优先选择 RabbitMQ 的条件：\n高级灵活的路由规则；\n消息时序控制（控制消息过期或者消息延迟）；\n高级的容错处理能力，在消费者更有可能处理消息不成功的情景中（瞬时或者持久）；\n更简单的消费者实现。\n\n优先选择 Kafka 的条件：\n严格的消息顺序；\n延长消息留存时间，包括过去消息重放的可能；\n传统解决方案无法满足的高伸缩能力。\n\n大部分情况下这两个消息平台都可以满足我们的要求。但是，它取决于我们的架构师，他们会选择最合适的工具。当做决策的时候，我们需要考虑上面着重强调的功能性差异和非功能性限制。\n这些限制如下：\n\n当前开发者对这两个消息平台的了解；\n托管云解决方案的可用性（如果适用）；\n每种解决方案的运营成本；\n适用于我们目标栈的 SDK 的可用性。\n\n当开发复杂的软件系统时，我们可能被诱导使用同一个消息平台去实现所有必须的消息用例。但是，从我的经验看，通常同时使用这两个消息平台能够带来更多的好处。\n例如，在一个事件驱动的架构系统中，我们可以使用 RabbitMQ 在服务之间发送命令，并且使用 Kafka 实现业务事件通知。\n原因是事件通知常常用于事件溯源，批量操作（ETL 风格），或者审计目的，因此 Kafka 的消息留存能力就显得很有价值。\n相反，命令一般需要在消费者端做额外处理，并且处理可以失败，所以需要高级的容错处理能力。\n这里，RabbitMQ 在功能上有很多闪光点。以后我可能会写一篇详细的文章来介绍，但是你必须记住–你的里程（mileage）可能会变化，因为适合性取决于你的特定需求。\n六、总结思想写这篇文章是由于我观察到许多开发者把这 RabbitMQ 和 Kafka 作为等价来看待。我希望通过这篇文章的帮助能够让你获得对这两种技术实现的深刻理解以及它们之间的技术差异。\n反过来通过它们之间的差异来影响这两个平台去给用例提供更好的服务。这两个消息平台都很棒，并且都能够给多个用例提供很好的服务。\n但是，作为解决方案架构师，取决于我们对每一个用例需求的理解，以及优化，然后选择最合适的解决方案。\n","categories":["运维","消息队列"]},{"title":"HTTP 基础问题","url":"/posts/20440/","content":"一、HTTP 基本概念HTTP 是什么？HTTP 是超文本传输协议，也就是HyperText Transfer Protocol。\n\n能否详细解释「超文本传输协议」？\n\nHTTP 的名字「超文本协议传输」，它可以拆成三个部分：\n\n超文本\n传输\n协议\n\n\n1. 「协议」\n在生活中，我们也能随处可见「协议」，例如：\n\n刚毕业时会签一个「三方协议」；\n找房子时会签一个「租房协议」；\n\n\n生活中的协议，本质上与计算机中的协议是相同的，协议的特点:\n\n「协」字，代表的意思是必须有两个以上的参与者。例如三方协议里的参与者有三个：你、公司、学校三个；租房协议里的参与者有两个：你和房东。\n「议」字，代表的意思是对参与者的一种行为约定和规范。例如三方协议里规定试用期期限、毁约金等；租房协议里规定租期期限、每月租金金额、违约如何处理等。\n\n针对 HTTP 协议，我们可以这么理解。\nHTTP 是一个用在计算机世界里的协议。它使用计算机能够理解的语言确立了一种计算机之间交流通信的规范（两个以上的参与者），以及相关的各种控制和错误处理方式（行为约定和规范）。\n2. 「传输」\n所谓的「传输」，很好理解，就是把一堆东西从 A 点搬到 B 点，或者从 B 点 搬到 A 点。\n别轻视了这个简单的动作，它至少包含两项重要的信息。\nHTTP 协议是一个双向协议。\n我们在上网冲浪时，浏览器是请求方 A，百度网站就是应答方 B。双方约定用 HTTP 协议来通信，于是浏览器把请求数据发送给网站，网站再把一些数据返回给浏览器，最后由浏览器渲染在屏幕，就可以看到图片、视频了。\n\n数据虽然是在 A 和 B 之间传输，但允许中间有中转或接力。\n就好像第一排的同学想传递纸条给最后一排的同学，那么传递的过程中就需要经过好多个同学（中间人），这样的传输方式就从「A &lt; — &gt; B」，变成了「A &lt;-&gt; N &lt;-&gt; M &lt;-&gt; B」。\n而在 HTTP 里，需要中间人遵从 HTTP 协议，只要不打扰基本的数据传输，就可以添加任意额外的东西。\n针对传输，我们可以进一步理解了 HTTP。\nHTTP 是一个在计算机世界里专门用来在两点之间传输数据的约定和规范。\n3. 「超文本」\nHTTP 传输的内容是「超文本」。\n我们先来理解「文本」，在互联网早期的时候只是简单的字符文字，但现在「文本」的涵义已经可以扩展为图片、视频、压缩包等，在 HTTP 眼里这些都算作「文本」。\n再来理解「超文本」，它就是超越了普通文本的文本，它是文字、图片、视频等的混合体，最关键有超链接，能从一个超文本跳转到另外一个超文本。\nHTML 就是最常见的超文本了，它本身只是纯文字文件，但内部用很多标签定义了图片、视频等的链接，再经过浏览器的解释，呈现给我们的就是一个文字、有画面的网页了。\nOK，经过了对 HTTP 里这三个名词的详细解释，就可以给出比「超文本传输协议」这七个字更准确更有技术含量的答案：\nHTTP 是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」。\n\n那「HTTP 是用于从互联网服务器传输超文本到本地浏览器的协议」，这种说法正确吗？\n\n这种说法是不正确的。因为也可以是「服务器&lt; – &gt;服务器」，所以采用两点之间的描述会更准确。\nHTTP 常见的状态码有哪些？\n1xx 类状态码属于提示信息，是协议处理中的一种中间状态，实际用到的比较少。\n2xx 类状态码表示服务器成功处理了客户端的请求，也是我们最愿意看到的状态。\n\n「200 OK」是最常见的成功状态码，表示一切正常。如果是非 HEAD 请求，服务器返回的响应头都会有 body 数据。\n「204 No Content」也是常见的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。\n「206 Partial Content」是应用于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，而是其中的一部分，也是服务器处理成功的状态。\n\n3xx 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是重定向。\n\n「301 Moved Permanently」表示永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问。\n「302 Found」表示临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问。\n\n301 和 302 都会在响应头里使用字段 Location，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。\n\n「304 Not Modified」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲文件，也称缓存重定向，也就是告诉客户端可以继续使用缓存资源，用于缓存控制。\n\n4xx 类状态码表示客户端发送的报文有误，服务器无法处理，也就是错误码的含义。\n\n「400 Bad Request」表示客户端请求的报文有错误，但只是个笼统的错误。\n「403 Forbidden」表示服务器禁止访问资源，并不是客户端的请求出错。\n「404 Not Found」表示请求的资源在服务器上不存在或未找到，所以无法提供给客户端。\n\n5xx 类状态码表示客户端请求报文正确，但是服务器处理时内部发生了错误，属于服务器端的错误码。\n\n「500 Internal Server Error」与 400 类型，是个笼统通用的错误码，服务器发生了什么错误，我们并不知道。\n「501 Not Implemented」表示客户端请求的功能还不支持，类似“即将开业，敬请期待”的意思。\n「502 Bad Gateway」通常是服务器作为网关或代理时返回的错误码，表示服务器自身工作正常，访问后端服务器发生了错误。\n「503 Service Unavailable」表示服务器当前很忙，暂时无法响应客户端，类似“网络服务正忙，请稍后重试”的意思。\n\nHTTP 常见字段有哪些？Host 字段\n客户端发送请求时，用来指定服务器的域名。\n\nHost: www.A.com\n\n有了 Host 字段，就可以将请求发往「同一台」服务器上的不同网站。\nContent-Length 字段\n服务器在返回数据时，会有 Content-Length 字段，表明本次回应的数据长度。\n\nContent-Length: 1000\n\n如上面则是告诉浏览器，本次服务器回应的数据长度是 1000 个字节，后面的字节就属于下一个回应了。\n大家应该都知道 HTTP 是基于 TCP 传输协议进行通信的，而使用了 TCP 传输协议，就会存在一个“粘包”的问题，HTTP 协议通过设置回车符、换行符作为 HTTP header 的边界，通过 Content-Length 字段作为 HTTP body 的边界，这两个方式都是为了解决“粘包”的问题。具体什么是 TCP 粘包，可以看这篇文章：如何理解是 TCP 面向字节流协议？(opens new window)\nConnection 字段\nConnection 字段最常用于客户端要求服务器使用「HTTP 长连接」机制，以便其他请求复用。\n\nHTTP 长连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\n\nHTTP/1.1 版本的默认连接都是长连接，但为了兼容老版本的 HTTP，需要指定 Connection 首部字段的值为 Keep-Alive。\nConnection: Keep-Alive\n\n开启了 HTTP Keep-Alive 机制后， 连接就不会中断，而是保持连接。当客户端发送另一个请求时，它会使用同一个连接，一直持续到客户端或服务器端提出断开连接。\nPS：大家不要把 HTTP Keep-Alive 和 TCP Keepalive 搞混了，这两个虽然长的像，但是不是一个东西，具体可以看我这篇文章：TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？(opens new window)\nContent-Type 字段\nContent-Type 字段用于服务器回应时，告诉客户端，本次数据是什么格式。\n\nContent-Type: text/html; Charset=utf-8\n\n上面的类型表明，发送的是网页，而且编码是UTF-8。\n客户端请求的时候，可以使用 Accept 字段声明自己可以接受哪些数据格式。\nAccept: */*\n\n上面代码中，客户端声明自己可以接受任何格式的数据。\nContent-Encoding 字段\nContent-Encoding 字段说明数据的压缩方法。表示服务器返回的数据使用了什么压缩格式\n\nContent-Encoding: gzip\n\n上面表示服务器返回的数据采用了 gzip 方式压缩，告知客户端需要用此方式解压。\n客户端在请求时，用 Accept-Encoding 字段说明自己可以接受哪些压缩方法。\nAccept-Encoding: gzip, deflate\n\n\n二、GET 与 POSTGET 和 POST 有什么区别？根据 RFC 规范，GET 的语义是从服务器获取指定的资源，这个资源可以是静态的文本、页面、图片视频等。GET 请求的参数位置一般是写在 URL 中，URL 规定只能支持 ASCII，所以 GET 请求的参数只允许 ASCII 字符 ，而且浏览器会对 URL 的长度有限制（HTTP协议本身对 URL长度并没有做任何规定）。\n比如，你打开我的文章，浏览器就会发送 GET 请求给服务器，服务器就会返回文章的所有文字及资源。\n\n根据 RFC 规范，POST 的语义是根据请求负荷（报文body）对指定的资源做出处理，具体的处理方式视资源类型而不同。POST 请求携带数据的位置一般是写在报文 body 中，body 中的数据可以是任意格式的数据，只要客户端与服务端协商好即可，而且浏览器不会对 body 大小做限制。\n比如，你在我文章底部，敲入了留言后点击「提交」（暗示你们留言），浏览器就会执行一次 POST 请求，把你的留言文字放进了报文 body 里，然后拼接好 POST 请求头，通过 TCP 协议发送给服务器。\n\nGET 和 POST 方法都是安全和幂等的吗？先说明下安全和幂等的概念：\n\n在 HTTP 协议里，所谓的「安全」是指请求方法不会「破坏」服务器上的资源。\n所谓的「幂等」，意思是多次执行相同的操作，结果都是「相同」的。\n\n如果从 RFC 规范定义的语义来看：\n\nGET 方法就是安全且幂等的，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。所以，可以对 GET 请求的数据做缓存，这个缓存可以做到浏览器本身上（彻底避免浏览器发请求），也可以做到代理上（如nginx），而且在浏览器中 GET 请求可以保存为书签。\nPOST 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是不安全的，且多次提交数据就会创建多个资源，所以不是幂等的。所以，浏览器一般不会缓存 POST 请求，也不能把 POST 请求保存为书签。\n\n做个简要的小结。\nGET 的语义是请求获取指定的资源。GET 方法是安全、幂等、可被缓存的。\nPOST 的语义是根据请求负荷（报文主体）对指定的资源做出处理，具体的处理方式视资源类型而不同。POST 不安全，不幂等，（大部分实现）不可缓存。\n注意， 上面是从 RFC 规范定义的语义来分析的。\n但是实际过程中，开发者不一定会按照 RFC 规范定义的语义来实现 GET 和 POST 方法。比如：\n\n可以用 GET 方法实现新增或删除数据的请求，这样实现的 GET 方法自然就不是安全和幂等。\n可以用 POST 方法实现查询数据的请求，这样实现的 POST 方法自然就是安全和幂等。\n\n曾经有个笑话，有人写了个博客，删除博客用的是 GET 请求，他觉得没人访问就连鉴权都没做。然后 Google 服务器爬虫爬了一遍，他所有博文就没了。。。\n如果「安全」放入概念是指信息是否会被泄漏的话，虽然 POST 用 body 传输数据，而 GET 用 URL 传输，这样数据会在浏览器地址拦容易看到，但是并不能说 GET 不如 POST 安全的。\n因为 HTTP 传输的内容都是明文的，虽然在浏览器地址拦看不到 POST 提交的 body 数据，但是只要抓个包就都能看到了。\n所以，要避免传输过程中数据被窃取，就要使用 HTTPS 协议，这样所有 HTTP 的数据都会被加密传输。\n\nGET 请求可以带 body 吗？\n\nRFC 规范并没有规定 GET 请求不能带 body 的。理论上，任何请求都可以带 body 的。只是因为 RFC 规范定义的 GET 请求是获取资源，所以根据这个语义不需要用到 body。\n另外，URL 中的查询参数也不是 GET 所独有的，POST 请求的 URL 中也可以有参数的。\n三、HTTP 缓存技术HTTP 缓存有哪些实现方式？对于一些具有重复性的 HTTP 请求，比如每次请求得到的数据都一样的，我们可以把这对「请求-响应」的数据都缓存在本地，那么下次就直接读取本地的数据，不必在通过网络获取服务器的响应了，这样的话 HTTP/1.1 的性能肯定肉眼可见的提升。\n所以，避免发送 HTTP 请求的方法就是通过缓存技术，HTTP 设计者早在之前就考虑到了这点，因此 HTTP 协议的头部有不少是针对缓存的字段。\nHTTP 缓存有两种实现方式，分别是强制缓存和协商缓存。\n什么是强制缓存？强缓存指的是只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边。\n如下图中，返回的是 200 状态码，但在 size 项中标识的是 from disk cache，就是使用了强制缓存。\n\n强缓存是利用下面这两个 HTTP 响应头部（Response Header）字段实现的，它们都用来表示资源在客户端缓存的有效期：\n\nCache-Control， 是一个相对时间；\nExpires，是一个绝对时间；\n\n如果 HTTP 响应头部同时有 Cache-Control 和 Expires 字段的话，Cache-Control 的优先级高于 Expires 。\nCache-control 选项更多一些，设置更加精细，所以建议使用 Cache-Control 来实现强缓存。具体的实现流程如下：\n\n当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 Cache-Control，Cache-Control 中设置了过期时间大小；\n浏览器再次请求访问服务器中的该资源时，会先通过请求资源的时间与 Cache-Control 中设置的过期时间大小，来计算出该资源是否过期，如果没有，则使用该缓存，否则重新请求服务器；\n服务器再次收到请求后，会再次更新 Response 头部的 Cache-Control。\n\n什么是协商缓存？当我们在浏览器使用开发者工具的时候，你可能会看到过某些请求的响应码是 304，这个是告诉浏览器可以使用本地缓存的资源，通常这种通过服务端告知客户端是否可以使用缓存的方式被称为协商缓存。\n\n上图就是一个协商缓存的过程，所以协商缓存就是与服务端协商之后，通过协商结果来判断是否使用本地缓存。\n协商缓存可以基于两种头部来实现。\n第一种：请求头部中的 If-Modified-Since 字段与响应头部中的 Last-Modified 字段实现，这两个字段的意思是：\n\n响应头部中的 Last-Modified：标示这个响应资源的最后修改时间；\n请求头部中的 If-Modified-Since：当资源过期了，发现响应头中具有 Last-Modified 声明，则再次发起请求的时候带上 Last-Modified 的时间，服务器收到请求后发现有 If-Modified-Since 则与被请求资源的最后修改时间进行对比（Last-Modified），如果最后修改时间较新（大），说明资源又被改过，则返回最新资源，HTTP 200 OK；如果最后修改时间较旧（小），说明资源无新修改，响应 HTTP 304 走缓存。\n\n第二种：请求头部中的 If-None-Match 字段与响应头部中的 ETag 字段，这两个字段的意思是：\n\n响应头部中 Etag：唯一标识响应资源；\n请求头部中的 If-None-Match：当资源过期时，浏览器发现响应头里有 Etag，则再次向服务器发起请求时，会将请求头 If-None-Match 值设置为 Etag 的值。服务器收到请求后进行比对，如果资源没有变化返回 304，如果资源变化了返回 200。\n\n第一种实现方式是基于时间实现的，第二种实现方式是基于一个唯一标识实现的，相对来说后者可以更加准确地判断文件内容是否被修改，避免由于时间篡改导致的不可靠问题。\n如果在第一次请求资源的时候，服务端返回的 HTTP 响应头部同时有 Etag 和 Last-Modified 字段，那么客户端再下一次请求的时候，如果带上了 ETag 和 Last-Modified 字段信息给服务端，这时 Etag 的优先级更高，也就是服务端先会判断 Etag 是否变化了，如果 Etag 有变化就不用在判断 Last-Modified 了，如果 Etag 没有变化，然后再看 Last-Modified。\n为什么 ETag 的优先级更高？这是因为 ETag 主要能解决 Last-Modified 几个比较难以解决的问题：\n\n在没有修改文件内容情况下文件的最后修改时间可能也会改变，这会导致客户端认为这文件被改动了，从而重新请求；\n可能有些文件是在秒级以内修改的，If-Modified-Since 能检查到的粒度是秒级的，使用 Etag就能够保证这种需求下客户端在 1 秒内能刷新多次；\n有些服务器不能精确获取文件的最后修改时间。\n\n注意，协商缓存这两个字段都需要配合强制缓存中 Cache-Control 字段来使用，只有在未能命中强制缓存的时候，才能发起带有协商缓存字段的请求。\n下图是强制缓存和协商缓存的工作流程：\n\n当使用 ETag 字段实现的协商缓存的过程：\n\n当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 ETag 唯一标识，这个唯一标识的值是根据当前请求的资源生成的；\n\n当浏览器再次请求访问服务器中的该资源时，首先会先检查强制缓存是否过期：\n\n如果没有过期，则直接使用本地缓存；\n如果缓存过期了，会在 Request 头部加上 If-None-Match 字段，该字段的值就是 ETag 唯一标识；\n\n\n服务器再次收到请求后，\n会根据请求中的 If-None-Match 值与当前请求的资源生成的唯一标识进行比较\n：\n\n如果值相等，则返回 304 Not Modified，不会返回资源；\n如果不相等，则返回 200 状态码和返回资源，并在 Response 头部加上新的 ETag 唯一标识；\n\n\n如果浏览器收到 304 的请求响应状态码，则会从本地缓存中加载资源，否则更新资源。\n\n\n四、HTTP 特性到目前为止，HTTP 常见到版本有 HTTP/1.1，HTTP/2.0，HTTP/3.0，不同版本的 HTTP 特性是不一样的。\n这里先用 HTTP/1.1 版本给大家介绍，其他版本的后续也会介绍。\nHTTP/1.1 的优点有哪些？HTTP 最突出的优点是「简单、灵活和易于扩展、应用广泛和跨平台」。\n1. 简单\nHTTP 基本的报文格式就是 header + body，头部信息也是 key-value 简单文本的形式，易于理解，降低了学习和使用的门槛。\n2. 灵活和易于扩展\nHTTP 协议里的各类请求方法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员自定义和扩充。\n同时 HTTP 由于是工作在应用层（ OSI 第七层），则它下层可以随意变化，比如：\n\nHTTPS 就是在 HTTP 与 TCP 层之间增加了 SSL/TLS 安全传输层；\nHTTP/1.1 和 HTTP/2.0 传输协议使用的是 TCP 协议，而到了 HTTP/3.0 传输协议改用了 UDP 协议。\n\n3. 应用广泛和跨平台\n互联网发展至今，HTTP 的应用范围非常的广泛，从台式机的浏览器到手机上的各种 APP，从看新闻、刷贴吧到购物、理财、吃鸡，HTTP 的应用遍地开花，同时天然具有跨平台的优越性。\nHTTP/1.1 的缺点有哪些？HTTP 协议里有优缺点一体的双刃剑，分别是「无状态、明文传输」，同时还有一大缺点「不安全」。\n1. 无状态双刃剑\n无状态的好处，因为服务器不会去记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务。\n无状态的坏处，既然服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦。\n例如登录-&gt;添加购物车-&gt;下单-&gt;结算-&gt;支付，这系列操作都要知道用户的身份才行。但服务器不知道这些请求是有关联的，每次都要问一遍身份信息。\n这样每操作一次，都要验证信息，这样的购物体验还能愉快吗？别问，问就是酸爽！\n对于无状态的问题，解法方案有很多种，其中比较简单的方式用 Cookie 技术。\nCookie 通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。\n相当于，在客户端第一次请求后，服务器会下发一个装有客户信息的「小贴纸」，后续客户端请求服务器的时候，带上「小贴纸」，服务器就能认得了了，\n\n2. 明文传输双刃剑\n明文意味着在传输过程中的信息，是可方便阅读的，比如 Wireshark 抓包都可以直接肉眼查看，为我们调试工作带了极大的便利性。\n但是这正是这样，HTTP 的所有信息都暴露在了光天化日下，相当于信息裸奔。在传输的漫长的过程中，信息的内容都毫无隐私可言，很容易就能被窃取，如果里面有你的账号密码信息，那你号没了。\n3. 不安全\nHTTP 比较严重的缺点就是不安全：\n\n通信使用明文（不加密），内容可能会被窃听。比如，账号信息容易泄漏，那你号没了。\n不验证通信方的身份，因此有可能遭遇伪装。比如，访问假的淘宝、拼多多，那你钱没了。\n无法证明报文的完整性，所以有可能已遭篡改。比如，网页上植入垃圾广告，视觉污染，眼没了。\n\nHTTP 的安全问题，可以用 HTTPS 的方式解决，也就是通过引入 SSL/TLS 层，使得在安全上达到了极致。\nHTTP/1.1 的性能如何？HTTP 协议是基于 TCP/IP，并且使用了「请求 - 应答」的通信模式，所以性能的关键就在这两点里。\n1. 长连接\n早期 HTTP/1.0 性能上的一个很大的问题，那就是每发起一个请求，都要新建一次 TCP 连接（三次握手），而且是串行请求，做了无谓的 TCP 连接建立和断开，增加了通信开销。\n为了解决上述 TCP 连接问题，HTTP/1.1 提出了长连接的通信方式，也叫持久连接。这种方式的好处在于减少了 TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载。\n持久连接的特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。\n\n当然，如果某个 HTTP 长连接超过一定时间没有任何数据交互，服务端就会主动断开这个连接。\n2. 管道网络传输\nHTTP/1.1 采用了长连接的方式，这使得管道（pipeline）网络传输成为了可能。\n即可在同一个 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。\n举例来说，客户端需要请求两个资源。以前的做法是，在同一个 TCP 连接里面，先发送 A 请求，然后等待服务器做出回应，收到后再发出 B 请求。那么，管道机制则是允许浏览器同时发出 A 请求和 B 请求，如下图：\n\n但是服务器必须按照接收请求的顺序发送对这些管道化请求的响应。\n如果服务端在处理 A 请求时耗时比较长，那么后续的请求的处理都会被阻塞住，这称为「队头堵塞」。\n所以，HTTP/1.1 管道解决了请求的队头阻塞，但是没有解决响应的队头阻塞。\nTIP\n注意!!!\n实际上 HTTP/1.1 管道化技术不是默认开启，而且浏览器基本都没有支持，所以后面所有文章讨论 HTTP/1.1 都是建立在没有使用管道化的前提。大家知道有这个功能，但是没有被使用就行了。\n3. 队头阻塞\n「请求 - 应答」的模式会造成 HTTP 的性能问题。为什么呢？\n因为当顺序发送的请求序列中的一个请求因为某种原因被阻塞时，在后面排队的所有请求也一同被阻塞了，会招致客户端一直请求不到数据，这也就是「队头阻塞」，好比上班的路上塞车。\n\n总之 HTTP/1.1 的性能一般般，后续的 HTTP/2 和 HTTP/3 就是在优化 HTTP 的性能。\n五、HTTP 与 HTTPSHTTP 与 HTTPS 有哪些区别？\nHTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。\nHTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。\n两者的默认端口不一样，HTTP 默认端口号是 80，HTTPS 默认端口号是 443。\nHTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。\n\nHTTPS 解决了 HTTP 的哪些问题？HTTP 由于是明文传输，所以安全上存在以下三个风险：\n\n窃听风险，比如通信链路上可以获取通信内容，用户号容易没。\n篡改风险，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。\n冒充风险，比如冒充淘宝网站，用户钱容易没。\n\n\nHTTPS 在 HTTP 与 TCP 层之间加入了 SSL/TLS 协议，可以很好的解决了上述的风险：\n\n信息加密：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。\n校验机制：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。\n身份证书：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没。\n\n可见，只要自身不做「恶」，SSL/TLS 协议是能保证通信是安全的。\n\nHTTPS 是如何解决上面的三个风险的？\n\n\n混合加密的方式实现信息的机密性，解决了窃听的风险。\n摘要算法的方式来实现完整性，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。\n将服务器公钥放入到数字证书中，解决了冒充的风险。\n\n1. 混合加密\n通过混合加密的方式可以保证信息的机密性，解决了窃听的风险。\n\nHTTPS 采用的是对称加密和非对称加密结合的「混合加密」方式：\n\n在通信建立前采用非对称加密的方式交换「会话秘钥」，后续就不再使用非对称加密。\n在通信过程中全部使用对称加密的「会话秘钥」的方式加密明文数据。\n\n采用「混合加密」的方式的原因：\n\n对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。\n非对称加密使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。\n\n2. 摘要算法 + 数字签名\n为了保证传输的内容不被篡改，我们需要对内容计算出一个「指纹」，然后同内容一起传输给对方。\n对方收到后，先是对内容也计算出一个「指纹」，然后跟发送方发送的「指纹」做一个比较，如果「指纹」相同，说明内容没有被篡改，否则就可以判断出内容被篡改了。\n那么，在计算机里会用摘要算法（哈希函数）来计算出内容的哈希值，也就是内容的「指纹」，这个哈希值是唯一的，且无法通过哈希值推导出内容。\n\n通过哈希算法可以确保内容不会被篡改，但是并不能保证「内容 + 哈希值」不会被中间人替换，因为这里缺少对客户端收到的消息是否来源于服务端的证明。\n举个例子，你想向老师请假，一般来说是要求由家长写一份请假理由并签名，老师才能允许你请假。\n但是你有模仿你爸爸字迹的能力，你用你爸爸的字迹写了一份请假理由然后签上你爸爸的名字，老师一看到这个请假条，查看字迹和签名，就误以为是你爸爸写的，就会允许你请假。\n那作为老师，要如何避免这种情况发生呢？现实生活中的，可以通过电话或视频来确认是否是由父母发出的请假，但是计算机里可没有这种操作。\n那为了避免这种情况，计算机里会用非对称加密算法来解决，共有两个密钥：\n\n一个是公钥，这个是可以公开给所有人的；\n一个是私钥，这个必须由本人管理，不可泄露。\n\n这两个密钥可以双向加解密的，比如可以用公钥加密内容，然后用私钥解密，也可以用私钥加密内容，公钥解密内容。\n流程的不同，意味着目的也不相同：\n\n公钥加密，私钥解密。这个目的是为了保证内容传输的安全，因为被公钥加密的内容，其他人是无法解密的，只有持有私钥的人，才能解密出实际的内容；\n私钥加密，公钥解密。这个目的是为了保证消息不会被冒充，因为私钥是不可泄露的，如果公钥能正常解密出私钥加密的内容，就能证明这个消息是来源于持有私钥身份的人发送的。\n\n一般我们不会用非对称加密来加密实际的传输内容，因为非对称加密的计算比较耗费性能的。\n所以非对称加密的用途主要在于通过「私钥加密，公钥解密」的方式，来确认消息的身份，我们常说的数字签名算法，就是用的是这种方式，不过私钥加密内容不是内容本身，而是对内容的哈希值加密。\n\n私钥是由服务端保管，然后服务端会向客户端颁发对应的公钥。如果客户端收到的信息，能被公钥解密，就说明该消息是由服务器发送的。\n引入了数字签名算法后，你就无法模仿你爸爸的字迹来请假了，你爸爸手上持有着私钥，你老师持有着公钥。\n这样只有用你爸爸手上的私钥才对请假条进行「签名」，老师通过公钥看能不能解出这个「签名」，如果能解出并且确认内容的完整性，就能证明是由你爸爸发起的请假条，这样老师才允许你请假，否则老师就不认。\n3. 数字证书\n前面我们知道：\n\n可以通过哈希算法来保证消息的完整性；\n可以通过数字签名来保证消息的来源可靠性（能确认消息是由持有私钥的一方发送的）；\n\n但是这还远远不够，还缺少身份验证的环节，万一公钥是被伪造的呢？\n还是拿请假的例子，虽然你爸爸持有私钥，老师通过是否能用公钥解密来确认这个请假条是不是来源你父亲的。\n但是我们还可以自己伪造出一对公私钥啊！\n你找了个夜晚，偷偷把老师桌面上和你爸爸配对的公钥，换成了你的公钥，那么下次你在请假的时候，你继续模仿你爸爸的字迹写了个请假条，然后用你的私钥做个了「数字签名」。\n但是老师并不知道自己的公钥被你替换过了，所以他还是按照往常一样用公钥解密，由于这个公钥和你的私钥是配对的，老师当然能用这个被替换的公钥解密出来，并且确认了内容的完整性，于是老师就会以为是你父亲写的请假条，又允许你请假了。\n好家伙，为了一个请假，真的是斗智斗勇。\n后面你的老师和父亲发现了你伪造公私钥的事情后，决定重新商量一个对策来应对你这个臭家伙。\n正所谓魔高一丈，道高一尺。\n既然伪造公私钥那么随意，所以你爸把他的公钥注册到警察局，警察局用他们自己的私钥对你父亲的公钥做了个数字签名，然后把你爸爸的「个人信息 + 公钥 + 数字签名」打包成一个数字证书，也就是说这个数字证书包含你爸爸的公钥。\n这样，你爸爸如果因为家里确实有事要向老师帮你请假的时候，不仅会用自己的私钥对内容进行签名，还会把数字证书给到老师。\n老师拿到了数字证书后，首先会去警察局验证这个数字证书是否合法，因为数字证书里有警察局的数字签名，警察局要验证证书合法性的时候，用自己的公钥解密，如果能解密成功，就说明这个数字证书是在警察局注册过的，就认为该数字证书是合法的，然后就会把数字证书里头的公钥（你爸爸的）给到老师。\n由于通过警察局验证了数字证书是合法的，那么就能证明这个公钥就是你父亲的，于是老师就可以安心的用这个公钥解密出请假条，如果能解密出，就证明是你爸爸写的请假条。\n正是通过了一个权威的机构来证明你爸爸的身份，所以你的伪造公私钥这个小伎俩就没用了。\n在计算机里，这个权威的机构就是 CA （数字证书认证机构），将服务器公钥放在数字证书（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的。\n数字证书的工作流程，我也画了一张图，方便大家理解：\n\n通过数字证书的方式保证服务器公钥的身份，解决冒充的风险。\nHTTPS 是如何建立连接的？其间交互了什么？SSL/TLS 协议基本流程：\n\n客户端向服务器索要并验证服务器的公钥。\n双方协商生产「会话秘钥」。\n双方采用「会话秘钥」进行加密通信。\n\n前两步也就是 SSL/TLS 的建立过程，也就是 TLS 握手阶段。\nTLS 的「握手阶段」涉及四次通信，使用不同的密钥交换算法，TLS 握手流程也会不一样的，现在常用的密钥交换算法有两种：RSA 算法 (opens new window)和 ECDHE 算法 (opens new window)。\n基于 RSA 算法的 TLS 握手过程比较容易理解，所以这里先用这个给大家展示 TLS 握手过程，如下图：\n\nTLS 协议建立的详细流程：\n1. ClientHello\n首先，由客户端向服务器发起加密通信请求，也就是 ClientHello 请求。\n在这一步，客户端主要向服务器发送以下信息：\n（1）客户端支持的 TLS 协议版本，如 TLS 1.2 版本。\n（2）客户端生产的随机数（Client Random），后面用于生成「会话秘钥」条件之一。\n（3）客户端支持的密码套件列表，如 RSA 加密算法。\n2. SeverHello\n服务器收到客户端请求后，向客户端发出响应，也就是 SeverHello。服务器回应的内容有如下内容：\n（1）确认 TLS 协议版本，如果浏览器不支持，则关闭加密通信。\n（2）服务器生产的随机数（Server Random），也是后面用于生产「会话秘钥」条件之一。\n（3）确认的密码套件列表，如 RSA 加密算法。\n（4）服务器的数字证书。\n3.客户端回应\n客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。\n如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送如下信息：\n（1）一个随机数（pre-master key）。该随机数会被服务器公钥加密。\n（2）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。\n（3）客户端握手结束通知，表示客户端的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验。\n上面第一项的随机数是整个握手阶段的第三个随机数，会发给服务端，所以这个随机数客户端和服务端都是一样的。\n服务器和客户端有了这三个随机数（Client Random、Server Random、pre-master key），接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」。\n4. 服务器的最后回应\n服务器收到客户端的第三个随机数（pre-master key）之后，通过协商的加密算法，计算出本次通信的「会话秘钥」。\n然后，向客户端发送最后的信息：\n（1）加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信。\n（2）服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。\n至此，整个 TLS 的握手阶段全部结束。接下来，客户端与服务器进入加密通信，就完全是使用普通的 HTTP 协议，只不过用「会话秘钥」加密内容。\nTIP\n如果想深入学习基于 RSA 算法的 HTTPS 握手过程，可以看这篇，我通过抓包的方式，逐步分析每一个过程：HTTPS RSA 握手解析(opens new window)\n不过，基于 RSA 算法的 HTTPS 存在「前向安全」的问题：如果服务端的私钥泄漏了，过去被第三方截获的所有 TLS 通讯密文都会被破解。\n为了解决这个问题，后面就出现了 ECDHE 密钥协商算法，我们现在大多数网站使用的正是 ECDHE 密钥协商算法，关于 ECDHE 握手的过程可以看这篇文章：HTTPS ECDHE 握手解析(opens new window)\n\n客户端校验数字证书的流程是怎样的？\n\n接下来，详细说一下实际中数字证书签发和验证流程。\n如下图图所示，为数字证书签发和验证流程：\n\nCA 签发证书的过程，如上图左边部分：\n\n首先 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行 Hash 计算，得到一个 Hash 值；\n然后 CA 会使用自己的私钥将该 Hash 值加密，生成 Certificate Signature，也就是 CA 对证书做了签名；\n最后将 Certificate Signature 添加在文件证书上，形成数字证书；\n\n客户端校验服务端的数字证书的过程，如上图右边部分：\n\n首先客户端会使用同样的 Hash 算法获取该证书的 Hash 值 H1；\n通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使用 CA 的公钥解密 Certificate Signature 内容，得到一个 Hash 值 H2 ；\n最后比较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。\n\n但事实上，证书的验证过程中还存在一个证书信任链的问题，因为我们向 CA 申请的证书一般不是根证书签发的，而是由中间证书签发的，比如百度的证书，从下图你可以看到，证书的层级有三级：\n\n对于这种三级层级关系的证书的验证过程如下：\n\n客户端收到 baidu.com 的证书后，发现这个证书的签发者不是根证书，就无法根据本地已有的根证书中的公钥去验证 baidu.com 证书是否可信。于是，客户端根据 baidu.com 证书中的签发者，找到该证书的颁发机构是 “GlobalSign Organization Validation CA - SHA256 - G2”，然后向 CA 请求该中间证书。\n请求到证书后发现 “GlobalSign Organization Validation CA - SHA256 - G2” 证书是由 “GlobalSign Root CA” 签发的，由于 “GlobalSign Root CA” 没有再上级签发机构，说明它是根证书，也就是自签证书。应用软件会检查此证书有否已预载于根证书清单上，如果有，则可以利用根证书中的公钥去验证 “GlobalSign Organization Validation CA - SHA256 - G2” 证书，如果发现验证通过，就认为该中间证书是可信的。\n“GlobalSign Organization Validation CA - SHA256 - G2” 证书被信任后，可以使用 “GlobalSign Organization Validation CA - SHA256 - G2” 证书中的公钥去验证 baidu.com 证书的可信性，如果验证通过，就可以信任 baidu.com 证书。\n\n在这四个步骤中，最开始客户端只信任根证书 GlobalSign Root CA 证书的，然后 “GlobalSign Root CA” 证书信任 “GlobalSign Organization Validation CA - SHA256 - G2” 证书，而 “GlobalSign Organization Validation CA - SHA256 - G2” 证书又信任 baidu.com 证书，于是客户端也信任 baidu.com 证书。\n总括来说，由于用户信任 GlobalSign，所以由 GlobalSign 所担保的 baidu.com 可以被信任，另外由于用户信任操作系统或浏览器的软件商，所以由软件商预载了根证书的 GlobalSign 都可被信任。\n\n操作系统里一般都会内置一些根证书，比如我的 MAC 电脑里内置的根证书有这么多：\n\n这样的一层层地验证就构成了一条信任链路，整个证书信任链验证流程如下图所示：\n\n最后一个问题，为什么需要证书链这么麻烦的流程？Root CA 为什么不直接颁发证书，而是要搞那么多中间层级呢？\n这是为了确保根证书的绝对安全性，将根证书隔离地越严格越好，不然根证书如果失守了，那么整个信任链都会有问题。\nHTTPS 的应用数据是如何保证完整性的？TLS 在实现上分为握手协议和记录协议两层：\n\nTLS 握手协议就是我们前面说的 TLS 四次握手的过程，负责协商加密算法和生成对称密钥，后续用此密钥来保护应用程序数据（即 HTTP 数据）；\nTLS 记录协议负责保护应用程序数据并验证其完整性和来源，所以对 HTTP 数据加密是使用记录协议；\n\nTLS 记录协议主要负责消息（HTTP 数据）的压缩，加密及数据的认证，过程如下图：\n\n具体过程如下：\n\n首先，消息被分割成多个较短的片段,然后分别对每个片段进行压缩。\n接下来，经过压缩的片段会被加上消息认证码（MAC 值，这个是通过哈希算法生成的），这是为了保证完整性，并进行数据的认证。通过附加消息认证码的 MAC 值，可以识别出篡改。与此同时，为了防止重放攻击，在计算消息认证码时，还加上了片段的编码。\n再接下来，经过压缩的片段再加上消息认证码会一起通过对称密码进行加密。\n最后，上述经过加密的数据再加上由数据类型、版本号、压缩后的长度组成的报头就是最终的报文数据。\n\n记录协议完成后，最终的报文数据将传递到传输控制协议 (TCP) 层进行传输。\n如果你想详细了解记录协议是如何分片、压缩、计算 MAC 值、分组加密，可以看这篇：理解SSL/TLS系列 (四) 记录协议(opens new window)\nHTTPS 一定安全可靠吗？之前有读者在字节面试的时候，被问到：HTTPS 一定安全可靠吗？\n\n这个问题的场景是这样的：客户端通过浏览器向服务端发起 HTTPS 请求时，被「假基站」转发到了一个「中间人服务器」，于是客户端是和「中间人服务器」完成了 TLS 握手，然后这个「中间人服务器」再与真正的服务端完成 TLS 握手。\n\n具体过程如下：\n\n客户端向服务端发起 HTTPS 建立连接请求时，然后被「假基站」转发到了一个「中间人服务器」，接着中间人向服务端发起 HTTPS 建立连接请求，此时客户端与中间人进行 TLS 握手，中间人与服务端进行 TLS 握手；\n在客户端与中间人进行 TLS 握手过程中，中间人会发送自己的公钥证书给客户端，客户端验证证书的真伪，然后从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给中间人，中间人使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（A），后续客户端与中间人通信就用这个对称加密密钥来加密数据了。\n在中间人与服务端进行 TLS 握手过程中，服务端会发送从 CA 机构签发的公钥证书给中间人，从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给服务端，服务端使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（B），后续中间人与服务端通信就用这个对称加密密钥来加密数据了。\n后续的通信过程中，中间人用对称加密密钥（A）解密客户端的 HTTPS 请求的数据，然后用对称加密密钥（B）加密 HTTPS 请求后，转发给服务端，接着服务端发送 HTTPS 响应数据给中间人，中间人用对称加密密钥（B）解密 HTTPS 响应数据，然后再用对称加密密钥（A）加密后，转发给客户端。\n\n从客户端的角度看，其实并不知道网络中存在中间人服务器这个角色。那么中间人就可以解开浏览器发起的 HTTPS 请求里的数据，也可以解开服务端响应给浏览器的 HTTPS 响应数据。相当于，中间人能够 “偷看” 浏览器与服务端之间的 HTTPS 请求和响应的数据。\n但是要发生这种场景是有前提的，前提是用户点击接受了中间人服务器的证书。\n中间人服务器与客户端在 TLS 握手过程中，实际上发送了自己伪造的证书给浏览器，而这个伪造的证书是能被浏览器（客户端）识别出是非法的，于是就会提醒用户该证书存在问题。\n\n如果用户执意点击「继续浏览此网站」，相当于用户接受了中间人伪造的证书，那么后续整个 HTTPS 通信都能被中间人监听了。\n所以，这其实并不能说 HTTPS 不够安全，毕竟浏览器都已经提示证书有问题了，如果用户坚决要访问，那不能怪 HTTPS ，得怪自己手贱。\n另外，如果你的电脑中毒了，被恶意导入了中间人的根证书，那么在验证中间人的证书的时候，由于你操作系统信任了中间人的根证书，那么等同于中间人的证书是合法的，这种情况下，浏览器是不会弹出证书存在问题的风险提醒的。\n这其实也不关 HTTPS 的事情，是你电脑中毒了才导致 HTTPS 数据被中间人劫持的。\n所以，HTTPS 协议本身到目前为止还是没有任何漏洞的，即使你成功进行中间人攻击，本质上是利用了客户端的漏洞（用户点击继续访问或者被恶意导入伪造的根证书），并不是 HTTPS 不够安全。\n\n为什么抓包工具能截取 HTTPS 数据？\n\n很多抓包工具 之所以可以明文看到 HTTPS 数据，工作原理与中间人一致的。\n对于 HTTPS 连接来说，中间人要满足以下两点，才能实现真正的明文代理:\n\n中间人，作为客户端与真实服务端建立连接这一步不会有问题，因为服务端不会校验客户端的身份；\n中间人，作为服务端与真实客户端建立连接，这里会有客户端信任服务端的问题，也就是服务端必须有对应域名的私钥；\n\n中间人要拿到私钥只能通过如下方式：\n\n去网站服务端拿到私钥；\n去CA处拿域名签发私钥；\n自己签发证书，切要被浏览器信任；\n\n不用解释，抓包工具只能使用第三种方式取得中间人的身份。\n使用抓包工具进行 HTTPS 抓包的时候，需要在客户端安装 Fiddler 的根证书，这里实际上起认证中心（CA）的作用。\n抓包工具能够抓包的关键是客户端会往系统受信任的根证书列表中导入抓包工具生成的证书，而这个证书会被浏览器信任，也就是抓包工具给自己创建了一个认证中心 CA，客户端拿着中间人签发的证书去中间人自己的 CA 去认证，当然认为这个证书是有效的。\n\n如何避免被中间人抓取数据？\n\n我们要保证自己电脑的安全，不要被病毒乘虚而入，而且也不要点击任何证书非法的网站，这样 HTTPS 数据就不会被中间人截取到了。\n当然，我们还可以通过 HTTPS 双向认证来避免这种问题。\n一般我们的 HTTPS 是单向认证，客户端只会验证了服务端的身份，但是服务端并不会验证客户端的身份。\n\n如果用了双向认证方式，不仅客户端会验证服务端的身份，而且服务端也会验证客户端的身份。服务端一旦验证到请求自己的客户端为不可信任的，服务端就拒绝继续通信，客户端如果发现服务端为不可信任的，那么也中止通信。\n六、HTTP/1.1、HTTP/2、HTTP/3 演变HTTP/1.1 相比 HTTP/1.0 提高了什么性能？HTTP/1.1 相比 HTTP/1.0 性能上的改进：\n\n使用长连接的方式改善了 HTTP/1.0 短连接造成的性能开销。\n支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。\n\n但 HTTP/1.1 还是有性能瓶颈：\n\n请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 Body 的部分；\n发送冗长的首部。每次互相发送相同的首部造成的浪费较多；\n服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞；\n没有请求优先级控制；\n请求只能从客户端开始，服务器只能被动响应。\n\nHTTP/2 做了什么优化？HTTP/2 协议是基于 HTTPS 的，所以 HTTP/2 的安全性也是有保障的。\n\n那 HTTP/2 相比 HTTP/1.1 性能上的改进：\n\n头部压缩\n二进制格式\n并发传输\n服务器主动推送资源\n\n1. 头部压缩\nHTTP/2 会压缩头（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你消除重复的部分。\n这就是所谓的 HPACK 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。\n2. 二进制格式\nHTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了二进制格式，头信息和数据体都是二进制，并且统称为帧（frame）：头信息帧（Headers Frame）和数据帧（Data Frame）。\n\n这样虽然对人不友好，但是对计算机非常友好，因为计算机只懂二进制，那么收到报文后，无需再将明文的报文转成二进制，而是直接解析二进制报文，这增加了数据传输的效率。\n比如状态码 200 ，在 HTTP/1.1 是用 ‘2’’0’’0’ 三个字符来表示（二进制：00110010 00110000 00110000），共用了 3 个字节，如下图\n\n在 HTTP/2 对于状态码 200 的二进制编码是 10001000，只用了 1 字节就能表示，相比于 HTTP/1.1 节省了 2 个字节，如下图：\n\nHeader: :status: 200 OK 的编码内容为：1000 1000，那么表达的含义是什么呢？\n\n\n最前面的 1 标识该 Header 是静态表中已经存在的 KV。（至于什么是静态表，可以看这篇：HTTP/2 牛逼在哪？ (opens new window)）\n在静态表里，“:status: 200 ok” 静态表编码是 8，二进制即是 1000。\n\n因此，整体加起来就是 1000 1000。\n3. 并发传输\n我们都知道 HTTP/1.1 的实现是基于请求-响应模型的。同一个连接中，HTTP 完成一个事务（请求与响应），才能处理下一个事务，也就是说在发出请求等待响应的过程中，是没办法做其他事情的，如果响应迟迟不来，那么后续的请求是无法发送的，也造成了队头阻塞的问题。\n而 HTTP/2 就很牛逼了，引出了 Stream 概念，多个 Stream 复用在一条 TCP 连接。\n\n从上图可以看到，1 个 TCP 连接包含多个 Stream，Stream 里可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构成。Message 里包含一条或者多个 Frame，Frame 是 HTTP/2 最小单位，以二进制压缩格式存放 HTTP/1 中的内容（头部和包体）。\n针对不同的 HTTP 请求用独一无二的 Stream ID 来区分，接收端可以通过 Stream ID 有序组装成 HTTP 消息，不同 Stream 的帧是可以乱序发送的，因此可以并发不同的 Stream ，也就是 HTTP/2 可以并行交错地发送请求和响应。\n比如下图，服务端并行交错地发送了两个响应： Stream 1 和 Stream 3，这两个 Stream 都是跑在一个 TCP 连接上，客户端收到后，会根据相同的 Stream ID 有序组装成 HTTP 消息。\n\n4、服务器推送\nHTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务端不再是被动地响应，可以主动向客户端发送消息。\n客户端和服务器双方都可以建立 Stream， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。\n比如下图，Stream 1 是客户端向服务端请求的资源，属于客户端建立的 Stream，所以该 Stream 的 ID 是奇数（数字 1）；Stream 2 和 4 都是服务端主动向客户端推送的资源，属于服务端建立的 Stream，所以这两个 Stream 的 ID 是偶数（数字 2 和 4）。\n\n再比如，客户端通过 HTTP/1.1 请求从服务器那获取到了 HTML 文件，而 HTML 可能还需要依赖 CSS 来渲染页面，这时客户端还要再发起获取 CSS 文件的请求，需要两次消息往返，如下图左边部分：\n\n如上图右边部分，在 HTTP/2 中，客户端在访问 HTML 时，服务器可以直接主动推送 CSS 文件，减少了消息传递的次数。\n\nHTTP/2 有什么缺陷？\n\nHTTP/2 通过 Stream 的并发能力，解决了 HTTP/1 队头阻塞的问题，看似很完美了，但是 HTTP/2 还是存在“队头阻塞”的问题，只不过问题不是在 HTTP 这一层面，而是在 TCP 这一层。\nHTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题。\n\n举个例子，如下图：\n\n图中发送方发送了很多个 packet，每个 packet 都有自己的序号，你可以认为是 TCP 的序列号，其中 packet 3 在网络中丢失了，即使 packet 4-6 被接收方收到后，由于内核中的 TCP 数据不是连续的，于是接收方的应用层就无法从内核中读取到，只有等到 packet 3 重传后，接收方的应用层才可以从内核中读取到数据，这就是 HTTP/2 的队头阻塞问题，是在 TCP 层面发生的。\n所以，一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的所有的 HTTP 请求都必须等待这个丢了的包被重传回来。\nTIP\n如果想更进一步了解 HTTP/2 协议，可以看我这篇文章：HTTP/2 牛逼在哪？(opens new window)\nHTTP/3 做了哪些优化？前面我们知道了 HTTP/1.1 和 HTTP/2 都有队头阻塞的问题：\n\nHTTP/1.1 中的管道（ pipeline）虽然解决了请求的队头阻塞，但是没有解决响应的队头阻塞，因为服务端需要按顺序响应收到的请求，如果服务端处理某个请求消耗的时间比较长，那么只能等响应完这个请求后， 才能处理下一个请求，这属于 HTTP 层队头阻塞。\nHTTP/2 虽然通过多个请求复用一个 TCP 连接解决了 HTTP 的队头阻塞 ，但是一旦发生丢包，就会阻塞住所有的 HTTP 请求，这属于 TCP 层队头阻塞。\n\nHTTP/2 队头阻塞的问题是因为 TCP，所以 HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP！\n\nUDP 发送是不管顺序，也不管丢包的，所以不会出现像 HTTP/2 队头阻塞的问题。大家都知道 UDP 是不可靠传输的，但基于 UDP 的 QUIC 协议 可以实现类似 TCP 的可靠性传输。\nQUIC 有以下 3 个特点。\n\n无队头阻塞\n更快的连接建立\n连接迁移\n\n1、无队头阻塞\nQUIC 协议也有类似 HTTP/2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为就是一条 HTTP 请求。\nQUIC 有自己的一套机制可以保证传输的可靠性的。当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题。这与 HTTP/2 不同，HTTP/2 只要某个流中的数据包丢失了，其他流也会因此受影响。\n所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受影响。\n\n2、更快的连接建立\n对于 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。\nHTTP/3 在传输数据前虽然需要 QUIC 协议握手，但是这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。\n但是 HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是 QUIC 内部包含了 TLS，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS/1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商，如下图：\n\n甚至，在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。\n如下图右边部分，HTTP/3 当会话恢复时，有效负载数据与第一个数据包一起发送，可以做到 0-RTT（下图的右下角）：\n\n3、连接迁移\n基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。\n\n那么当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接。而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。\n而 QUIC 协议没有用四元组的方式来“绑定”连接，而是通过连接 ID 来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有丝毫卡顿感，达到了连接迁移的功能。\n所以， QUIC 是一个在 UDP 之上的伪 TCP + TLS + HTTP/2 的多路复用的协议。\nQUIC 是新协议，对于很多网络设备，根本不知道什么是 QUIC，只会当做 UDP，这样会出现新的问题，因为有的网络设备是会丢掉 UDP 包的，而 QUIC 是基于 UDP 实现的，那么如果网络设备无法识别这个是 QUIC 包，那么就会当作 UDP包，然后被丢弃。\nHTTP/3 现在普及的进度非常的缓慢，不知道未来 UDP 是否能够逆袭 TCP。\nTIP\n如果想更进一步了解 HTTP/3 和 QUIC 协议，可以看我这两篇文章：\n\nHTTP/3 强势来袭(opens new window)\n如何基于 UDP 协议实现可靠传输？(opens new window)\n\n\n参考资料：\n[1] 上野 宣.图解HTTP.人民邮电出版社.\n[2] 罗剑锋.透视HTTP协议.极客时间.\n[3] 陈皓.HTTP的前世今.酷壳CoolShell.https://coolshell.cn/articles/19840.html\n[4] 阮一峰.HTTP 协议入门.阮一峰的网络日志.http://www.ruanyifeng.com/blog/2016/08/http.html\n\n读者问答\n读者问：“https 和 http 相比，就是传输的内容多了对称加密，可以这么理解吗？”\n\n\n建立连接时候：https 比 http多了 TLS 的握手过程；\n传输内容的时候：https 会把数据进行加密，通常是对称加密数据；\n\n\n读者问：“ 我看文中 TLS 和 SSL 没有做区分，这两个需要区分吗？”\n\n这两实际上是一个东西。\nSSL 是洋文 “Secure Sockets Layer” 的缩写，中文叫做「安全套接层」。它是在上世纪 90 年代中期，由网景公司设计的。\n到了1999年，SSL 因为应用广泛，已经成为互联网上的事实标准。IETF 就在那年把 SSL 标准化。标准化之后的名称改为 TLS（是 “Transport Layer Security” 的缩写），中文叫做 「传输层安全协议」。\n很多相关的文章都把这两者并列称呼（SSL/TLS），因为这两者可以视作同一个东西的不同阶段。\n\n读者问：“为啥 SSL 的握手是 4 次？”\n\nSSL/TLS 1.2 需要 4 握手，需要 2 个 RTT 的时延，我文中的图是把每个交互分开画了，实际上把他们合在一起发送，就是 4 次握手：\n\n另外， SSL/TLS 1.3 优化了过程，只需要 1 个 RTT 往返时延，也就是只需要 3 次握手：\n\n","categories":["运维","Linux"]},{"title":"Docker 部署 Kafka","url":"/posts/30060/","content":"一、安装Zookeeperdocker run -d -e TZ=&quot;Asia/Shanghai&quot; -p 2181:2181 --name zk zookeeper\n二、安装Kafkadocker run  -d --name kafka -p 9092:9092 \\\t--link zk \\\t-e KAFKA_BROKER_ID=0 \\\t-e KAFKA_ZOOKEEPER_CONNECT=zk:2181/kafka \\\t-e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9092 \\\t-e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 \\\twurstmeister/kafka\nKAFKA_LISTENERS 和 KAFKA_ADVERTISED_LISTENERS 参数含义\n三、测试1.进入容器docker exec -it kafka bash\n\n2.启动生产者./opt/kafka_2.13-2.7.1/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n\n3.启动消费者另开一个终端进入容器\n./opt/kafka_2.13-2.7.1/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 -topic test --from-beginning\n\n4.发送消息在生产者中输入消息，在消费者中能接受到消息即为正常。\n","categories":["运维","消息队列","Kafka"]},{"title":"Kafka 核心配置","url":"/posts/59696/","content":"一、producer 核心配置Producer 配置官方文档\n1、acks ：发送应答（默认值：1）生产者在考虑完成请求之前要求leader收到的确认的数量。这控制了发送的记录的持久性。允许以下设置:\n\nacks=0：设置为0，则生产者将完全不等待来自服务器的任何确认。记录将立即添加到socket缓冲区，并被认为已发送。在这种情况下，不能保证服务器已经收到记录，重试配置将不会生效(因为客户机通常不会知道任何失败)。每个记录返回的偏移量总是-1。\n**acks=1:**leader会将记录写到本地日志中，但不会等待所有follower的完全确认。在这种情况下，如果leader在记录失败后立即失败，但在追随者复制记录之前失败，那么记录就会丢失。\n**acks=all / -1:**leader将等待完整的同步副本来确认记录。这保证了只要至少有一个同步副本仍然存在，记录就不会丢失。这是最有力的保证。这相当于acks=-1设置。\n\n2、batch.size：批量发送大小（默认：16384，16K）缓存到本地内存，批量发送大小，意思每次发送16K到broke。当多个记录被发送到同一个分区时，生产者将尝试将记录批处理成更少的请求。这有助于客户机和服务器上的性能。此配置以字节为单位控制默认批处理大小。\n3、bootstrap.servers：服务器地址broke服务器地址，多个用逗号割开。\n4、buffer.memory：生产者最大可用缓存 (默认：33554432，32M)生产者可以用来缓冲等待发送到服务器的记录的总内存字节。如果记录被发送的速度超过了它们可以被发送到服务器的速度，那么生产者将阻塞 max.block.ms。然后它会抛出一个异常。\n该设置应该大致与生成器将使用的总内存相对应，但不是硬绑定，因为生成器使用的并非所有内存都用于缓冲。一些额外的内存将用于压缩(如果启用了压缩)以及维护飞行中的请求。\n生产者产生的消息缓存到本地，每次批量发送batch.size大小到服务器。\n5、client.id：生产者ID(默认“”)请求时传递给服务器的id字符串。这样做的目的是通过允许在服务器端请求日志中包含逻辑应用程序名称，从而能够跟踪ip/端口之外的请求源。\n6、compression.type：压缩类型（默认值：producer）指定给定主题的最终压缩类型。此配置接受标准压缩编解码器(“gzip”、“snappy”、“lz4”、“zstd”)。它还接受“未压缩”，相当于没有压缩;以及“生产者”，即保留生产者设置的原始压缩编解码器。\n“gzip”：压缩效率高，适合高内存、CPU\n“snappy”：适合带宽敏感性，压缩力度大\nretries:失败重试次数（默认：2147483647）异常是RetriableException类型或者TransactionManager允许重试；\ntransactionManager.canRetry()后面会分析；先看看哪些异常是RetriableException类型异常。\n\n设置大于零的值将导致客户端重新发送任何发送失败并可能出现暂时错误的记录。请注意，此重试与客户端在接收到错误时憎恨记录没有什么不同。如果 delivery.timeout.ms 配置的超时时间在确认成功之前先过期，那么在重试次数用尽之前，生成请求就会失败。用户通常应该选择不设置这个配置，而是使用 delivery.timeout.ms 来控制重试行为。\n启用等幂需要此配置值大于0。如果设置了冲突配置且未显式启用幂等性，则禁用幂等性。\n允许重试，同时将 able.idempotence 设置为 false，将 max.in.flight.requests.per.connect 设置为1，这可能会改变记录的顺序，因为如果将两个批处理发送到一个分区，第一个失败并重试，但第二个成功，那么第二个批处理中的记录可能会首先出现。\nretry.backoff.ms：重试阻塞时间（默认：100）\n这避免了在某些失败场景下以紧密循环的方式重复发送请求。\n8、delivery.timeout.ms：传输时间（默认：120000，2分钟）调用 send ()返回后报告成功或失败的时间上限。这限制了记录在发送之前延迟的总时间、等待代理确认的时间(如果预期的话)以及允许可检索的发送失败的时间。如果遇到不可恢复的错误，重试已经用尽，或者该记录被添加到一批已经达到提前交付截止期限的记录中，生产者可能会报告未能在此配置之前发送记录。\n此配置的值应大于或等于 request.timeout.ms 和 linger.ms 之和。\n9、connections.max.idle.ms：关闭空闲连接时间（默认：540000） 在此配置指定的毫秒数之后关闭空闲连接。\n10、enable.idempotence：开启幂等（默认:false）当设置为“true”时，生产者将确保在流中准确地写入每个消息的副本。如果“false”，则由于代理失败而导致生产者重试，等等，可能会在流中写入重试消息的副本。请注意，启用幂等需要使用max.in.flight.requests.per.connection,连接小于或等于5，重试大于0且ack必须为“all”。如果用户没有显式地设置这些值，将选择合适的值。如果设置了不兼容的值，就会抛出ConfigException。\n11、max.in.flight.requests.per.connection：单个连接上发送的未确认请求的最大数量（默认：5）阻塞前客户端在单个连接上发送的未确认请求的最大数量。请注意，如果该设置设置为大于1，并且发送失败，则有由于重试(即，如果启用重试)。\n12、interceptor.classes：拦截器（默认：无）用作拦截器的类的列表。实现接口：org.apache.kafka.clients.producer。ProducerInterceptor接口允许将生产者接收到的记录发布到Kafka集群之前拦截它们(可能还会发生突变)。默认情况下，没有拦截器。\n13、key.serializer:key序列化器（默认无）实现org.apache.kafka.common. serialize .Serializer接口的key的序列化器类。String可配置：class org.apache.kafka.common.serialization.StringSerializer。\n14、value.serializer:value序列化器（默认无）序列化器类的值，该值实现org.apache.kafka.common. serialize .Serializer接口。String可配置：class org.apache.kafka.common.serialization.StringSerializer\n15、linger.ms：发送延迟时间（默认：0）为减少负载和客户端的请求数量，生产者不会一条一条发送，而是会逗留一段时间批量发送。batch.size 和 linger.ms 满足任何一个条件都会发送。\n16、max.block.ms：阻塞时间（默认：60000，一分钟）配置控制KafkaProducer.send()和KafkaProducer.partitionsFor()阻塞的时间。由于缓冲区已满或元数据不可用，也会阻塞。用户提供的序列化器或分区程序中的阻塞将不计入此超时。\n17、max.request.size：最大请求字节大小（默认：1048576，1M）请求的最大字节大小。此设置将限制生产者在单个请求中发送记录批的数量，以避免发送大量请求。这也有效地限制了最大记录批大小。注意，服务器对记录批处理大小有自己的上限，这可能与此不同。\n18、metric.reporters：自定义指标报告器用作指标报告器的类的列表。metricsreporter接口实现了org.apache.kafka.common.metrics.MetricsReporter接口，该接口允许插入将在创建新度量时得到通知的类。JmxReporter始终包含在注册JMX统计信息中。\n19、partitioner.class：自定义分区策略实现接口 org.apache.kafka.clients.producer.Partitioner，默认值：org.apache.kafka.clients.producer.internals.DefaultPartitioner\n20、request.timeout.ms：请求超时时间（默认：30000）配置控制客户机等待请求响应的最长时间。如果在超时超时之前没有收到响应，客户端将在需要时重新发送请求，或者在重试耗尽时失败请求。这个应该大于replica.lag.time.max。ms(代理配置)，以减少由于不必要的生产者重试而导致消息重复的可能性。\n21、receive.buffer.bytes（默认：32768，32K）读取数据时使用的TCP接收缓冲区(SO_RCVBUF)的大小。如果值是-1，将使用OS默认值。\n22、send.buffer.bytes（默认：131072,128K）发送数据时使用的TCP发送缓冲区(SO_SNDBUF)的大小。如果值是-1，将使用OS默认值。\nretry.backoff.ms：重试阻塞时间（默认：100）这避免了在某些失败场景下以紧密循环的方式重复发送请求。\n二、consumer核心配置Consumer 配置官方文档\n1、enable.auto.commit：开启自动提交（默认:true）如果为true，consumer的偏移量将在后台定期提交。\n2、auto.commit.interval.ms：自动提交频率（默认：5000）如果enable.auto.commit设置为true，则使用者偏移量自动提交到Kafka的频率(毫秒)。\nclient.id：客户ID便于跟踪日志。\n4、check.crcs：是否开启数据校验（默认：true）自动检查消耗的记录的CRC32。这确保不会发生对消息的在线或磁盘损坏。此检查增加了一些开销，因此在寻求极端性能的情况下可能禁用此检查。\n5、bootstrap.servers：服务器配置多个用都好隔开。\n6、connections.max.idle.ms：关闭空间连接时间（默认：540000）在此配置指定的毫秒数之后关闭空闲连接。\n7、group.id：群组（默认：“”）唯一标识用户群组，同一个group每个partition只会分配到一个consumer。\nmax.poll.records：拉起最大记录（默认：500）单次轮询()调用中返回的记录的最大数量。\n9、max.poll.interval.ms：拉取记录间隔（默认：300000，5分钟）使用消费者组管理时轮询()调用之间的最大延迟。这为使用者在获取更多记录之前空闲的时间设置了上限。如果在此超时过期之前没有调用poll()，则认为使用者失败，组将重新平衡，以便将分区重新分配给另一个成员。\n10、request.timeout.ms：请求超时时间（默认：30000 ，30S）配置控制客户机等待请求响应的最长时间。如果在超时超时之前没有收到响应，客户端将在需要时重新发送请求，或者在重试耗尽时失败请求。\n11、session.timeout.ms：consumer session超时用于检测worker程序失败的超时。worker定期发送心跳，以向代理表明其活性。如果在此会话超时过期之前代理没有接收到心跳，则代理将从组中删除。请注意，该值必须位于group.min.session.timeout在broker配置中配置的允许范围内group.max.session.timeout.ms。\n12、auto.offset.reset:初始偏移量 (默认：latest)如果Kafka中没有初始偏移量，或者服务器上不再存在当前偏移量(例如，因为该数据已被删除)，该怎么办:\nearliest:自动重置偏移到最早的偏移\nlatest:自动将偏移量重置为最新偏移量\nnone:如果没有为使用者的组找到以前的偏移量，则向使用者抛出exception\nanything else:向使用者抛出异常\n13、key.deserializer用于实现org.apache.kafka.common. serialize .Deserializer接口的key的反序列化类，class org.apache.kafka.common.serialization.StringDeserializer\n14、value.deserializer用于实现org.apache.kafka.common. serialize .Deserializer接口的value的反序列化类，class org.apache.kafka.common.serialization.StringDeserializer\n15、max.partition.fetch.bytes每个分区服务器将返回的最大数据量。记录由consumer成批提取。如果fetch的第一个非空分区中的第一个记录批处理大于这个限制，那么仍然会返回批处理，以确保使用者能够取得进展。broker接受的最大记录批处理大小是通过message.max定义的。字节(broker配置)或max.message。字节(topic配置)。看fetch.max.bytes用于限制consumer请求大小的字节。\n16、partition.assignment.strategy：consumer订阅分区策略（默认：class org.apache.kafka.clients.consumer.RangeAssignor）\n当使用组管理时，客户端将使用分区分配策略的类名在使用者实例之间分配分区所有权。\n17、fetch.max.bytes：拉取最大字节（默认：52428800，50M）服务器应该为获取请求返回的最大数据量。记录由使用者成批地获取，并且如果获取的第一个非空分区中的第一个记录批处理大于这个值，仍然会返回记录批处理，以确保使用者能够取得进展。因此，这不是一个绝对最大值。代理接受的最大记录批处理大小是通过message.max定义的。字节(代理配置)或max.message。字节(主题配置)。请注意，使用者并行执行多个获取。\n18、heartbeat.interval.ms：心跳时间（默认：3000, 3S）使用Kafka的组管理工具时，从心跳到消费者协调器的预期时间。心跳被用来确保消费者的会话保持活跃，并在新消费者加入或离开组时促进再平衡。该值必须设置为小于session.timeout.ms的1/3。它可以调整甚至更低，以控制正常再平衡的预期时间。\n19、fetch.max.wait.ms：拉取阻塞时间（默认：500）如果没有足够的数据立即满足fetch.min.bytes提供的要求，服务器在响应fetch请求之前将阻塞的最长时间。\n20、fetch.min.bytes：拉取最小字节数（默认：1）服务器应该为获取请求返回的最小数据量。如果没有足够的数据可用，请求将等待那么多数据累积后再响应请求。默认的1字节设置意味着，只要数据的一个字节可用，或者获取请求超时等待数据到达，就会响应获取请求。将此设置为大于1的值将导致服务器等待更大数量的数据累积，这可以稍微提高服务器吞吐量，但代价是增加一些延迟。\n21、exclude.internal.topics:公开内部topic(默认：true)是否应该将来自内部主题(如偏移量)的记录公开给使用者，consumer共享offset。如果设置为true，从内部主题接收记录的唯一方法是订阅它。\n22、isolation.level(隔离级别：默认：read_uncommitted）控制如何以事务方式读取写入的消息。如果设置为read_committed, consumer.poll()将只返回已提交的事务消息。如果设置为read_uncommitted’(默认)，consumer.poll()将返回所有消息，甚至是已经中止的事务消息。在任何一种模式下，非事务性消息都将无条件返回。\n回到顶部\n三、broke配置1、zookeeper.connect：zk地址多个用逗号隔开。\n2、advertised.host.name（默认：null）不赞成使用:\n在server.properties 里还有另一个参数是解决这个问题的， advertised.host.name参数用来配置返回的host.name值，把这个参数配置为外网IP地址即可。\n这个参数默认没有启用，默认是返回的java.net.InetAddress.getCanonicalHostName的值，在我的mac上这个值并不等于hostname的值而是返回IP，但在linux上这个值就是hostname的值。\n3、advertised.listenershostname和端口注册到zk给生产者和消费者使用的，如果没有设置，将会使用listeners的配置，如果listeners也没有配置，将使用java.net.InetAddress.getCanonicalHostName()来获取这个hostname和port，对于ipv4，基本就是localhost了。\n4、auto.create.topics.enable（自动创建topic，默认：true）第一次发动消息时，自动创建topic。\n5、auto.leader.rebalance.enable：自动rebalance(默认：true)支持自动领导平衡。如果需要，后台线程定期检查并触发leader balance。\n6、background.threads：处理线程（默认：10）用于各种后台处理任务的线程数。\n7、broker.id 默认：-1此服务器的broke id。如果未设置，将生成唯一的代理id。为了避免zookeeper生成的broke id和用户配置的broke id之间的冲突，生成的代理id从reserve .broker.max开始id + 1。\n8、compression.type：压缩类型，默认：producer指定给定主题的最终压缩类型。此配置接受标准压缩编解码器(“gzip”、“snappy”、“lz4”、“zstd”)。它还接受“未压缩”，相当于没有压缩;以及“producer”，即保留producer设置的原始压缩编解码器。\n9、delete.topic.enable 删除topic(默认：true)允许删除主题。如果关闭此配置，则通过管理工具删除主题将无效。\n10、leader.imbalance.check.interval.seconds（rebalance检测频率，默认：300）控制器触发分区rebalance检查的频率。\n11、leader.imbalance.per.broker.percentage（触发rebalance比率，默认：10，10%）每个broke允许的lead不平衡比率。如果控制器超过每个broke的这个值，控制器将触发一个leader balance。该值以百分比指定。\n12、log.dir（日志目录，默认：/tmp/kafka-logs）保存日志数据的目录。\n13、log.dirs保存日志数据的目录。如果未设置，则为日志中的值。使用dir。\n14、log.flush.interval.messages（默认：9223372036854775807）在将消息刷新到磁盘之前，日志分区上累积的消息数量\n15、log.flush.interval.ms（默认：null）任何主题中的消息在刷新到磁盘之前保存在内存中的最长时间。如果没有设置，则使用log.flush.scheduler.interval.ms中的值。\n16、log.flush.offset.checkpoint.interval.ms（默认：60000）作为日志恢复点的上次刷新的持久记录的更新频率。\n17、log.retention.bytes 保存日志文件的最大值（默认：-1）删除前日志的最大大小。\n18、log.retention.hours日志文件最大保存时间（小时）默认：168，一周日志文件最大保存时间。\n19、log.retention.minutes日志文件最大保存时间（分钟）默认：null20、log.retention.ms日志文件最大保存时间（毫秒）默认：null21、log.roll.hours:新segment产生时间，默认：168，一周即使文件没有到达log.segment.bytes，只要文件创建时间到达此属性，就会创建新文件。\n22、log.roll.ms :新segment产生时间滚出新日志段之前的最大时间(以毫秒为单位)。如果未设置，则为log.roll中的值使用。\n23、log.segment.bytes：segment文件最大值，默认：1073741824（1G）24、log.segment.delete.delay.ms：segment删除等待时间， 默认：60000从文件系统中删除文件之前等待的时间量。\n25、message.max.bytes 最大batch size 默认：1000012，0.9MKafka允许的最大记录batch size。如果增加了这个值，并且存在大于0.10.2的使用者，那么还必须增加consumer的fetch大小，以便他们能够获取这么大的记录批。在最新的消息格式版本中，记录总是按批进行分组，以提高效率。在以前的消息格式版本中，未压缩记录没有分组成批，这种限制只适用于单个记录。可以使用主题级别max.message设置每个主题。字节的配置。\n26、min.insync.replicas（insync中最小副本值）当producer将acks设置为“all”(或“-1”)时，min.insync。副本指定必须确认写操作成功的最小副本数量。如果不能满足这个最小值，则生产者将引发一个异常(要么是NotEnoughReplicas，要么是NotEnoughReplicasAfterAppend)。\n当一起使用时，min.insync.replicas和ack允许您执行更大的持久性保证。一个典型的场景是创建一个复制因子为3的主题，设置min.insync复制到2个，用“all”配置发送。将确保如果大多数副本没有收到写操作，则生产者将引发异常。\n27、num.io.threads，默认：8服务器用于处理请求的线程数，其中可能包括磁盘I/O。\n28、num.network.threads，默认：3服务器用于接收来自网络的请求和向网络发送响应的线程数。\n29、num.recovery.threads.per.data.dir 默认：1每个数据目录在启动时用于日志恢复和在关闭时用于刷新的线程数。\n30、num.replica.alter.log.dirs.threads 默认：null可以在日志目录(可能包括磁盘I/O)之间移动副本的线程数。\n31、num.replica.fetchers从leader复制数据到follower的线程数。\n32、offset.metadata.max.bytes 默认：4096与offset提交关联的metadata的最大大小。\n33、offsets.commit.timeout.ms 默认：5000偏移量提交将被延迟，直到偏移量主题的所有副本收到提交或达到此超时。这类似于生产者请求超时。\n34、offsets.topic.num.partitions 默认：50偏移量提交主题的分区数量(部署后不应更改)。\n35、offsets.topic.replication.factor 副本大小,默认：3低于上述值，主题将创建失败。\n36、offsets.topic.segment.bytes 默认104857600 ,100Msegment映射文件（index）文件大小，以便加快日志压缩和缓存负载。\n37、queued.max.requests 默认：500阻塞网络线程之前允许排队的请求数。\n38、replica.fetch.min.bytes默认：1每个fetch响应所需的最小字节。如果字节不够，则等待replicaMaxWaitTimeMs。\n39、replica.lag.time.max.ms 默认：10000如果follower 没有发送任何获取请求，或者至少在这段时间没有消耗到leader日志的结束偏移量，那么leader将从isr中删除follower 。\n40、transaction.max.timeout.ms 默认：900000事务执行最长时间，超时则抛出异常。\n41、unclean.leader.election.enable 默认：false是否选举ISR以外的副本作为leader,会导致数据丢失。\n42、zookeeper.connection.timeout.ms客户端等待与zookeeper建立连接的最长时间。如果未设置，则用zookeeper.session.timeout中的值。\n43、zookeeper.max.in.flight.requests阻塞之前consumer将发送给Zookeeper的未确认请求的最大数量。\n44、group.max.session.timeout.ms注册使用者允许的最大会话超时。超时时间越长，消费者在心跳之间处理消息的时间就越多，而检测故障的时间就越长。\n45、group.min.session.timeout.ms注册使用者允许的最小会话超时。更短的超时导致更快的故障检测，但代价是更频繁的用户心跳，这可能会压倒broke资源。\n46、num.partitions 默认：1每个主题的默认日志分区数量。\n","categories":["运维","消息队列","Kafka"]},{"title":"既然有 HTTP 协议，为什么还要有 RPC？","url":"/posts/24265/","content":"一、从 TCP 聊起作为一个程序员，假设我们需要在 A 电脑的进程发一段数据到 B 电脑的进程，我们一般会在代码里使用 Socket 进行编程。\n这时候，我们可选项一般也就 TCP 和 UDP 二选一。TCP 可靠，UDP 不可靠。除非是马总这种神级程序员（早期 QQ 大量使用 UDP），否则，只要稍微对可靠性有些要求，普通人一般无脑选 TCP 就对了。\n类似下面这样。\nfd = socket(AF_INET,SOCK_STREAM,0);\n\n其中 SOCK_STREAM，是指使用字节流传输数据，说白了就是 TCP 协议。\n在定义了 Socket 之后，我们就可以愉快的对这个 Socket 进行操作，比如用 bind() 绑定 IP 端口，用 connect() 发起建连。\n\n在连接建立之后，我们就可以使用 send() 发送数据，recv() 接收数据。\n光这样一个纯裸的 TCP 连接，就可以做到收发数据了，那是不是就够了？\n不行，这么用会有问题。\n二、使用纯裸 TCP 会有什么问题八股文常背，TCP 是有三个特点，面向连接、可靠、基于字节流。\n\n这三个特点真的概括的非常精辟，这个八股文我们没白背。\n每个特点展开都能聊一篇文章，而今天我们需要关注的是基于字节流这一点。\n字节流可以理解为一个双向的通道里流淌的数据，这个数据其实就是我们常说的二进制数据，简单来说就是一大堆 01 串。纯裸 TCP 收发的这些 01 串之间是没有任何边界的，你根本不知道到哪个地方才算一条完整消息。\n\n正因为这个没有任何边界的特点，所以当我们选择使用 TCP 发送”夏洛”和”特烦恼”的时候，接收端收到的就是”夏洛特烦恼”，这时候接收端没发区分你是想要表达”夏洛”+”特烦恼”还是”夏洛特”+”烦恼”。\n\n这就是所谓的粘包问题，之前也写过一篇专门的文章 (opens new window)聊过这个问题。\n说这个的目的是为了告诉大家，纯裸 TCP 是不能直接拿来用的，你需要在这个基础上加入一些自定义的规则，用于区分消息边界。\n于是我们会把每条要发送的数据都包装一下，比如加入消息头，消息头里写清楚一个完整的包长度是多少，根据这个长度可以继续接收数据，截取出来后它们就是我们真正要传输的消息体。\n\n而这里头提到的消息头，还可以放各种东西，比如消息体是否被压缩过和消息体格式之类的，只要上下游都约定好了，互相都认就可以了，这就是所谓的协议。\n每个使用 TCP 的项目都可能会定义一套类似这样的协议解析标准，他们可能有区别，但原理都类似。\n于是基于 TCP，就衍生了非常多的协议，比如 HTTP 和 RPC。\n三、HTTP 和 RPC我们回过头来看网络的分层图。\n\nTCP 是传输层的协议，而基于 TCP 造出来的 HTTP 和各类 RPC 协议，它们都只是定义了不同消息格式的应用层协议而已。\nHTTP 协议（Hyper Text Transfer Protocol），又叫做超文本传输协议。我们用的比较多，平时上网在浏览器上敲个网址就能访问网页，这里用到的就是 HTTP 协议。\n\n而 RPC（Remote Procedure Call），又叫做远程过程调用。它本身并不是一个具体的协议，而是一种调用方式。\n举个例子，我们平时调用一个本地方法就像下面这样。\nres = localFunc(req)\n\n如果现在这不是个本地方法，而是个远端服务器暴露出来的一个方法 remoteFunc，如果我们还能像调用本地方法那样去调用它，这样就可以屏蔽掉一些网络细节，用起来更方便，岂不美哉？\nres = remoteFunc(req)\n\n\n基于这个思路，大佬们造出了非常多款式的 RPC 协议，比如比较有名的gRPC，thrift。\n值得注意的是，虽然大部分 RPC 协议底层使用 TCP，但实际上它们不一定非得使用 TCP，改用 UDP 或者 HTTP，其实也可以做到类似的功能。\n\n到这里，我们回到文章标题的问题。\n\n既然有 HTTP 协议，为什么还要有 RPC？\n\n其实，TCP 是70年代出来的协议，而 HTTP 是 90 年代才开始流行的。而直接使用裸 TCP 会有问题，可想而知，这中间这么多年有多少自定义的协议，而这里面就有80年代出来的 RPC。\n所以我们该问的不是既然有 HTTP 协议为什么要有 RPC，而是为什么有 RPC 还要有 HTTP 协议。\n\n那既然有 RPC 了，为什么还要有 HTTP 呢？\n\n现在电脑上装的各种联网软件，比如 xx管家，xx卫士，它们都作为客户端（Client）需要跟服务端（Server）建立连接收发消息，此时都会用到应用层协议，在这种 Client/Server (C/S) 架构下，它们可以使用自家造的 RPC 协议，因为它只管连自己公司的服务器就 ok 了。\n但有个软件不同，浏览器（Browser），不管是 Chrome 还是 IE，它们不仅要能访问自家公司的服务器（Server），还需要访问其他公司的网站服务器，因此它们需要有个统一的标准，不然大家没法交流。于是，HTTP 就是那个时代用于统一 Browser/Server (B/S) 的协议。\n也就是说在多年以前，HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。**很多软件同时支持多端，比如某度云盘，既要支持**网页版**，还要支持手机端和 PC 端**，如果通信协议都用 HTTP 的话，那服务器只用同一套就够了。而 RPC 就开始退居幕后，一般用于公司内部集群里，各个微服务之间的通讯。\n那这么说的话，都用 HTTP 得了，还用什么 RPC？\n仿佛又回到了文章开头的样子，那这就要从它们之间的区别开始说起。\n四、HTTP 和 RPC 有什么区别我们来看看 RPC 和 HTTP 区别比较明显的几个点。\n服务发现首先要向某个服务器发起请求，你得先建立连接，而建立连接的前提是，你得知道 IP 地址和端口。这个找到服务对应的 IP 端口的过程，其实就是服务发现。\n在 HTTP 中，你知道服务的域名，就可以通过 DNS 服务去解析得到它背后的 IP 地址，默认 80 端口。\n而 RPC 的话，就有些区别，一般会有专门的中间服务去保存服务名和IP信息，比如 Consul 或者 Etcd，甚至是 Redis。想要访问某个服务，就去这些中间服务去获得 IP 和端口信息。由于 DNS 也是服务发现的一种，所以也有基于 DNS 去做服务发现的组件，比如CoreDNS。\n可以看出服务发现这一块，两者是有些区别，但不太能分高低。\n底层连接形式以主流的 HTTP/1.1 协议为例，其默认在建立底层 TCP 连接之后会一直保持这个连接（Keep Alive），之后的请求和响应都会复用这条连接。\n而 RPC 协议，也跟 HTTP 类似，也是通过建立 TCP 长链接进行数据交互，但不同的地方在于，RPC 协议一般还会再建个连接池，在请求量大的时候，建立多条连接放在池内，要发数据的时候就从池里取一条连接出来，用完放回去，下次再复用，可以说非常环保。\n\n由于连接池有利于提升网络请求性能，所以不少编程语言的网络库里都会给 HTTP 加个连接池，比如 Go 就是这么干的。\n可以看出这一块两者也没太大区别，所以也不是关键。\n传输的内容基于 TCP 传输的消息，说到底，无非都是消息头 Header 和消息体 Body。\nHeader 是用于标记一些特殊信息，其中最重要的是消息体长度。\nBody 则是放我们真正需要传输的内容，而这些内容只能是二进制 01 串，毕竟计算机只认识这玩意。所以 TCP 传字符串和数字都问题不大，因为字符串可以转成编码再变成 01 串，而数字本身也能直接转为二进制。但结构体呢，我们得想个办法将它也转为二进制 01 串，这样的方案现在也有很多现成的，比如 Json，Protobuf。\n这个将结构体转为二进制数组的过程就叫序列化，反过来将二进制数组复原成结构体的过程叫反序列化。\n\n对于主流的 HTTP/1.1，虽然它现在叫超文本协议，支持音频视频，但 HTTP 设计初是用于做网页文本展示的，所以它传的内容以字符串为主。Header 和 Body 都是如此。在 Body 这块，它使用 Json 来序列化结构体数据。\n我们可以随便截个图直观看下。\n\n可以看到这里面的内容非常多的冗余，显得非常啰嗦。最明显的，像 Header 里的那些信息，其实如果我们约定好头部的第几位是 Content-Type，就不需要每次都真的把”Content-Type”这个字段都传过来，类似的情况其实在 body 的 Json 结构里也特别明显。\n而 RPC，因为它定制化程度更高，可以采用体积更小的 Protobuf 或其他序列化协议去保存结构体数据，同时也不需要像 HTTP 那样考虑各种浏览器行为，比如 302 重定向跳转啥的。因此性能也会更好一些，这也是在公司内部微服务中抛弃 HTTP，选择使用 RPC 的最主要原因。\n\n\n当然上面说的 HTTP，其实特指的是现在主流使用的 HTTP/1.1，HTTP/2 在前者的基础上做了很多改进，所以性能可能比很多 RPC 协议还要好，甚至连 gRPC 底层都直接用的 HTTP/2。\n\n那么问题又来了，为什么既然有了 HTTP/2，还要有 RPC 协议？\n\n这个是由于 HTTP/2 是 2015 年出来的。那时候很多公司内部的 RPC 协议都已经跑了好些年了，基于历史原因，一般也没必要去换了。\n五、总结\n纯裸 TCP 是能收发数据，但它是个无边界的数据流，上层需要定义消息格式用于定义消息边界。于是就有了各种协议，HTTP 和各类 RPC 协议就是在 TCP 之上定义的应用层协议。\nRPC 本质上不算是协议，而是一种调用方式，而像 gRPC 和 Thrift 这样的具体实现，才是协议，它们是实现了 RPC 调用的协议。目的是希望程序员能像调用本地方法那样去调用远端的服务方法。同时 RPC 有很多种实现方式，不一定非得基于 TCP 协议。\n从发展历史来说，HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。很多软件同时支持多端，所以对外一般用 HTTP 协议，而内部集群的微服务之间则采用 RPC 协议进行通讯。\nRPC 其实比 HTTP 出现的要早，且比目前主流的 HTTP/1.1 性能要更好，所以大部分公司内部都还在使用 RPC。\nHTTP/2.0 在 HTTP/1.1 的基础上做了优化，性能可能比很多 RPC 协议都要好，但由于是这几年才出来的，所以也不太可能取代掉 RPC。\n\n","categories":["运维","Linux"]},{"title":"漫谈 Kafka","url":"/posts/11915/","content":"一、Rebalance机制Rebalance 描述的是 consumer group 中的 consumer 与 topic  下的 partition 重新匹配的过程。\n1.何时发生Rebalance\nconsumer group 中消费者个数发生变化，例如新消费者加入或者消费者宕机\nconsumer 消费超时\nconsumer group 订阅的 topic 个数发生变化\nconsumer group 订阅的 topic  的 partition 数量发生变化\n\n2.GroupCoordinator（协调者）coordinator 是 Rebalance 机制中非常重要的一个角色。\n每个消费者组都会有一个 coordinator，负责管理组内消费者，但其并不负责消费者组内 partition 分配，而是由 leader consumer 进行分配。\n1）_consumer_offsets Topic__consumer_offsets 是 Kafka 内部使用的一个 topic，专门用来存储消费者组的消费情况，默认有 50 个 partition，每个 partition 三个  follower。\n每个消费者组的消费情况存储在哪个 partition 上是根据 abs(GroupId.hashCode()) % NumPartitions 计算得来的。\n其中 NumPartitions 是 _consumer_offsets topic 的 partition 数量，默认 50。\n每个 broker 在启动时，都会启动一个 coordinator，但 只有 _consumer_offsets 的 partition 的 leader 所在 broker 上的 coordinator 才会直接与 consumer 进行交互，其他 coordinator 只是作为备份，一旦 leader 的 broker 挂掉之后进行主备切换。\n2）影响 consumer 与 coordinator 的参数消费者以 heartbeat.interval.ms 参数的频率向 coordinator 发送心跳，当消费者在 session.timeout.ms 参数配置的时间内没有向 coordinator 发送任何请求，会认为该消费者挂了。\n一般来说 heartbeat.interval.ms 要小于 session.timeout.ms，不然因为偶然的网络原因少了一次心跳或者下次心跳还没来得及发送就判定消费者挂了，那就会频繁的 rebalance，对于吞吐量是有很大影响的。\n但是如果 session.timeout.ms 设的时间太长了，就会导致 coordinator 需要较长的时间才能判定消费者宕机，对性能也是有影响的。\n在 kafka 0.10.1 之后的版本中，将 session.timeout.ms 和 max.poll.interval.ms 解耦了。\n也就是说在 new KafkaConsumer 对象后，在 while 循环中执行 consumer.poll 拉取消息过程中，实际运行了两个线程，一个是 consumer 消费消息的线程，一个是定时发送心跳给 coordinator 的线程（这个线程对开发是透明的）。\n如果 consumer 消费消息耗时较长，也不会影响给 coordinator 发送心跳，就不会出现 consumer 在正常消费，但 coordinator 认为 consumer 已经宕机的情况。只要 max.poll.interval.ms 参数可以包含 consumer 的消费时长是没问题的。\n由于是独立的线程，如果 consumer 真的宕机了，也就不用等到 max.poll.interval.ms之后才能检测出 consumer 宕机了。\n在 kafka 0.10.1 之前，由于心跳包的发送和消息消费是在一个线程中的，如果消息处理的时间超过 session.timeout.ms，消息还没消费完就被 coordinator 踢出消费者组了。\n3.Rebalance 过程coordinator 发生 rebalance 时，coordinator 并不会主动通知组内所有消费者重新加入消费者组，而是在 consumer 发送心跳的时候，将 rebalance 的情况通过心跳响应返回给 consumer。\nrebalance 整体可以分为两个步骤：Joining The Group 和 Synchronizing Group State。\n1）Joining The Group这一阶段，consumer 向 coordinator 请求加入消费者组，coordinator 会从所有 consumer 中随机选择一个作为 leader consumer。\n2）Synchronizing Group Stateleader consumer 从 coordinator 获取所有消费者的信息，并将消费者的 partition 分配信息封装为 SyncGroup 请求。\nleader consumer 并不直接与其他 consumer 进行交互，而是将 SyncGroup 发送给 coordinator，再由 coordinator 将分配结果发送给 consumer。\n如果 leader consumer 因为特殊原因导致分配 partition 失败（coordinator 通过超时的方式检测），那么 coordinator 会重新要求该消费者组中所有  consumer 重新进行 Joining The Group。\n4.Generation 机制当 consumer 消费超时（超过 max.poll.interval.ms），coordinator 会将该 consumer 从消费者组中剔除，进行 Rebalance，将当前 consumer 负责的 partition 分配给其他 consumer，如果此时该 consumer 消费完成提交 offset，会抛出如下异常：\n\nCommit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing max.poll.interval.ms or by reducing the maximum size of batches returned in poll() with max.poll.records.\n\nGeneration 机制就是为了防止上述情况下 offset 提交成功的。\ncoordinator 每进行一次 rebalance，就会重新设置 generation 标记。\n例如第一次 rebalance 标记是 1，第二次 rebalance 标记是 2，消费者在提交 offset 时会将 generation 一同提交，coordinate 发现标记已经过时，就会拒绝 consumer 提交消息。\nGeneration 机制可能会导致上一代 consumer 和当前代 consumer 消费相同的消息，所以 consumer 在消费消息时需要实现幂等性。\n5.Leader ConsumerLeader Consumer 是在 Joining The Group 阶段中随机选举的，负责其消费者组中各个 consumer 的 partition 分配。\n除此之外，还负责监控消费者组订阅的 Topic，一旦发现订阅的 Topic 发生变化， Leader Consumer 会通知 coordinator 进行 rebalance。\n二、Partition 副本同步机制kafka 一个 Partition 可以有多个副本，采用主备模式，只有 leador partition 可以进行读写，副本只是同步 leador 的数据。\n\nISR(in-sync Replica) 与 OSR(out-sync Replica)如果一个 follower 落后 leader 不超过某个时间阈值，那么该 follower 就在 ISR 中，否则放在 OSR 中。\nISR 和 OSR 并不是一成不变的。\n当 ISR 中某个 follower 不符合条件时，会进入到 OSR 中；\n相反，当 OSR 中某个 follower 跟上了 leader 会进入到 ISR。\nHW（high watermark）所有 ISR 中最小的 LEO 即为 HW。\nLEO（log end offset）partition 日志文件中下一条待写入消息的 offset。无论 leader 还是 follower 都有。\nfirstUnstableOffsetkafka 支持事物消息，firstUnstableOffset 指的是第一条未提交的消息的 offset，从 firstUnstableOffset 开始，到 LEO 之前都是未提交的消息。\nlastStableOffset最后一条已提交的消息的 offset。\nlogStartOffset初始消息的 offset。\n1.Partition中哪些消息可以消费？当允许消费未提交的消息时，从 logStartOffset 开始，到 HW 之前的消息都是可以消费。\n当不允许消费未提交的消息时，只有从 logStartOffset 开始，到 lastStableOffset 的消息是可以消费的。\n2.同步过程当 leader 收到生产者的一条消息时，LEO 通常会自增 1，而 follower 需要从 leader fetch到数据后，才能增加 LEO。\nfollower 发出 fetch 请求同步数据时，会携带自身的 LEO，leader 会更新 remote LEO 和 partition 的 HW，然后将数据返回给 follower。\nfollower 获取到数据后有三种情况：\n\nfollower 的 LEO 小于 leader 的 logStartOffset：说明 follower 的数据太老了，会把自身的数据删除，重新获取 leader 从 logStartOffset 到 LEO 之前的数据；\nfollower 的 LEO 大于等于 leader logStartOffset 且小于等于 leader LEO：正常同步缺失的数据；\nfollower 的 LEO 大于 leader LEO：删除 follower 多余的 offset，与 leader 保持一致。\n\nfollower 会用 leader HW 和自身 LEO 的最小值进行更新自身 HW。\n数据丢失当 follower 还没有同步到 leader 最新数据，leader 挂掉，follower 成为 leader，部分数据会存在丢失的情况。\n三、消息高可靠消息可靠发送1）ACKS\nacks = 0：生产者在成功写入消息之前不会等待broker的响应，一旦因为网络或其他原因导致broker没有收到消息，消息将会丢失。但由于不需要等待 broker 响应，所以吞吐量最高；\nacks = 1：当生产者收到leader写入消息成功的ack后，即认为消息发送成功。当消息无法写入 leader ，将会收到一个错误响应，为避免消息丢失，生产者将会重新发送消息。若leader成功接收消息但还未同步给 follower，leader 挂掉，重新选举的leader没有该消息，此时消息将会丢失。该方式的吞吐量取决于使用异步发送还是同步发送；\nacks = all/-1：ISR 中所有follower全部写入消息成功后，生产者才会收到ack。该模式的安全性最高，但延迟也最高。\n\n2）是否允许从OSR选举leaderunclean.leader.election.enable:false 配置，禁止选举 ISR 之外（即 OSR 中）的 follower 成为 leader。\n如果允许从 OSR 中选举 leader，即使 ack = all/-1 也会存在消息丢失的情况。\n但是如果不允许从 OSR 中选举leader，在极端情况下 ISR 中的broker全部挂掉，此时就无法选举出leader，只能等待 ISR 中leader恢复，该 partition 也就无法提供服务，牺牲了可用性，但保证了消息的可靠性。\n3）retries 重试次数当生产者发送了消息，因为网络原因或其他原因导致没有收到ack，会进行重试。\n4）最小同步副本数broker 的 min.insync.replicas 参数可以控制消息至少被写入多少个副本才算写入成功，默认值为 1。\n如果消息同步的副本数量小于配置的该值，生产者将收到错误响应，保证消息不丢失。\n该配置可以与 acks 相结合，平衡性能。\n\nmin.insync.replicas = 2 &amp;&amp; ack = all &amp;&amp; ISR=&#123;1,2&#125;  时，ISR中两个follower同步完成生产者就会收到成功响应；\nmin.insync.replicas = 2 &amp;&amp; ISR=&#123;1&#125;  时，若 ack = all  不能成功写入，若 ack = 0 或 1 可以成功写入；\nmin.insync.replicas = 2 &amp;&amp; ack = all &amp;&amp; ISR=&#123;1,2,3&#125;  时，需要 ISR 中所有 follower 都写入成功后才会收到成功响应。\n\n消息可靠消费\n手动提交offset\n减小broker服务器的刷盘间隔\n事物消息\n\n四、消费者无法消费offsets.topic.replication.factor \noffsets topic 的复制因子(设置更高以确保可用性)。在集群大小满足此复制因子要求之前，内部主题创建将失败。\n文档链接 https://kafka.apache.org/documentation/#brokerconfigs_offsets.topic.replication.factor\n","categories":["运维","消息队列","Kafka"]},{"title":"RabbitMQ","url":"/posts/33708/","content":"普通集群模式\nRabbitMQ 的普通集群模式是不能高可用的，因为其他节点只同步元数据。其中消息只在其所在节点中保存。客户端可以连接任意节点向队列发送或消费消息，若非队列数据所在节点，则该节点会进行路由转发。\n元数据：\n队列数据：队列名称和属性\n交换器元数据：交换器名称、类型、属性\n绑定元数据：描述了如果将消息路由到队列\nvhost 元数据：为 vhost 内的队列、交换器和绑定提供命名空间和安全属性\n\n为什么只同步元数据？\n存储空间：每个节点都保存全量数据，若节点间存储空间大小不一样，会影响消息堆积能力\n性能：消息的发布者需要将消息复制到每一个集群节点\n\n集群节点类型\n磁盘节点：配置信息、元数据、消息都存储在磁盘上\n内存节点：配置信息、元数据、消息都存储在内存中。内存节点维护了磁盘节点的地址，会将前边说的信息同步至磁盘节点进行持久化。其性能优于磁盘节点。\n\nRabbitMQ 要求集群汇总至少有一个磁盘节点，当节点加入和离开集群时，必须通知磁盘节点（如果集群中唯一的磁盘节点崩溃，则不能进行创建队列、创建交换器、创建绑定、添加用户、更改权限、添加和删除集群节点）。如果唯一的磁盘节点崩溃，集群是可以保持运行的，但不能更改任何东西。因此建议在集群中设值两个磁盘节点。\n","categories":["运维","消息队列","RabbitMQ"]},{"title":"Elasticsearch 详解","url":"/posts/30870/","content":"一、生活中的数据搜索引擎是对数据的检索，所以我们先从生活中的数据说起。\n我们生活中的数据总体分为两种：结构化数据 和 非结构化数据 。\n结构化数据 ：也称作行数据，是由二维表结构来逻辑表达和实现的数据，严格地遵循数据格式与长度规范，主要通过关系型数据库进行存储和管理。指具有固定格式或有限长度的数据，如数据库，元数据等。\n非结构化数据 ：又可称为全文数据，不定长或无固定格式，不适于由数据库二维表来表现，包括所有格式的办公文档、XML、HTML、word文档，邮件，各类报表、图片和咅频、视频信息等。\n\n说明：如果要更细致的区分的话，XML、HTML可划分为 半结构化数据 。因为它们也具有自己特定的标签格式，所以既可以根据需要按结构化数据来处理，也可抽取出纯文本按非结构化数据来处理。\n\n根据两种数据分类，搜索也相应的分为两种：结构化数据搜索 和非结构化数据搜索 。\n对于结构化数据，因为它们具有特定的结构，所以我们一般都是可以通过关系型数据库（mysql，oracle等）的 二维表（table）的方式存储和搜索，也可以建立索引。\n对于非结构化数据，也即对全文数据的搜索主要有两种方法：顺序扫描法 ，全文检索 。\n顺序扫描 ：通过文字名称也可了解到它的大概搜索方式，即按照顺序扫描的方式查询特定的关键字。例如给你一张报纸，让你找到该报纸中“平安”的文字在哪些地方出现过。你肯定需要从头到尾把报纸阅读扫描一遍然后标记出关键字在哪些版块出现过以及它的出现位置。\n这种方式无疑是最耗时的最低效的，如果报纸排版字体小，而且版块较多甚至有多份报纸，等你扫描完你的眼睛也差不多了。\n全文搜索 ：对非结构化数据顺序扫描很慢，我们是否可以进行优化？把我们的非结构化数据想办法弄得有一定结构不就行了吗？将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对此有一定结构的数据进行搜索，从而达到搜索相对较快的目的。\n这种方式就构成了全文检索的基本思路。这部分从非结构化数据中提取出的然后重新组织的信息，我们称之索引 。这种方式的主要工作量在前期索引的创建，但是对于后期搜索却是快速高效的。\n二、先说说 Lucene通过对生活中数据的类型作了一个简短了解之后，我们知道关系型数据库的SQL检索是处理不了这种非结构化数据的。这种非结构化数据的处理需要依赖全文搜索，而目前市场上开放源代码的最好全文检索引擎工具包就属于 apache 的 Lucene了。\n但是 Lucene 只是一个工具包，它不是一个完整的全文检索引擎。Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。\n目前以 Lucene 为基础建立的开源可用全文搜索引擎主要是 Solr 和 Elasticsearch。\nSolr 和 Elasticsearch 都是比较成熟的全文搜索引擎，能完成的功能和性能也基本一样。但是 ES 本身就具有分布式的特性和易安装使用的特点，而Solr的分布式需要借助第三方来实现，例如通过使用ZooKeeper来达到分布式协调管理。\n不管是 Solr 还是 Elasticsearch 底层都是依赖于 Lucene，而 Lucene 能实现全文搜索主要是因为它实现了倒排索引 的查询结构。\n如何理解倒排索引呢？假如现有三份数据文档，文档的内容如下分别是：\n\nJava is the best programming language.\nPHP is the best programming language.\nJavascript is the best programming language.\n\n为了创建倒排索引，我们通过分词器将每个文档的内容域拆分成单独的词（我们称它为词条或 Term），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示：\nTerm          Doc_1    Doc_2   Doc_3-------------------------------------Java        |   X   |        |is          |   X   |   X    |   Xthe         |   X   |   X    |   Xbest        |   X   |   X    |   Xprogramming |   x   |   X    |   Xlanguage    |   X   |   X    |   XPHP         |       |   X    |Javascript  |       |        |   X-------------------------------------\n\n这种结构由文档中所有不重复词的列表构成，对于其中每个词都有一个文档列表与之关联。这种由属性值来确定记录的位置的结构就是倒排索引 。带有倒排索引的文件我们称为倒排文件。\n我们将上面的内容转换为图的形式来说明倒排索引的结构信息，如下图所示，\n\n其中主要有如下几个核心术语需要理解：\n\n词条(Term) ：索引里面最小的存储和查询单元，对于英文来说是一个单词，对于中文来说一般指分词后的一个词。\n词典(Term Dictionary) ：或字典，是词条Term的集合。搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。\n倒排表(Post list) ：一个文档通常由多个词组成，倒排表记录的是某个词在哪些文档里出现过以及出现的位置。每条记录称为一个倒排项(Posting)。倒排表记录的不单是文档编号，还存储了词频等信息。\n倒排文件(Inverted File) ：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件被称之为倒排文件，倒排文件是存储倒排索引的物理文件。\n\n从上图我们可以了解到倒排索引主要由两个部分组成：词典 和倒排文件 。词典和倒排表是Lucene中很重要的两种数据结构，是实现快速检索的重要基石。词典和倒排文件是分两部分存储的，词典在内存中而倒排文件存储在磁盘上。\n三、核心概念一些基础知识的铺垫之后我们正式进入今天的主角Elasticsearch的介绍， ES是使用Java编写的一种开源搜索引擎，它在内部使用Lucene做索引与搜索，通过对Lucene的封装，隐藏了Lucene的复杂性，取而代之的提供一套简单一致的 RESTful API。\n然而，Elasticsearch 不仅仅是 Lucene，并且也不仅仅只是一个全文搜索引擎。它可以被下面这样准确的形容：\n\n一个分布式的实时文档存储，每个字段可以被索引与搜索。\n一个分布式实时分析搜索引擎。\n能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据。\n\n官网对Elasticsearch的介绍是Elasticsearch 是一个分布式 、可扩展 、近实时的 搜索与数据分析引擎。我们通过一些核心概念来看下Elasticsearch 是如何做到分布式，可扩展和近实时搜索的。\n集群（Cluster）ES的集群搭建很简单，不需要依赖第三方协调管理组件，自身内部就实现了集群的管理功能。ES集群由一个或多个Elasticsearch节点组成，每个节点配置相同的 cluster.name 即可加入集群，默认值为 “elasticsearch”。确保不同的环境中使用不同的集群名称，否则最终会导致节点加入错误的集群。\n一个Elasticsearch服务启动实例就是一个节点（Node）。节点通过node.name来设置节点名称，如果不设置则在启动时给节点分配一个随机通用唯一标识符作为名称。\n发现机制那么有一个问题，ES内部是如何通过一个相同的设置cluster.name 就能将不同的节点连接到同一个集群的？答案是Zen Discovery。\nZen Discovery是Elasticsearch的内置默认发现模块（发现模块的职责是发现集群中的节点以及选举master节点）。它提供单播和基于文件的发现，并且可以扩展为通过插件支持云环境和其他形式的发现。Zen Discovery 与其他模块集成，例如，节点之间的所有通信都使用Transport模块完成。节点使用发现机制通过Ping的方式查找其他节点。\nElasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。\n如果集群的节点运行在不同的机器上，使用单播，你可以为 Elasticsearch 提供一些它应该去尝试连接的节点列表。当一个节点联系到单播列表中的成员时，它就会得到整个集群所有节点的状态，然后它会联系 master 节点，并加入集群。\n这意味着单播列表不需要包含集群中的所有节点， 它只是需要足够的节点，当一个新节点联系上其中一个并且说上话就可以了。如果你使用 master 候选节点作为单播列表，你只要列出三个就可以了。这个配置在 elasticsearch.yml 文件中：\ndiscovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2:port&quot;]\n\n节点启动后先 ping ，如果discovery.zen.ping.unicast.hosts 有设置，则 ping 设置中的 host ，否则尝试 ping localhost 的几个端口， Elasticsearch 支持同一个主机启动多个节点， Ping 的 response 会包含该节点的基本信息以及该节点认为的 master 节点。选举开始，先从各节点认为的 master 中选，规则很简单，按照 id 的字典序排序，取第一个。如果各节点都没有认为的 master ，则从所有节点中选择，规则同上。\n这里有个限制条件就是 discovery.zen.minimum_master_nodes ，如果节点数达不到最小值的限制，则循环上述过程，直到节点数足够可以开始选举。最后选举结果是肯定能选举出一个 master ，如果只有一个 local 节点那就选出的是自己。如果当前节点是 master ，则开始等待节点数达到 discovery.zen.minimum_master_nodes，然后提供服务。如果当前节点不是 master ，则尝试加入 master 。Elasticsearch 将以上服务发现以及选主的流程叫做 ZenDiscovery 。\n由于它支持任意数目的集群（ 1- N ），所以不能像 Zookeeper 那样限制节点必须是奇数，也就无法用投票的机制来选主，而是通过一个规则，只要所有的节点都遵循同样的规则，得到的信息都是对等的，选出来的主节点肯定是一致的。但分布式系统的问题就出在信息不对等的情况，这时候很容易出现脑裂（ Split-Brain ）的问题，大多数解决方案就是设置一个 quorum 值，要求可用节点必须大于 quorum （一般是超过半数节点），才能对外提供服务。而 Elasticsearch 中，这个 quorum 的配置就是 discovery.zen.minimum_master_nodes 。\n节点的角色每个节点既可以是候选主节点 也可以是数据节点 ，通过在配置文件../config/elasticsearch.yml中设置即可，默认都为true。\nnode.master: true  //是否候选主节点node.data: true    //是否数据节点\n\n数据节点 负责数据的存储和相关的操作，例如对数据进行增、删、改、查和聚合等操作，所以数据节点（data节点）对机器配置要求比较高，对CPU、内存和I/O的消耗很大。通常随着集群的扩大，需要增加更多的数据节点来提高性能和可用性。\n候选主节点 可以被选举为主节点（master节点），集群中只有候选主节点才有选举权和被选举权，其他节点不参与选举的工作。主节点负责创建索引、删除索引、跟踪哪些节点是群集的一部分，并决定哪些分片分配给相关的节点、追踪集群中节点的状态等，稳定的主节点对集群的健康是非常重要的。\n\n一个节点既可以是候选主节点也可以是数据节点，但是由于数据节点对CPU、内存核I/0消耗都很大，所以如果某个节点既是数据节点又是主节点，那么可能会对主节点产生影响从而对整个集群的状态产生影响。\n因此为了提高集群的健康性，我们应该对Elasticsearch集群中的节点做好角色上的划分和隔离。可以使用几个配置较低的机器群作为候选主节点群。\n主节点和其他节点之间通过Ping的方式互检查，主节点负责Ping所有其他节点，判断是否有节点已经挂掉。其他节点也通过Ping的方式判断主节点是否处于可用状态。\n虽然对节点做了角色区分，但是用户的请求可以发往任何一个节点，并由该节点负责分发请求、收集结果等操作，而不需要主节点转发，这种节点可称之为协调节点 ，协调节点是不需要指定和配置的，集群中的任何节点都可以充当协调节点的角色。\n脑裂现象同时如果由于网络或其他原因导致集群中选举出多个Master节点，使得数据更新时出现不一致，这种现象称之为脑裂 ，即集群中不同的节点对于master的选择出现了分歧，出现了多个master竞争。\n“脑裂”问题可能有以下几个原因造成：\n\n网络问题 ：集群间的网络延迟导致一些节点访问不到master，认为master挂掉了从而选举出新的master，并对master上的分片和副本标红，分配新的主分片\n节点负载 ：主节点的角色既为master又为data，访问量较大时可能会导致ES停止响应（假死状态）造成大面积延迟，此时其他节点得不到主节点的响应认为主节点挂掉了，会重新选取主节点。\n内存回收 ：主节点的角色既为master又为data，当data节点上的ES进程占用的内存较大，引发JVM的大规模内存回收，造成ES进程失去响应。\n\n为了避免脑裂现象的发生，我们可以从原因着手通过以下几个方面来做出优化措施：\n\n适当调大响应时间，减少误判通过参数discovery.zen.ping_timeout设置节点状态的响应时间，默认为3s，可以适当调大，如果master在该响应时间的范围内没有做出响应应答，判断该节点已经挂掉了。调大参数（如6s，discovery.zen.ping_timeout:6），可适当减少误判。\n选举触发我们需要在候选集群中的节点的配置文件中设置参数discovery.zen.munimum_master_nodes的值，这个参数表示在选举主节点时需要参与选举的候选主节点的节点数，默认值是1，官方建议取值(master_eligibel_nodes/2) + 1，其中master_eligibel_nodes为候选主节点的个数。这样做既能防止脑裂现象的发生，也能最大限度地提升集群的高可用性，因为只要不少于discovery.zen.munimum_master_nodes个候选节点存活，选举工作就能正常进行。当小于这个值的时候，无法触发选举行为，集群无法使用，不会造成分片混乱的情况。\n角色分离即是上面我们提到的候选主节点和数据节点进行角色分离，这样可以减轻主节点的负担，防止主节点的假死状态发生，减少对主节点“已死”的误判。\n\n分片（Shards）ES支持PB级全文搜索，当索引上的数据量太大的时候，ES通过水平拆分的方式将一个索引上的数据拆分出来分配到不同的数据块上，拆分出来的数据库块称之为一个分片 。\n这类似于MySql的分库分表，只不过Mysql分库分表需要借助第三方组件而ES内部自身实现了此功能。\n在一个多分片的索引中写入数据时，通过路由来确定具体写入哪一个分片中，所以在创建索引的时候需要指定分片的数量，并且分片的数量一旦确定就不能修改。\n分片的数量和下面介绍的副本数量都是可以通过创建索引时的settings来配置，ES默认为一个索引创建5个主分片, 并分别为每个分片创建一个副本。\nPUT /myIndex&#123;   &quot;settings&quot; : &#123;      &quot;number_of_shards&quot; : 5,      &quot;number_of_replicas&quot; : 1   &#125;&#125;\n\nES通过分片的功能使得索引在规模上和性能上都得到提升，每个分片都是Lucene中的一个索引文件，每个分片必须有一个主分片和零到多个副本。\n副本（Replicas）副本就是对分片的Copy，每个主分片都有一个或多个副本分片，当主分片异常时，副本可以提供数据的查询等操作。主分片和对应的副本分片是不会在同一个节点上的，所以副本分片数的最大值是 n -1（其中n为节点数）。\n对文档的新建、索引和删除请求都是写操作，必须在主分片上面完成之后才能被复制到相关的副本分片，ES为了提高写入的能力这个过程是并发写的，同时为了解决并发写的过程中数据冲突的问题，ES通过乐观锁的方式控制，每个文档都有一个 _version （版本）号，当文档被修改时版本号递增。一旦所有的副本分片都报告写成功才会向协调节点报告成功，协调节点向客户端报告成功。\n\n从上图可以看出为了达到高可用，Master节点会避免将主分片和副本分片放在同一个节点上。\n假设这时节点Node1服务宕机了或者网络不可用了，那么主节点上主分片S0也就不可用了。幸运的是还存在另外两个节点能正常工作，这时ES会重新选举新的主节点，而且这两个节点上存在我们的所需要的S0的所有数据，我们会将S0的副本分片提升为主分片，这个提升主分片的过程是瞬间发生的。此时集群的状态将会为 yellow。\n为什么我们集群状态是 yellow 而不是 green 呢？虽然我们拥有所有的2个主分片，但是同时设置了每个主分片需要对应两份副本分片，而此时只存在一份副本分片。所以集群不能为 green 的状态。如果我们同样关闭了 Node2 ，我们的程序依然可以保持在不丢任何数据的情况下运行，因为Node3 为每一个分片都保留着一份副本。\n如果我们重新启动Node1 ，集群可以将缺失的副本分片再次进行分配，那么集群的状态又将恢复到原来的正常状态。如果Node1依然拥有着之前的分片，它将尝试去重用它们，只不过这时Node1节点上的分片不再是主分片而是副本分片了，如果期间有更改的数据只需要从主分片上复制修改的数据文件即可。\n\n小结:\n1、将数据分片是为了提高可处理数据的容量和易于进行水平扩展，为分片做副本是为了提高集群的稳定性和提高并发量。2、副本是乘法，越多消耗越大，但也越保险。分片是除法，分片越多，单分片数据就越少也越分散。3、副本越多，集群的可用性就越高，但是由于每个分片都相当于一个Lucene的索引文件，会占用一定的文件句柄、内存及CPU，并且分片间的数据同步也会占用一定的网络带宽，所以索引的分片数和副本数也不是越多越好。\n\n映射（Mapping）映射是用于定义ES对索引中字段的存储类型、分词方式和是否存储等信息，就像数据库中的 schema ，描述了文档可能具有的字段或属性、每个字段的数据类型。只不过关系型数据库建表时必须指定字段类型，而ES对于字段类型可以不指定然后动态对字段类型猜测，也可以在创建索引时具体指定字段的类型。\n对字段类型根据数据格式自动识别的映射称之为动态映射（Dynamic mapping） ，我们创建索引时具体定义字段类型的映射称之为静态映射 或显示映射（Explicit mapping） 。\n在讲解动态映射和静态映射的使用前，我们先来了解下ES中的数据有哪些字段类型？之后我们再讲解为什么我们创建索引时需要建立静态映射而不使用动态映射。\nES（v6.8）中字段数据类型主要有以下几类：\n\n\n\n类别\n数据类型\n\n\n\n核心类型\ntext, keywords, long, integer, short, double, data, boolean等等\n\n\n复杂类型\nObject, Nested\n\n\n地理类型\ngeo_point, geo_shape\n\n\n特殊类型\nip, completion, token_count, join等等\n\n\n…….\n…\n\n\ntext 用于索引全文值的字段，例如电子邮件正文或产品说明。这些字段是被分词的，它们通过分词器传递 ，以在被索引之前将字符串转换为单个术语的列表。分析过程允许Elasticsearch搜索单个单词中每个完整的文本字段。文本字段不用于排序，很少用于聚合。\nkeyword 用于索引结构化内容的字段，例如电子邮件地址，主机名，状态代码，邮政编码或标签。它们通常用于过滤，排序，和聚合。keyword字段只能按其确切值进行搜索。\n通过对字段类型的了解我们知道有些字段需要明确定义的，例如某个字段是text类型还是keword类型差别是很大的，时间字段也许我们需要指定它的时间格式，还有一些字段我们需要指定特定的分词器等等。如果采用动态映射是不能精确做到这些的，自动识别常常会与我们期望的有些差异。\n所以创建索引给的时候一个完整的格式应该是指定分片和副本数以及Mapping的定义，如下：\nPUT my_index&#123;   &quot;settings&quot; : &#123;      &quot;number_of_shards&quot; : 5,      &quot;number_of_replicas&quot; : 1   &#125;  &quot;mappings&quot;: &#123;    &quot;_doc&quot;: &#123;      &quot;properties&quot;: &#123;        &quot;title&quot;:    &#123; &quot;type&quot;: &quot;text&quot;  &#125;,        &quot;name&quot;:     &#123; &quot;type&quot;: &quot;text&quot;  &#125;,        &quot;age&quot;:      &#123; &quot;type&quot;: &quot;integer&quot; &#125;,        &quot;created&quot;:  &#123;          &quot;type&quot;:   &quot;date&quot;,          &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot;        &#125;      &#125;    &#125;  &#125;&#125;\n\n四、基本使用在决定使用 Elasticsearch 的时候首先要考虑的是版本问题，Elasticsearch （排除 0.x 和 1.x）目前有如下常用的稳定的主版本：2.x，5.x，6.x，7.x（current）。你可能会发现没有 3.x 和 4.x，ES 从 2.4.6 直接跳到了 5.0.0。\n其实是为了ELK（ElasticSearch, logstash, kibana）技术栈的版本统一，免的给用户带来混乱。在 Elasticsearch 是 2.x （2.x 的最后一版 2.4.6 的发布时间是 July 25, 2017） 的情况下，kibana 已经是 4.x（Kibana 4.6.5 的发布时间是 July 25, 2017），那么在 kibana 的下一主版本肯定是 5.x 了，所以 Elasticsearch 直接将自己的主版本发布为 5.0.0 了。统一之后，我们选版本就不会犹豫困惑了，我们选定 elasticsearch 的版本后再选择相同版本的 kibana 就行了，不用担忧版本不兼容的问题。\nElasticsearch是使用Java构建，所以除了注意 ELK 技术的版本统一，我们在选择 Elasticsearch 的版本的时候还需要注意 JDK的版本。因为每个大版本所依赖的 JDK版本也不同，目前7.2版本已经可以支持 jdk11。\n安装使用1、下载和解压Elasticsearch，无需安装解压后即可用，解压后目录如下。\n\nbin：二进制系统指令目录，包含启动命令和安装插件命令等。\nconfig：配置文件目录。\ndata：数据存储目录。\nlib：依赖包目录。\nlogs：日志文件目录。\nmodules：模块库，例如x-pack的模块。\nplugins：插件目录。\n\n2、安装目录下运行 bin/elasticsearch来启动 ES。3、默认在9200端口运行，请求curl http://localhost:9200/ 或者浏览器输入http://localhost:9200，得到一个 JSON 对象，其中包含当前节点、集群、版本等信息。\n&#123;  &quot;name&quot; : &quot;U7fp3O9&quot;,  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,  &quot;cluster_uuid&quot; : &quot;-Rj8jGQvRIelGd9ckicUOA&quot;,  &quot;version&quot; : &#123;    &quot;number&quot; : &quot;6.8.1&quot;,    &quot;build_flavor&quot; : &quot;default&quot;,    &quot;build_type&quot; : &quot;zip&quot;,    &quot;build_hash&quot; : &quot;1fad4e1&quot;,    &quot;build_date&quot; : &quot;2019-06-18T13:16:52.517138Z&quot;,    &quot;build_snapshot&quot; : false,    &quot;lucene_version&quot; : &quot;7.7.0&quot;,    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;  &#125;,  &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125;\n\n集群健康状态要检查群集运行状况，我们可以在 Kibana 控制台中运行以下命令 GET /_cluster/health，得到如下信息：\n&#123;  &quot;cluster_name&quot; : &quot;wujiajian&quot;,  &quot;status&quot; : &quot;yellow&quot;,  &quot;timed_out&quot; : false,  &quot;number_of_nodes&quot; : 1,  &quot;number_of_data_nodes&quot; : 1,  &quot;active_primary_shards&quot; : 9,  &quot;active_shards&quot; : 9,  &quot;relocating_shards&quot; : 0,  &quot;initializing_shards&quot; : 0,  &quot;unassigned_shards&quot; : 5,  &quot;delayed_unassigned_shards&quot; : 0,  &quot;number_of_pending_tasks&quot; : 0,  &quot;number_of_in_flight_fetch&quot; : 0,  &quot;task_max_waiting_in_queue_millis&quot; : 0,  &quot;active_shards_percent_as_number&quot; : 64.28571428571429&#125;\n\n集群状态通过 绿，黄，红 来标识\n\n绿色 ：集群健康完好，一切功能齐全正常，所有分片和副本都可以正常工作。\n黄色 ：预警状态，所有主分片功能正常，但至少有一个副本是不能正常工作的。此时集群是可以正常工作的，但是高可用性在某种程度上会受影响。\n红色 ：集群不可正常使用。某个或某些分片及其副本异常不可用，这时集群的查询操作还能执行，但是返回的结果会不准确。对于分配到这个分片的写入请求将会报错，最终会导致数据的丢失。\n\n当集群状态为红色时，它将会继续从可用的分片提供搜索请求服务，但是你需要尽快修复那些未分配的分片。\n五、机制原理ES的基本概念和基本操作介绍完了之后我们可能还有很多疑惑，它们内部是如何运行的？主分片和副本分片是如何同步的？创建索引的流程是什么样的？ES如何将索引数据分配到不同的分片上的？以及这些索引数据是如何存储的？为什么说ES是近实时 搜索引擎而文档的 CRUD (创建-读取-更新-删除) 操作是实时的？以及Elasticsearch 是怎样保证更新被持久化在断电时也不丢失数据？还有为什么删除文档不会立刻释放空间？带着这些疑问我们进入接下来的内容。\n写索引原理下图描述了3个节点的集群，共拥有12个分片，其中有4个主分片（S0、S1、S2、S3）和8个副本分片（R0、R1、R2、R3），每个主分片对应两个副本分片，节点1是主节点（Master节点）负责整个集群的状态。\n\n写索引是只能写在主分片上，然后同步到副本分片。这里有四个主分片，一条数据ES是根据什么规则写到特定分片上的呢？这条索引数据为什么被写到S0上而不写到S1或S2上？那条数据为什么又被写到S3上而不写到S0上了？\n首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的：\nshard = hash(routing) % number_of_primary_shards\n\nrouting 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到余数 。这个在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。\n这就解释了为什么我们要在创建索引的时候就确定好主分片的数量并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。\n由于在ES集群中每个节点通过上面的计算公式都知道集群中的文档的存放位置，所以每个节点都有处理读写请求的能力。在一个写请求被发送到某个节点后，该节点即为前面说过的协调节点，协调节点会根据路由公式计算出需要写到哪个分片上，再将请求转发到该分片的主分片节点上。\n假如此时数据通过路由计算公式取余后得到的值是 shard = hash(routing) % 4 = 0，则具体流程如下：\n\n\n客户端向ES1节点（协调节点）发送写请求，通过路由计算公式得到值为0，则当前数据应被写到主分片S0上。\nES1节点将请求转发到S0主分片所在的节点ES3，ES3接受请求并写入到磁盘。\n并发将数据复制到两个副本分片R0上，其中通过乐观并发控制数据的冲突。一旦所有的副本分片都报告成功，则节点ES3将向协调节点报告成功，协调节点向客户端报告成功。\n\n存储原理上面介绍了在ES内部索引的写处理流程，这个流程是在ES的内存中执行的，数据被分配到特定的分片和副本上之后，最终是存储到磁盘上的，这样在断电的时候就不会丢失数据。具体的存储路径可在配置文件../config/elasticsearch.yml中进行设置，默认存储在安装目录的data文件夹下。建议不要使用默认值，因为若ES进行了升级，则有可能导致数据全部丢失。\npath.data: /path/to/data  //索引数据path.logs: /path/to/logs  //日志记录\n\n分段存储索引文档以段的形式存储在磁盘上，何为段 ？索引文件被拆分为多个子文件，则每个子文件叫作段 ， 每一个段本身都是一个倒排索引，并且段具有不变性，一旦索引的数据被写入硬盘，就不可再修改。在底层采用了分段的存储模式，使它在读写时几乎完全避免了锁的出现，大大提升了读写性能。\n段被写入到磁盘后会生成一个提交点 ，提交点是一个用来记录所有提交后段信息的文件。一个段一旦拥有了提交点，就说明这个段只有读的权限，失去了写的权限。相反，当段在内存中时，就只有写的权限，而不具备读数据的权限，意味着不能被检索。\n\n段 的概念提出主要是因为：在早期全文检索中为整个文档集合建立了一个很大的倒排索引，并将其写入磁盘中。如果索引有更新，就需要重新全量创建一个索引来替换原来的索引。这种方式在数据量很大时效率很低，并且由于创建一次索引的成本很高，所以对数据的更新不能过于频繁，也就不能保证时效性。\n索引文件分段存储并且不可修改，那么新增、更新和删除如何处理呢？\n\n新增，新增很好处理，由于数据是新的，所以只需要对当前文档新增一个段就可以了。\n删除，由于不可修改，所以对于删除操作，不会把文档从旧的段中移除而是通过新增一个.del文件，文件中会列出这些被删除文档的段信息。这个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。\n更新，不能修改旧的段来进行反映文档的更新，其实更新相当于是删除和新增这两个动作组成。会将旧的文档在.del文件中标记删除，然后文档的新版本被索引到一个新的段中。可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就会被移除。\n\n段被设定为不可修改具有一定的优势也有一定的缺点，优势主要表现在：\n\n不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。\n一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。\n其它缓存(像filter缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。\n写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和 需要被缓存到内存的索引的使用量。\n\n段的不变性的缺点如下：\n\n当对旧数据进行删除时，旧数据不会马上被删除，而是在.del文件中被标记为删除。而旧数据只能等到段更新时才能被移除，这样会造成大量的空间浪费。\n若有一条数据频繁的更新，每次更新都是新增新的标记旧的，则会有大量的空间浪费。\n每次新增数据时都需要新增一个段来存储数据。当段的数量太多时，对服务器的资源例如文件句柄的消耗会非常大。\n在查询的结果中包含所有的结果集，需要排除被标记删除的旧数据，这增加了查询的负担。\n\n延迟写策略介绍完了存储的形式，那么索引是写入到磁盘的过程是这怎样的？是否是直接调 fsync 物理性地写入磁盘？\n\n答案是显而易见的，如果是直接写入到磁盘上，磁盘的I/O消耗上会严重影响性能，那么当写数据量大的时候会造成ES停顿卡死，查询也无法做到快速响应。如果真是这样ES也就不会称之为近实时 全文搜索引擎了。\n\n为了提升写的性能，ES并没有每新增一条数据就增加一个段到磁盘上，而是采用延迟写 的策略。\n每当有新增的数据时，就将其先写入到内存中，在内存和磁盘之间是文件系统缓存，当达到默认的时间（1秒钟）或者内存的数据达到一定量时，会触发一次刷新（Refresh），将内存中的数据生成到一个新的段上并缓存到文件缓存系统 上，稍后再被刷新到磁盘中并生成提交点 。\n这里的内存使用的是ES的JVM内存，而文件缓存系统使用的是操作系统的内存。新的数据会继续的被写入内存，但内存中的数据并不是以段的形式存储的，因此不能提供检索功能。由内存刷新到文件缓存系统的时候会生成了新的段，并将段打开以供搜索使用，而不需要等到被刷新到磁盘。\n在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 refresh （即内存刷新到文件缓存系统）。默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是近实时搜索 ，因为文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。我们也可以手动触发 refresh，POST /_refresh 刷新所有索引，POST /nba/_refresh刷新指定的索引。\n\nTips ：尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产&gt; 环境下每次索引一个文档都去手动刷新。而且并不是所有的情况都需要每秒刷新。可能你正在使用 Elasticsearch 索引大量的日志文件， 你可能想优化索引速度而不是&gt; 近实时搜索， 这时可以在创建索引时在settings中通过调大refresh_interval = &quot;30s&quot; 的值 ， 降低每个索引的刷新频率，设值时需要注意后面带上时间单位，否则默认是毫秒。当refresh_interval = -1时表示关闭索引的自动刷新。\n\n虽然通过延时写的策略可以减少数据往磁盘上写的次数提升了整体的写入能力，但是我们知道文件缓存系统也是内存空间，属于操作系统的内存，只要是内存都存在断电或异常情况下丢失数据的危险。\n为了避免丢失数据，Elasticsearch添加了事务日志（Translog） ，事务日志记录了所有还没有持久化到磁盘的数据。添加了事务日志后整个写索引的流程如下图所示。\n\n\n一个新文档被索引之后，先被写入到内存中，但是为了防止数据的丢失，会追加一份数据到事务日志中。不断有新的文档被写入到内存，同时也都会记录到事务日志中。这时新数据还不能被检索和查询。\n当达到默认的刷新时间或内存中的数据达到一定量后，会触发一次 refresh，将内存中的数据以一个新段形式刷新到文件缓存系统中并清空内存。这时虽然新段未被提交到磁盘，但是可以提供文档的检索功能且不能被修改。\n随着新文档索引不断被写入，当日志数据大小超过512M或者时间超过30分钟时，会触发一次 flush。内存中的数据被写入到一个新段同时被写入到文件缓存系统，文件系统缓存中数据通过 fsync 刷新到磁盘中，生成提交点，日志文件被删除，创建一个空的新日志。\n\n通过这种方式当断电或需要重启时，ES不仅要根据提交点去加载已经持久化过的段，还需要工具Translog里的记录，把未持久化的数据重新持久化到磁盘上，避免了数据丢失的可能。\n段合并由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。每一个段都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个段然后合并查询结果，所以段越多，搜索也就越慢。\nElasticsearch通过在后台定期进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档不会被拷贝到新的大段中。合并的过程中不会中断索引和搜索。\n\n段合并在进行索引和搜索时会自动进行，合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中，这些段既可以是未提交的也可以是已提交的。合并结束后老的段会被删除，新的段被 flush 到磁盘，同时写入一个包含新段（已排除旧的被合并的段）的新提交点，新的段被打开可以用来搜索。\n段合并的计算量庞大， 而且还要吃掉大量磁盘 I/O，段合并会拖累写入速率，如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制，所以搜索仍然有足够的资源很好地执行。\n六、性能优化存储设备磁盘在现代服务器上通常都是瓶颈。Elasticsearch 重度使用磁盘，你的磁盘能处理的吞吐量越大，你的节点就越稳定。这里有一些优化磁盘 I/O 的技巧：\n\n使用 SSD。就像其他地方提过的， 他们比机械磁盘优秀多了。\n使用 RAID 0。条带化 RAID 会提高磁盘 I/O，代价显然就是当一块硬盘故障时整个就故障了。不要使用镜像或者奇偶校验 RAID 因为副本已经提供了这个功能。\n另外，使用多块硬盘，并允许 Elasticsearch 通过多个 path.data 目录配置把数据条带化分配到它们上面。\n不要使用远程挂载的存储，比如 NFS 或者 SMB/CIFS。这个引入的延迟对性能来说完全是背道而驰的。\n如果你用的是 EC2，当心 EBS。即便是基于 SSD 的 EBS，通常也比本地实例的存储要慢。\n\n内部索引优化\nElasticsearch为了能快速找到某个term，先将所有的term排个序，然后根据二分法查找term，时间复杂度为logN，就像通过字典查找一样，这就是Term Dictionary 。现在再看起来，似乎和传统数据库通过B-Tree的方式类似。\n但是如果term太多，term dictionary也会很大，放内存不现实，于是有了Term Index ，就像字典里的索引页一样，A开头的有哪些term，分别在哪页，可以理解term index是一颗树。这棵树不会包含所有的term，它包含的是term的一些前缀。通过term index可以快速地定位到term dictionary的某个offset，然后从这个位置再往后顺序查找。\n在内存中用FST方式压缩term index，FST以字节的方式存储所有的term，这种压缩方式可以有效的缩减存储空间，使得term index足以放进内存，但这种方式也会导致查找时需要更多的CPU资源。演示地址：Build your own FST\n对于存储在磁盘上的倒排表同样也采用了压缩技术减少存储所占用的空间，更多可以阅读 Frame of Reference and Roaring Bitmaps。\n调整配置参数\n给每个文档指定有序的具有压缩良好的序列模式ID，避免随机的UUID-4 这样的 ID，这样的ID压缩比很低，会明显拖慢 Lucene。\n对于那些不需要聚合和排序的索引字段禁用Doc values。Doc Values是有序的基于document =&gt; field value的映射列表；\n不需要做模糊检索的字段使用 keyword类型代替 text 类型，这样可以避免在建立索引前对这些文本进行分词。\n如果你的搜索结果不需要近实时的准确度，考虑把每个索引的 index.refresh_interval 改到 30s 。如果你是在做大批量导入，导入期间你可以通过设置这个值为 -1 关掉刷新，还可以通过设置 index.number_of_replicas: 0关闭副本。别忘记在完工的时候重新开启它。\n避免深度分页查询建议使用Scroll进行分页查询。普通分页查询时，会创建一个from + size的空优先队列，每个分片会返回from + size 条数据，默认只包含文档id和得分score给协调节点，如果有n个分片，则协调节点再对（from + size）× n 条数据进行二次排序，然后选择需要被取回的文档。当from很大时，排序过程会变得很沉重占用CPU资源严重。\n减少映射字段，只提供需要检索，聚合或排序的字段。其他字段可存在其他存储设备上，例如Hbase，在ES中得到结果后再去Hbase查询这些字段。\n创建索引和查询时指定路由routing值，这样可以精确到具体的分片查询，提升查询效率。路由的选择需要注意数据的分布均衡。\n\nJVM调优\n确保堆内存最小值（ Xms ）与最大值（ Xmx ）的大小是相同的，防止程序在运行时改变堆内存大小。Elasticsearch 默认安装后设置的堆内存是 1 GB。可通过../config/jvm.option文件进行配置，但是最好不要超过物理内存的50%和超过32GB。\nGC 默认采用CMS的方式，并发但是有STW的问题，可以考虑使用G1收集器。\nES非常依赖文件系统缓存（Filesystem Cache），快速搜索。一般来说，应该至少确保物理上有一半的可用内存分配到文件系统缓存。\n\n","categories":["运维"],"tags":["Elasticsearch"]}]